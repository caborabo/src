1
00:00:14,150 --> 00:00:14,910
Welcome everyone.

2
00:00:14,970 --> 00:00:16,329
I'm excited to be here.

3
00:00:16,460 --> 00:00:18,030
Thank you for joining me today.

4
00:00:18,730 --> 00:00:23,710
So imagine that you are hosting a
party and you're not sure how many

5
00:00:23,740 --> 00:00:25,760
people or how many guests will show up.

6
00:00:26,410 --> 00:00:30,089
You could reserve a fixed
number of table, but what if

7
00:00:30,629 --> 00:00:32,400
more people come than expected?

8
00:00:32,945 --> 00:00:36,085
Or what if just half people show up?

9
00:00:36,615 --> 00:00:42,174
So you either be headache fighting
some for more space for more people

10
00:00:42,245 --> 00:00:47,185
or stuck with empty table And waiting
resources that you reserve for So that

11
00:00:47,185 --> 00:00:52,775
is a lot like traditional kubernetes
capacity scaling is working today

12
00:00:52,985 --> 00:00:57,724
You allocate capacity upfront, but
the reality is things always change.

13
00:00:58,185 --> 00:01:01,645
Hi, my name is Jacques Ri, a
solution architect from AWS.

14
00:01:01,874 --> 00:01:06,975
So I'm here to walk you through some
capability of Carpenter and demo as well.

15
00:01:07,405 --> 00:01:09,165
So let's dive in.

16
00:01:10,925 --> 00:01:15,695
So now what if the table that
we mentioned at your party can

17
00:01:16,415 --> 00:01:21,245
automatically be expanded or changed
based on how many guests arrive?

18
00:01:21,675 --> 00:01:23,475
No S, no scrambling.

19
00:01:23,555 --> 00:01:25,995
And yeah, so always the right side.

20
00:01:26,385 --> 00:01:31,055
That is what exactly how
Carpenter does for our cluster.

21
00:01:31,525 --> 00:01:36,485
So let's see some of the challenges
that we have today in the current test.

22
00:01:37,470 --> 00:01:40,940
And also Amazon EKS with
cluster also autoscaler.

23
00:01:41,950 --> 00:01:47,970
So here you have Like deployment
and also you have the existing node

24
00:01:47,970 --> 00:01:51,000
already that run two parts, right?

25
00:01:51,310 --> 00:01:57,120
So we have two vcpu here for m5 large
and then the Scaling up mechanism

26
00:01:57,120 --> 00:02:03,010
kicks in maybe come from data or HPA
depending on the cluster and we have

27
00:02:03,020 --> 00:02:13,870
1 2 3 4 5 6 7 7 parts in pending state
so Some of the parts will be scheduled

28
00:02:13,880 --> 00:02:18,150
on the existing node and some of the
parts will need additional capacity.

29
00:02:18,480 --> 00:02:20,380
So that's why we need cluster autoscaler.

30
00:02:20,900 --> 00:02:24,440
And cluster autoscaler works
with autoscaling group.

31
00:02:25,190 --> 00:02:25,980
So here.

32
00:02:26,550 --> 00:02:33,100
We can have a new instance here,
which are m5 blush for two instances.

33
00:02:33,200 --> 00:02:37,709
Some of the pending part will go to
the second one and just one more part

34
00:02:37,759 --> 00:02:39,920
will go to the third instance here.

35
00:02:40,250 --> 00:02:43,900
We can spot two things
here for the challenge.

36
00:02:44,030 --> 00:02:49,139
The first thing is that it takes
longer time to spin up a new

37
00:02:49,159 --> 00:02:51,119
instance for scheduling the pods.

38
00:02:51,159 --> 00:02:54,199
Because cluster autoscaler
needs to work with autoscaling

39
00:02:54,209 --> 00:02:57,559
group and then autoscaling group
needs to work with EC2 API.

40
00:02:58,499 --> 00:03:01,559
So there are multiple steps
to spin up a new instant here.

41
00:03:02,049 --> 00:03:07,029
This is a program number one and
the program number two, if you

42
00:03:07,519 --> 00:03:12,839
can notify or observe that on the
third instance, we have only one

43
00:03:12,840 --> 00:03:15,280
part running and we still have 1.

44
00:03:15,320 --> 00:03:16,710
5 CPU left.

45
00:03:16,760 --> 00:03:17,870
This one is underutilized.

46
00:03:18,580 --> 00:03:19,750
So we would like to.

47
00:03:20,405 --> 00:03:26,465
try to find the right size of the node
that fit the parts that we need to run.

48
00:03:26,735 --> 00:03:29,495
So that is the thing that we
will will need to be solved.

49
00:03:31,020 --> 00:03:35,460
And coming into the third
challenge that we have for exiting

50
00:03:35,460 --> 00:03:37,260
cluster with cluster autoscaler.

51
00:03:37,790 --> 00:03:41,450
So if you can see here, this is the
deployment that we already deployed.

52
00:03:41,510 --> 00:03:45,660
At this based on the CPU
requests and CPU consumption.

53
00:03:45,950 --> 00:03:49,070
But how about when we
have a new deployment?

54
00:03:49,280 --> 00:03:52,970
That need a GPU workload or
another type of instance.

55
00:03:53,430 --> 00:03:57,670
So now so this part or this
deployment cannot be scheduled

56
00:03:57,680 --> 00:04:02,110
because this capacity that we have
doesn't support the GPU workload.

57
00:04:02,340 --> 00:04:08,230
So the things here is that for
platform team, they need to create

58
00:04:08,240 --> 00:04:11,000
a new compute, new node group.

59
00:04:11,070 --> 00:04:13,710
So the first one is for
the compute node group.

60
00:04:14,060 --> 00:04:16,080
And another one is for GPU node group.

61
00:04:16,460 --> 00:04:21,820
So here, after that the autoscaling
can spin a new type of instance

62
00:04:21,830 --> 00:04:25,210
to support a deployment which GPU.

63
00:04:25,580 --> 00:04:26,190
Now it's done.

64
00:04:26,740 --> 00:04:32,805
So we have Three main challenge challenges
here and we would like to see what kind

65
00:04:32,805 --> 00:04:38,405
of things that carpenter can make it
Like more efficient or better let's see.

66
00:04:38,475 --> 00:04:43,025
So this is the exiting one when we have
pending part and we don't have enough

67
00:04:43,045 --> 00:04:47,825
capacity cluster auto, autoscaler
kicks in and then cluster autoscaler,

68
00:04:47,895 --> 00:04:49,495
we work with autoscaling group.

69
00:04:49,645 --> 00:04:54,175
After that autoscaling group,
we'll spin up a EC2 instance by

70
00:04:54,175 --> 00:04:56,205
calling the API to EC2 fleet.

71
00:04:56,565 --> 00:05:00,115
So multiple steps, just like I
mentioned, but for Carpenter.

72
00:05:00,600 --> 00:05:05,560
So we don't need these two components
anymore and we don't need to go

73
00:05:05,600 --> 00:05:07,350
through the auto scaling group.

74
00:05:07,530 --> 00:05:09,490
Also the node group concept will be gone.

75
00:05:10,120 --> 00:05:15,210
And we can bypass that by replacing
those two modules by Carpenter.

76
00:05:16,020 --> 00:05:21,120
And Carpenter can spin up
their EC2 on our behalf.

77
00:05:21,670 --> 00:05:26,090
The instant that Carpenter spin
up, it will be always the cheapest

78
00:05:26,090 --> 00:05:27,890
one depending on the configuration.

79
00:05:28,380 --> 00:05:32,990
And also it will be always the
right size for the pending parts.

80
00:05:33,060 --> 00:05:34,190
So let's see how it works.

81
00:05:36,450 --> 00:05:41,570
So here we have two instants that
running nine parts of our deployment.

82
00:05:42,030 --> 00:05:44,820
The type of the C5 to Eclash.

83
00:05:45,360 --> 00:05:46,490
And now we don't.

84
00:05:46,575 --> 00:05:51,515
We don't run cluster autoscaler and
autoscaler in our cluster anymore.

85
00:05:51,525 --> 00:05:54,155
We have EKS and also Carpenter installed.

86
00:05:54,935 --> 00:05:57,675
How about now we have the GPU workload.

87
00:05:58,035 --> 00:06:04,985
So now if we configure the Carpenter
configuration correctly, so Carpenter

88
00:06:05,035 --> 00:06:09,765
can spin up a new workload which
support the GPU workload automatically.

89
00:06:09,955 --> 00:06:11,885
And you don't need to do anything.

90
00:06:11,905 --> 00:06:15,455
You just configure in the node pool
configuration, which we will go

91
00:06:15,485 --> 00:06:17,715
through in the couple of slides.

92
00:06:18,355 --> 00:06:23,235
So here the new workload can just
be scheduled in the new instance

93
00:06:23,255 --> 00:06:25,335
that coming up from Carpenter.

94
00:06:26,975 --> 00:06:29,125
The, I think this one is
the third problem, right?

95
00:06:29,125 --> 00:06:31,010
But we can see the first and.

96
00:06:31,380 --> 00:06:35,080
The first one is already solved
because we bypassed auto scaling group.

97
00:06:35,530 --> 00:06:39,540
But this one is the third one, but
we will revisit the second one later.

98
00:06:40,960 --> 00:06:44,120
So here is the configuration
that you can do.

99
00:06:44,230 --> 00:06:44,990
That I mentioned.

100
00:06:45,320 --> 00:06:48,700
That you can configure the
node pool for Carpentry.

101
00:06:50,110 --> 00:06:52,710
How it will behave in your cluster.

102
00:06:53,045 --> 00:06:57,925
You can have multiple not to as
well, but depending on the situation.

103
00:06:58,555 --> 00:07:03,935
So here we have the configuration
that we can choose from the

104
00:07:04,025 --> 00:07:05,905
different types of instance.

105
00:07:05,985 --> 00:07:06,635
So you can.

106
00:07:06,990 --> 00:07:12,620
Use y card here like you can use
only C or C5 or C6 or something

107
00:07:12,620 --> 00:07:14,760
like that You can use M.

108
00:07:14,770 --> 00:07:20,710
You can use R and you can also specify
the size like nano, micro, smaller,

109
00:07:20,780 --> 00:07:23,740
large, X large Something like that.

110
00:07:23,740 --> 00:07:29,005
So So depending on the use case, you
can specify wildcard or a specific one.

111
00:07:29,965 --> 00:07:33,425
And how about the availability zone?

112
00:07:33,435 --> 00:07:39,845
You can also specify here that you
allow Carpenter to spin up a node inside

113
00:07:40,095 --> 00:07:42,995
that particular availability zone.

114
00:07:42,995 --> 00:07:48,284
Or you can choose either
architecture for the CPU.

115
00:07:48,805 --> 00:07:55,635
By using xic or 64 here, or you
can also use the arm 64 A MB 64.

116
00:07:55,685 --> 00:07:56,015
Sorry.

117
00:07:56,945 --> 00:08:01,345
And you can also use ARM 64
which is Graviton on AWS.

118
00:08:01,585 --> 00:08:07,035
So Graviton provide better price
for performance compared to X.

119
00:08:07,965 --> 00:08:14,595
So if your application is already
tested that it supports arm, so

120
00:08:14,595 --> 00:08:15,480
you can use ARM here as well.

121
00:08:16,670 --> 00:08:19,320
And the final one is the capacity type.

122
00:08:19,740 --> 00:08:23,810
You can either go with on demand
or you can utilize spot as well.

123
00:08:24,100 --> 00:08:29,140
If you specify both of them, it
will try to prioritize the spot.

124
00:08:30,130 --> 00:08:31,690
And then you have the limit here.

125
00:08:31,720 --> 00:08:37,880
The limit specify that that this node
pool can spin up maximum 100 CPU only.

126
00:08:39,535 --> 00:08:44,455
When it exceeds 100 CPU, it
cannot spin up a new node anymore.

127
00:08:46,465 --> 00:08:50,775
Okay, for the node pool, you can
have a single one for one cluster.

128
00:08:50,825 --> 00:08:54,385
So this one is quite simple, maybe
in the test environment or dev

129
00:08:54,385 --> 00:09:00,105
environment, or you have a unified
node pool configuration that supports

130
00:09:00,125 --> 00:09:01,505
all the workloads in this cluster.

131
00:09:01,905 --> 00:09:04,825
So it's quite simple for the
single one, single node pool.

132
00:09:05,830 --> 00:09:12,120
But for a multiple one, you can have maybe
multiple team that need the different

133
00:09:12,120 --> 00:09:17,430
requirements for the node configuration,
or you have the accuracy hardware.

134
00:09:17,710 --> 00:09:23,210
Some of the team need the TPU workload
or like you have different image or a

135
00:09:23,210 --> 00:09:28,180
MI in your workload, like one workload
use battle rocket and one workload.

136
00:09:28,180 --> 00:09:29,800
Just use EKS optimize.

137
00:09:30,000 --> 00:09:30,820
Or something like that.

138
00:09:30,820 --> 00:09:35,160
So you can have multiple one to
support different kinds of workload.

139
00:09:36,520 --> 00:09:40,890
And you can also put the
weight on the notepad as well.

140
00:09:41,130 --> 00:09:47,240
If you try to prioritize the higher
weight notepad first, and then when

141
00:09:47,250 --> 00:09:52,500
it run out, when it reached the limit,
maybe the first one can spin up only

142
00:09:52,500 --> 00:09:57,530
like 10 CPU, depending on the, maybe
saving plan or reserving standard you

143
00:09:57,530 --> 00:10:02,050
bought from AWS, and then other than
that, maybe utilize spot or on demand.

144
00:10:02,460 --> 00:10:11,570
So you can use where the node pool
as well Okay so here is the Challenge

145
00:10:11,620 --> 00:10:17,000
number two that we have some of
the node that are in underutilized

146
00:10:17,220 --> 00:10:24,395
status so you can notice here on
the Two instance on the right side.

147
00:10:24,655 --> 00:10:29,895
We have only one pod running and
it's the m5 They are m5 egg large,

148
00:10:29,925 --> 00:10:34,135
which is quite big compared to the
workload is running then inside

149
00:10:34,145 --> 00:10:40,065
these two workloads So in carpenter
configuration, you can also specify

150
00:10:40,065 --> 00:10:43,395
that When will you do the consolidation?

151
00:10:43,495 --> 00:10:47,570
Consolidation mean that it will try
to move the workload together Try to

152
00:10:47,570 --> 00:10:49,480
bin packet to save the cost for you.

153
00:10:49,580 --> 00:10:54,120
And you can also specify the time
that it will do the disruption

154
00:10:54,140 --> 00:10:55,500
or move the workload for you.

155
00:10:55,700 --> 00:10:58,680
So here I specified as zero second.

156
00:10:58,710 --> 00:11:03,670
So if you try to do consideration
every time that it sees that

157
00:11:03,680 --> 00:11:08,010
the node is underutilized it's
quite aggressive here, but for.

158
00:11:08,380 --> 00:11:14,200
Demo purpose I just would like to spin
it up quite fast or do the consolidation

159
00:11:14,240 --> 00:11:18,870
quite fast You can also specify
like one hour or 24 hours or one day

160
00:11:18,870 --> 00:11:21,610
two days depending on the use case.

161
00:11:23,220 --> 00:11:30,120
So now it can group together for those
two pots to inside the exiting capacity.

162
00:11:30,780 --> 00:11:34,710
And this is the first que first
case, and here is the second one.

163
00:11:35,340 --> 00:11:41,180
So the same you have here M five
egg large and two two of the

164
00:11:41,180 --> 00:11:43,660
pots are running separately.

165
00:11:43,960 --> 00:11:48,440
And when you enable the
consolidation, you try to find.

166
00:11:49,385 --> 00:11:56,165
A node that fit these two parts and spin
that up and try to drain these two parts

167
00:11:56,165 --> 00:11:58,115
to a new instant to save you some cost.

168
00:11:58,925 --> 00:12:00,635
Let's see here.

169
00:12:00,645 --> 00:12:05,985
If we group together those two parts
and then you try if we move to the

170
00:12:05,985 --> 00:12:07,875
new instant, which is M5 blush.

171
00:12:07,985 --> 00:12:10,755
So if we reduce the
overall cost of cluster.

172
00:12:11,255 --> 00:12:12,985
Yeah you can see that we solve.

173
00:12:13,990 --> 00:12:16,680
All of the three challenges
that I presented back then.

174
00:12:18,580 --> 00:12:20,410
So how about some other features as well?

175
00:12:20,780 --> 00:12:23,780
Recently we launched
a carpenter version 1.

176
00:12:23,810 --> 00:12:24,050
0.

177
00:12:24,380 --> 00:12:29,570
So this feature also support that as
well, that you can do automatically

178
00:12:29,600 --> 00:12:31,510
upgraded by detecting drift.

179
00:12:31,700 --> 00:12:37,110
Of the cabinet configuration There is a
custom resource another custom resource.

180
00:12:37,130 --> 00:12:43,790
It's called ec2 node class for here you
can specify the image that you are that

181
00:12:43,790 --> 00:12:48,970
you would like to use the security group
and also the Availability zone as well.

182
00:12:49,460 --> 00:12:53,725
Here this, under the spec, we
specify that we would like to use

183
00:12:54,185 --> 00:12:58,035
the latest image that it comes out.

184
00:12:58,035 --> 00:13:02,660
So it means that if AWS
update the EKS optimized a MI.

185
00:13:03,160 --> 00:13:08,010
For the new version, Carpenter will
try to upgrade every workload under

186
00:13:08,030 --> 00:13:14,470
Carpenter and upgrade to the new one
and also try to drain the pods for us.

187
00:13:15,020 --> 00:13:19,730
So this is, this can be done
automatically and it will force every

188
00:13:19,730 --> 00:13:25,360
time that we have a new, um, AMI for
The ami that you are using so the

189
00:13:25,360 --> 00:13:29,500
new version comes and then it will be
Automatically upgrade but sometimes

190
00:13:29,510 --> 00:13:30,880
you can also do something like this.

191
00:13:30,890 --> 00:13:35,050
You can pin the version that you would
like to use this one when the updates

192
00:13:35,050 --> 00:13:40,995
come out, it will not be automatically
updated and then you can change the

193
00:13:41,065 --> 00:13:45,645
value inside the EC2 node class later
when you would like to do the upgrade.

194
00:13:46,305 --> 00:13:52,395
And this one can be specified with
the custom AMI that you have as well.

195
00:13:52,455 --> 00:13:54,075
So quite flexible for this one.

196
00:13:55,225 --> 00:13:59,675
So now coming into the demo
that I prepared for you.

197
00:14:00,625 --> 00:14:02,115
So let's see here.

198
00:14:03,265 --> 00:14:03,945
What do we have?

199
00:14:04,555 --> 00:14:04,985
Let's see.

200
00:14:07,215 --> 00:14:10,735
So for the nodes, we have two nodes here.

201
00:14:10,845 --> 00:14:17,095
The type is a three, three,
three large, two nodes.

202
00:14:17,195 --> 00:14:20,165
And I think, so let's see the workload.

203
00:14:20,665 --> 00:14:22,105
What kind of workload that we have.

204
00:14:23,450 --> 00:14:26,440
Okay, we have coordinates in kube system.

205
00:14:26,550 --> 00:14:28,310
We have carpenter.

206
00:14:28,350 --> 00:14:29,390
This is the controller.

207
00:14:29,730 --> 00:14:35,100
So I spin up for two pods for under
the deployment because I would

208
00:14:35,100 --> 00:14:39,300
like to provide the resiliency and
Then I have the my workload here.

209
00:14:39,480 --> 00:14:46,019
The name is inflate and the replica
is zero now let's see What kind of

210
00:14:46,020 --> 00:14:52,305
configuration that I have in that
inflate deployment So it's quite basic.

211
00:14:52,805 --> 00:14:56,165
The name is inflate, and
the name space is inflate.

212
00:14:56,465 --> 00:15:01,245
And the CPU request here is one CPU.

213
00:15:01,905 --> 00:15:04,835
I didn't specify the memory requirement.

214
00:15:05,345 --> 00:15:09,215
And it will just pull down
from the public ECR repository.

215
00:15:09,950 --> 00:15:13,620
And it did nothing it just reserved
the cpu that's all I think for

216
00:15:13,620 --> 00:15:17,350
now and How about the node pool
configuration that we went through?

217
00:15:17,740 --> 00:15:19,540
So what kind of thing that I configure?

218
00:15:19,970 --> 00:15:24,269
Let's see Okay, I think

219
00:15:26,299 --> 00:15:28,009
Yeah, we have only one node pool here.

220
00:15:28,009 --> 00:15:29,899
The name is the default.

221
00:15:30,159 --> 00:15:32,459
You can have multiple
just like I mentioned.

222
00:15:32,929 --> 00:15:37,679
And the version is version one, of course,
because we launched as a stable version.

223
00:15:38,199 --> 00:15:42,709
Consolidation policy now is when
it's empty and we change later.

224
00:15:43,239 --> 00:15:46,519
And the consolidation
time is five seconds.

225
00:15:47,129 --> 00:15:52,299
The limit here, this node tool can
spin up only 10 CPU only, after that

226
00:15:52,319 --> 00:15:54,299
it cannot spin up a new instance.

227
00:15:55,029 --> 00:15:56,459
Architecture is x86.

228
00:15:58,439 --> 00:16:05,599
The capacity type is on demand only and
Instant type here this node tool can

229
00:16:05,599 --> 00:16:10,549
spin up a C type, M type, and R type.

230
00:16:10,569 --> 00:16:14,144
So let's see What type of
things that it will happen

231
00:16:14,144 --> 00:16:15,784
when we scaled up our workload?

232
00:16:17,904 --> 00:16:18,434
Yep.

233
00:16:19,404 --> 00:16:21,974
So we spin up five replica here.

234
00:16:22,474 --> 00:16:26,974
And so we have five
pending pots now or so.

235
00:16:26,974 --> 00:16:30,784
You can see it's detect quite fast
and it's spin up a new one for us.

236
00:16:30,814 --> 00:16:33,094
It's C six a two X large.

237
00:16:33,364 --> 00:16:37,024
It's not ready yet because it take
some time for the node to be ready,

238
00:16:37,444 --> 00:16:42,424
but yeah, it's super fast to detect
and this one is faster than auto

239
00:16:42,424 --> 00:16:47,284
scaling group and cluster auto scaling,
so it takes some time for that.

240
00:16:47,784 --> 00:16:51,414
So now, it's also spin up two parts.

241
00:16:51,734 --> 00:16:55,494
It is the cool proxy and also couplet.

242
00:16:56,854 --> 00:17:01,164
Yeah, so all of them already scheduled
five parts and it's the flat right

243
00:17:01,164 --> 00:17:04,094
side for This kind of pending parts.

244
00:17:04,484 --> 00:17:09,994
So let's see the logs to understand it
further this is the log from carpenter.

245
00:17:10,864 --> 00:17:17,519
So here you request for five parts that
we see in the pending parts, right?

246
00:17:18,179 --> 00:17:22,949
And then Carpenter, we try to come
up with the right size instant.

247
00:17:23,219 --> 00:17:27,169
So here we request for five CPU,
the number of part is seven because

248
00:17:27,229 --> 00:17:29,269
it include kubelet and kubeproxy.

249
00:17:29,649 --> 00:17:33,409
So we try to search through the
instant type that we specify in

250
00:17:33,409 --> 00:17:36,819
the node pool, like 45, 46 yeah.

251
00:17:36,889 --> 00:17:41,859
It's in that time and it came
up with c6a2xlash in this

252
00:17:42,099 --> 00:17:47,169
availability zone with on demand
type and this one support 58 pods.

253
00:17:47,389 --> 00:17:51,229
This one is depending on the number
of elastic network interface.

254
00:17:51,739 --> 00:17:56,859
Now it's already registered and
ready to to host the pods for you.

255
00:17:57,589 --> 00:17:58,619
Yeah, that is a lot.

256
00:17:58,949 --> 00:18:00,749
How about now we scaled up to 20.

257
00:18:02,054 --> 00:18:06,744
So some of them can be scheduled
in the existing node, and some of

258
00:18:06,744 --> 00:18:10,054
them will be scheduled in the new
instance that's coming up here.

259
00:18:11,124 --> 00:18:12,114
So it comes up to C6a

260
00:18:14,554 --> 00:18:20,044
large, but we still have some
pending pods there, 13, right?

261
00:18:20,614 --> 00:18:24,124
I think C6a large maybe can
support like one or two.

262
00:18:24,624 --> 00:18:27,724
Let's see now it's not ready yet.

263
00:18:27,924 --> 00:18:31,994
So we need to wait until it's ready
to see what kind of pending part

264
00:18:31,994 --> 00:18:37,644
that we have Yeah, just speed just
schedule just one pot inside the

265
00:18:37,654 --> 00:18:42,784
new instance and we still have 12
parts that is in pending state here.

266
00:18:43,224 --> 00:18:47,594
Yeah, you can see here as
well and So can you guess?

267
00:18:48,454 --> 00:18:54,614
Why character don't spin up anyone because
it reached the limit that we specified

268
00:18:54,614 --> 00:19:01,004
that this node pool can spin up only
just like 10 CPU only and now it reached

269
00:19:01,004 --> 00:19:04,894
the limit so How about we increase that?

270
00:19:06,174 --> 00:19:13,484
Let's see Yeah, we will increase that to
1000 from 10 then hopefully it can spin

271
00:19:13,484 --> 00:19:17,284
up a new node to support 12 pending parts.

272
00:19:17,344 --> 00:19:18,414
So now it's quite fast.

273
00:19:18,414 --> 00:19:20,854
You can see so now it's
been up a C6A4XLarge

274
00:19:23,224 --> 00:19:23,554
here.

275
00:19:23,554 --> 00:19:25,454
The limit already changed is 1000.

276
00:19:25,454 --> 00:19:31,579
C6A4XLarge and it will fit
over the 12 pending parts here.

277
00:19:32,995 --> 00:19:39,164
Soon, so I need to wait for a
cool plan and cool proxy here

278
00:19:41,279 --> 00:19:41,679
Okay.

279
00:19:41,709 --> 00:19:42,509
Yeah, it's there.

280
00:19:42,649 --> 00:19:44,419
And it's all the pods is running.

281
00:19:44,809 --> 00:19:46,769
I would like to add arm as well.

282
00:19:46,809 --> 00:19:48,859
The ground, we don't
want to set some costs.

283
00:19:49,209 --> 00:19:52,519
And I also would like
to add the spot as well.

284
00:19:52,759 --> 00:19:56,919
For a spot, you need to make sure that
your workload can tolerate for the risk

285
00:19:56,959 --> 00:19:58,789
disruption that we call back the spot.

286
00:19:59,879 --> 00:20:00,179
Okay.

287
00:20:00,429 --> 00:20:05,219
Let's check the configuration that
we add the spot and also the arm.

288
00:20:05,379 --> 00:20:06,989
I just architecture as well.

289
00:20:06,990 --> 00:20:07,004
Okay.

290
00:20:08,374 --> 00:20:15,584
It's up there somewhere here so you
specify X86 which is AMD 64 and arm 64.

291
00:20:15,764 --> 00:20:20,084
Yeah crowd it on one and Spots as well.

292
00:20:20,244 --> 00:20:24,084
I think something will comes up here
when it fight a spot for us because

293
00:20:24,084 --> 00:20:26,814
it's cheaper It's deleting this note.

294
00:20:27,264 --> 00:20:33,254
So it's right try to drain some pots
out Already drain to the extinct one.

295
00:20:33,264 --> 00:20:39,369
Maybe the lower one And
coming up with the spot soon.

296
00:20:41,339 --> 00:20:41,819
Okay.

297
00:20:42,149 --> 00:20:43,419
Let's wait a little bit.

298
00:20:43,999 --> 00:20:44,629
Let's see.

299
00:20:45,629 --> 00:20:46,149
Okay here.

300
00:20:46,509 --> 00:20:53,949
So we have C 60 G means Graviton, which is
arm and from on demand, we change to spot.

301
00:20:53,949 --> 00:20:57,329
Here is much cheaper, like up to 70%.

302
00:20:57,829 --> 00:20:58,979
Cheaper than the only one.

303
00:20:58,979 --> 00:20:59,399
Okay.

304
00:21:00,409 --> 00:21:05,989
Just wait to it to be ready and then
it will try to drain the pods for us It

305
00:21:06,179 --> 00:21:08,819
will call it on and also drain for us.

306
00:21:09,189 --> 00:21:09,349
Yeah

307
00:21:11,819 --> 00:21:17,089
Okay, now it's working for a spot
and grab it on which help us to

308
00:21:17,089 --> 00:21:22,084
save some cost of our cluster Can
we it will delete the old one here?

309
00:21:23,444 --> 00:21:28,564
How about we change the consolidation
policies policy from when empty to

310
00:21:28,614 --> 00:21:35,794
when empty or underutilized Let's
see and here Consolidation policy

311
00:21:35,804 --> 00:21:40,244
from when empty to when empty or
underutilized under five second

312
00:21:42,644 --> 00:21:47,624
As I say you can specify like one hour
or 24 four hour do the consolidation if

313
00:21:47,934 --> 00:21:50,234
you don't want to have any disruption

314
00:21:52,419 --> 00:21:58,459
Okay, I will try to Reduce the number
of the workload here so that we have

315
00:21:58,459 --> 00:22:04,129
some underutilized node here we have one
five pots and 20 percent of consumption

316
00:22:04,129 --> 00:22:10,819
for CPU and It will try to come up
with a new node and drain for us No

317
00:22:12,389 --> 00:22:23,949
to make sure that our cluster is like
a more utilized Yep Now we have it.

318
00:22:24,179 --> 00:22:26,149
This one is also underutilized.

319
00:22:26,269 --> 00:22:30,629
So we'll try to spin up a new one
and then move all the parts for us.

320
00:22:30,819 --> 00:22:30,989
Yeah.

321
00:22:32,959 --> 00:22:36,939
Deleting the other one
that don't need anymore.

322
00:22:38,999 --> 00:22:43,159
So this is how it works for
consolidation and also some other

323
00:22:43,169 --> 00:22:47,269
things like architecture, spot and
also the type that we can select.

324
00:22:48,019 --> 00:22:50,699
How about now we would like
to do 10 and toleration.

325
00:22:51,199 --> 00:22:56,789
So we specify that Carpenter will
spin up a part, a node with a 10.

326
00:22:57,039 --> 00:22:59,139
The key is my name is Chakri.

327
00:23:00,259 --> 00:23:04,179
Here, so the node that spin up
by Carpenter will have this 10.

328
00:23:04,679 --> 00:23:07,809
So the deployment need to have
the toleration for this one.

329
00:23:08,359 --> 00:23:13,029
I will try to increase the
number here of the workload.

330
00:23:13,864 --> 00:23:17,664
Oh, unfortunately that it will
fall into the node that we have

331
00:23:17,664 --> 00:23:19,464
already, then it can be scheduled.

332
00:23:20,034 --> 00:23:21,884
So now we add some more.

333
00:23:22,854 --> 00:23:26,674
So these some of the pods
here will be in pending.

334
00:23:27,724 --> 00:23:30,854
Because The Carpentry cannot
spin up a node for it.

335
00:23:31,104 --> 00:23:35,444
Deployment doesn't have a
toleration for the 10 key chakras.

336
00:23:36,254 --> 00:23:37,594
So cannot spin up here.

337
00:23:37,934 --> 00:23:42,164
So now we would like to add the
toleration into the deployment so that

338
00:23:42,304 --> 00:23:44,294
Carpentry can spin up a node for us.

339
00:23:45,384 --> 00:23:45,964
So let's see.

340
00:23:46,964 --> 00:23:47,404
Okay.

341
00:23:47,534 --> 00:23:51,184
Just add the toleration
here in our deployment.

342
00:23:51,294 --> 00:23:51,874
Let's check.

343
00:23:52,874 --> 00:23:54,784
Oh, it's just spin up a new one for us.

344
00:23:54,844 --> 00:23:57,334
You can see ccgs for Eclodge.

345
00:23:57,804 --> 00:24:01,044
So let's see in the deployment
that we have the iteration.

346
00:24:01,544 --> 00:24:02,564
That I just added.

347
00:24:03,564 --> 00:24:04,274
Okay.

348
00:24:05,464 --> 00:24:07,784
Here is the iteration.

349
00:24:07,804 --> 00:24:11,654
That's why Carpenter can
spin up a new node for us.

350
00:24:11,704 --> 00:24:13,454
And yeah, schedule our parts.

351
00:24:13,954 --> 00:24:14,324
Boop.

352
00:24:15,644 --> 00:24:16,184
Okay.

353
00:24:17,844 --> 00:24:23,914
Okay, that's all I think I think the
final one Is about the ac spread so

354
00:24:23,914 --> 00:24:28,444
carpentry will respect anything that
you configure in your deployment So

355
00:24:28,444 --> 00:24:31,204
here I would like to spread my ac.

356
00:24:31,254 --> 00:24:35,594
From each other And you can see
that a lot of things that happening.

357
00:24:35,694 --> 00:24:41,234
So you try to spin up a new node
for supporting different ac So this

358
00:24:41,234 --> 00:24:46,694
one will ensure that your workload
will have higher resiliency.

359
00:24:48,144 --> 00:24:53,204
So this one is the configuration that
I specify in the deployment that I

360
00:24:53,214 --> 00:24:55,264
would like to use topology spread.

361
00:24:55,744 --> 00:24:59,124
That if it cannot satisfy,
will not schedule.

362
00:24:59,214 --> 00:25:00,774
You can see that do not schedule that.

363
00:25:01,874 --> 00:25:05,114
You can schedule anyway as well,
but this one I would like to show

364
00:25:05,114 --> 00:25:08,374
that Carpenter, we respect anything
that we put in a deployment.

365
00:25:08,874 --> 00:25:13,344
So we would like to check when
it's done that we have multiple

366
00:25:13,344 --> 00:25:15,239
AC for our workload here.

367
00:25:15,689 --> 00:25:19,319
So you can split, you can,
okay, it's deleting one of

368
00:25:19,319 --> 00:25:20,979
them, maybe just wait for it.

369
00:25:21,179 --> 00:25:24,649
So here we have AC1A, 1B, 1C.

370
00:25:24,669 --> 00:25:26,579
So let's see again because it's deleting.

371
00:25:27,419 --> 00:25:29,829
Yep, I think we have all of the AC here.

372
00:25:29,879 --> 00:25:32,229
A, B, C for our workload.

373
00:25:32,339 --> 00:25:35,119
Yeah, you try to balance the AC for us.

374
00:25:36,189 --> 00:25:37,589
That's all I would like to show.

375
00:25:37,789 --> 00:25:40,749
A lot of things that you
can explore by yourself.

376
00:25:41,039 --> 00:25:44,839
You can also read to the best
practice that we have online.

377
00:25:45,449 --> 00:25:49,499
So for onboarding to Carpenter, so
you can install Carpenter by using

378
00:25:49,509 --> 00:25:55,819
Hamshark and One best practice here
that do not run the Carpenter on the

379
00:25:55,829 --> 00:25:57,974
node that It's managed by Carpenter.

380
00:25:58,054 --> 00:26:01,494
Otherwise, it will just delete the node
that runs Carpenter and stop working.

381
00:26:01,924 --> 00:26:04,884
You can run Carpenter on Fargate
or different node groups.

382
00:26:05,394 --> 00:26:08,614
And you can also migrate from
Cluster, Autoscaler as well.

383
00:26:08,634 --> 00:26:09,614
The guide is there.

384
00:26:09,754 --> 00:26:10,834
And you can explore.

385
00:26:11,334 --> 00:26:12,064
So let's sum up.

386
00:26:12,899 --> 00:26:16,629
Also, CloudHunter is fast, it's quite
simple, but it's quite powerful.

387
00:26:17,009 --> 00:26:21,739
It's cost effective because it will try to
reduce the cost for you by consolidation.

388
00:26:22,229 --> 00:26:26,469
And also it's Kubernetes because
it's open source and now we have,

389
00:26:26,589 --> 00:26:29,149
we support multiple cloud already.

390
00:26:29,409 --> 00:26:31,979
And yeah, I think it's awesome.

391
00:26:33,359 --> 00:26:34,129
Thank you so much.

