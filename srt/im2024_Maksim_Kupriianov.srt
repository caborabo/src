1
00:00:00,130 --> 00:00:02,580
Hello and welcome to Con 42.

2
00:00:02,930 --> 00:00:04,310
I am Maxim Kupriyanov.

3
00:00:04,360 --> 00:00:09,659
My talk today is network incidents or
what to know before the chaos strikes.

4
00:00:10,140 --> 00:00:11,690
Few words about myself.

5
00:00:11,960 --> 00:00:16,400
I work as a Site Reliability Engineer,
Production Engineer, before that System

6
00:00:16,400 --> 00:00:22,055
Administrator, For a lot of years,
and I've seen a lot, I'd say, I wish

7
00:00:22,324 --> 00:00:26,134
I see them less, but, yes, that's
true, a lot of different incidents,

8
00:00:26,194 --> 00:00:31,075
and, those of them who were connected
to the, or root cause by the network

9
00:00:31,075 --> 00:00:33,785
services were the most devastating ones.

10
00:00:34,570 --> 00:00:39,210
let's dive deep what is actually
a network incident and, like,

11
00:00:39,260 --> 00:00:40,800
why it's so devastating.

12
00:00:41,330 --> 00:00:44,900
First of all, a network incident
is just an expected event that

13
00:00:44,909 --> 00:00:48,920
disrupts or degrades critical
network services, impacting their

14
00:00:48,940 --> 00:00:50,750
availability, integrity, or performance.

15
00:00:51,050 --> 00:00:51,560
Sounds?

16
00:00:52,135 --> 00:00:54,915
Very similar to any other incident.

17
00:00:55,065 --> 00:01:00,345
But, point is, critical network services.

18
00:01:00,355 --> 00:01:03,214
Let's take a look what they are.

19
00:01:03,924 --> 00:01:08,405
And these are DNS, DHCP, Routing, VPN.

20
00:01:08,435 --> 00:01:10,304
These are infrastructure services.

21
00:01:10,685 --> 00:01:14,014
They are security services including
firewall intrusion detection

22
00:01:14,044 --> 00:01:19,425
prevention, security certificate
centers, physical access control.

23
00:01:19,925 --> 00:01:21,395
These are directory services.

24
00:01:21,445 --> 00:01:25,985
yes, so you can't authenticate or
authorize your users, the cloud

25
00:01:25,985 --> 00:01:31,555
and container orchestration, or
just web services which deliver

26
00:01:31,555 --> 00:01:35,685
your traffic, CDN, load balancers,
or web service architecture.

27
00:01:36,015 --> 00:01:38,295
That's a very sensitive problem.

28
00:01:38,650 --> 00:01:43,010
point sensitive layer of
software for any organization.

29
00:01:43,930 --> 00:01:48,410
And, why this network incidents
dealing with this critical

30
00:01:48,420 --> 00:01:50,250
services are so disruptive.

31
00:01:50,470 --> 00:01:55,420
First of all, they frequent,
I'd say almost every time they

32
00:01:55,420 --> 00:01:57,090
lead to cascading failures.

33
00:01:57,410 --> 00:01:59,400
for example, if a DNS.

34
00:01:59,605 --> 00:02:04,865
server failed, very likely that a lot
of services that are still dependent

35
00:02:04,865 --> 00:02:07,395
on DNS will start to malfunction.

36
00:02:07,645 --> 00:02:10,275
By the way, please never depend on DNS.

37
00:02:10,305 --> 00:02:12,305
If you can avoid it by any cost.

38
00:02:13,385 --> 00:02:15,205
But still, a lot of
services still do that.

39
00:02:15,665 --> 00:02:18,655
then you lose your monitoring
and diagnostic tools.

40
00:02:18,695 --> 00:02:20,175
Not them, but access to them.

41
00:02:20,185 --> 00:02:21,079
Usually it's very fast.

42
00:02:21,170 --> 00:02:25,680
Frequent story, then downtime
affects multiple stakeholders.

43
00:02:25,720 --> 00:02:30,090
Yes People will come to you or
your network engineer or site

44
00:02:30,200 --> 00:02:31,550
reliability engineer with a question.

45
00:02:31,950 --> 00:02:32,620
What's going on?

46
00:02:33,440 --> 00:02:34,940
We can't work.

47
00:02:35,020 --> 00:02:40,540
Nothing is working do something with that
Difficulty in triage modern networks are

48
00:02:40,540 --> 00:02:43,070
very complicated, especially big ones.

49
00:02:43,420 --> 00:02:47,145
It takes a lot of time to find
the root cause to triage it

50
00:02:47,195 --> 00:02:51,525
and to fix it afterwards, time
consuming troubleshooting.

51
00:02:51,695 --> 00:02:52,175
Yes.

52
00:02:52,205 --> 00:02:53,995
Time is just money.

53
00:02:53,995 --> 00:02:56,955
In that case, if your business
is not working, you're losing

54
00:02:56,955 --> 00:02:59,795
money, increased security risk.

55
00:02:59,875 --> 00:03:02,065
that's, can happen for two reasons.

56
00:03:02,595 --> 00:03:04,475
First of all, you can like.

57
00:03:05,060 --> 00:03:10,080
Firewall or whatever can be malfunctioning
and another story is you can make a

58
00:03:10,080 --> 00:03:15,040
temporary exemption for fixing the issue
And it could lead to the attackers to come

59
00:03:15,090 --> 00:03:22,250
in and do something very violent Okay,
of course reduced business operations.

60
00:03:22,280 --> 00:03:27,520
That's basically It's all it, what
it mean, let's, take a look at a

61
00:03:27,520 --> 00:03:31,420
couple of examples of the recently
happened, network incidents, with

62
00:03:31,420 --> 00:03:38,480
the big tech companies happen like in
2020, 2021, there is no 2023 and 24,

63
00:03:38,540 --> 00:03:40,570
because I was not able to find them.

64
00:03:40,630 --> 00:03:41,000
there were no.

65
00:03:42,210 --> 00:03:46,530
incidents, but still some
of them probably happen.

66
00:03:47,140 --> 00:03:47,460
Okay.

67
00:03:47,490 --> 00:03:50,580
let's start from just
briefly cloudflare outage.

68
00:03:50,860 --> 00:03:53,160
It's, happened in July, 2022.

69
00:03:53,485 --> 00:03:54,225
20 minutes.

70
00:03:54,235 --> 00:03:58,445
Basically, it was a misconfiguration
in router, which, lead to

71
00:03:58,445 --> 00:04:01,365
the, router malfunctioning and
overloading one of the sites.

72
00:04:02,115 --> 00:04:06,675
then, Google cloud outage,
authentication issue, Google accounts

73
00:04:06,675 --> 00:04:08,975
authentication failure was in that time.

74
00:04:09,745 --> 00:04:14,465
So services like Gmail, YouTube, and
Google drive were down for, about an hour.

75
00:04:14,975 --> 00:04:17,675
Then, Microsoft Azure and teams outage.

76
00:04:18,180 --> 00:04:20,550
Actually, teams were not
working for three, three hours.

77
00:04:20,600 --> 00:04:24,960
It happens again because the
Active Directory failure.

78
00:04:25,040 --> 00:04:29,670
So authentication, Fastly
CDN outage, bugging triggered

79
00:04:29,680 --> 00:04:31,100
by customer configuration.

80
00:04:31,110 --> 00:04:39,350
You see a one hour Facebook outage, six
hours that happened because the software

81
00:04:39,410 --> 00:04:44,360
was not correctly handled in several
conditions happened in the network.

82
00:04:44,820 --> 00:04:45,190
Just.

83
00:04:46,310 --> 00:04:47,560
Error in developing.

84
00:04:48,400 --> 00:04:52,349
Amazon web service, AWS
outage, 6 hours and 30 minutes.

85
00:04:52,350 --> 00:04:56,210
And again, there's actually
a software issue, I'd say.

86
00:04:57,405 --> 00:04:59,055
what we, can take from this?

87
00:04:59,655 --> 00:05:05,495
In the IT world, network incidents
are as certain as sunrise, so if you

88
00:05:05,565 --> 00:05:09,985
never had them, you probably will
have them at some point of time.

89
00:05:11,475 --> 00:05:13,375
The anatomy of a network incident.

90
00:05:13,835 --> 00:05:18,425
Let's dive inside of how things
usually start and develop.

91
00:05:18,795 --> 00:05:22,795
first of all, initial situation, you
have a very healthy green system.

92
00:05:23,120 --> 00:05:26,210
The network is fine, systems
are fine, everything is working

93
00:05:26,210 --> 00:05:28,930
perfectly, but then, hardware failure.

94
00:05:29,440 --> 00:05:32,190
For instance, it could be a
power supply of your router,

95
00:05:32,190 --> 00:05:34,120
switch, or line card, whatever.

96
00:05:34,480 --> 00:05:37,400
It could be more tricky stuff,
I've seen from my experience like

97
00:05:37,400 --> 00:05:41,370
a malfunctioning central processing
unit which start to mess up data.

98
00:05:42,100 --> 00:05:47,050
Things could go immediately wrong or they
can take some time to develop, but still.

99
00:05:47,330 --> 00:05:50,310
software issue, two
situations could happen here.

100
00:05:50,560 --> 00:05:53,105
First, just the software bug.

101
00:05:53,125 --> 00:05:56,025
Sometimes it happens by
the way, quite frequently.

102
00:05:56,505 --> 00:05:59,035
another story is unsupported scenario.

103
00:05:59,085 --> 00:06:02,735
And that's is very frequent
nowadays because, unfortunately

104
00:06:02,735 --> 00:06:04,095
developers are not, gods.

105
00:06:04,095 --> 00:06:05,145
They don't know everything.

106
00:06:05,645 --> 00:06:10,005
the systems, Sometimes can predict
that they can be put in such a

107
00:06:10,005 --> 00:06:15,355
situation that they just can't handle
and that's also software issue human

108
00:06:15,375 --> 00:06:20,635
error Usually, it's a misconfiguration
story untested configuration or

109
00:06:20,635 --> 00:06:25,401
just wrong configuration Mistype,
whatever it can Security breaches.

110
00:06:25,401 --> 00:06:31,225
It's usually some kind of external
factor, but someone trying to a break

111
00:06:31,225 --> 00:06:36,062
your perimeter or someone trying to
like make a denial of service for your,

112
00:06:36,522 --> 00:06:40,282
for service or just external factors.

113
00:06:40,312 --> 00:06:43,642
This is usually the most
simple to trash things.

114
00:06:44,162 --> 00:06:50,627
It could be, the flat, or it could
be, kind of fiber cut, or Again, from

115
00:06:50,627 --> 00:06:54,917
my experience, I've seen quite crazy
things, with the animal, somehow

116
00:06:54,917 --> 00:06:59,757
got into the transformer, caused the
short circuit, which caused the fire

117
00:07:00,677 --> 00:07:03,077
and it's caused the power outage.

118
00:07:04,207 --> 00:07:05,167
It can happen as well.

119
00:07:05,167 --> 00:07:08,977
Unfortunately, we can do anything
with that, just external factors.

120
00:07:09,167 --> 00:07:11,357
and my favorite market related events.

121
00:07:12,257 --> 00:07:16,057
I was working for several years
for the marketplace and usually

122
00:07:16,057 --> 00:07:18,407
Black Fridays were very hot for us.

123
00:07:18,662 --> 00:07:25,132
And when a Black Friday starts, a
lot of people come to your services.

124
00:07:25,152 --> 00:07:28,832
A lot of people start
to make transactions.

125
00:07:29,342 --> 00:07:33,542
And it, it depends a lot on
how professional you are in

126
00:07:33,542 --> 00:07:35,552
capacity planning before that.

127
00:07:36,862 --> 00:07:40,452
So you are very close to
the overload at this point.

128
00:07:40,492 --> 00:07:41,432
And this can happen.

129
00:07:41,652 --> 00:07:45,822
Sometimes it happens at,
random, unpredicted moments.

130
00:07:45,872 --> 00:07:46,322
It could be.

131
00:07:46,832 --> 00:07:48,902
Market is chaotic sometimes.

132
00:07:50,532 --> 00:07:55,522
first of all, after the initial trigger,
again, it could develop for a week,

133
00:07:55,592 --> 00:07:58,042
but usually it starts right after that.

134
00:07:58,382 --> 00:07:59,162
What do you see?

135
00:07:59,412 --> 00:08:00,642
You see the service outage.

136
00:08:00,672 --> 00:08:01,482
That's obvious.

137
00:08:01,482 --> 00:08:04,512
You, for instance, a file server
can stop working, database,

138
00:08:04,512 --> 00:08:10,042
communication system, anything,
customer facing application, websites.

139
00:08:10,782 --> 00:08:12,422
It can start to fail.

140
00:08:12,922 --> 00:08:19,732
other terms, all the monitoring and other
tools, send out flood of alerts, multiple

141
00:08:19,742 --> 00:08:24,802
systems and services detect failures,
the team could be overwhelmed with, with

142
00:08:24,812 --> 00:08:27,732
these alerts, or it could be even worse.

143
00:08:28,037 --> 00:08:32,687
When you don't start, you don't
receive any alerts at all because like

144
00:08:33,037 --> 00:08:37,937
system is unreachable completely or
just malfunctioning, user complaints.

145
00:08:37,937 --> 00:08:43,457
Of course, employees and customers, the
usual very social media starts to, to

146
00:08:43,457 --> 00:08:49,727
make some, posts, like any other media
can start to join this party, whatever.

147
00:08:49,727 --> 00:08:52,027
So yeah, this happens.

148
00:08:52,847 --> 00:08:53,687
What's next?

149
00:08:53,787 --> 00:08:58,417
unfortunately, network incidents,
they, very rarely remain isolated.

150
00:08:59,287 --> 00:09:02,147
First of all, interdependent
systems start to fail.

151
00:09:02,877 --> 00:09:06,307
For instance, authentication
service goes down.

152
00:09:06,797 --> 00:09:10,057
All the services dependent on the
user authentication authorization

153
00:09:10,157 --> 00:09:12,527
can fail as well afterwards.

154
00:09:13,307 --> 00:09:16,957
And, applications can start, to be slow.

155
00:09:16,957 --> 00:09:17,031
for listening.

156
00:09:17,292 --> 00:09:23,152
or just failing or in the responsive
and, the worst thing about that,

157
00:09:23,372 --> 00:09:28,042
which can happen and that's frequently
happen, they start to retry, if one

158
00:09:28,112 --> 00:09:32,772
operation fail, application start
to retry the operation and the

159
00:09:32,822 --> 00:09:36,722
application before that application
also start to retry the operation.

160
00:09:36,912 --> 00:09:38,102
And they create such a.

161
00:09:38,842 --> 00:09:44,632
big wave of requests that they usually
kill their underlying services completely.

162
00:09:45,862 --> 00:09:48,852
Traffic bottlenecks,
that's a network thing.

163
00:09:48,862 --> 00:09:53,482
So if you, if the network incident,
if it didn't, it happened because of

164
00:09:53,482 --> 00:09:58,142
a road misconfiguration or maybe a
fiber cut or whatever, your links could

165
00:09:58,272 --> 00:10:03,462
be overloaded and it could lead to,
to all the service using the slings.

166
00:10:03,622 --> 00:10:05,842
failing resource exhaustion.

167
00:10:06,802 --> 00:10:09,812
That's what I was talking
about the many retries.

168
00:10:10,172 --> 00:10:14,422
Each device has its own hardware
limits, and if you start to send to a

169
00:10:14,422 --> 00:10:18,542
server, a million requests per minute
instead of a hundred, as you did

170
00:10:18,602 --> 00:10:23,542
previously, yeah, probably it will,
not handle it, it will, it will fail.

171
00:10:26,407 --> 00:10:31,617
Then, you'll lose visibility and control,
and that's a very difficult thing because

172
00:10:31,627 --> 00:10:35,897
first of all, your inaccessible monitoring
tools are become, your monitoring

173
00:10:35,897 --> 00:10:41,367
tools, Solib story becomes inaccessible,
it's a very frequent thing can happen,

174
00:10:41,407 --> 00:10:45,217
and, you just don't know what happened
in your network with your services.

175
00:10:45,907 --> 00:10:51,447
remote access issue, you can just lose
access to your network, completely,

176
00:10:52,087 --> 00:10:55,057
because of authentication, because
of physical failure, anything,

177
00:10:55,127 --> 00:10:56,967
it just doesn't exist anymore.

178
00:10:57,517 --> 00:11:01,447
or a communication breakdown,
for example, email or messaging

179
00:11:01,507 --> 00:11:05,156
applications can be down, it's
difficult to coordinate, to communicate.

180
00:11:05,607 --> 00:11:09,737
Teams just don't know what's going
on like implode like individual

181
00:11:09,767 --> 00:11:14,797
people's people can can't understand
what's going on what to do

182
00:11:17,157 --> 00:11:21,247
and Incident may expose the
network to additional threats.

183
00:11:21,967 --> 00:11:27,377
Why it can happen not only because
of the network incident happen, but

184
00:11:28,377 --> 00:11:34,472
because of the you know How you try
to fix that sometimes for that, some

185
00:11:34,512 --> 00:11:41,622
holes in firewalls are made or just
firewall is disabled or some, simplified,

186
00:11:41,732 --> 00:11:47,162
applications are landed, which can
handle the situation, but it can help,

187
00:11:47,382 --> 00:11:53,696
but sometimes, attackers just Wait
for exactly this to happen to go on.

188
00:11:54,496 --> 00:12:00,176
So be aware, usually it's a very
bad practice to decrease, security

189
00:12:00,176 --> 00:12:02,036
level during the incidents.

190
00:12:04,096 --> 00:12:07,726
And of course, team face
multiple obstacles in terms

191
00:12:07,726 --> 00:12:09,126
of like stress and pressure.

192
00:12:09,496 --> 00:12:15,046
this network incidents, they are usually
very, Now, people get very nervous because

193
00:12:15,066 --> 00:12:20,476
nothing is working and you face a stress
and pressure from, your stakeholders,

194
00:12:20,536 --> 00:12:26,216
and from your team members, whatever, you
have prioritization dilemmas to solve.

195
00:12:26,216 --> 00:12:30,996
So what start to do first, fixing
this or that you'd never don't know

196
00:12:31,136 --> 00:12:33,406
you need someone to consult to, or.

197
00:12:34,611 --> 00:12:35,521
It's difficult.

198
00:12:35,791 --> 00:12:39,571
Communication with stakeholders,
they are being very tough because

199
00:12:39,621 --> 00:12:44,321
you are fixing, the service, you
don't have time to talk to someone,

200
00:12:44,331 --> 00:12:48,941
to anyone else, and stakeholders,
they are, they just, they want the

201
00:12:48,941 --> 00:12:51,511
service to get, to be working back.

202
00:12:51,891 --> 00:12:56,111
They, try to, communicate with
you, but you're not answering.

203
00:12:56,151 --> 00:12:57,441
All this stuff is happens.

204
00:12:59,131 --> 00:13:03,221
what are the key, take outs
from, this anatomy story?

205
00:13:03,691 --> 00:13:08,161
Network incidents have multiple triggers,
so anything can start an incident.

206
00:13:08,721 --> 00:13:13,071
Sometimes you can never thought about
it, like I said, an animal transformer.

207
00:13:13,541 --> 00:13:14,201
How could that be?

208
00:13:14,651 --> 00:13:15,641
Did it get there?

209
00:13:15,651 --> 00:13:16,201
I don't know.

210
00:13:16,901 --> 00:13:19,111
interconnectedness, magnitudes, impacts.

211
00:13:19,311 --> 00:13:23,731
Network services are very critical
because a lot of other services depend

212
00:13:23,731 --> 00:13:25,731
on them and this is a cascading event.

213
00:13:25,951 --> 00:13:26,841
It's a chain event.

214
00:13:27,181 --> 00:13:28,741
things can go pretty bad.

215
00:13:29,191 --> 00:13:31,941
loss of visibility complicates response.

216
00:13:32,461 --> 00:13:34,021
Yes, of course.

217
00:13:34,171 --> 00:13:38,561
If you don't see what's happening
and you, very frequently you

218
00:13:38,561 --> 00:13:40,911
can't even, tell, if the network.

219
00:13:41,321 --> 00:13:43,021
is faulty or something else.

220
00:13:48,541 --> 00:13:49,281
Preparation.

221
00:13:50,001 --> 00:13:54,211
Now, let's talk about, how
can you be prepared for such

222
00:13:54,211 --> 00:13:55,871
a network incident event?

223
00:13:58,391 --> 00:14:00,551
Key steps you have to take.

224
00:14:01,051 --> 00:14:04,701
First, you need a kind
of a network redundancy.

225
00:14:04,851 --> 00:14:10,361
And by that, I not only mean like
just having multiple network path, or,

226
00:14:10,391 --> 00:14:13,041
setting up multiple routers or whatever.

227
00:14:13,571 --> 00:14:15,701
geographical redundancy as well.

228
00:14:16,031 --> 00:14:20,811
because, for the external factory events,
it could, happen that, this particular

229
00:14:20,871 --> 00:14:26,277
area, the you need, to be present
somewhere else to continue the operations.

230
00:14:26,297 --> 00:14:27,267
you have to do this.

231
00:14:27,267 --> 00:14:32,137
You have to, and this have to
exist before, the incident strikes.

232
00:14:33,412 --> 00:14:36,292
Out of band management
network is a crucial story.

233
00:14:36,812 --> 00:14:40,392
if the network is the issue,
it means that you are losing

234
00:14:40,462 --> 00:14:42,942
access to your services at all.

235
00:14:42,952 --> 00:14:45,372
You get access to your devices at all.

236
00:14:45,702 --> 00:14:50,242
And the only way you can, you can do
something with your network services,

237
00:14:50,242 --> 00:14:54,502
devices, whatever, you can, right
to a data center with the console

238
00:14:54,502 --> 00:14:56,822
cable, and fix something from there.

239
00:14:57,072 --> 00:15:01,622
So if you think before, and you,
can build out of band management

240
00:15:01,632 --> 00:15:04,102
network, things can go pretty well.

241
00:15:04,112 --> 00:15:05,932
You just connect, from the.

242
00:15:06,897 --> 00:15:10,747
I'd say opposite side, you make
the changes, you fix the issue.

243
00:15:11,537 --> 00:15:13,487
Data backs up, backups, of course.

244
00:15:13,577 --> 00:15:18,127
Sometimes network incidents can lead to
a data fail, data loss, unfortunately.

245
00:15:18,487 --> 00:15:20,737
And data backups is crucial for this.

246
00:15:21,277 --> 00:15:26,167
Especially, just to remind, data backups
should not be held in the same location

247
00:15:26,227 --> 00:15:28,787
as the system services are working.

248
00:15:29,787 --> 00:15:35,787
Failover systems, that's pretty obvious
story if you know that any server can

249
00:15:35,807 --> 00:15:39,437
fail, You need something to overtake it.

250
00:15:42,457 --> 00:15:48,647
very important step to take is to conduct
regular system audits and health check.

251
00:15:48,697 --> 00:15:53,047
And by this, I don't just mean,
having a PDF with all green marks

252
00:15:53,047 --> 00:15:54,577
checks that everything's fine.

253
00:15:55,017 --> 00:15:56,477
It should be a continuous thing.

254
00:15:56,897 --> 00:15:59,507
and we're talking about network
assessments and hardware

255
00:15:59,507 --> 00:16:01,137
maintenance, capacity planning.

256
00:16:01,517 --> 00:16:06,957
It should be, You should work on this,
all the time, continuously and subset

257
00:16:06,957 --> 00:16:09,357
of monitoring tools and dashboards.

258
00:16:09,407 --> 00:16:12,617
It's definitely must be
accessible out of band.

259
00:16:12,867 --> 00:16:16,217
Just imagine your network is not
working services, not working.

260
00:16:16,267 --> 00:16:18,917
how do you understand what's
going on actually inside of it?

261
00:16:19,247 --> 00:16:22,556
You need something to monitor
this, some access tools.

262
00:16:22,557 --> 00:16:24,307
You can, rent some services.

263
00:16:24,857 --> 00:16:28,447
elsewhere, export some,
part of data there.

264
00:16:28,847 --> 00:16:31,987
usually it's not a kind
of secret thing of data.

265
00:16:31,987 --> 00:16:34,447
You just some, general health metrics.

266
00:16:34,457 --> 00:16:36,487
You need to have them somewhere.

267
00:16:39,057 --> 00:16:43,376
another very important thing
is to develop a comprehensive

268
00:16:43,376 --> 00:16:45,087
documentation and playbooks.

269
00:16:45,097 --> 00:16:50,627
Usually, all the company I was working
with, It's usually a painful thing

270
00:16:50,637 --> 00:16:54,807
to have a comprehensive documentation
because things are, systems are, they

271
00:16:54,807 --> 00:16:57,547
are being, developed, continuously.

272
00:16:57,557 --> 00:17:00,947
They are changing continuously,
and the best documentation

273
00:17:00,947 --> 00:17:02,517
you can have is in your code.

274
00:17:02,897 --> 00:17:08,857
But, For these network related incidents,
you need to have at least playbooks, and

275
00:17:08,857 --> 00:17:14,267
these playbooks and network documentation,
they should be available remotely, out

276
00:17:14,267 --> 00:17:19,527
of band, external backup, whatever, you
need to have it remotely, you won't have

277
00:17:19,557 --> 00:17:22,947
an access to them when systems will fail.

278
00:17:23,277 --> 00:17:27,907
So just have a copy somewhere
else and regularly check that

279
00:17:27,907 --> 00:17:29,567
you, can access this copy.

280
00:17:29,877 --> 00:17:33,997
Because a crazy thing can happen, people
can just, forget their, alternative

281
00:17:33,997 --> 00:17:35,547
password, forget where they store it.

282
00:17:38,987 --> 00:17:41,227
Okay, and, finally, drills.

283
00:17:41,837 --> 00:17:45,567
You need to like to fire
drills in your network.

284
00:17:45,587 --> 00:17:47,167
it's very important.

285
00:17:47,547 --> 00:17:51,327
they should be done on the regular
basis, drills and simulations.

286
00:17:51,337 --> 00:17:55,477
So you need to be able to take
down your systems, components

287
00:17:55,477 --> 00:17:56,957
of it to see how things goes.

288
00:17:56,967 --> 00:18:00,697
For instance, what will happen
if the DNS server will go down?

289
00:18:00,767 --> 00:18:03,077
Just turn it off in some location.

290
00:18:03,407 --> 00:18:04,707
See what will happen.

291
00:18:05,062 --> 00:18:08,532
If you have some, health budget
on your systems, ensure that your

292
00:18:08,532 --> 00:18:12,762
team is prepared and, evaluate
and update the response plans.

293
00:18:12,792 --> 00:18:19,792
Yes, this seems like slightly, a
lot of work, but if you want to save

294
00:18:19,792 --> 00:18:24,072
some, time, money, nerves, whatever,
just mental health during the network

295
00:18:24,072 --> 00:18:29,212
incidents, if you will do this, you will
be prepared for this network incidents.

296
00:18:30,432 --> 00:18:31,222
let's recap.

297
00:18:31,892 --> 00:18:35,162
Preparations is an ongoing process.

298
00:18:36,052 --> 00:18:40,842
Always have a backup
plan, fire regular drills.

299
00:18:41,982 --> 00:18:45,032
And like with all of that,
the benefits outweigh costs.

300
00:18:45,042 --> 00:18:48,282
Yes, you'll spend a lot on this,
a lot of time, a lot of money,

301
00:18:48,712 --> 00:18:50,852
but you better do this than not.

302
00:18:52,077 --> 00:18:56,657
So let me give you, some tips on
how to, better behave during the

303
00:18:56,657 --> 00:19:03,417
incident, to, to fix it sooner, to,
get less penalties from that, and,

304
00:19:03,437 --> 00:19:05,337
first recommendation is stay calm.

305
00:19:06,122 --> 00:19:08,172
It's probably the most important one.

306
00:19:08,172 --> 00:19:09,202
Stay calm.

307
00:19:09,252 --> 00:19:13,042
if you get in panic, into panic, then
probably you won't be as effective.

308
00:19:13,432 --> 00:19:17,332
the, the network incident
will take much longer time.

309
00:19:17,332 --> 00:19:18,822
It will be harder to fix, whatever.

310
00:19:19,022 --> 00:19:19,832
Stay calm.

311
00:19:20,897 --> 00:19:23,997
the second step, so you should take
the gather initial information.

312
00:19:24,007 --> 00:19:26,377
There could be logs, there could
be other from your systems.

313
00:19:26,527 --> 00:19:30,847
And for that to happen, you need
to out of band access or backup.

314
00:19:30,917 --> 00:19:33,647
sometimes you're there still
accessible and available.

315
00:19:33,867 --> 00:19:33,967
It's a good thing.

316
00:19:34,322 --> 00:19:39,042
It's just depending on how lucky
you are, also user reports.

317
00:19:39,052 --> 00:19:45,652
So sometimes they find the problem before
any other systems will come into play.

318
00:19:46,192 --> 00:19:50,302
And you should determine the scope
of the impact, which systems are.

319
00:19:50,792 --> 00:19:55,752
affected and, like how many of them,
then, establish incident command.

320
00:19:55,772 --> 00:19:57,732
This is again, a very important step.

321
00:19:57,792 --> 00:19:58,632
you have to take it.

322
00:19:59,112 --> 00:20:02,922
you need to assign someone
as an incident manager.

323
00:20:02,942 --> 00:20:06,482
It doesn't mean that the key is
the manager, the key, the captain,

324
00:20:06,482 --> 00:20:11,502
whatever, but he is the point of
content contact for everyone else.

325
00:20:12,042 --> 00:20:17,582
And while Some of your team will,
be very deep diving inside the

326
00:20:17,582 --> 00:20:20,002
debugging, triaging, fixing things.

327
00:20:20,682 --> 00:20:24,382
This captain will maintain the
current state of the incident and

328
00:20:24,382 --> 00:20:28,762
will communicate with another teams
or communicate with the stakeholders,

329
00:20:29,302 --> 00:20:31,452
and synchronize the team activity.

330
00:20:31,522 --> 00:20:35,772
So it's very important and it's
better be, prepared beforehand.

331
00:20:36,012 --> 00:20:40,142
and that, I'll talk about training
previously, and drills, so you should

332
00:20:40,142 --> 00:20:46,192
find the way how to, who will be
assigned to this, incident manager role.

333
00:20:46,812 --> 00:20:48,512
then secure communication channel.

334
00:20:48,972 --> 00:20:53,902
Your team need to have an effective
way to communicate during incidents.

335
00:20:54,552 --> 00:20:59,122
It could happen that, for instance,
if you use, company wide messenger

336
00:20:59,122 --> 00:21:01,552
or whatever, it could be broken.

337
00:21:01,797 --> 00:21:04,457
Because of the network incident,
especially it's true for big

338
00:21:04,457 --> 00:21:07,367
companies which have their,
own communication solutions.

339
00:21:07,997 --> 00:21:12,557
You should have alternative one and
it, everybody should know how to

340
00:21:12,557 --> 00:21:14,707
use it and how to get inside of it.

341
00:21:16,417 --> 00:21:21,257
A lot of incidents happened when
like people were just not aware that

342
00:21:21,267 --> 00:21:25,077
they have a kind of communication
IRC channel or alternative messenger

343
00:21:25,097 --> 00:21:27,237
or how to reach other people.

344
00:21:27,367 --> 00:21:31,107
Sometimes they just were not
able to even find the phone of

345
00:21:31,107 --> 00:21:33,017
their colleague to contact with.

346
00:21:33,627 --> 00:21:35,327
So please do this beforehand.

347
00:21:35,957 --> 00:21:38,067
And, inform stakeholders.

348
00:21:38,177 --> 00:21:40,287
here like honesty is the key.

349
00:21:40,307 --> 00:21:45,452
you better Use a very simple, no technical
terms, in a very simple words, you

350
00:21:45,452 --> 00:21:50,712
should describe to them, what's going
on and what's are your, thoughts about

351
00:21:50,712 --> 00:21:55,882
it very briefly, but they should know,
if they won't get the information.

352
00:21:55,937 --> 00:21:59,217
things can get very, worse
for you, after the incident.

353
00:21:59,217 --> 00:22:02,067
So you better be, communicative with them.

354
00:22:03,277 --> 00:22:05,797
then you have to prioritize
what to fix first.

355
00:22:06,077 --> 00:22:07,337
It's also very important.

356
00:22:07,747 --> 00:22:12,417
Identify a critical service, identify
which service, you can sacrifice,

357
00:22:12,777 --> 00:22:17,257
maybe not completely, but just to put
it in the second stage or whatever,

358
00:22:17,787 --> 00:22:19,967
and determine the root cause.

359
00:22:19,977 --> 00:22:19,997
yeah.

360
00:22:21,497 --> 00:22:26,107
Again, everything is important here, so
determine the root cause as well, and

361
00:22:26,117 --> 00:22:28,297
develop an action plan, and follow it.

362
00:22:28,787 --> 00:22:32,097
These three rules to what to do
with this sounds very simple, but

363
00:22:32,147 --> 00:22:35,377
usually it's very difficult to
do, but still, this is important.

364
00:22:35,417 --> 00:22:37,547
Develop action plan and
follow it all the time.

365
00:22:37,557 --> 00:22:41,747
So if, different people from a team start
to do different things, not according

366
00:22:41,747 --> 00:22:44,127
to plan, things could get even worse.

367
00:22:46,027 --> 00:22:51,527
Communication, everyone, who are, working,
not working affected by the incident

368
00:22:51,537 --> 00:22:53,777
internally, maybe sometimes externally.

369
00:22:53,837 --> 00:22:58,007
And external clients, whatever
should know what is going on.

370
00:22:58,327 --> 00:22:59,627
They don't need technical details.

371
00:22:59,897 --> 00:23:04,207
Usually what they want to know is
like the, when you will fix it and,

372
00:23:04,217 --> 00:23:09,827
what are the, could we run something,
even if these things are failing,

373
00:23:09,867 --> 00:23:11,677
or could we do something with that?

374
00:23:11,677 --> 00:23:12,487
Something like that.

375
00:23:12,487 --> 00:23:17,117
So please establish a communication
protocol and do regular updates

376
00:23:17,417 --> 00:23:18,467
with the current status.

377
00:23:18,767 --> 00:23:21,097
That's incident manager is for.

378
00:23:21,097 --> 00:23:25,127
Sometimes there is an alternative role
as a journalist, but it's for very

379
00:23:25,127 --> 00:23:27,097
big companies, for very big incidents.

380
00:23:27,437 --> 00:23:29,837
Usually incident manager
is pretty fine with that.

381
00:23:30,312 --> 00:23:34,662
inform all affected parties, and,
maintain open lines within the team.

382
00:23:34,672 --> 00:23:38,472
First of all, you need to
share the findings somehow,

383
00:23:38,472 --> 00:23:39,762
internal chat, something.

384
00:23:40,152 --> 00:23:42,932
Second, don't be afraid to ask for help.

385
00:23:42,972 --> 00:23:46,462
Don't sit inside of the problem alone.

386
00:23:47,487 --> 00:23:50,507
Just share it with people
and let them help you.

387
00:23:50,567 --> 00:23:55,007
Sometimes you never know, where it can
get help from, but there are a lot of

388
00:23:55,007 --> 00:23:59,897
small, smart people around and they have
their own experience and they can get, the

389
00:23:59,897 --> 00:24:05,922
idea, or sometimes they can like, just the
Hence to help whatever, and stay positive.

390
00:24:06,122 --> 00:24:06,412
Yeah.

391
00:24:06,462 --> 00:24:08,262
that's, like it's hard, I know.

392
00:24:08,352 --> 00:24:12,362
but if you're not calm, if you're
not positive, it just will take, you

393
00:24:12,362 --> 00:24:16,342
will be less effective and it will
take longer time, to fix the issue.

394
00:24:18,277 --> 00:24:22,207
You have to talk about common pitfalls
which can happen during the incident.

395
00:24:22,217 --> 00:24:24,817
Number one is not rushing without a plan.

396
00:24:25,367 --> 00:24:31,517
It will just add more chaos
to your incident situation.

397
00:24:31,887 --> 00:24:34,867
The second one is poor communication.

398
00:24:35,542 --> 00:24:38,022
Either within your team
or with the stakeholders.

399
00:24:38,052 --> 00:24:40,802
Poor communication within the
team will add more stress,

400
00:24:40,802 --> 00:24:42,452
will make you less effective.

401
00:24:43,302 --> 00:24:46,952
Poor communication with the stakeholders,
not giving them information about

402
00:24:46,962 --> 00:24:54,872
the current state, will probably make
them disappointed in you as a service

403
00:24:54,882 --> 00:24:56,872
provider or employee or whatever.

404
00:24:57,882 --> 00:25:00,462
Number three is ignoring
protocols and procedures.

405
00:25:00,522 --> 00:25:02,142
Please don't do that.

406
00:25:02,492 --> 00:25:05,852
Several protocols and procedures
were, introduced, not just for

407
00:25:05,852 --> 00:25:10,862
fun, but for security or legal
reasons and, like avoiding them

408
00:25:11,092 --> 00:25:13,222
could make things even worse.

409
00:25:14,052 --> 00:25:18,872
the fourth one, overlooking the root
cause, if you missed with that, you

410
00:25:18,952 --> 00:25:23,802
can heal some kind of symptoms and
the situation can reappear very soon.

411
00:25:24,002 --> 00:25:28,322
look deeper and the final, but
also very important thing is

412
00:25:28,322 --> 00:25:29,842
neglecting team well being.

413
00:25:29,872 --> 00:25:36,562
And what I mean by that is that if the
incident is quite long, like it could last

414
00:25:36,622 --> 00:25:43,602
hours or sometimes days, the team could be
overstressed, and, like prone to mistakes.

415
00:25:44,132 --> 00:25:45,102
so if you.

416
00:25:45,322 --> 00:25:50,812
can introduce kind of rotation within
the team or something like this,

417
00:25:50,822 --> 00:25:53,002
it could help think about that.

418
00:25:54,532 --> 00:25:56,542
and finally, let's recap.

419
00:25:56,942 --> 00:26:01,462
so first of all, during the
incident, stay calm and positive.

420
00:26:02,032 --> 00:26:04,952
It's the most important rule,
as I said, please do that.

421
00:26:05,582 --> 00:26:08,932
second, effective communication is
critical within the team with the

422
00:26:08,932 --> 00:26:14,122
stakeholders, both very important with
the clients, of course, teamwork matters.

423
00:26:14,442 --> 00:26:16,242
Don't be afraid to ask for help.

424
00:26:16,722 --> 00:26:20,792
Always share all your findings,
just work as a team, work together.

425
00:26:20,842 --> 00:26:25,792
You will solve this story, this
incident faster and most efficient.

426
00:26:26,372 --> 00:26:27,742
Prioritize actions.

427
00:26:28,102 --> 00:26:28,762
you should.

428
00:26:29,142 --> 00:26:31,592
Take the approach the
problem step by step.

429
00:26:31,702 --> 00:26:35,742
Take the most critical, step,
make it, then next one, and so on.

430
00:26:36,282 --> 00:26:40,182
And avoid the pitfalls we
described in the previous slide.

431
00:26:41,532 --> 00:26:46,372
When the incident is over, you still
need several things to be done.

432
00:26:47,647 --> 00:26:51,907
First of all, analyze what went
wrong, and by that I mean that you

433
00:26:51,907 --> 00:26:56,217
need to have a post incident review
procedure and root cause analysis.

434
00:26:56,547 --> 00:27:01,717
You need to clearly understand why the
incident happened, what was done well

435
00:27:01,727 --> 00:27:03,967
during it, and what could be improved.

436
00:27:04,077 --> 00:27:05,347
These three things.

437
00:27:05,737 --> 00:27:09,387
Then create a feedback loop, and
by that You should update all

438
00:27:09,387 --> 00:27:13,507
the runbooks of the systems that
were involved in the incident.

439
00:27:13,967 --> 00:27:17,787
They could be outdated, they could be
wrong, they could be just not present.

440
00:27:17,997 --> 00:27:22,247
Because if you don't do this, you are
waiting for the history to repeat itself.

441
00:27:22,947 --> 00:27:28,717
And finally, Every network incident
is, troublemaking, at least.

442
00:27:28,797 --> 00:27:30,327
It could be very expensive, finally.

443
00:27:31,147 --> 00:27:34,877
But it's an invaluable source
of the learning experience.

444
00:27:35,017 --> 00:27:38,827
Please use it to strengthen your
system, your infrastructure.

445
00:27:39,207 --> 00:27:40,827
to train your team, finally.

446
00:27:41,487 --> 00:27:43,977
And, stay calm, be positive about that.

447
00:27:43,997 --> 00:27:46,222
It's already happened to
you, you already planned it.

448
00:27:46,822 --> 00:27:48,122
pass through the incident.

449
00:27:49,052 --> 00:27:54,112
like I hope network incidents will
never happen to your company, but if

450
00:27:54,122 --> 00:27:57,132
they will, you are now ready for them.

451
00:27:57,492 --> 00:27:58,952
And thank you for your attention.

452
00:27:59,092 --> 00:28:00,372
It was Con 42.

453
00:28:00,472 --> 00:28:02,112
If you have any questions,
you are welcome.

