1
00:00:02,240 --> 00:00:18,289
Today, I'm going to talk about
building efficient and frugal image

2
00:00:18,350 --> 00:00:23,829
data processing pipelines, which is
based on my experience in Amazon.

3
00:00:26,535 --> 00:00:28,265
And this is the agenda.

4
00:00:28,535 --> 00:00:32,205
we will first overview the image
processing use cases in the

5
00:00:32,205 --> 00:00:34,815
large, in the large industry.

6
00:00:35,225 --> 00:00:40,454
the most challenges that we face,
the current state of art solution and

7
00:00:40,465 --> 00:00:42,864
the future invasion we have it for.

8
00:00:43,864 --> 00:00:50,354
And here there are so many examples of
the application of image data, medical

9
00:00:50,384 --> 00:00:55,924
imaging, object recognition for vehicles,
product image and recommendation

10
00:00:55,934 --> 00:00:57,944
for social media and e commerce.

11
00:00:58,454 --> 00:01:03,385
and recently due to AI, its
importance continues to grow, for

12
00:01:03,395 --> 00:01:05,625
multimodal AI training specifically.

13
00:01:07,595 --> 00:01:11,385
most of the challenges come
from large image data sites.

14
00:01:11,745 --> 00:01:17,415
the high storage cost, for storing
vast amounts of image data and

15
00:01:17,425 --> 00:01:21,765
very intense computation when
processing the, image data.

16
00:01:21,935 --> 00:01:26,715
Image quality and the time it
consumes is very long as well.

17
00:01:26,735 --> 00:01:30,994
Sometimes it can become a bottom leg
of the infrastructure for any real

18
00:01:30,995 --> 00:01:33,465
time and time sensitive applications.

19
00:01:33,945 --> 00:01:39,604
and network bandwidth, transforming
those data over network is very

20
00:01:39,604 --> 00:01:41,424
expensive and costly as well.

21
00:01:42,464 --> 00:01:46,514
And most of the time we are
very limited on those resources.

22
00:01:47,514 --> 00:01:52,244
And the goal of this efficient and
frugal pipeline is very important,

23
00:01:53,384 --> 00:02:00,034
such as we're able to minimize the
processing time and optimize the storage

24
00:02:00,054 --> 00:02:06,189
costs and utilization, to have all
Better resource allocation throughput.

25
00:02:07,189 --> 00:02:10,819
The first important thing is
choosing the right image formats.

26
00:02:11,339 --> 00:02:15,989
there are several popular image formats
to consider, but each with its own

27
00:02:16,019 --> 00:02:22,339
advantages and trade off, mostly in
terms of compression and image quality.

28
00:02:22,779 --> 00:02:23,549
there's there.

29
00:02:23,549 --> 00:02:28,549
Here are some commonly used image
formats, like such as, JPEG is the

30
00:02:28,549 --> 00:02:33,449
popular choice for photographers and
the image with complex color gradients.

31
00:02:34,019 --> 00:02:37,609
it offers a good balance between
file size and image quality.

32
00:02:38,059 --> 00:02:45,009
whereas PNG is more versatile
and lossless, suitable for images

33
00:02:45,009 --> 00:02:47,989
with sharp edges and transparency.

34
00:02:48,989 --> 00:02:53,789
And I feel like I would like to go
more details about this popular choice.

35
00:02:53,919 --> 00:02:58,639
JPEG, which we'll be using a lot in
our image processing infrastructure

36
00:03:00,229 --> 00:03:06,039
is known for joined photographic,
photographic experts group, which

37
00:03:06,209 --> 00:03:07,869
actually I'm new to this as well.

38
00:03:08,339 --> 00:03:14,859
it's widely used lossy compression
format for photographers and, As I said,

39
00:03:14,859 --> 00:03:19,519
it has the good balance between the
size and the quality, and also supports

40
00:03:19,599 --> 00:03:25,219
adjustable compression level, allowing
for fine grained control over the tradeoff

41
00:03:25,249 --> 00:03:27,249
between file size and image quality.

42
00:03:28,009 --> 00:03:31,319
it's widely used in web
browsers and editing software.

43
00:03:33,609 --> 00:03:40,569
And the PNG, also new to me, Portable
Network Graphics, that is, unlike

44
00:03:40,599 --> 00:03:45,619
JPEG, PNG preserves all image
data, ensuring that no quality is

45
00:03:45,619 --> 00:03:48,199
lost even after multiple edits.

46
00:03:49,269 --> 00:03:55,229
It supports a wider range of colors and
transparency than JPEG, making it very

47
00:03:55,229 --> 00:03:57,799
ideal for logos, icons, and graphics.

48
00:03:59,609 --> 00:04:02,999
And there is some breakdown of
the next generation image formats.

49
00:04:03,399 --> 00:04:09,529
Yeah, I think, for example,
WebP, that's all for both

50
00:04:09,529 --> 00:04:11,659
lossy and lossless compression.

51
00:04:12,129 --> 00:04:17,659
H, like HIC, that was high efficiency
video coding for compression.

52
00:04:18,209 --> 00:04:18,819
and AVIF.

53
00:04:20,184 --> 00:04:23,764
that's based on the AV1 video codec.

54
00:04:24,234 --> 00:04:28,604
it's very promising, even better
than compression that WebP and HIC.

55
00:04:29,604 --> 00:04:33,364
in the realm of image compression,
there's a fundamental trade off

56
00:04:33,374 --> 00:04:35,404
between file size and image quality.

57
00:04:35,814 --> 00:04:41,594
mostly, Like for the low C compression,
it can achieve higher compression ratios.

58
00:04:41,894 --> 00:04:45,524
and it's achieved by selectively
discarding some image data.

59
00:04:46,124 --> 00:04:50,394
It's mostly suitable for application
where minor quality loss is acceptable,

60
00:04:50,854 --> 00:04:56,909
for lossless compression that will come,
that will pr Preserve all image data, but

61
00:04:56,949 --> 00:05:04,079
resolve in larger size is mostly ideal for
application where image fidelity is very

62
00:05:04,119 --> 00:05:07,709
critical, such as medical and archive.

63
00:05:07,709 --> 00:05:10,024
Sometimes, there are.

64
00:05:10,194 --> 00:05:14,974
Yeah, there's a bunch of recommended
compression libraries and tools provide

65
00:05:15,534 --> 00:05:22,704
pre built functionality for encoding and
decoding various image formats that enable

66
00:05:22,794 --> 00:05:27,704
efficient compression and decompression
of image with our application,

67
00:05:29,644 --> 00:05:34,254
mostly including the algorithm and
hardware acceleration capabilities,

68
00:05:36,494 --> 00:05:39,164
which can have an improvement
of the performance.

69
00:05:40,224 --> 00:05:47,474
And this also isolated to the logical
image handling process, which can lead

70
00:05:47,514 --> 00:05:51,584
us to focus on our business use case
when dealing with image processing

71
00:05:52,994 --> 00:05:59,654
here is a way to choose the right image
formats using adaptive compression.

72
00:06:00,774 --> 00:06:01,254
that.

73
00:06:02,804 --> 00:06:07,454
The adaptive compression that can
optimize image storage and the

74
00:06:07,464 --> 00:06:11,404
bandwidth by dynamically adjusting
compression levels based on the

75
00:06:11,694 --> 00:06:13,734
contact and the quality requirements.

76
00:06:15,304 --> 00:06:19,664
Yeah, this is the benefits can
provide us good, better user

77
00:06:19,684 --> 00:06:21,334
experience and cost saving.

78
00:06:22,674 --> 00:06:24,224
And this is how it works.

79
00:06:24,684 --> 00:06:31,104
It will first do the analysis of the
content to identify areas with varying

80
00:06:31,104 --> 00:06:33,334
levels of detail and the complexity.

81
00:06:34,764 --> 00:06:40,314
Based on that, it will have different
compression level, such as smooth

82
00:06:40,344 --> 00:06:44,994
background can be compressed more,
while faces and edges need to compress

83
00:06:45,254 --> 00:06:48,014
less to preserve the visual quality.

84
00:06:49,414 --> 00:06:54,834
And then, then the last is final
encoded image, to a smaller size will

85
00:06:54,834 --> 00:06:57,054
maintain the acceptable, the quality

86
00:06:58,054 --> 00:07:04,374
and the, when it came to infrastructure
is very popular to leveraging cloud

87
00:07:04,794 --> 00:07:11,404
native services for cloud storage
and other, and other system, system

88
00:07:11,404 --> 00:07:13,404
handling that I will go over later.

89
00:07:15,149 --> 00:07:20,739
yeah, cloud computing offers, significant,
offers lots of service that will enhance

90
00:07:20,759 --> 00:07:25,529
the efficiency and the cost effective
of the image processing pipelines, and

91
00:07:25,739 --> 00:07:30,199
these are the popular choices, it will
offer the benefits of, scalability

92
00:07:31,339 --> 00:07:37,179
to Durability, cost effectiveness,
and accessibility, and the image data

93
00:07:37,219 --> 00:07:43,109
can be assessed from anywhere with an
internet connection, facilitating the

94
00:07:43,349 --> 00:07:45,859
collaboration and remote processing.

95
00:07:46,859 --> 00:07:51,759
and we are also using a lot of serverless
computing for on demand image processing.

96
00:07:52,129 --> 00:07:55,579
the popular choice is AWS Lambda.

97
00:07:55,579 --> 00:07:58,509
Yeah, we'll be using AWS Lambda mostly.

98
00:07:59,364 --> 00:08:03,054
and, this is more like we can run
code in response to events without

99
00:08:03,754 --> 00:08:04,924
provisioning and managing servers.

100
00:08:05,724 --> 00:08:10,924
and also these on demand execution
model is good for image processing

101
00:08:10,924 --> 00:08:17,004
such as resizing, resizing, thumbnail
generation, and format conversion.

102
00:08:18,004 --> 00:08:23,354
There are also cloud based image
processing APIs that is very handy, such

103
00:08:23,354 --> 00:08:28,704
as, yeah, we'll be using recognition a
lot, but, comparably Google and Azure

104
00:08:28,704 --> 00:08:31,774
also offer a similar service like that.

105
00:08:32,584 --> 00:08:37,684
and, it helps us with the object
detection, image classification, facial

106
00:08:37,684 --> 00:08:41,034
recognition, and contact or moderation.

107
00:08:41,984 --> 00:08:45,914
Yeah, which is save us a lot of effort
building and maintaining our own models.

108
00:08:46,914 --> 00:08:50,754
And here are some strategy
for optimizing cloud costs.

109
00:08:51,314 --> 00:08:57,064
using spot instance that take advantage
of unused cloud computing capacity.

110
00:08:58,014 --> 00:09:02,154
having the right size resources,
choosing the appropriate instance

111
00:09:02,154 --> 00:09:07,674
type that can match the workload
to, and to avoid over provisioning.

112
00:09:07,674 --> 00:09:07,704
Okay.

113
00:09:08,619 --> 00:09:14,229
Leverage reserved instance, which is
committed to using specific instance

114
00:09:14,229 --> 00:09:20,639
tabs for longer duration, and we can
get discount pricing of that, and, yeah,

115
00:09:20,639 --> 00:09:26,009
monitor and analyze user pattern so
we can identify where we can optimize.

116
00:09:27,454 --> 00:09:29,974
iteratively, budget alert.

117
00:09:30,064 --> 00:09:36,534
Yeah, definitely having the alert to
notify when the cloud spending is exceed

118
00:09:36,844 --> 00:09:39,374
the limits, which can happen quite often.

119
00:09:40,374 --> 00:09:46,684
Data per and distribute processing and the
parallelism, which is also very helpful.

120
00:09:46,954 --> 00:09:51,334
that is how we divided an image
data into smaller chunks and the

121
00:09:51,354 --> 00:09:56,714
process in parallel to achieve
better throughput and efficiency.

122
00:09:59,204 --> 00:10:03,634
And this approach allows us to
leverage the combined processing power

123
00:10:03,634 --> 00:10:09,294
of multiple machines and definitely
speed up our pipeline in the end.

124
00:10:10,294 --> 00:10:17,194
I can give an example of the parallel
task for image processing, such as, such

125
00:10:17,194 --> 00:10:23,514
as we can break the task into independent
subtasks, Resizing, filtering, feature

126
00:10:23,534 --> 00:10:30,334
extraction, and you're able to do these
independent steps, simultaneously, Or in

127
00:10:30,334 --> 00:10:36,384
a concurrent manner, And, optional, if
necessary, we can synchronize the result

128
00:10:36,384 --> 00:10:38,514
of the subtax and obtain the final output.

129
00:10:39,514 --> 00:10:43,484
When it comes to distribute processing
and parallelism, there are also

130
00:10:43,484 --> 00:10:49,484
several, known frameworks that, we can
leverage, Apache Spark, the versatile

131
00:10:49,504 --> 00:10:54,194
distributed computing framework well
suited for the big data processing.

132
00:10:54,584 --> 00:10:59,655
It offers high level APIs for working
with large data sets, and, Dusk

133
00:11:00,605 --> 00:11:05,095
is a flexible library for parallel
program, parallel computing in Python.

134
00:11:05,825 --> 00:11:11,235
and Ray, is a general purpose
cluster computing framework that is

135
00:11:11,255 --> 00:11:16,595
good at, it offers a simple API for
parallelizing Python code and supports

136
00:11:16,595 --> 00:11:18,385
various machine learning library.

137
00:11:19,445 --> 00:11:23,875
These frameworks provide the tools
and abstractions to distribute your

138
00:11:23,895 --> 00:11:28,265
image processing workload across
multiple machines and enable you to

139
00:11:28,275 --> 00:11:30,615
process large data sets efficiently.

140
00:11:31,615 --> 00:11:33,455
And next one is containerization

141
00:11:35,705 --> 00:11:37,205
and orchestration.

142
00:11:38,475 --> 00:11:42,895
Yeah, these two technologies can
simplify the deployment and the

143
00:11:42,895 --> 00:11:48,965
management of image processing pipelines,
such as the containerization like

144
00:11:49,005 --> 00:11:54,835
Docker allows you to package your
application and dependency into portable

145
00:11:54,835 --> 00:11:59,025
containers, and these containers can
run consistently across different

146
00:11:59,045 --> 00:12:06,709
environments, ensuring reproductivity,
reproductivity reproducible and

147
00:12:06,819 --> 00:12:13,139
eliminating compatibility issues and
orchestration, such as Kubernetes,

148
00:12:13,519 --> 00:12:19,109
automating the deployment, scaling and the
management of continualized applications.

149
00:12:19,659 --> 00:12:23,849
this will simplify the process of
deploying your image processing pipeline

150
00:12:23,859 --> 00:12:31,399
across multiple machines and ensure that
they run real, real, Reliably, rather

151
00:12:31,399 --> 00:12:33,549
reliable, even in the face of failures.

152
00:12:34,549 --> 00:12:41,469
And continuous optimization and the cost
motor cost monitoring, as is the crucial

153
00:12:41,469 --> 00:12:47,159
part to, help us remain efficient and
budget friendly, performing tools, happy

154
00:12:47,159 --> 00:12:53,039
you to identify performance bottlenecks by
measuring the execution time of different

155
00:12:53,039 --> 00:12:59,610
stage, once we identify any bottom
like, Bottom next, we can apply various

156
00:12:59,640 --> 00:13:06,370
techniques such as algorithm improvements,
using more efficient algorithm or data

157
00:13:06,380 --> 00:13:14,360
structure, hardware acceleration, leverage
GPU or specialized hardware accelerator.

158
00:13:15,155 --> 00:13:18,325
to offload computationally
intensive tasks.

159
00:13:19,325 --> 00:13:24,755
Parameter tuning, which is like
experiment with different parameter

160
00:13:24,755 --> 00:13:28,625
settings to find the optimal
configuration for the workload.

161
00:13:29,625 --> 00:13:36,614
Yeah, and, caching and memorization
are also very valuable to optimizing

162
00:13:36,634 --> 00:13:38,094
the image processing pipelines.

163
00:13:38,644 --> 00:13:43,054
for caching, we will store the
result of expansive operation.

164
00:13:43,604 --> 00:13:48,114
and this is very beneficial for image
transformations and feature expressions,

165
00:13:48,164 --> 00:13:52,834
feature extractions, which is applied
repeatedly to the same images.

166
00:13:53,664 --> 00:13:58,904
Memorization, will cache function
calls result based on input arguments,

167
00:13:59,274 --> 00:14:02,594
providing redundant computations
when a function is called multiple

168
00:14:02,594 --> 00:14:04,544
times with the same inputs.

169
00:14:06,424 --> 00:14:11,954
And, yeah, this is how mostly we, to,
we are doing for the computationally

170
00:14:12,654 --> 00:14:15,504
intensive and frequent repeated
operations for optimization.

171
00:14:16,504 --> 00:14:21,424
The cloud providers also offer a lot
of tools to service to track resources,

172
00:14:21,424 --> 00:14:26,234
usage and associated costs, which
can, which will give us valuable

173
00:14:26,234 --> 00:14:31,584
insights in the financial footprint
and, yeah, there's a lot of them.

174
00:14:31,644 --> 00:14:35,054
And, we mostly using cloud
watch from a double eyes.

175
00:14:36,779 --> 00:14:41,879
yeah, and it help us to gain the
better visibility into our cloud

176
00:14:41,879 --> 00:14:46,749
experience and make informed decision,
most of the time in the important

177
00:14:46,769 --> 00:14:48,379
business leadership meetings,

178
00:14:49,379 --> 00:14:54,689
the future of image processing is
very, bright with lots of, With lots

179
00:14:54,699 --> 00:15:00,219
of advanced, advanced progress in
artificial intelligence and machine

180
00:15:00,219 --> 00:15:06,469
learning, lots of techniques like deep
learning and neural networks can enabling

181
00:15:06,689 --> 00:15:09,769
sophisticated image analysis tasks.

182
00:15:10,259 --> 00:15:13,699
however, the image
processing technologies is.

183
00:15:15,584 --> 00:15:22,574
can cause some ethical implications,
such as privacy, bias, and transparency.

184
00:15:22,904 --> 00:15:26,744
They need to be carefully addressed
to ensure the responsible and

185
00:15:26,794 --> 00:15:28,504
ethical use of image data.

186
00:15:29,214 --> 00:15:34,389
yeah, that's a hard and controversial
topic where to strike the

187
00:15:34,389 --> 00:15:38,219
balance between the innovation
and the ethical considerations.

188
00:15:39,099 --> 00:15:46,169
for now, we will be evaluating the race
based case by case, mostly about who

189
00:15:46,199 --> 00:15:52,929
our, how, who are the customers of our
images, and based on how much benefits

190
00:15:53,019 --> 00:15:57,629
we can bring to them and, the risks that
we're going to take, to make the, to

191
00:15:57,629 --> 00:15:59,709
make the balance and informal choice.

192
00:16:00,709 --> 00:16:04,689
Here is the overview of
my, of my presentation.

193
00:16:04,979 --> 00:16:10,289
First, we go through the image data
use cases and its applications.

194
00:16:10,739 --> 00:16:16,429
and second, we go through what is the main
challenge, with the large image datasets.

195
00:16:17,109 --> 00:16:19,669
And then we go through the
state of the art solution.

196
00:16:20,594 --> 00:16:26,444
Which in the industry, regarding to
having the right formats, compression

197
00:16:26,444 --> 00:16:32,014
techniques, using cloud native
services, distribute processing and

198
00:16:32,084 --> 00:16:35,524
parallelism, container and orchestration.

199
00:16:36,019 --> 00:16:39,109
And how to optimize and monitor the cost.

200
00:16:39,759 --> 00:16:44,469
and then we through more provocative
thoughts about the future applying,

201
00:16:44,899 --> 00:16:49,679
about the image processing, choices
that we need to make in order to achieve

202
00:16:49,719 --> 00:16:52,099
both innovation and safety measure.

203
00:16:53,099 --> 00:16:54,289
so here, so here.

204
00:16:54,289 --> 00:16:54,589
Yeah.

205
00:16:54,589 --> 00:16:58,709
Thank you so much for, participating
this, this presentation.

206
00:16:59,079 --> 00:17:01,039
And, here is my contact detail.

207
00:17:01,499 --> 00:17:05,469
so feel free to contact me for any
questions and the future follow up.

208
00:17:05,519 --> 00:17:11,819
If you want to collaborate more on the
image processing pipeline and, have, hope

209
00:17:11,819 --> 00:17:13,909
you have a good time at the conference.

