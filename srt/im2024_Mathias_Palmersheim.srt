1
00:00:00,299 --> 00:00:00,809
Hey, everyone.

2
00:00:00,810 --> 00:00:04,250
My name is Matthias Palmersheim, and today
I want to talk to you about how to monitor

3
00:00:04,250 --> 00:00:06,240
your monitoring and why it's important.

4
00:00:07,570 --> 00:00:08,560
A little bit about me.

5
00:00:08,560 --> 00:00:12,129
I'm a solutions engineer at Victoria
Metrics, and Victoria Metrics

6
00:00:12,180 --> 00:00:15,839
makes an amazing time series
database called Victoria Metrics.

7
00:00:16,149 --> 00:00:19,999
We also make some utilities for getting
your data not only into Victoria Metrics,

8
00:00:20,049 --> 00:00:22,329
but other time series databases as well.

9
00:00:22,669 --> 00:00:27,049
We are also starting to make a tool
called Victoria Logs for aggregating logs.

10
00:00:27,634 --> 00:00:31,455
And when I'm not working on that at my
day job, I like to take a lot of the

11
00:00:31,455 --> 00:00:36,105
utilities we make as well as some others
and aggregate it together into an easy

12
00:00:36,105 --> 00:00:38,215
to deploy monitoring and logging system.

13
00:00:38,215 --> 00:00:43,085
So that way you don't have to have a giant
Kubernetes cluster or an expensive cloud

14
00:00:43,085 --> 00:00:45,504
service to get started with observability.

15
00:00:46,724 --> 00:00:50,185
So if you couldn't tell by that
intro, I'm a huge observability nerd.

16
00:00:50,585 --> 00:00:53,925
And Actually, on my first
date with my wife, I wouldn't

17
00:00:53,925 --> 00:00:55,265
shut up about observability.

18
00:00:55,265 --> 00:00:59,505
And there was a betting pool in
the office to see if I would talk

19
00:00:59,514 --> 00:01:01,005
about observability the whole time.

20
00:01:02,215 --> 00:01:06,654
And the reason why I became obsessed with
observability is one of my first IT jobs.

21
00:01:06,655 --> 00:01:09,515
It was a IT manager at a factory.

22
00:01:09,544 --> 00:01:14,235
And when I got there, I didn't really
have any monitoring system, so to speak.

23
00:01:14,665 --> 00:01:18,695
And what would happen is an application
would go down because I didn't have

24
00:01:18,695 --> 00:01:20,775
a monitoring system to help me.

25
00:01:21,265 --> 00:01:24,505
Prevent that from happening in the first
place and get alerts for things like,

26
00:01:24,785 --> 00:01:28,275
Hey, hard drives filling up and the
hard drive would just fill up and an

27
00:01:28,275 --> 00:01:33,175
application would fail and my alerting
system at the time was users coming into

28
00:01:33,175 --> 00:01:37,154
my office and informing me that Hey,
this change that was made recently took

29
00:01:37,154 --> 00:01:40,705
down the application and now the whole
office can't work and that's costing the

30
00:01:40,705 --> 00:01:44,975
company tens of thousands of dollars an
hour and that caused a lot of stress.

31
00:01:44,985 --> 00:01:50,435
So along with not having context as to why
things were broken, I had to deal with the

32
00:01:50,445 --> 00:01:55,105
stress of having somebody there reminding
me how expensive this problem was.

33
00:01:57,005 --> 00:02:00,445
So I could get nice automated
notifications instead of

34
00:02:00,455 --> 00:02:02,025
angry users in my office.

35
00:02:02,055 --> 00:02:05,324
I implemented a monitoring system
that collected the telemetry and that

36
00:02:05,324 --> 00:02:10,715
telemetry could tell me that, Hey,
this resource is overused, or this

37
00:02:10,725 --> 00:02:12,209
thing is being slow before it finds me.

38
00:02:12,460 --> 00:02:16,510
failed and although I was angry
that now I had alerts, things

39
00:02:16,510 --> 00:02:17,810
weren't going down as often.

40
00:02:17,830 --> 00:02:23,340
And, when people would come into my
office after they broke, I would usually

41
00:02:23,340 --> 00:02:27,380
already be working on the problem
and that helps lower stress a ton.

42
00:02:30,139 --> 00:02:31,239
but I had a problem though.

43
00:02:31,259 --> 00:02:34,799
A lot of the notifications I was
getting from my monitoring system

44
00:02:34,799 --> 00:02:39,479
were coming through the same channels
as things like status updates or.

45
00:02:39,834 --> 00:02:43,204
people, just like general
announcements, Oh, Hey, you need to

46
00:02:43,204 --> 00:02:44,784
park in the other parking lot today.

47
00:02:45,204 --> 00:02:48,355
And so that led to a lot of alert
fatigue because every time I got

48
00:02:48,355 --> 00:02:54,554
an email, I got the like stress of,
is this just a simple update that

49
00:02:54,554 --> 00:02:55,905
I needed to park somewhere else?

50
00:02:55,924 --> 00:03:00,284
Or is this a tens of thousands
of dollar an hour problem?

51
00:03:00,624 --> 00:03:05,005
So I looked into noisy
notification systems that were

52
00:03:05,005 --> 00:03:06,464
dedicated for critical alerts.

53
00:03:06,464 --> 00:03:08,374
So it was super easy to set up.

54
00:03:08,949 --> 00:03:10,619
D and D override in my phone.

55
00:03:11,629 --> 00:03:15,169
And the other nice things about
this is that it would make

56
00:03:15,169 --> 00:03:16,829
an individual responsible.

57
00:03:16,999 --> 00:03:21,089
So instead of just shouting out, Hey, the
system's broken, somebody should fix this.

58
00:03:21,129 --> 00:03:23,949
It would assign somebody to the, incident.

59
00:03:25,119 --> 00:03:28,089
And the way I decide if
something is an important.

60
00:03:28,859 --> 00:03:32,979
notification and it should be able
to override like the notification

61
00:03:32,989 --> 00:03:37,469
prefaces on my phone is that if it
costs life, liberty or property.

62
00:03:37,559 --> 00:03:43,629
So if it could cause physical harm, if the
system is down, if it causes compliance

63
00:03:43,629 --> 00:03:47,859
issues and the government's going to
get involved, or if it can cause loss

64
00:03:47,859 --> 00:03:51,889
to property and property could just mean
that it reaches a certain cost threshold

65
00:03:51,889 --> 00:03:56,329
and it's losing the company more money
than you would like for an outage.

66
00:03:56,569 --> 00:04:00,779
So I was starting to feel pretty happy
about my existing observability system.

67
00:04:01,149 --> 00:04:05,089
I was able to get noisy notifications when
things were breaking instead of email.

68
00:04:05,809 --> 00:04:07,739
But then I had a philosophical question.

69
00:04:09,109 --> 00:04:12,059
Is what happens if the
monitoring system fails?

70
00:04:12,319 --> 00:04:14,619
So if my monitoring
system fell over at 2 a.

71
00:04:14,619 --> 00:04:16,749
m., does anyone get paged?

72
00:04:17,749 --> 00:04:22,624
So this is the problem that
I was running into where the

73
00:04:22,624 --> 00:04:23,974
monitoring system would fail.

74
00:04:24,014 --> 00:04:28,104
And I would think that everything's
fine, whether or not the applications

75
00:04:28,124 --> 00:04:32,384
behind or that the monitoring
system was responsible for were up.

76
00:04:32,944 --> 00:04:37,954
And to solve this problem, you just
deploy a second monitoring system.

77
00:04:37,984 --> 00:04:41,924
So you set up a monitoring of
monitoring system that only monitors

78
00:04:41,954 --> 00:04:44,254
the primary monitoring system.

79
00:04:44,524 --> 00:04:47,754
And then from the perspective of
the primary monitoring system,

80
00:04:48,574 --> 00:04:51,074
you It's just another application.

81
00:04:51,074 --> 00:04:53,324
So it's just like the ERP system.

82
00:04:53,344 --> 00:04:57,684
It sends metrics and it alerts you
when things aren't going as expected.

83
00:04:58,724 --> 00:04:59,934
There's a problem with this.

84
00:05:00,894 --> 00:05:04,794
The monitoring and monitoring system
is also just another application can

85
00:05:04,794 --> 00:05:08,604
fail and multiple applications can fail
at the same time for whatever reason.

86
00:05:10,274 --> 00:05:12,369
So this problem could be solved.

87
00:05:12,669 --> 00:05:16,969
Just be solved by adding more and more
monitoring systems, but you never quite

88
00:05:16,979 --> 00:05:22,309
get to 100 percent availability you
get more nines but with more nines of

89
00:05:22,359 --> 00:05:27,109
Availability you also get more costs and
it's harder to maintain the knowledge

90
00:05:27,109 --> 00:05:32,339
if you have all those monitoring systems
so usually the sweet spot is two But

91
00:05:32,339 --> 00:05:36,309
you don't just deploy two applications
to the same region and in the same

92
00:05:36,329 --> 00:05:43,399
infrastructure as code inventory You need
to make sure that the applications are

93
00:05:43,399 --> 00:05:47,999
deployed in a way that they're isolated
from failures as much as possible.

94
00:05:48,629 --> 00:05:54,039
And before I get into all the ways that
this can go wrong, I'm not implying that

95
00:05:54,039 --> 00:05:55,249
you and your teammates aren't smart.

96
00:05:55,289 --> 00:05:57,689
I'm saying that humans aren't perfect.

97
00:05:57,729 --> 00:06:02,204
We make mistakes, but Usually
different groups of humans don't

98
00:06:02,204 --> 00:06:03,694
make mistakes at the same time.

99
00:06:03,694 --> 00:06:07,544
So you want to make a different group of
humans as responsible for your monitoring

100
00:06:07,544 --> 00:06:09,374
and monitoring as much as possible.

101
00:06:10,244 --> 00:06:13,704
And another thing you need to do is to
make sure that your change management

102
00:06:13,704 --> 00:06:17,524
processes are aware that these two
systems shouldn't be updated at the same

103
00:06:17,524 --> 00:06:21,884
time because upgrades or configuration
changes are frequent source of outages.

104
00:06:22,424 --> 00:06:25,924
So if you touch both systems, if
you're allowed to touch both systems

105
00:06:25,924 --> 00:06:29,314
at the same time, then there's a high
likelihood both fail at the same time.

106
00:06:31,704 --> 00:06:35,124
So the different tools that we need to
make are the different technical ways that

107
00:06:35,124 --> 00:06:37,034
we can prevent both systems from failing.

108
00:06:38,544 --> 00:06:42,724
are to use different notification
services because again, notifications

109
00:06:42,724 --> 00:06:44,924
are apps and apps fail sometimes.

110
00:06:45,414 --> 00:06:48,264
you want to make sure that they're
in different infrastructure providers

111
00:06:48,294 --> 00:06:53,294
because usually, different infrastructure
providers don't have simultaneous outages.

112
00:06:53,804 --> 00:06:58,224
If you can't get it inside of a different
infrastructure provider or a different

113
00:06:58,224 --> 00:07:02,324
cloud service, at least try to get it
inside of different regions and separate

114
00:07:02,324 --> 00:07:04,394
deployments within the same cloud service.

115
00:07:06,314 --> 00:07:11,164
So in summary, your monitoring and
monitoring is another monitoring

116
00:07:11,164 --> 00:07:15,754
system, but it's only responsible for
the primary monitoring system because

117
00:07:15,844 --> 00:07:20,284
it gets really confusing if you have
different monitoring systems available

118
00:07:20,294 --> 00:07:21,994
for certain business applications.

119
00:07:22,344 --> 00:07:26,174
It can be harder to test the system
too, because not only do you have to

120
00:07:26,184 --> 00:07:31,314
get the okay from your boss to test and
intentionally fail a system, you have to.

121
00:07:33,010 --> 00:07:36,810
Get approval for a test that could
have impacts to other teams as well.

122
00:07:37,620 --> 00:07:42,990
And the monitoring and monitoring
system is just treated by the monitoring

123
00:07:42,990 --> 00:07:44,690
system as another application.

124
00:07:46,820 --> 00:07:49,419
So the minimum requirement is to
do the most important thing in

125
00:07:49,419 --> 00:07:53,710
observability, which is make sure
that your applications are available.

126
00:07:53,989 --> 00:07:57,999
This is going to be cheaper to store
and easier to configure, because in

127
00:07:57,999 --> 00:08:02,239
most cases you're just configuring
a connection between two things or

128
00:08:02,569 --> 00:08:07,010
feeding a list of URLs to some service
and it's performing health checks.

129
00:08:07,530 --> 00:08:11,739
But the downside is that you
don't get the, you don't get a

130
00:08:11,739 --> 00:08:14,989
lot of preventative alerts unless
like the applications being slow.

131
00:08:15,579 --> 00:08:19,949
and so what happens if you don't get
those contextual things is that when

132
00:08:19,949 --> 00:08:23,459
the application goes down, you have
a much higher mean time to resolution

133
00:08:25,329 --> 00:08:29,339
because you have to figure out what went
wrong as well as fix what went wrong.

134
00:08:30,259 --> 00:08:33,649
And again, this is responsible
for keeping the most important

135
00:08:33,649 --> 00:08:35,449
applications in your business online.

136
00:08:35,829 --> 00:08:37,199
So you really should.

137
00:08:38,494 --> 00:08:42,064
treat it like another application and give
it those preventative measures, give it

138
00:08:42,064 --> 00:08:46,274
really nice dashboards, have run books
and all those other things that every

139
00:08:46,274 --> 00:08:48,434
other application in your business gets.

140
00:08:50,144 --> 00:08:53,014
And even if you are treating it
like another application, your

141
00:08:53,014 --> 00:08:56,244
applications should have health
checks that alert you if they fail.

142
00:08:58,029 --> 00:08:59,589
So what are the approaches we can do this?

143
00:08:59,619 --> 00:09:03,389
I'm gonna get into some like quick
and dirty approaches that if you

144
00:09:03,519 --> 00:09:08,029
can't get a full blown Dedicated
monitoring of monitoring system.

145
00:09:08,409 --> 00:09:13,459
It's better than nothing So the first
instance of that is gonna be a heartbeat

146
00:09:14,799 --> 00:09:20,649
so Heartbeat is just one system
communicating with another and if that

147
00:09:20,649 --> 00:09:24,924
communication doesn't happen for a certain
period of time You An alert will fire.

148
00:09:25,324 --> 00:09:30,814
this is, again, it's the simplest up down
that you could get is just communication

149
00:09:30,814 --> 00:09:33,034
between these two systems as it happened.

150
00:09:33,374 --> 00:09:39,734
And the downside is that the heartbeat
is usually like a dedicated health check.

151
00:09:40,594 --> 00:09:46,414
It's not like a, as accurate of a
representation as some of the other

152
00:09:46,424 --> 00:09:49,054
health checks we're going to go
through a little bit in the talk.

153
00:09:49,054 --> 00:09:52,884
So a good example would be
the, ANAG system with Nagios.

154
00:09:54,194 --> 00:09:58,134
And that's just, if the ANAG application
hasn't contacted the server in a

155
00:09:58,134 --> 00:09:59,774
certain amount of time, it will fire.

156
00:09:59,894 --> 00:10:03,394
There's obviously false positive
risks with that because your phone

157
00:10:03,394 --> 00:10:06,774
could be having, issues connecting
to LTE or something like that.

158
00:10:07,874 --> 00:10:12,024
And the other problem is that's just
shouting into an area saying, Hey,

159
00:10:12,104 --> 00:10:16,674
this is broken rather than, doing what
they do in CPR training, which is say.

160
00:10:17,074 --> 00:10:20,814
Hey, somebody calling, like you point
to somebody and say, you call an

161
00:10:20,814 --> 00:10:23,134
ambulance or you get a defibrillator.

162
00:10:23,744 --> 00:10:26,334
Another example of this
is Grafana on call.

163
00:10:26,464 --> 00:10:29,894
If you self host Grafana on call,
you can sign up for a cloud account.

164
00:10:29,914 --> 00:10:32,764
And then if the self hosted
version doesn't talk to the cloud

165
00:10:32,764 --> 00:10:36,954
account for a certain amount of
time, then you get a notification

166
00:10:36,964 --> 00:10:38,144
that there's a missing heartbeat.

167
00:10:38,194 --> 00:10:42,184
But this only covers the notification
system and not everything as a whole.

168
00:10:43,614 --> 00:10:47,274
So this kind of works, but you should
probably look into something better.

169
00:10:47,574 --> 00:10:52,454
if you're using a cloud vendor for your
monitoring, then usually they have a

170
00:10:52,454 --> 00:10:56,174
status page, and hopefully they're not
self hosting it, because if you are

171
00:10:56,174 --> 00:11:01,669
self hosting, your own status page, then
there's a higher likelihood that both

172
00:11:01,679 --> 00:11:04,959
the application and the status page fail
at the same time, because there could be

173
00:11:04,989 --> 00:11:09,239
overlap in the infrastructure overlap in
the humans that cause both the systems to

174
00:11:09,239 --> 00:11:11,719
fail, but they're really easy to set up.

175
00:11:11,809 --> 00:11:17,229
If you go, if you just like search online
for cloud vendors, status page, they'll

176
00:11:17,229 --> 00:11:21,119
give you this information and give
you like point and click instructions

177
00:11:21,129 --> 00:11:25,439
for getting this into email or Slack,
but the downside is those are, Those

178
00:11:25,439 --> 00:11:27,379
aren't noisy notification system.

179
00:11:27,629 --> 00:11:31,409
People commonly will mute, will mute
things and it's really tricky to get the

180
00:11:31,409 --> 00:11:35,879
settings just right to where your status
update, or people asking for status

181
00:11:35,879 --> 00:11:39,609
updates don't bother you after hours,
but, Cloud Render going down would.

182
00:11:41,009 --> 00:11:44,639
And this usually doesn't work
for self hosting solutions.

183
00:11:46,024 --> 00:11:50,584
A bit of a better version of this is
going to be health check services.

184
00:11:50,594 --> 00:11:54,614
So if you're self hosting, you could
self host something like uptime Kuma, but

185
00:11:54,674 --> 00:11:58,314
getting another team to host this inside
of your organization is going to be tricky

186
00:11:58,324 --> 00:11:59,944
because you have to convince people.

187
00:12:00,329 --> 00:12:04,079
that, hey, I know I'm on the observability
team, but this team that isn't the

188
00:12:04,079 --> 00:12:07,829
observability team should manage the
service that does observability things.

189
00:12:08,509 --> 00:12:11,349
there's also cloud options for this,
but again, if you're self hosting,

190
00:12:11,349 --> 00:12:15,589
this can be really tricky because you
either have to allow access to your

191
00:12:15,589 --> 00:12:20,469
monitoring application, which can be a
security risk and getting the security

192
00:12:20,469 --> 00:12:24,369
team on board with this could be a
problem, or you have to, manage an

193
00:12:24,379 --> 00:12:26,189
agent inside of your infrastructure.

194
00:12:26,934 --> 00:12:29,664
to, beacon out information
to that service as well.

195
00:12:30,964 --> 00:12:33,684
But these do hook into
noisy notifications.

196
00:12:34,154 --> 00:12:36,814
and they do require a bit of
extra configuration because it's

197
00:12:36,814 --> 00:12:39,954
not just subscribing to a feed
or setting up a simple heartbeat.

198
00:12:39,964 --> 00:12:44,114
You do have to give a list of URLs and
if possible, you should give the correct

199
00:12:44,114 --> 00:12:49,914
responses because sometimes you get a 200
HTTP code and a valid SSL certificate,

200
00:12:50,224 --> 00:12:55,464
but on that, On that web page or in the
response, you say, Hey, even though I'm

201
00:12:55,464 --> 00:12:58,104
serving HTTP traffic, I'm not healthy.

202
00:12:58,454 --> 00:13:03,604
So if you can configure look for a string
in a response, definitely set that up.

203
00:13:04,104 --> 00:13:08,594
And again, all of these are minimal
context to it's just a simple is the thing

204
00:13:08,594 --> 00:13:11,014
working or not and maybe a response time.

205
00:13:13,944 --> 00:13:15,994
So the first system
that I would recommend.

206
00:13:16,399 --> 00:13:20,379
that I would say is an adequate monitoring
of monitoring system is to deploy

207
00:13:20,379 --> 00:13:22,449
two independent monitoring systems.

208
00:13:22,859 --> 00:13:27,679
this has the widest range of quality, so
you could do it the lazy way and deploy

209
00:13:27,679 --> 00:13:31,339
a smaller version of the exact same
tooling with the exact same version in

210
00:13:31,339 --> 00:13:35,999
the same infrastructure as code inventory
with no change management controls.

211
00:13:36,099 --> 00:13:39,269
And this obviously isn't
the best solution because.

212
00:13:40,644 --> 00:13:44,254
something inside of that region or inside
of that Kubernetes cluster could fail.

213
00:13:44,624 --> 00:13:47,614
you could find out that there was
a regression in an update and take

214
00:13:47,614 --> 00:13:49,244
out both systems at the same time.

215
00:13:49,504 --> 00:13:52,184
If you use the same notification
system and that fails, you,

216
00:13:52,374 --> 00:13:53,644
it's really hard to tell.

217
00:13:53,645 --> 00:13:59,394
So doing all those things right
is also tricky from a bureaucratic

218
00:13:59,414 --> 00:14:02,644
perspective too, because you have to
convince your boss that, hey, we need to

219
00:14:02,644 --> 00:14:04,594
deploy an application in a new region.

220
00:14:05,174 --> 00:14:05,724
Or.

221
00:14:06,449 --> 00:14:12,839
Another thing is you have to justify
of setting up change management

222
00:14:12,839 --> 00:14:16,439
processes, which can be difficult
and it can be hard to get people

223
00:14:16,439 --> 00:14:17,989
to follow the instructions on that.

224
00:14:19,754 --> 00:14:22,664
If you do this approach, you
should definitely test it.

225
00:14:22,674 --> 00:14:25,834
So a way you could test it is
to break your monitoring of

226
00:14:25,844 --> 00:14:29,364
monitoring system and then make
sure that your primary monitoring

227
00:14:29,364 --> 00:14:30,934
system is sending notifications.

228
00:14:30,984 --> 00:14:34,524
And when you do this, make sure
that somebody's periodically just

229
00:14:34,524 --> 00:14:38,094
like refreshing dashboards or some
tests to make sure that the primary

230
00:14:38,094 --> 00:14:40,074
monitoring system is working as well.

231
00:14:40,334 --> 00:14:43,434
So that way you can figure out
if there's any interdependencies

232
00:14:43,474 --> 00:14:44,784
between the two systems.

233
00:14:47,019 --> 00:14:50,839
another thing you can do if you're self
hosting is purchase a cloud service.

234
00:14:50,839 --> 00:14:53,209
And if you're approaching your cloud
service, you can either purchase a

235
00:14:53,209 --> 00:14:57,549
different cloud service that monitors
the first cloud service, or you

236
00:14:57,549 --> 00:15:01,689
could set up an on prem system that
monitor monitors the cloud service.

237
00:15:02,099 --> 00:15:05,439
the upside of this is now you're
definitely having different humans

238
00:15:05,439 --> 00:15:09,829
manage the systems, and those humans are
probably using different upstream vendors.

239
00:15:09,899 --> 00:15:13,289
a lot of the time you can control
where your cloud service is deployed.

240
00:15:14,109 --> 00:15:16,689
And so that way they're
geographically isolated.

241
00:15:17,884 --> 00:15:22,074
and I know cloud monitoring bills have
a bad rap for being super expensive,

242
00:15:22,154 --> 00:15:25,164
but because it's one application,
it's not that big of a deal.

243
00:15:25,164 --> 00:15:29,424
And that application is managed
usually by the observity observability

244
00:15:29,874 --> 00:15:31,764
experts in your organization.

245
00:15:31,774 --> 00:15:34,904
So they're aware of things that
can lower costs like react.

246
00:15:35,199 --> 00:15:37,409
relabeling rules or streaming aggregation.

247
00:15:38,169 --> 00:15:41,249
And this one, you can
still misconfigure it.

248
00:15:41,299 --> 00:15:44,939
but it's lower because it's a
shared responsibility model rather

249
00:15:44,939 --> 00:15:46,699
than a, you're responsible model.

250
00:15:47,239 --> 00:15:51,119
And the best version of this
is to purchase a dedicated

251
00:15:51,129 --> 00:15:52,819
monitoring of monitoring solution.

252
00:15:52,819 --> 00:15:57,579
So Victoria metrics offers this as an
part of some of our enterprise plans.

253
00:15:58,129 --> 00:16:02,209
But the downside of it is it's the most
expensive because along with paying

254
00:16:02,209 --> 00:16:05,594
for a separate monitoring system.

255
00:16:05,594 --> 00:16:10,734
So a separate instance of Victoria metrics
to monitor your on prem Victoria metrics.

256
00:16:10,774 --> 00:16:18,174
You're paying for the really smart humans
to enrich the already rich notifications.

257
00:16:19,654 --> 00:16:22,484
But again, that's going to
be the best experience by far

258
00:16:22,484 --> 00:16:27,759
because along with a really well
supported observability system.

259
00:16:27,779 --> 00:16:31,169
You get the smart people behind
it that can help you, work

260
00:16:31,169 --> 00:16:32,309
through the issues as well.

261
00:16:34,479 --> 00:16:37,259
So in summary, this doesn't
have to be super expensive.

262
00:16:37,289 --> 00:16:39,359
It doesn't have to be super difficult.

263
00:16:39,619 --> 00:16:44,469
And if you can't get like permission
to deploy a separate monitoring and

264
00:16:44,479 --> 00:16:49,484
monitoring system, there's options
that are better than nothing.

265
00:16:49,854 --> 00:16:53,534
And you can mix and match the
approaches to fit your availability

266
00:16:53,534 --> 00:16:55,554
requirements or fit your use case.

267
00:16:55,854 --> 00:17:00,194
And the most important thing is we get to
answer the age old philosophical question.

268
00:17:00,224 --> 00:17:02,984
If your monitoring system falls
over in the middle of the night,

269
00:17:03,054 --> 00:17:04,664
does somebody get paged at 2 AM?

270
00:17:05,274 --> 00:17:07,304
After this talk, the answer should be yes.

