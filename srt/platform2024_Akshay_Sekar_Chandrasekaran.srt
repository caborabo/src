1
00:00:14,029 --> 00:00:18,470
So this is the agenda for the secure
AI model sharing and deployment.

2
00:00:18,990 --> 00:00:23,889
First we're going to look at the usage
of Gen AI and platform engineering.

3
00:00:24,279 --> 00:00:28,790
Next we're going to have a look at
different security risks and the Gen

4
00:00:28,790 --> 00:00:33,910
AI After which we will discuss the
different security techniques for

5
00:00:33,910 --> 00:00:39,330
different GNI models, and what the
best practices for sharing GNI models

6
00:00:39,399 --> 00:00:41,400
internally and externally would be.

7
00:00:42,010 --> 00:00:47,870
We would then go ahead with understanding
secure GNI deployment strategies, which

8
00:00:47,880 --> 00:00:53,240
essentially relates to how models,
GNI models can be deployed securely,

9
00:00:53,270 --> 00:00:55,510
efficiently in a corporate environment.

10
00:00:56,060 --> 00:01:01,449
Next, we will look something specific
to security with respect to GNA models,

11
00:01:01,450 --> 00:01:06,800
which essentially relates to trusted
execution environments, which generally

12
00:01:06,800 --> 00:01:09,640
means how, isolated processes can be run.

13
00:01:10,170 --> 00:01:14,780
And we would then go ahead with
discussing the automated security

14
00:01:14,789 --> 00:01:19,650
in GNA operations, which covers the
different security techniques that

15
00:01:19,660 --> 00:01:21,940
can be applied in GNA operations.

16
00:01:22,300 --> 00:01:25,169
Finally, we will conclude
by looking at a case study.

17
00:01:25,505 --> 00:01:30,935
of how an organization can use
these tactics to, apply Gen AI to

18
00:01:30,955 --> 00:01:34,915
their corporate environment and
the different trends of, security,

19
00:01:34,975 --> 00:01:37,115
Gen AI security as a whole.

20
00:01:37,665 --> 00:01:42,594
So coming to Gen AI in platform
engineering, so Gen AI, Gen AI

21
00:01:42,624 --> 00:01:47,184
as it relates to, it's a field
of, artificial intelligence that

22
00:01:47,184 --> 00:01:48,824
is able to generate answers.

23
00:01:49,234 --> 00:01:53,224
So it's able to generate
images, text, videos, et cetera.

24
00:01:53,534 --> 00:01:58,084
So over the past one to two
years, Gen AI application scope

25
00:01:58,194 --> 00:02:00,244
has been booming pretty much.

26
00:02:00,844 --> 00:02:06,194
It has been able to generate codes, system
configurations, as well as look at large

27
00:02:06,194 --> 00:02:11,414
chunk of logs and emit that information,
which makes it pretty much easier for

28
00:02:11,544 --> 00:02:16,004
the person at the user end to look at
the logs and just summarize the logs.

29
00:02:16,594 --> 00:02:20,754
but the main security risks here
are that Gen AI inherits a lot of

30
00:02:20,794 --> 00:02:23,114
the, security risks in AI models.

31
00:02:23,604 --> 00:02:25,864
So essentially it's inheriting the bias.

32
00:02:26,264 --> 00:02:30,474
Essentially it's also inheriting the
privacy related challenges as well as

33
00:02:30,844 --> 00:02:34,814
the hallucinations and explainability
based, which is based on the data,

34
00:02:34,844 --> 00:02:36,584
the specific AI model is trained.

35
00:02:37,064 --> 00:02:40,174
This is a specific reason why
security is such an important

36
00:02:40,174 --> 00:02:41,684
aspect of Gen AI as a whole.

37
00:02:42,374 --> 00:02:46,094
security is an important aspect because
it can be used to protect the intellectual

38
00:02:46,354 --> 00:02:50,664
property of enterprise organizations,
basically protecting them against a

39
00:02:50,664 --> 00:02:55,584
different kind of attacks, as well as
helping continuously monitor these models

40
00:02:56,204 --> 00:03:00,844
which emit the output to make sure the
data that's being feeded in is appropriate

41
00:03:00,884 --> 00:03:02,984
and is not biased or corrupted in any way.

42
00:03:03,364 --> 00:03:06,614
let's discuss the different
security risks in GNI model sharing.

43
00:03:06,614 --> 00:03:06,714
Thank you.

44
00:03:07,204 --> 00:03:10,584
the first risk we're going to be
discussing about is data poisoning attack.

45
00:03:10,814 --> 00:03:13,514
So we're going to, for each of
this attack, we're going to be

46
00:03:13,624 --> 00:03:16,854
looking at the risk detection as
well as the general mitigation.

47
00:03:17,234 --> 00:03:22,154
So data poisoning attack basically relates
to manipulation of the data, maybe by

48
00:03:22,154 --> 00:03:26,384
corruption of the data, potentially
leading to skewing of the outputs.

49
00:03:27,034 --> 00:03:31,234
Maybe it's drifting from the original,
standard way of what the exact output is.

50
00:03:32,084 --> 00:03:37,424
A good example for this as let's say, if
I'm gonna ask Jenny more, what addition of

51
00:03:37,424 --> 00:03:41,984
two numbers, maybe one plus one is instead
of maybe answering two, it's gonna answer

52
00:03:42,014 --> 00:03:45,734
four because the data that it was trained
on is basically skewed and corrupted.

53
00:03:46,204 --> 00:03:50,884
So the way to detect these kind of
attacks is employed data validation at

54
00:03:50,884 --> 00:03:55,024
the starting before training the morning,
as well as provenance checks, which

55
00:03:55,024 --> 00:04:00,394
is basically validation checks to make
sure these kind of outputs are detected.

56
00:04:00,424 --> 00:04:02,984
And, nipping the bud appropriately.

57
00:04:03,334 --> 00:04:08,614
It's very important and crucial to
examine the outputs of the DNA models so

58
00:04:08,614 --> 00:04:13,744
that one can understand if it's emitting
hallucinations or any kind of bias or any

59
00:04:13,744 --> 00:04:16,874
kind of imaginations that can be there.

60
00:04:17,194 --> 00:04:21,394
the mitigation for this kind of attack
would be to try to regularly retrain the

61
00:04:21,404 --> 00:04:25,514
model with a clean set of data, making
sure there's validation checks and

62
00:04:25,514 --> 00:04:29,434
there's no any kind of tampering with
the data, which is an important aspect.

63
00:04:29,994 --> 00:04:32,974
So the integrity of the data
must not be compromised here.

64
00:04:33,404 --> 00:04:37,304
The next thing, attack we're going to look
at is called as model inversion attack.

65
00:04:37,554 --> 00:04:41,444
So in this attack, the, the
attack basically exploits the

66
00:04:41,444 --> 00:04:44,974
model output to infer sensitive
details from training data.

67
00:04:45,204 --> 00:04:47,974
So this actually risks privacy breaches.

68
00:04:48,264 --> 00:04:51,374
An example of this would be
emitting private information or

69
00:04:51,824 --> 00:04:53,674
health care information, which is.

70
00:04:54,794 --> 00:04:56,524
information of a person.

71
00:04:56,844 --> 00:05:00,464
This has the potential to
breach regulations, their

72
00:05:00,465 --> 00:05:04,064
health regulation, regulatory
standards like the HIPAA in the U.

73
00:05:04,064 --> 00:05:04,224
S.

74
00:05:04,224 --> 00:05:09,824
So if organizations don't take care
of this kind of, attack, it could lead

75
00:05:09,824 --> 00:05:13,594
to breach in violations and leading
to reputational damage as well as

76
00:05:13,594 --> 00:05:15,614
financial, big financial charges.

77
00:05:16,144 --> 00:05:20,234
So the way to detect these kind of attacks
would be to implement different privacy

78
00:05:20,234 --> 00:05:22,634
and output scrubbing to add randomness.

79
00:05:22,710 --> 00:05:26,099
to restrict sensitive data leakage.

80
00:05:26,349 --> 00:05:30,339
So essentially it's important to
make sure appropriate policies and

81
00:05:30,339 --> 00:05:34,739
standards are in place before the
models are being trained and after the

82
00:05:34,739 --> 00:05:39,129
models are trained to make sure the
models follow the specific policies.

83
00:05:39,199 --> 00:05:43,359
And basically adding randomness
to the data, making sure that it

84
00:05:43,379 --> 00:05:47,339
does not emit any kind of sensitive
information is of paramount importance.

85
00:05:47,844 --> 00:05:51,294
because once the confidential information
of the specific person is out,

86
00:05:51,304 --> 00:05:54,674
there's nothing one can do to, scrap
that information off the Internet.

87
00:05:55,104 --> 00:05:58,654
the way to mitigate this
would be to enhance security

88
00:05:58,654 --> 00:06:00,324
by strict access controls.

89
00:06:00,604 --> 00:06:05,134
Essentially, giving an audit trail
of people who have created the model

90
00:06:05,134 --> 00:06:10,394
or maybe who have tried to feed
misappropriated input or invalid

91
00:06:10,394 --> 00:06:14,094
input trying to make the model emit
something that it's not supposed to.

92
00:06:14,429 --> 00:06:18,549
looking at this query, it's easy to
understand, what the model, how the

93
00:06:18,559 --> 00:06:22,239
model is trying to behave as well as if
it's trying to emit any sensitive data.

94
00:06:22,809 --> 00:06:27,199
So next let's go to different
security principles that can

95
00:06:27,199 --> 00:06:29,309
be employed in Gen AI models.

96
00:06:29,699 --> 00:06:32,149
So the first principle we're
going to look at is zero trust.

97
00:06:32,614 --> 00:06:37,464
So Zero Trust is a principle that
is based on the model of never

98
00:06:37,484 --> 00:06:39,174
trust anything, always verify.

99
00:06:39,444 --> 00:06:43,034
This essentially means that even if
a request is within a corporate, a

100
00:06:43,034 --> 00:06:48,724
demilitarized zone, which is essentially a
sectioned, zone of the corporate network,

101
00:06:48,945 --> 00:06:50,885
it's not trusted, it's still verified.

102
00:06:51,205 --> 00:06:54,275
So the three methods I'm going
to be discussing here, the first

103
00:06:54,295 --> 00:06:56,165
method is continuous verification.

104
00:06:56,255 --> 00:06:59,155
The second is minimizing insider
threats, and the third is

105
00:06:59,205 --> 00:07:01,075
adaptability to dynamic environments.

106
00:07:01,980 --> 00:07:05,100
Let's go into detail into
each one of these, starting

107
00:07:05,100 --> 00:07:06,460
with continuous verification.

108
00:07:06,710 --> 00:07:10,920
So continuous validation of the
data inputs is essential because the

109
00:07:11,250 --> 00:07:15,180
relationship between the data inputs
and outputs happen at the rapid pace

110
00:07:15,180 --> 00:07:17,789
in GNA models, often in real time.

111
00:07:18,089 --> 00:07:22,299
So zero trust architecture here
ensures that the validation of

112
00:07:22,309 --> 00:07:24,579
data here is constant to make sure.

113
00:07:25,225 --> 00:07:29,495
any unauthorized access or integrity
of the data when, with which the

114
00:07:29,495 --> 00:07:31,375
model is trained is not compromised.

115
00:07:31,705 --> 00:07:35,205
Next, look at, let's look at
minimized, minimized insider threats.

116
00:07:35,695 --> 00:07:40,675
Gen AI systems are also
susceptible to, insider threats.

117
00:07:41,035 --> 00:07:44,160
There might be people who might be
trying to internally corrupt the data

118
00:07:44,160 --> 00:07:48,305
or maybe, leak the, model configurations
or training data of the specific model.

119
00:07:48,565 --> 00:07:54,155
So zero trust may help mitigate the
insider threat, insider threats by

120
00:07:54,175 --> 00:07:58,355
limiting access to only people who
are needed on an as needed basis.

121
00:07:58,655 --> 00:08:02,705
So this makes sure there's a specific
risk profile and they're not able

122
00:08:02,705 --> 00:08:05,405
to access content other than what
they are authorized to access.

123
00:08:06,135 --> 00:08:09,725
next, let's go to adaptability
into dynamic environments.

124
00:08:09,745 --> 00:08:14,565
So these days with every company moving
to the cloud, zero trust is the way to go

125
00:08:14,565 --> 00:08:22,590
in terms of our security because Zerotest
has a way to continuously monitor changing

126
00:08:22,610 --> 00:08:24,410
requirements for different services.

127
00:08:25,020 --> 00:08:29,360
So one could use, for example, in a
cloud system like maybe AWS, one could

128
00:08:29,360 --> 00:08:34,340
use something like an API gateway that
could basically monitor all the logging

129
00:08:34,350 --> 00:08:38,840
based information on who accessed what
and retain these logs for compliance

130
00:08:38,870 --> 00:08:43,440
purposes, which would be helpful in trying
to understand who accessed what and when.

131
00:08:43,955 --> 00:08:46,745
the next set of security principles
we are gonna be looking at

132
00:08:46,745 --> 00:08:49,040
is, ai, API based principles.

133
00:08:49,280 --> 00:08:54,520
Basically even trying to develop a gen AI
model, one has to use, one of these APIs,

134
00:08:54,730 --> 00:08:59,350
rod by one of the huge vendors, like maybe
chat GPT, which is belongs open AI, andro

135
00:08:59,350 --> 00:09:01,840
pick, or maybe hanging face, et cetera.

136
00:09:02,080 --> 00:09:06,355
So some of this techniques for,
secure, gene model charging involve

137
00:09:06,355 --> 00:09:08,545
ho homo homomorphic encryption.

138
00:09:08,785 --> 00:09:11,155
So homomorphic encryption is basically.

139
00:09:11,485 --> 00:09:15,765
carried out on a ciphertext, generating
an encrypted result, which when

140
00:09:15,955 --> 00:09:19,345
decrypted matches the result of
operations performed on the plaintext.

141
00:09:19,895 --> 00:09:23,345
So homomorphic encryption enables
the processing of sensitive

142
00:09:23,385 --> 00:09:24,985
data without exposing it.

143
00:09:25,265 --> 00:09:29,575
For an example, a Gen AI model could
train on an encrypted data set, ensuring

144
00:09:29,575 --> 00:09:33,375
the underlying data remains confidential
throughout the computation process.

145
00:09:33,945 --> 00:09:36,885
So organizations can leverage
homomorphic encryption.

146
00:09:37,660 --> 00:09:41,760
to change in, Gen A models and encrypted
data from multiple sources without

147
00:09:41,860 --> 00:09:43,590
accessing their own data, directly.

148
00:09:43,780 --> 00:09:47,480
So this makes sure any confidential
data is not accessed directly.

149
00:09:47,720 --> 00:09:49,590
we look at an example down the line.

150
00:09:49,880 --> 00:09:53,210
So this is crucial in industries
like finance or healthcare where

151
00:09:53,210 --> 00:09:55,070
privacy is of paramount importance.

152
00:09:55,570 --> 00:10:00,260
There are also regulations in terms
of what access can be accessed within

153
00:10:00,370 --> 00:10:02,230
a country or outside a country.

154
00:10:02,480 --> 00:10:03,040
So for this.

155
00:10:04,455 --> 00:10:07,695
is used for this specific
reason, homomorphic encryption is

156
00:10:07,975 --> 00:10:10,205
specific, specifically important.

157
00:10:10,515 --> 00:10:14,598
So looking at the secure
ABA strategies, so robust

158
00:10:14,598 --> 00:10:18,385
authentication and authentication
is of paramount importance here.

159
00:10:18,645 --> 00:10:24,315
So using a modern authentication
mechanism like OAuth2 or OpenID to

160
00:10:24,315 --> 00:10:28,305
verify the identity of users and
services is of paramount importance.

161
00:10:28,720 --> 00:10:32,220
There needs to be fine grained
authorization controls to make sure

162
00:10:32,230 --> 00:10:36,060
users only have appropriate access to the
data and actions based on their roles.

163
00:10:36,490 --> 00:10:40,290
So someone could employ something
like a privileged access management or

164
00:10:40,320 --> 00:10:44,310
just in time access which essentially
provides access only for a specific

165
00:10:44,310 --> 00:10:48,703
window for the specific data so that
their access is automatically removed

166
00:10:48,703 --> 00:10:53,110
without people having to monitor if that
person is having access for a longer

167
00:10:53,110 --> 00:10:55,440
period of time that wouldn't be intended.

168
00:10:55,950 --> 00:11:01,350
So API need to be also be protected from
denial of service attacks that can, this

169
00:11:01,350 --> 00:11:05,890
can be done by, using features like rate
limiting, which essentially times out

170
00:11:05,890 --> 00:11:09,760
or locks out the user depending on the
number of requests that's being sent on

171
00:11:09,760 --> 00:11:13,260
a per minute or a 10 plus periodic basis.

172
00:11:13,600 --> 00:11:18,920
One could also use, CDN systems
like Cloudflare or Amazon AWS

173
00:11:19,700 --> 00:11:22,810
who have custom algorithms to
protect against such attacks.

174
00:11:23,810 --> 00:11:28,380
One should be basically utilize a
transport layer security to encrypt data

175
00:11:28,440 --> 00:11:30,770
transmitted between clients and JNI APIs.

176
00:11:31,230 --> 00:11:36,680
This makes sure that the attacker
or a hacker is not able to sniff

177
00:11:36,680 --> 00:11:40,100
on this data and tamper the
sensitive data during transmission.

178
00:11:42,080 --> 00:11:47,160
As we discussed before, using API gateways
to manage, monitor and secure traffic to

179
00:11:47,160 --> 00:11:51,960
and from JNI services is of, can also be
used and it's of paramount importance.

180
00:11:52,400 --> 00:11:55,560
So gateways basically provide
additional layers of security,

181
00:11:55,560 --> 00:11:59,480
such as IP whitelisting, detection,
logging for forensic analysis.

182
00:12:00,560 --> 00:12:04,590
Next, let's look at different
deployment strategies and how one

183
00:12:04,630 --> 00:12:07,370
could securely deploy GNI models.

184
00:12:08,190 --> 00:12:11,930
So one could use digital
signatures, real time monitoring,

185
00:12:12,000 --> 00:12:15,710
and automated response protocols
when trying to securely deploy GNI

186
00:12:18,130 --> 00:12:18,230
models.

187
00:12:18,800 --> 00:12:22,490
So in terms of digital signal
notification, one could use that to

188
00:12:22,490 --> 00:12:26,930
understand the model authenticity and
implement secure key management and

189
00:12:26,930 --> 00:12:32,650
version control to ensure that any changes
that are made are properly documented

190
00:12:32,650 --> 00:12:38,235
and any models are not, are not, are,
the original models are not un altered.

191
00:12:39,230 --> 00:12:41,870
So real time monitoring
plays a crucial role here.

192
00:12:42,215 --> 00:12:45,265
So advanced tools can be used
to detect anomalies in gen

193
00:12:45,265 --> 00:12:49,135
IBM utilizing sophisticated
anomaly detection algorithms.

194
00:12:49,525 --> 00:12:53,445
one could also use automated
response protocols by integrating

195
00:12:53,445 --> 00:12:56,645
automated responses to swiftly
address, detected anomalies.

196
00:12:56,935 --> 00:13:01,535
One could enhance security with feedback
loops that could refine and improve this

197
00:13:01,535 --> 00:13:03,695
detection process based on the insights.

198
00:13:04,375 --> 00:13:07,405
next, next, let's look at trust
and execution environments, which

199
00:13:07,405 --> 00:13:09,215
is a specific concept of security.

200
00:13:09,785 --> 00:13:11,595
as it relates to gen AI models.

201
00:13:11,945 --> 00:13:15,795
in terms of trusted execution
environments, it essentially means

202
00:13:15,795 --> 00:13:20,415
there's an isolated, pro, execution
environment in a processor that provides

203
00:13:20,425 --> 00:13:24,135
secure, computing environment for
the particular data, isolating any

204
00:13:24,135 --> 00:13:28,075
sensitive data and model processing
from the main, hardware or operating

205
00:13:28,075 --> 00:13:32,825
system to make sure people are, any
unauthorized person is not able to access.

206
00:13:33,535 --> 00:13:37,035
we have for a real time
application of this as, processes

207
00:13:37,045 --> 00:13:38,255
that are used in phones.

208
00:13:38,575 --> 00:13:44,115
Many of the sensitive data that we use,
like maybe, calls or messages or anything

209
00:13:44,115 --> 00:13:48,885
that we discuss with a Google Assistant
or an Alexa Assistant, it's executed in

210
00:13:48,885 --> 00:13:53,305
a trusted, executable environment, so
this data is not accessible, even if a

211
00:13:53,305 --> 00:13:55,235
person has administrative privileges.

212
00:13:55,640 --> 00:13:59,760
or the processor that's performing
the normal tasks for a phone is

213
00:13:59,760 --> 00:14:00,970
not able to access this data.

214
00:14:02,040 --> 00:14:05,430
One could use, so trusted
execution environments offer

215
00:14:05,430 --> 00:14:06,880
enhanced security features.

216
00:14:07,200 --> 00:14:11,210
They basically ensure data integrity
and confidentiality through

217
00:14:11,210 --> 00:14:14,915
encryption and integrity checks
along with strict access controls.

218
00:14:14,915 --> 00:14:17,799
this is more versatile in computing.

219
00:14:18,169 --> 00:14:21,629
That's to say, trusted execution
environments are used mostly

220
00:14:21,639 --> 00:14:23,019
in cloud and edge computing.

221
00:14:23,374 --> 00:14:27,934
These are crucial for GNI applications
because given the interaction between the

222
00:14:28,054 --> 00:14:33,134
input and the output and it's almost in
real time, the amount of computing power

223
00:14:33,134 --> 00:14:37,694
it takes and the speed of the response
is of paramount importance, which is

224
00:14:37,724 --> 00:14:41,914
why, industries like health care and
finance can use something like a trusted

225
00:14:41,914 --> 00:14:46,824
execution environment to make sure that
confidential data is processed separately.

226
00:14:47,444 --> 00:14:51,374
Next, let's look at the different,
security, operations one could

227
00:14:52,134 --> 00:14:53,424
automate in JNI operations.

228
00:14:53,684 --> 00:14:56,624
So one could perform automated
vulnerability scanning.

229
00:14:56,894 --> 00:15:01,704
So these tools basically identify
address vulnerabilities within the AI

230
00:15:01,704 --> 00:15:05,724
models, data processes, integrating
them into a CACD pipeline for early

231
00:15:05,724 --> 00:15:07,504
detection and continuous compliance.

232
00:15:08,054 --> 00:15:10,884
one could use behavioral
analytics and code analysis.

233
00:15:11,479 --> 00:15:16,449
these essentially ensure they monitor
the AI system, look for their behaviors,

234
00:15:16,509 --> 00:15:21,579
analyze the code to flag anomalies and
non security risks, thereby enhancing real

235
00:15:21,579 --> 00:15:23,529
time security and developer awareness.

236
00:15:23,949 --> 00:15:27,429
One could also use proactive
security and compliance by continuous

237
00:15:27,449 --> 00:15:29,149
integration of security assessments.

238
00:15:29,724 --> 00:15:32,884
making sure that a proactive
security posture is maintained.

239
00:15:33,224 --> 00:15:39,054
This reduces the security depth and the
amount of work developer teams have to

240
00:15:39,054 --> 00:15:44,064
do while also maintaining compliance and
giving visibility to entire development

241
00:15:44,064 --> 00:15:49,274
and IT operations team, making sure IT
bugs and security bugs are fixed before

242
00:15:49,274 --> 00:15:50,794
these models are deployed for use.

243
00:15:51,434 --> 00:15:55,834
Now let's look at an example case study
of how an organization would unam such,

244
00:15:55,934 --> 00:16:00,234
some of the techniques described So,
let's take an example of a healthcare

245
00:16:00,234 --> 00:16:03,774
industry that's maybe trying to
modernize their security process, and

246
00:16:03,774 --> 00:16:05,344
they're trying to use GNI to do this.

247
00:16:05,834 --> 00:16:10,294
So the healthcare company basically needs
to abide by HIPAA regulations in the U.

248
00:16:10,294 --> 00:16:14,684
S., and they're trying to use GNI systems
to improve the diagnostic accuracy,

249
00:16:14,924 --> 00:16:19,364
thereby training the model on a vast
set of anonymized patient image data.

250
00:16:19,809 --> 00:16:23,849
So the, here, as you can infer, as
we previously discussed, they're

251
00:16:23,849 --> 00:16:28,209
using the homomorphic encryption and
the concept of trusted processor,

252
00:16:28,449 --> 00:16:31,719
trusted execution environment
to separately process the data.

253
00:16:32,029 --> 00:16:35,339
This application basically uses
homomorphic encryption to allow the

254
00:16:35,339 --> 00:16:39,999
GNA model to process encrypted images
directly, ensuring the data domain

255
00:16:40,099 --> 00:16:41,619
is secure even during computation.

256
00:16:42,519 --> 00:16:45,769
they then use digital signature,
they can use digital signatures.

257
00:16:46,149 --> 00:16:52,909
to verify the model integrity before each
use, and each tested execution environment

258
00:16:52,929 --> 00:16:58,269
can be used for secure model execution,
isolating the confidential data, making

259
00:16:58,269 --> 00:17:02,679
sure that the data is accessed only
within the specific computation period

260
00:17:02,719 --> 00:17:04,789
and not accessed outside that environment.

261
00:17:05,149 --> 00:17:08,939
So let's look at the future trends
of how Gen AI will affect security.

262
00:17:09,404 --> 00:17:13,324
So there's a lots of research going
on in terms of quantum cryptography,

263
00:17:13,574 --> 00:17:17,934
proactive security models and the,
compliance and privacy regulations.

264
00:17:18,174 --> 00:17:22,494
So in terms of quantum resistant
encryption, one, one could look at

265
00:17:22,494 --> 00:17:26,484
different applications such as development
of post quantum cryptography, which

266
00:17:26,514 --> 00:17:31,174
aims to secure systems against quantum
computing threats, where the standards are

267
00:17:31,174 --> 00:17:33,184
developed by a big institution like nist.

268
00:17:33,904 --> 00:17:38,334
one in terms of predictive security models
with Gen AI, one could use Gen AI to

269
00:17:38,334 --> 00:17:43,344
analyze extensive data sets, predictive
security models, which basically are able

270
00:17:43,344 --> 00:17:47,304
to foresee vulnerabilities and basically
mitigate those vulnerabilities and

271
00:17:47,354 --> 00:17:49,114
enabling proactive different strategies.

272
00:17:49,714 --> 00:17:56,114
integrating and standardization of,
for adaptability of these, Gen AI

273
00:17:56,114 --> 00:17:57,474
models is of paramount importance.

274
00:17:57,819 --> 00:18:01,839
So this emphasizes the need for interation
of new cryptographic solutions into

275
00:18:01,839 --> 00:18:06,899
existing systems and standard and
standardization for, widespread adoption.

276
00:18:08,339 --> 00:18:12,139
Coming to the conclusion in today's
evolving board, the role of securities

277
00:18:12,139 --> 00:18:16,349
of paramount importance, especially
in the, role of application of

278
00:18:16,349 --> 00:18:21,524
attribution intelligence, it's not only
important to protect against threats,

279
00:18:21,704 --> 00:18:25,394
but also to maintain the trust and
confidence of users and stakeholders.

280
00:18:25,864 --> 00:18:29,974
As AI technologies keep improving at a
rapid pace, so will the sophistication of

281
00:18:30,184 --> 00:18:32,054
threats against these kind of AI models.

282
00:18:32,484 --> 00:18:37,444
It's crucial to have security in mind
and from the start, and set of product

283
00:18:37,474 --> 00:18:40,994
development, and integrate security
across each stage of the product

284
00:18:41,064 --> 00:18:42,634
life cycle rather than at the end.

285
00:18:42,904 --> 00:18:47,604
This is crucial in making sure security
bugs are squashed and teams are able

286
00:18:47,604 --> 00:18:49,394
to manage their workload efficiently.

287
00:18:49,679 --> 00:18:53,839
The complex landscape of AI and
security demonstrates the necessity of

288
00:18:53,839 --> 00:18:58,344
a proactive approach where security is
not an afterthought, but a foundational

289
00:18:58,344 --> 00:19:00,594
component of all AI initiatives.

290
00:19:00,884 --> 00:19:04,504
thank you for, this opportunity
for any questions or feedback.

291
00:19:04,714 --> 00:19:08,824
Please reach out to me, via the
LinkedIn, username listed on the slide.

292
00:19:09,184 --> 00:19:09,424
Thank you.

