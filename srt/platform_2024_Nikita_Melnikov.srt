1
00:00:14,060 --> 00:00:15,810
My name is Nikita Melnikov.

2
00:00:16,379 --> 00:00:18,500
I am VP of engineering at Atlantic money.

3
00:00:19,060 --> 00:00:23,030
and today we'll talk about affordable
concurrency, about financial

4
00:00:23,030 --> 00:00:25,920
systems, about its challenges.

5
00:00:26,270 --> 00:00:30,980
And, how to use Acro model
to solve everything we need

6
00:00:31,350 --> 00:00:33,429
in, this kind of systems.

7
00:00:33,930 --> 00:00:37,540
So about me, as I've said before, I'm
VP of engineering at Atlantic money.

8
00:00:38,285 --> 00:00:41,465
Also, I am Axe Sync of Bank and
the Axe Sync of Investments.

9
00:00:41,775 --> 00:00:43,935
I have more than 10 years in FinTech.

10
00:00:44,255 --> 00:00:45,905
I was working on high load systems.

11
00:00:45,905 --> 00:00:49,134
So we have more than 300,
000 surplus per second.

12
00:00:49,545 --> 00:00:52,755
And also I love Scala,
Golang, Postgres and Kafka.

13
00:00:53,275 --> 00:00:53,895
Let's begin.

14
00:00:55,624 --> 00:00:57,645
Let's take a look on the agenda.

15
00:00:58,115 --> 00:01:00,045
we will cover a few topics.

16
00:01:00,115 --> 00:01:02,355
So we'll go from, the problem.

17
00:01:02,905 --> 00:01:05,005
Do the solution that I propose.

18
00:01:05,575 --> 00:01:08,095
So the problem is about
financial transactions and,

19
00:01:08,095 --> 00:01:09,975
what problems could we have?

20
00:01:10,605 --> 00:01:14,265
after that, we will shift to traditional
approach and their limitations.

21
00:01:15,045 --> 00:01:19,395
And, we, of course, speak
about a synchronous processing

22
00:01:19,595 --> 00:01:23,305
about Kafka, how to implement a
synchronous processing using Kafka.

23
00:01:23,555 --> 00:01:24,575
Why do we need it?

24
00:01:24,635 --> 00:01:25,925
How it could help us.

25
00:01:26,175 --> 00:01:28,755
And the main topic is actor model.

26
00:01:28,755 --> 00:01:29,205
Of course.

27
00:01:30,275 --> 00:01:31,575
let's start.

28
00:01:32,495 --> 00:01:34,935
So what is financial transaction?

29
00:01:34,985 --> 00:01:42,005
we will use our company as an
example, and, we will review the

30
00:01:42,005 --> 00:01:45,235
standard process for our cases.

31
00:01:46,054 --> 00:01:49,225
A customer can create
a transfer in our app.

32
00:01:49,675 --> 00:01:55,595
So for example, you want to
send 100 to 90 euros to someone.

33
00:01:56,470 --> 00:02:01,820
you're creating the transfer in our
app, you send 100 payment to our app

34
00:02:02,370 --> 00:02:06,230
and we can send euros to your recipient.

35
00:02:07,670 --> 00:02:09,050
Let's simplify it a bit.

36
00:02:09,090 --> 00:02:11,640
So we are waiting for dollars.

37
00:02:11,960 --> 00:02:15,569
We run some checks, we send euros.

38
00:02:16,479 --> 00:02:17,379
Sounds simple, right?

39
00:02:17,429 --> 00:02:22,339
but in fact, what happens
in the real world?

40
00:02:23,259 --> 00:02:23,819
First of all.

41
00:02:25,325 --> 00:02:31,755
As we discussed before, customer creates
USD to Euro transfer system, waits

42
00:02:31,755 --> 00:02:33,775
for USD the same as before, right?

43
00:02:34,755 --> 00:02:36,975
System writes the payment details.

44
00:02:36,984 --> 00:02:42,075
So we should know, when we've
got your payment, amount, sender,

45
00:02:42,105 --> 00:02:46,955
recipient, your bank information, and
any other information that we need.

46
00:02:47,945 --> 00:02:49,305
After that system runs on checks.

47
00:02:50,130 --> 00:02:50,940
What checks?

48
00:02:51,280 --> 00:02:54,780
For example, we must check sanction lists.

49
00:02:55,180 --> 00:02:58,840
We should check your payment
in our anti fraud system.

50
00:02:59,390 --> 00:03:01,770
We should check your payment limits.

51
00:03:02,220 --> 00:03:08,070
We should calculate fees and many
more, as many as you can imagine.

52
00:03:08,790 --> 00:03:12,940
Of course, we have to exchange
currencies to send another

53
00:03:12,940 --> 00:03:14,480
currency to your recipient.

54
00:03:16,560 --> 00:03:19,680
And finally, we can send
EUR to the recipient.

55
00:03:21,465 --> 00:03:22,985
Where is the problem?

56
00:03:23,905 --> 00:03:27,905
Of course, there can be many
problems in terms of compliance,

57
00:03:28,455 --> 00:03:31,235
operations, finance, and so on.

58
00:03:31,705 --> 00:03:35,305
But we will review only one
technical problem today.

59
00:03:35,965 --> 00:03:37,465
The problem is lost update.

60
00:03:38,355 --> 00:03:39,815
Let's take a look on the diagram.

61
00:03:39,965 --> 00:03:40,925
What happens here?

62
00:03:41,425 --> 00:03:47,615
So when transfer system receive
your payment, we should find

63
00:03:47,645 --> 00:03:49,275
proper transfer in the database.

64
00:03:50,555 --> 00:03:54,515
After finding, we should check
the sender and the recipient.

65
00:03:54,765 --> 00:03:59,375
For example, in our compliance service,
we are getting, okay, everything is fine.

66
00:03:59,385 --> 00:04:00,675
So we can go further.

67
00:04:01,105 --> 00:04:05,075
We are processing the request,
doing some correlations, applying

68
00:04:05,075 --> 00:04:07,055
fees, exchanging currencies.

69
00:04:07,155 --> 00:04:09,015
And finally, we should
update the transfers.

70
00:04:09,065 --> 00:04:12,125
So just to set the status,
let everything is fine.

71
00:04:12,155 --> 00:04:14,955
So we've paid out, the
money for your recipient.

72
00:04:15,905 --> 00:04:16,855
Where are the problem?

73
00:04:17,395 --> 00:04:24,085
The problem is When compliance
system decided to cancel the transfer

74
00:04:24,115 --> 00:04:28,345
at the same time, while we are
processing the original transfer.

75
00:04:29,545 --> 00:04:36,485
So the process looks mostly the same,
but after successful checks in compliance

76
00:04:36,485 --> 00:04:38,875
service can decide to cancel the transfer.

77
00:04:39,095 --> 00:04:44,815
So it hits in point transfer slash
cancel, and we should select transfer for

78
00:04:44,825 --> 00:04:47,445
restriction and update transfer as well.

79
00:04:48,400 --> 00:04:52,750
Let's take a look on the
transactions here on the left.

80
00:04:53,140 --> 00:04:56,570
We have compliance
transaction on the right.

81
00:04:56,680 --> 00:05:00,030
We have our payment, to our system.

82
00:05:00,380 --> 00:05:02,190
So we are starting the transaction.

83
00:05:02,540 --> 00:05:04,140
We are selecting transfer.

84
00:05:04,830 --> 00:05:05,870
We are getting result.

85
00:05:05,960 --> 00:05:07,410
So result of the same.

86
00:05:07,410 --> 00:05:12,730
So ID one status created, everything
should be fine in both transactions.

87
00:05:13,595 --> 00:05:16,905
After that, we are setting
different statuses for compliance.

88
00:05:16,905 --> 00:05:20,765
We should set status
canceled for our payment.

89
00:05:20,825 --> 00:05:22,885
We should set status payment received.

90
00:05:24,085 --> 00:05:24,845
Okay, cool.

91
00:05:25,025 --> 00:05:26,275
We're committing the changes.

92
00:05:27,235 --> 00:05:29,305
Where is the problem?

93
00:05:29,305 --> 00:05:31,865
The problem is here.

94
00:05:33,065 --> 00:05:38,575
If we will select transfer right
after these changes, we can get.

95
00:05:39,430 --> 00:05:40,660
undefined status.

96
00:05:40,800 --> 00:05:45,720
So because we have two concurrent
transactions, and we don't know

97
00:05:45,740 --> 00:05:50,420
what exactly we want to have
at the end, we cannot guarantee

98
00:05:51,250 --> 00:05:53,050
the order of the statuses.

99
00:05:53,260 --> 00:05:56,780
So the status can be undefined, it
can be cancelled, it can be payment

100
00:05:56,800 --> 00:05:59,000
received, or any other, of course.

101
00:06:00,110 --> 00:06:02,740
Let's try to solve it.

102
00:06:03,870 --> 00:06:08,840
And we'll talk about For additional
approaches, of course, I don't

103
00:06:08,860 --> 00:06:13,300
have the time to provide all
possible options, how to solve it.

104
00:06:13,870 --> 00:06:17,980
And also we are limited by a
technology that we use now.

105
00:06:17,990 --> 00:06:21,520
let's imagine we have
postgres database, right?

106
00:06:22,010 --> 00:06:26,830
And we can solve it like, in,
in that way, we have database

107
00:06:26,830 --> 00:06:28,910
transaction, how it looks like.

108
00:06:30,200 --> 00:06:32,250
The same as before, we're
starting the transaction.

109
00:06:33,240 --> 00:06:40,590
We are selecting transfer for update
now, and we can commit the changes.

110
00:06:42,070 --> 00:06:46,750
Everything is okay, but let's try
to implement it on the diagram.

111
00:06:47,580 --> 00:06:51,910
So this is the same diagram as before,
except just the one small thing.

112
00:06:52,400 --> 00:06:55,990
In fact, just to selecting the transfer,
we are selecting it for update.

113
00:06:56,730 --> 00:06:59,460
we are starting in from,
payment receiving process.

114
00:06:59,790 --> 00:07:01,230
So we're selecting for update.

115
00:07:01,240 --> 00:07:03,440
We're checking sender and recipient.

116
00:07:03,740 --> 00:07:04,690
Everything is fine.

117
00:07:05,060 --> 00:07:09,670
At that moment, compliance system decided
to cancel and restrict transferred.

118
00:07:10,670 --> 00:07:13,260
It does select for updates as well.

119
00:07:13,910 --> 00:07:18,880
but it should wait until we have
one open transaction for update for

120
00:07:18,880 --> 00:07:21,370
one single row or for many rows.

121
00:07:21,540 --> 00:07:25,590
Postgres knows that everything
should be blocked until the

122
00:07:25,600 --> 00:07:29,390
first process will be completed.

123
00:07:30,940 --> 00:07:36,610
so the one, the first process when we are
receiving the payment is, painting now.

124
00:07:36,720 --> 00:07:38,509
So we are selected transfer.

125
00:07:38,529 --> 00:07:39,939
We are doing some calculations.

126
00:07:40,600 --> 00:07:47,060
We are working on the processing and
finally we do update on the step six.

127
00:07:48,270 --> 00:07:52,990
Once we committed changes, we can
continue with the second process.

128
00:07:53,900 --> 00:07:58,309
We're selecting, we already sent
select for update, requests from

129
00:07:58,309 --> 00:08:00,670
compliance service to our database.

130
00:08:00,920 --> 00:08:04,690
And, we were waiting for the
result and now the result is

131
00:08:04,690 --> 00:08:06,960
finally here and we can continue.

132
00:08:07,320 --> 00:08:08,530
everything is solved actually.

133
00:08:08,550 --> 00:08:10,310
So everything is super simple here.

134
00:08:10,310 --> 00:08:13,350
So there are only two,
database transactions.

135
00:08:13,350 --> 00:08:18,969
We don't have any new abstractions or any
new tools, but where's the limitation?

136
00:08:20,460 --> 00:08:22,209
Let's make a quick calculation.

137
00:08:22,340 --> 00:08:29,659
So let's imagine we have processing time,
average processing time, five seconds.

138
00:08:29,660 --> 00:08:33,120
And we have 100 operations per second.

139
00:08:33,480 --> 00:08:36,970
That means that we should
have 500 active transactions.

140
00:08:38,315 --> 00:08:40,355
Doesn't seem complicated, right?

141
00:08:40,555 --> 00:08:43,185
Authors definitely could handle this.

142
00:08:44,025 --> 00:08:45,465
And yes and no.

143
00:08:45,905 --> 00:08:49,105
there are two drawbacks.

144
00:08:49,405 --> 00:08:51,405
First of all, is resources.

145
00:08:51,584 --> 00:08:54,194
we have many active transactions.

146
00:08:54,999 --> 00:08:59,329
that do nothing actually, because we
are waiting, we are going to external

147
00:08:59,329 --> 00:09:05,139
services, we are writing some audit data,
we are getting data from another service.

148
00:09:05,139 --> 00:09:06,239
So many things.

149
00:09:06,269 --> 00:09:13,179
So while, we do this transaction
is active, and we cannot work with

150
00:09:13,229 --> 00:09:18,529
this, locked row, for example,
with this And the second point

151
00:09:18,539 --> 00:09:20,759
is about your connection pools.

152
00:09:20,949 --> 00:09:26,734
So I believe most of you have a
connection pooling system in your apps.

153
00:09:26,744 --> 00:09:31,874
So for example, it can be limited up
to 16 connections to your Postgres

154
00:09:31,874 --> 00:09:36,334
instance and, everything should be fine
actually, because we have different

155
00:09:36,344 --> 00:09:42,234
transfers, but the problem will be
if you have multiple concurrent.

156
00:09:44,184 --> 00:09:49,204
Transactions in the database,
let's imagine we have 16 concurrent

157
00:09:49,204 --> 00:09:50,754
process at the same time.

158
00:09:51,244 --> 00:09:53,144
So that means that the first.

159
00:09:53,509 --> 00:09:57,089
Connection will be acquired
bar by actual transaction.

160
00:09:57,099 --> 00:10:01,129
So we are, we got the
result from the database.

161
00:10:01,339 --> 00:10:04,939
We start doing something, we are
processing it to five seconds.

162
00:10:04,999 --> 00:10:07,779
And finally we can
release during this time.

163
00:10:08,139 --> 00:10:12,399
The second operation will be
waiting and the third and the

164
00:10:12,479 --> 00:10:15,209
fifth and the sixth and so on.

165
00:10:15,269 --> 00:10:15,859
the.

166
00:10:16,334 --> 00:10:22,764
Contention of the, on the lock and on
the database, in fact, if we want to

167
00:10:22,764 --> 00:10:27,804
work with, the same transfer, it will be
quite a problem because at the end you

168
00:10:27,844 --> 00:10:32,305
can end up with, just, empty connection
pool because every connection is busy.

169
00:10:33,310 --> 00:10:38,230
and you cannot run anything
else on these connection pools.

170
00:10:38,510 --> 00:10:40,330
You cannot serve customers request.

171
00:10:40,350 --> 00:10:42,440
You cannot serve in other transfers.

172
00:10:42,620 --> 00:10:43,470
here's the problem.

173
00:10:44,270 --> 00:10:44,630
Okay.

174
00:10:45,760 --> 00:10:52,219
Let's try to solve it in, in
other way, let's try locks.

175
00:10:52,779 --> 00:10:58,789
Of course, locks can be implemented in
different systems in different ways.

176
00:10:58,809 --> 00:11:01,719
But in fact, we'll today,
we'll talk about local locks.

177
00:11:02,459 --> 00:11:03,599
And there's the muted locks.

178
00:11:04,029 --> 00:11:05,689
Let's start with local locks.

179
00:11:06,279 --> 00:11:10,229
in Golang, it can be done
via just a mutex object.

180
00:11:11,269 --> 00:11:12,059
How it works.

181
00:11:12,419 --> 00:11:15,189
we should run something on the transfer.

182
00:11:15,519 --> 00:11:17,889
We are trying to acquire the mutex.

183
00:11:19,269 --> 00:11:21,189
And defer a function to unlock it.

184
00:11:21,389 --> 00:11:27,549
if mutex was already locked,
The goroutine will be.

185
00:11:28,239 --> 00:11:33,309
We'll be painting, actually, we will
wait until, the log will be resolved.

186
00:11:34,179 --> 00:11:35,279
Everything is simple, right?

187
00:11:35,379 --> 00:11:38,039
But the problem is obvious, actually.

188
00:11:38,089 --> 00:11:42,774
if you have multiple nodes, You have
multiple endpoints or processes or

189
00:11:42,774 --> 00:11:48,144
whatever, or maybe, your consistent
rotor on engine X or any, another

190
00:11:48,314 --> 00:11:53,634
balancer was broken or after start,
you can end up with, this scheme.

191
00:11:54,024 --> 00:11:59,144
So one node holds a lock
local lock for one transfer.

192
00:11:59,514 --> 00:12:06,199
And the knock note too, got a request
for the same transfer with different That

193
00:12:06,199 --> 00:12:08,359
means that locks doesn't work at all.

194
00:12:08,369 --> 00:12:09,489
So you don't need it.

195
00:12:11,079 --> 00:12:13,419
let's try another approach.

196
00:12:14,039 --> 00:12:16,559
So this is about distributed locks.

197
00:12:17,179 --> 00:12:22,319
of course it's become a bit, a bit
complex because you shouldn't produce

198
00:12:22,320 --> 00:12:25,939
a new infrastructure things here.

199
00:12:26,219 --> 00:12:30,929
And also you should understand how
it works and your engineers as well.

200
00:12:31,279 --> 00:12:32,869
So let's start with the diagram.

201
00:12:32,869 --> 00:12:36,869
So the key difference actually
is that instead of just a

202
00:12:39,169 --> 00:12:43,789
Mutex in a single node application, you
have distributed lock manager, how it

203
00:12:43,789 --> 00:12:50,769
works, node one can request for lock in a
lock manager, and if it was not acquired

204
00:12:50,779 --> 00:12:53,139
by another process, we can run the lock.

205
00:12:53,309 --> 00:12:56,254
If node two, will try to
request the same lock.

206
00:12:56,754 --> 00:13:02,364
It will wait so it works the same so if
node one has the access it can safely

207
00:13:02,374 --> 00:13:08,014
access some resources and If everything
is done So we can just release the lock

208
00:13:08,264 --> 00:13:12,824
and right after that lock manager will
run the lock to the second node Everything

209
00:13:12,824 --> 00:13:18,529
is super simple here About storages,
actually, there are a lot of options.

210
00:13:18,849 --> 00:13:21,209
We will just, just name a few.

211
00:13:21,210 --> 00:13:25,759
Hazelcast, Zookeeper, etcd,
Console, Redis, whatever you need,

212
00:13:25,839 --> 00:13:29,969
whatever you know, whatever you
like, so you can use everything.

213
00:13:30,989 --> 00:13:32,809
what about limitations here?

214
00:13:33,629 --> 00:13:39,909
Limitations are quite, Quite
complex, actually, because, under

215
00:13:39,999 --> 00:13:45,239
load, it can be quite challenging
to understand how the system will

216
00:13:45,299 --> 00:13:48,239
work because of problem of ordering.

217
00:13:49,059 --> 00:13:53,889
So let's imagine you have multiple nodes,
for example, a cluster with 16 nodes.

218
00:13:54,409 --> 00:13:59,849
they can be placed in a different
zones, in a different data centers, in

219
00:13:59,849 --> 00:14:07,709
a different networks, because of many,
things just have different timing to

220
00:14:07,789 --> 00:14:13,499
access your, lock manager, for example,
Hazelcast, or, for example, Zookeeper.

221
00:14:13,515 --> 00:14:14,435
this is the first problem.

222
00:14:14,475 --> 00:14:22,275
So we can't understand why we've got
the lock to the note that, for example,

223
00:14:22,345 --> 00:14:25,345
call it later than the first one.

224
00:14:25,485 --> 00:14:29,135
So it is quite challenging to
understand how it works just

225
00:14:29,225 --> 00:14:31,145
because of problem of the ordering.

226
00:14:32,655 --> 00:14:33,425
Timeouts.

227
00:14:33,975 --> 00:14:36,075
there are actually multiple timeouts.

228
00:14:36,135 --> 00:14:38,575
So first of all, it is
log acquisition timeout.

229
00:14:38,575 --> 00:14:39,565
So we.

230
00:14:39,895 --> 00:14:41,935
As a engineer should understand.

231
00:14:41,935 --> 00:14:45,255
So I trying to get a
lock in our lock manager.

232
00:14:45,615 --> 00:14:50,515
we should be able to, set some time
out because we cannot wait, infinite

233
00:14:50,575 --> 00:14:54,265
time for this log because it just,
it will just hang in the process.

234
00:14:54,315 --> 00:14:58,645
and, we should understand how
to set this time out, how to

235
00:14:58,645 --> 00:14:59,685
wear and see this time out.

236
00:15:00,350 --> 00:15:04,870
And, the second point is how
to manage this time out when

237
00:15:04,870 --> 00:15:06,590
we already acquired the lock.

238
00:15:07,110 --> 00:15:13,390
let's imagine we've spent three seconds
to wait for a lock and now we should

239
00:15:13,390 --> 00:15:18,470
limit the time, when we process, when we
run actual logic, backends logic, right?

240
00:15:19,350 --> 00:15:21,420
and we already have two points here.

241
00:15:21,420 --> 00:15:24,090
So we should respect timeouts.

242
00:15:24,400 --> 00:15:29,220
We should, respect holding timeouts and we
should understand how to work with them.

243
00:15:29,430 --> 00:15:32,720
What should we do if
we didn't get the lock?

244
00:15:33,230 --> 00:15:34,830
Probably everything should be fine, right?

245
00:15:34,830 --> 00:15:37,540
So we just didn't do anything.

246
00:15:38,380 --> 00:15:44,960
What if we got the lock, but we
lost the lock or the process was

247
00:15:44,960 --> 00:15:47,570
crashed when we hold the lock?

248
00:15:48,645 --> 00:15:51,915
this is challenging actually, so because
you should respect it in your code.

249
00:15:51,925 --> 00:15:56,405
So every process should understand
that it can be, it can be dropped.

250
00:15:57,235 --> 00:16:00,425
And the server can fail and
how to restore the process.

251
00:16:01,075 --> 00:16:04,665
And you should understand this
partial state that was in, that was

252
00:16:04,665 --> 00:16:07,425
implemented during the lock holding.

253
00:16:08,715 --> 00:16:10,675
And, of course, potential deadlocks.

254
00:16:10,885 --> 00:16:15,655
That means that, for example, we
want to hold a lock for transfer, for

255
00:16:15,655 --> 00:16:24,457
check, for customer, for recipient,
and, The modify the state, we

256
00:16:24,517 --> 00:16:26,657
need to touch every object there.

257
00:16:26,907 --> 00:16:31,347
and potentially it can be challenging
to understand and to go and see

258
00:16:31,347 --> 00:16:33,037
that there is no deadlock installed.

259
00:16:33,057 --> 00:16:37,237
Deadlock is when, for example,
we've got a lock for transfer.

260
00:16:37,627 --> 00:16:41,317
We are trying to get lock for recipient,
but other process started with the

261
00:16:41,317 --> 00:16:43,317
recipient and after the transfer.

262
00:16:43,487 --> 00:16:44,507
So the second.

263
00:16:45,142 --> 00:16:50,142
The second transaction got a lock
for cpn and after that it's trying

264
00:16:50,592 --> 00:16:55,562
to get a lock for transfer, but it
holds in this lock is for this, the

265
00:16:55,562 --> 00:16:56,932
first note for the first process.

266
00:16:57,182 --> 00:17:01,035
and it is quite challenging and, I don't
know how to guarantee that there is no

267
00:17:01,035 --> 00:17:05,215
deadlocks at all instead of just, testing.

268
00:17:05,215 --> 00:17:08,355
But, testing is super complex
in distributed systems.

269
00:17:08,355 --> 00:17:12,255
So I don't have so many times to
test distributed system on my own.

270
00:17:13,195 --> 00:17:13,555
Okay.

271
00:17:13,885 --> 00:17:17,195
Let's write the switch to
a synchronous processing.

272
00:17:17,935 --> 00:17:18,345
Let's.

273
00:17:18,935 --> 00:17:23,185
First, define what the transfer model is.

274
00:17:23,835 --> 00:17:26,135
Actually, it is a finite
state machine, right?

275
00:17:26,335 --> 00:17:32,665
because transfer has multiple statuses,
State transitions occur via commons.

276
00:17:33,135 --> 00:17:39,335
Each state defines a load commons and
commons trigger actions and state changes.

277
00:17:40,265 --> 00:17:40,755
That's all.

278
00:17:40,765 --> 00:17:42,435
So finish state machine is same, right?

279
00:17:42,435 --> 00:17:46,385
So it's just a, switching before
states because of something, because

280
00:17:46,385 --> 00:17:48,375
of some events or commons or whatever.

281
00:17:48,465 --> 00:17:53,895
So we cannot go from payment received
to payment waiting just because,

282
00:17:53,945 --> 00:17:55,805
it's impossible in our domain.

283
00:17:56,625 --> 00:18:04,135
Let's take a look on the code First of
all, let's define transfer for simplicity.

284
00:18:04,135 --> 00:18:09,050
It could have only ID
and status What is face?

285
00:18:09,220 --> 00:18:10,730
I'm here being a state machine.

286
00:18:10,740 --> 00:18:13,380
So we have a transfer created.

287
00:18:14,130 --> 00:18:19,800
We have common receive payment
and we can move our transfer

288
00:18:19,800 --> 00:18:21,000
to payment received, right?

289
00:18:21,350 --> 00:18:23,480
So let's define statuses.

290
00:18:23,800 --> 00:18:27,810
We have created payment, received
checks, sent checks, painting and so on.

291
00:18:27,810 --> 00:18:33,940
So all our tree of choices and,
transitions from status to status.

292
00:18:34,440 --> 00:18:36,790
And of course we need to define a command.

293
00:18:36,920 --> 00:18:41,020
So a command is something that can
happen in your, in your application.

294
00:18:41,030 --> 00:18:45,100
It can be done via HTTP endpoints,
your PC endpoints, broker

295
00:18:45,100 --> 00:18:47,430
messages, time or whatever.

296
00:18:47,480 --> 00:18:51,550
in fact, we need just something to
tell to the transfer that it should

297
00:18:51,560 --> 00:18:53,949
move from one step to another.

298
00:18:55,330 --> 00:18:57,500
and let's take a look on the code.

299
00:18:57,540 --> 00:18:58,840
How would we handle this?

300
00:18:58,840 --> 00:19:03,130
But in this common, of course, we,
we won't review everything here.

301
00:19:03,170 --> 00:19:05,760
We will start with something super simple.

302
00:19:05,770 --> 00:19:08,360
So payment, receive payment common.

303
00:19:09,320 --> 00:19:11,110
First of all, we should check the status.

304
00:19:11,880 --> 00:19:12,700
Got to simplify it.

305
00:19:12,720 --> 00:19:15,840
We ignore error handling
here, error cases and so on.

306
00:19:15,840 --> 00:19:17,300
So just the pure logic.

307
00:19:17,705 --> 00:19:18,785
We should check the status.

308
00:19:19,335 --> 00:19:20,885
we've got a receive payment command.

309
00:19:20,915 --> 00:19:26,215
We know that status should
be only status created.

310
00:19:27,055 --> 00:19:29,325
If not, we should throw some error.

311
00:19:30,525 --> 00:19:32,395
We need to write payment details.

312
00:19:32,695 --> 00:19:34,415
We have to set status.

313
00:19:34,735 --> 00:19:36,605
We have to save the state.

314
00:19:37,005 --> 00:19:42,525
And we should tell to the transfer
that we need to run some checks

315
00:19:43,495 --> 00:19:45,285
to move the transfer further.

316
00:19:46,335 --> 00:19:46,775
Okay.

317
00:19:46,785 --> 00:19:46,845
Thank you.

318
00:19:46,845 --> 00:19:46,974
You're welcome.

319
00:19:47,405 --> 00:19:47,745
Cool.

320
00:19:48,925 --> 00:19:50,885
Requirements for asynchronous processing.

321
00:19:50,895 --> 00:19:52,965
So now we know the model.

322
00:19:53,015 --> 00:19:56,535
Now we understand the transfer
is actual FAST SAM model.

323
00:19:56,825 --> 00:19:58,925
And, we need comments, we need messages.

324
00:19:59,045 --> 00:19:59,685
we need states.

325
00:20:00,610 --> 00:20:04,390
Let's, make some
requirements for our system.

326
00:20:04,720 --> 00:20:07,970
First of all, we need
communication through messages.

327
00:20:08,300 --> 00:20:11,710
messages are comments
or events or whatever.

328
00:20:11,800 --> 00:20:16,430
This is something that we can tell
from one service to another service

329
00:20:16,680 --> 00:20:18,960
that, it needs to run something.

330
00:20:19,875 --> 00:20:23,215
Also, we need one at a
time message handling.

331
00:20:23,555 --> 00:20:29,125
That means that if we got a command
for transfer A, only one command

332
00:20:29,215 --> 00:20:31,865
can be executed for this transfer.

333
00:20:32,275 --> 00:20:36,325
If we have the second, it
should be queued somehow.

334
00:20:37,730 --> 00:20:39,880
Also, we need durable message store.

335
00:20:40,240 --> 00:20:45,370
That means that we should keep our
messages for some time because of,

336
00:20:45,420 --> 00:20:49,630
node failures, location, failures,
data centers, handlers, and many more

337
00:20:49,650 --> 00:20:50,900
because of many reasons, actually.

338
00:20:50,900 --> 00:20:55,790
So because of we, we would need
to replay messages somehow.

339
00:20:55,990 --> 00:21:00,400
Maybe we need analytics, maybe we
deployed something broken and we need

340
00:21:00,430 --> 00:21:03,000
to restore the state, on, on some.

341
00:21:03,220 --> 00:21:09,590
Date or time, and let's take a closer
on communication through messages.

342
00:21:10,090 --> 00:21:11,780
Actually, here is super simple, right?

343
00:21:11,950 --> 00:21:16,050
So we have a producer, we have
a consumer, we have something in

344
00:21:16,080 --> 00:21:18,630
the middle to deliver messages.

345
00:21:18,780 --> 00:21:20,180
Let's call it message broker.

346
00:21:20,620 --> 00:21:24,130
producers send messages to broker
deliver messages to consumer.

347
00:21:24,250 --> 00:21:27,840
Consumer tells to the broker that
everything is fine, so we can continue.

348
00:21:28,250 --> 00:21:30,810
And producer can send another message.

349
00:21:31,435 --> 00:21:34,825
The same one at a time message handling.

350
00:21:35,005 --> 00:21:36,885
So we already discussed, right?

351
00:21:36,885 --> 00:21:41,835
So we want to send three messages,
but at the same time, message

352
00:21:41,835 --> 00:21:44,245
consumer can handle only one.

353
00:21:45,365 --> 00:21:52,305
Our transfer queue is something that
can order the messages by producing

354
00:21:52,305 --> 00:21:59,545
time, delivering message one by
one to the consumer, and Transfer

355
00:21:59,545 --> 00:22:03,915
queues should wait until we've got
acknowledgement of that message was

356
00:22:04,075 --> 00:22:05,585
processed and everything is fine.

357
00:22:06,405 --> 00:22:08,345
And the same for message
two, for message three.

358
00:22:08,475 --> 00:22:10,545
we can think about it as a lock.

359
00:22:10,805 --> 00:22:16,875
So we, we have written something to
the queue one by one, and it will

360
00:22:16,885 --> 00:22:20,705
be consumed by the consumer one
by one as well, in the same order.

361
00:22:20,705 --> 00:22:20,765
Thank you.

362
00:22:22,405 --> 00:22:23,345
Durable message store.

363
00:22:23,735 --> 00:22:30,215
So it can be, there can be a lot of
examples actually, because we have many

364
00:22:30,285 --> 00:22:32,635
things that can break our system, right?

365
00:22:32,635 --> 00:22:36,445
So we need just, something
to replay the message.

366
00:22:36,545 --> 00:22:42,635
If our message consumer was broken or
even if external service system is broken.

367
00:22:43,705 --> 00:22:44,185
That's all.

368
00:22:45,215 --> 00:22:45,625
Okay.

369
00:22:46,135 --> 00:22:48,595
Let's talk about Kafka.

370
00:22:49,275 --> 00:22:50,165
And of course.

371
00:22:50,250 --> 00:22:53,306
Kafka is one of my favorite
technologies overall.

372
00:22:53,306 --> 00:22:57,780
So yeah, let's begin with some
basics because we need it just

373
00:22:57,780 --> 00:22:58,970
to understand how it works.

374
00:22:59,840 --> 00:23:05,870
Kafka have topic as a root
entity, in the architecture.

375
00:23:05,900 --> 00:23:07,340
So topic is just a feed.

376
00:23:07,340 --> 00:23:14,960
It, can contain multiple partitions, one,
two, three, four, five, Unlimited, of

377
00:23:14,960 --> 00:23:20,450
course, like a reasonably unlimited, and
Kafka is, distributed system by default.

378
00:23:20,460 --> 00:23:25,820
So it can be, deployed in multiple, in
multiple servers in multiple locations.

379
00:23:25,970 --> 00:23:30,060
So let's just think that we
have free brokers and they can

380
00:23:30,090 --> 00:23:33,125
coordinate the data somehow.

381
00:23:33,545 --> 00:23:37,345
So one topic has three
partitions, and each partition

382
00:23:37,355 --> 00:23:39,305
will be placed on the broker.

383
00:23:39,475 --> 00:23:46,105
Of course, there are replication,
some guarantees, some settings,

384
00:23:46,355 --> 00:23:47,935
but we don't need it for today.

385
00:23:49,215 --> 00:23:52,795
About partition of sense, as I
told before, so we can think about

386
00:23:52,805 --> 00:23:55,755
this message queue as a, as a lock.

387
00:23:56,115 --> 00:24:01,780
So partition one partition can have
Message zero, message one, message

388
00:24:01,780 --> 00:24:06,850
three, and, partition is the,
actually the sequence of the messages.

389
00:24:07,040 --> 00:24:10,020
So one topic can have multiple partitions.

390
00:24:10,270 --> 00:24:15,850
One partition can have multiple
messages ordered by, date time.

391
00:24:17,080 --> 00:24:18,020
About routing.

392
00:24:18,460 --> 00:24:19,960
the most interesting topic here.

393
00:24:20,700 --> 00:24:23,810
Let's think about
producers and partitions.

394
00:24:24,050 --> 00:24:26,180
Let's imagine we have multiple producers.

395
00:24:26,625 --> 00:24:28,415
And we have multiple partitions.

396
00:24:28,655 --> 00:24:32,495
So how can we know,
where to place a message?

397
00:24:32,525 --> 00:24:35,915
Because a partitions can be
written, in concurrent mode.

398
00:24:35,925 --> 00:24:39,805
So we can write to partition
one and partition two, multiple

399
00:24:39,805 --> 00:24:40,675
messages at the same time.

400
00:24:40,675 --> 00:24:41,535
There is no problem.

401
00:24:42,465 --> 00:24:44,575
there are multiple options.

402
00:24:44,585 --> 00:24:45,845
First of all, of course, it is possible.

403
00:24:46,605 --> 00:24:47,505
around Robin.

404
00:24:48,195 --> 00:24:51,855
This is we will write the partition
zero, partition one, partition

405
00:24:51,855 --> 00:24:54,335
three, partition zero, partition
one, partition three, and so on.

406
00:24:55,255 --> 00:25:00,195
But the interesting thing here is
what exactly we need is the key.

407
00:25:00,445 --> 00:25:06,655
Kafka will calculate a hash of the
key and the message for the key

408
00:25:06,765 --> 00:25:09,065
will be placed into the partition.

409
00:25:09,285 --> 00:25:11,865
So for example, we have
a transfer message.

410
00:25:12,385 --> 00:25:16,155
We have a key equals to transfer ID ABC.

411
00:25:16,605 --> 00:25:22,255
So if producer one and producer
two, we'll write the same message

412
00:25:22,255 --> 00:25:24,475
with the same key to the topic.

413
00:25:24,715 --> 00:25:29,855
So that means that both messages
will be placed into one partition

414
00:25:29,905 --> 00:25:31,015
into the same partition.

415
00:25:31,255 --> 00:25:32,395
So this is.

416
00:25:33,585 --> 00:25:38,145
this is the thing that we should keep in
mind, because this is, how it will help

417
00:25:38,145 --> 00:25:39,935
us to solve overall concurrency things.

418
00:25:41,770 --> 00:25:44,750
And let's speak about producer guarantees.

419
00:25:44,990 --> 00:25:46,280
There are a lot of settings.

420
00:25:46,280 --> 00:25:49,010
So of course, Kafka is
a quite complex system.

421
00:25:49,130 --> 00:25:52,890
in terms of, how to, set up it
properly and correctly, but let's

422
00:25:52,900 --> 00:25:54,810
speak about the most simple things.

423
00:25:54,820 --> 00:25:56,380
we have an ax setting.

424
00:25:56,380 --> 00:26:00,500
So that means that when producer
right message to the topic.

425
00:26:01,135 --> 00:26:07,505
We can set this ACK settings, if it equals
to zero, no ACK management is needed.

426
00:26:07,525 --> 00:26:12,985
So that means that Kafka, leader
instance will get the message

427
00:26:13,325 --> 00:26:14,755
and said, okay, I've got it.

428
00:26:14,835 --> 00:26:15,615
Everything is okay.

429
00:26:16,550 --> 00:26:20,130
Of course, it has low latency
because we don't write anything

430
00:26:20,220 --> 00:26:23,390
to the disk or in other servers.

431
00:26:23,600 --> 00:26:27,120
And of course, there is no delivery
guarantees because, if we get, if we

432
00:26:27,210 --> 00:26:32,340
have a call to Kafka with x0, Kafka
said that, okay, everything is fine.

433
00:26:32,340 --> 00:26:33,310
So I got the message.

434
00:26:33,470 --> 00:26:37,180
And right after that, the
leader replica failed.

435
00:26:37,740 --> 00:26:41,720
That means that there is no message
still because we didn't wait for that.

436
00:26:42,620 --> 00:26:47,210
X ones, meaning the leader
acknowledges the right in case of

437
00:26:47,210 --> 00:26:49,930
leader failure, data loss is possible.

438
00:26:50,190 --> 00:26:55,300
So in this case, so the leader
replica will tell to the producer

439
00:26:55,310 --> 00:26:56,990
that, okay, I got your message.

440
00:26:57,480 --> 00:27:02,580
I written everything to, to my
store and, but if the failure

441
00:27:02,580 --> 00:27:05,150
will occur, data loss is possible.

442
00:27:05,170 --> 00:27:06,950
As I said here on the slide.

443
00:27:07,750 --> 00:27:09,550
And that's all in sync replica.

444
00:27:09,550 --> 00:27:12,690
So does it mean that all
followers of their, leader,

445
00:27:13,210 --> 00:27:14,660
should acknowledge the message?

446
00:27:14,750 --> 00:27:18,040
Of course it has highest
durability and highest latency.

447
00:27:19,480 --> 00:27:19,840
Okay.

448
00:27:20,430 --> 00:27:23,150
let's revisit our requirements
for a synchronous processing.

449
00:27:23,310 --> 00:27:29,820
Again, communication through messages
done, the durable message store done.

450
00:27:31,250 --> 00:27:35,270
One at a time message can be,
this is interesting thing.

451
00:27:36,540 --> 00:27:37,460
Consumer requirements.

452
00:27:37,650 --> 00:27:40,620
We want to have one at a
time message processing.

453
00:27:40,830 --> 00:27:43,610
Each transfer should be
processed by a single consumer.

454
00:27:43,970 --> 00:27:46,060
Consumers should be able
to scale horizontally.

455
00:27:46,560 --> 00:27:49,190
And now we have an
abstraction as consumer group.

456
00:27:49,650 --> 00:27:53,700
So we want to scale our application
by servers horizontally.

457
00:27:53,880 --> 00:27:56,280
So It is super simple in Kafka, actually.

458
00:27:56,280 --> 00:27:59,280
So we have a topic, we have multiple
partitions, we have consumer group.

459
00:27:59,520 --> 00:28:04,230
So we can tell Kafka that, this
instance, this consumer, is

460
00:28:04,230 --> 00:28:06,470
related to consumer group A.

461
00:28:06,970 --> 00:28:12,780
and when we Deploy two instances,
this, the same consumer group

462
00:28:12,830 --> 00:28:15,340
will be respected in, in Kafka.

463
00:28:15,340 --> 00:28:20,140
So partition one will be, distributed
to consumer instance one, partition

464
00:28:20,140 --> 00:28:22,400
two and three to consumer instance two.

465
00:28:23,280 --> 00:28:24,190
here's the limitation.

466
00:28:24,200 --> 00:28:25,450
So you can't have.

467
00:28:26,890 --> 00:28:32,090
Consumer instances, more than a
number of partitions in your topic.

468
00:28:32,230 --> 00:28:36,420
So just keep in mind, if you have,
for example, 16 partitions in your

469
00:28:36,420 --> 00:28:40,720
topic, the maximum number of your
applications, application instances

470
00:28:40,760 --> 00:28:46,160
in the same group will be 16 as
well, and we've got everything we

471
00:28:46,160 --> 00:28:49,390
need messaging system using Kafka.

472
00:28:49,390 --> 00:28:54,070
So here is everything that we,
We decided to change, right?

473
00:28:54,070 --> 00:28:59,860
So we combined the system from,
abstractions that Kafka provides to us.

474
00:29:00,280 --> 00:29:04,590
their units, For producers, we have
compliance commons, we have verification

475
00:29:04,640 --> 00:29:07,640
commons, we have payments commons,
we have many commons producers.

476
00:29:08,130 --> 00:29:13,590
We have partitions for our transfer
topic, for transfer A, for transfer B,

477
00:29:13,590 --> 00:29:16,110
for transfer C, and we have consumers.

478
00:29:16,670 --> 00:29:23,730
So here is on the diagram, we have a
routine of how we can handle and going

479
00:29:23,740 --> 00:29:26,180
see one at time processing, right?

480
00:29:26,230 --> 00:29:31,510
Because in the consumer group, we
go and see that, the order of events

481
00:29:31,540 --> 00:29:34,460
within a single partition is ordered.

482
00:29:36,130 --> 00:29:40,510
Also, we have consumer groups,
so we can handle multiple

483
00:29:40,570 --> 00:29:42,270
partitions at the same time.

484
00:29:42,550 --> 00:29:48,300
And there is no option to get the, Message
for transfer in multiple consumers at

485
00:29:48,320 --> 00:29:53,050
the same time, just because of routing,
because we have, because Kafka will route.

486
00:29:53,820 --> 00:29:56,990
transfer a into partition one,
transfer B into partition, do you

487
00:29:56,990 --> 00:29:58,420
transfer C into partition three?

488
00:29:58,420 --> 00:29:59,030
And that's all.

489
00:30:00,290 --> 00:30:00,640
Okay.

490
00:30:01,460 --> 00:30:03,140
what about the actor model actually here?

491
00:30:03,480 --> 00:30:04,780
let's review.

492
00:30:05,340 --> 00:30:06,750
It's core concepts.

493
00:30:08,340 --> 00:30:11,840
First of all, actor is
a fundamental units.

494
00:30:12,930 --> 00:30:18,520
A single message passing, state
isolation, sequential message

495
00:30:18,520 --> 00:30:23,560
processing, location transparency,
fault tolerance, and scalability.

496
00:30:25,405 --> 00:30:30,495
It can be implemented in our
asynchronous system, right?

497
00:30:30,535 --> 00:30:33,705
Most of this, but let's
do important disclaimer.

498
00:30:34,145 --> 00:30:39,895
This is not full implementation of actor
model as Erlang or Kafka or maybe another

499
00:30:39,895 --> 00:30:44,695
framework, we will use the actor model as
a concept here just to build the system.

500
00:30:45,205 --> 00:30:49,315
There is no need to implement all feature
because we don't need supervision,

501
00:30:49,425 --> 00:30:53,255
location, transparency, et cetera, because
we just don't need it because Kafka

502
00:30:53,795 --> 00:30:57,885
got, all complexity, into their domain.

503
00:30:57,895 --> 00:31:00,385
So we could think about
actors as a product.

504
00:31:00,845 --> 00:31:04,925
processor of commons with some
mailbox of messages for processing

505
00:31:05,195 --> 00:31:09,785
and for and state isolation in our
actual state of the transfer or

506
00:31:09,785 --> 00:31:11,945
maybe user or recipient or whatever.

507
00:31:12,025 --> 00:31:17,725
we only need the fact that we
can, Interact within, messages.

508
00:31:17,775 --> 00:31:19,105
We have state isolation.

509
00:31:19,305 --> 00:31:21,245
We have sequential message processing.

510
00:31:21,415 --> 00:31:25,445
We call it a one at a time message
processing before we have full

511
00:31:25,485 --> 00:31:30,155
tolerance because if our server will
fail, consumer, and restarted after,

512
00:31:30,475 --> 00:31:34,595
it will join consumer group and
Kafka will rebalance, We'll rebalance

513
00:31:34,595 --> 00:31:38,955
partitions over new set of consumers
and of course, scalability, so we can

514
00:31:38,965 --> 00:31:41,915
deploy as many consumers as we want.

515
00:31:42,085 --> 00:31:46,255
Of course, it, it has a limitation up
to the number of, of, topic partitions.

516
00:31:47,965 --> 00:31:48,415
Okay.

517
00:31:48,765 --> 00:31:51,185
Let's try to implement
Acro model in our system.

518
00:31:51,285 --> 00:31:51,615
Yeah.

519
00:31:51,655 --> 00:31:54,435
We will ignore some things
of factor model here.

520
00:31:54,575 --> 00:31:58,395
It's just an inspiration and how
it can be implemented in, in a

521
00:31:58,395 --> 00:31:59,905
tool that can help us to build it.

522
00:32:01,785 --> 00:32:03,635
Let's start with interfaces.

523
00:32:03,745 --> 00:32:05,495
So we have, we need to have a storage.

524
00:32:05,795 --> 00:32:07,105
the storage is quite simple.

525
00:32:07,115 --> 00:32:11,945
So we have to create a, new, just to
produce a new structure to work with that.

526
00:32:12,175 --> 00:32:15,475
So key here is for a key as for a state.

527
00:32:16,295 --> 00:32:18,505
So new, get, and put,
everything is simple.

528
00:32:18,695 --> 00:32:20,815
and as we discussed
before, we have a package.

529
00:32:20,815 --> 00:32:26,805
Postgres, it can be sharded, maybe
not, so it doesn't matter, so it

530
00:32:27,105 --> 00:32:32,475
can be Cassandra, it can be whatever
you need, even local caches, Redis,

531
00:32:32,495 --> 00:32:35,025
just MongoDB, whatever you need.

532
00:32:36,225 --> 00:32:40,605
We have an actor, so actor
should have just one single

533
00:32:40,615 --> 00:32:42,625
function, it should be receive.

534
00:32:43,575 --> 00:32:47,075
Receive function gets state and comment.

535
00:32:47,785 --> 00:32:49,845
And should return new state.

536
00:32:50,115 --> 00:32:56,025
So if it's just a function that knows
about current state and the comment

537
00:32:56,555 --> 00:33:04,775
and produces new state or a force
error mailbox, this is after mailbox

538
00:33:04,775 --> 00:33:10,815
for our Kafka, we have to know about
key state and comment, and that's all.

539
00:33:11,235 --> 00:33:14,265
So we will deploy our application.

540
00:33:15,040 --> 00:33:19,180
And we will start consuming,
messages from, transfer topic.

541
00:33:19,930 --> 00:33:27,470
And, this actor mailbox, we'll know about,
roads in key state and comment to call

542
00:33:27,510 --> 00:33:30,690
actor and the fourth common producer.

543
00:33:31,020 --> 00:33:35,970
Common producer is something
just to tell to, to a actor

544
00:33:36,060 --> 00:33:38,560
about a new common for message.

545
00:33:38,920 --> 00:33:41,860
So of course it should know
about key and column as well.

546
00:33:41,870 --> 00:33:48,920
So for example, through this interface,
we can tell to, compliance system

547
00:33:48,950 --> 00:33:53,930
or even self transfer, topic about
a, Hey, you have a new comment here.

548
00:33:54,915 --> 00:33:55,395
Oh yeah.

549
00:33:55,395 --> 00:33:57,865
And this is about what,
what this types means here.

550
00:33:58,795 --> 00:34:02,965
Kafka consumer, actually,
everything is super simple here.

551
00:34:03,145 --> 00:34:08,495
So record, Kafka record, has a key
and has a value key and value, just

552
00:34:08,495 --> 00:34:10,855
the pure area of bytes and the soul.

553
00:34:10,865 --> 00:34:15,495
So we have to somehow to decode
it into the time that we need.

554
00:34:16,085 --> 00:34:18,675
also we have a storage,
so we will try to get.

555
00:34:18,960 --> 00:34:22,380
the state from storage, if
it's not found, we will try to

556
00:34:22,410 --> 00:34:24,420
create a new, new stage here.

557
00:34:24,940 --> 00:34:30,890
After that, we pass state and
comment into, actor and we are

558
00:34:30,890 --> 00:34:32,030
getting new state, that's all.

559
00:34:32,030 --> 00:34:34,690
So everything we need is
just to update the state.

560
00:34:35,870 --> 00:34:37,100
And here is the problem.

561
00:34:37,790 --> 00:34:40,950
of course, we have a database and Kafka.

562
00:34:41,250 --> 00:34:44,070
So that means that we have a
problem with double writes.

563
00:34:45,055 --> 00:34:48,205
Double write problem is when you
have a Postgres in Kafka and you

564
00:34:48,255 --> 00:34:53,245
cannot go and see the transaction
here because if Postgres will fail,

565
00:34:53,665 --> 00:34:55,725
overall, transaction will fail.

566
00:34:56,035 --> 00:34:59,985
If Kafka will fail, we write, we
write state to Postgres, but we

567
00:34:59,985 --> 00:35:01,795
cannot send, messages to Kafka.

568
00:35:01,965 --> 00:35:03,195
So let's adjust.

569
00:35:03,335 --> 00:35:05,605
our code a bit.

570
00:35:05,905 --> 00:35:07,175
It goes old box pattern.

571
00:35:07,185 --> 00:35:08,755
Actually, it's quite popular.

572
00:35:08,755 --> 00:35:11,325
So I believe many of you know about it.

573
00:35:11,715 --> 00:35:18,305
so we have a new transfer, new
transaction, and our storage should take

574
00:35:18,665 --> 00:35:21,275
transaction as a, as an argument as well.

575
00:35:22,025 --> 00:35:26,835
Everything we need is to produce not
only new state, but effects as well.

576
00:35:26,905 --> 00:35:30,825
Effect is something that
should be executed later.

577
00:35:30,855 --> 00:35:32,395
For example, thinking to Kafka.

578
00:35:32,995 --> 00:35:38,405
in our case, we have new state
and array of facts, and we need

579
00:35:38,415 --> 00:35:41,335
to persist everything altogether
in one single transaction.

580
00:35:41,385 --> 00:35:42,825
So there is no Kafka at all.

581
00:35:42,975 --> 00:35:49,225
If you need to send something to
compliance, actor or user actor, you can

582
00:35:49,235 --> 00:35:51,565
just put everything in your database.

583
00:35:53,165 --> 00:35:55,065
Here is, everything is solved actually.

584
00:35:55,115 --> 00:36:01,625
but the last thing is how to deliver
your messages from Postgres to Kafka.

585
00:36:02,025 --> 00:36:05,595
actually you have many choices
since you have a Debezium, you

586
00:36:05,595 --> 00:36:09,465
have Debezium connector, so it can
listen for while log and, deliver

587
00:36:09,465 --> 00:36:11,825
messages to Kafka in your topic.

588
00:36:11,825 --> 00:36:14,975
Also, you can write your own,
your way, how to manage this.

589
00:36:14,975 --> 00:36:16,435
It's just up to you.

590
00:36:18,100 --> 00:36:22,410
Another important thing is about toxic
messages, as we discussed before,

591
00:36:22,510 --> 00:36:25,590
we will try to consume the message.

592
00:36:25,630 --> 00:36:31,430
If something goes wrong, for example,
we've got a transfer common, receive

593
00:36:31,430 --> 00:36:33,590
payment, but payment was already received.

594
00:36:33,600 --> 00:36:36,030
So that means that, we should.

595
00:36:36,440 --> 00:36:38,950
return some error and ignore the message.

596
00:36:39,440 --> 00:36:44,080
And of course, if you have a large
core base or many developers, so it's

597
00:36:44,190 --> 00:36:48,440
possible that you potentially can miss
something, and, process it incorrectly.

598
00:36:48,460 --> 00:36:49,720
here can be toxic messages.

599
00:36:49,720 --> 00:36:53,080
Of course, if you can, you can
build, many things on top of it,

600
00:36:53,080 --> 00:36:54,650
but, let's discuss it a bit later.

601
00:36:55,110 --> 00:36:57,490
toxic messages are messages
that cannot be proceed.

602
00:36:58,210 --> 00:36:58,780
messages.

603
00:36:59,350 --> 00:37:04,370
can be caused by incorrect message
format, incorrect message version,

604
00:37:04,710 --> 00:37:08,140
incorrect message data, or logic errors.

605
00:37:08,370 --> 00:37:13,180
For example, incorrect actor state
and incorrect message processing.

606
00:37:13,820 --> 00:37:14,790
What to do with that?

607
00:37:14,800 --> 00:37:18,240
So for example, you can
implement, Delayed messages.

608
00:37:18,240 --> 00:37:23,100
You can implement a counter in
some system just to count how many

609
00:37:23,100 --> 00:37:24,870
times your external service failed.

610
00:37:25,120 --> 00:37:28,460
And right after that, you can
mark your message as a toxic.

611
00:37:28,460 --> 00:37:32,020
So you cannot process it,
10 times in 10 seconds.

612
00:37:32,030 --> 00:37:37,710
That means that it should be toxic or,
division by zero, or, incorrect status

613
00:37:37,710 --> 00:37:41,390
for your state or anything else, actually.

614
00:37:41,400 --> 00:37:42,900
So the fact that, that.

615
00:37:43,625 --> 00:37:46,915
The toxic messages are messages
that you should ignore, but

616
00:37:46,945 --> 00:37:48,365
you should handle it somehow.

617
00:37:49,275 --> 00:37:53,185
So this is just a simple,
dead letters queue.

618
00:37:53,455 --> 00:37:57,475
So that means that, new state,
after receive method returned.

619
00:37:58,315 --> 00:38:03,685
Toxic error, and we want to
handle this letter somehow later.

620
00:38:03,885 --> 00:38:06,825
So this is dead letters queue
right in the Kafka as well.

621
00:38:07,115 --> 00:38:10,545
And, we are producing the same
record to, dead letters queue.

622
00:38:10,765 --> 00:38:15,275
We can run some monitoring analytics,
anomaly detection and many more.

623
00:38:16,475 --> 00:38:19,835
And let's, let's review final
implementation for Kafka consumer.

624
00:38:20,395 --> 00:38:21,315
That's before we have.

625
00:38:21,450 --> 00:38:24,820
We have state and storage.

626
00:38:25,030 --> 00:38:27,910
We have new state effects and error.

627
00:38:28,130 --> 00:38:32,270
If error is toxic, so we can
send it to dead letters for

628
00:38:32,520 --> 00:38:34,790
investigation or for alerting.

629
00:38:35,270 --> 00:38:38,350
And, we have our box
pattern here and that's all.

630
00:38:38,700 --> 00:38:45,240
So actually this is implementation
for your Kafka consumer to work

631
00:38:45,550 --> 00:38:49,420
with, It was a transfer or with
your actor, everything's done here.

632
00:38:50,240 --> 00:38:55,650
And, let's just take a look
on a small function here.

633
00:38:55,680 --> 00:38:56,460
So this is.

634
00:38:57,390 --> 00:39:01,610
Or how to handle comments, in
your transfer actor as an example.

635
00:39:01,620 --> 00:39:03,260
we've got received payment comment.

636
00:39:03,670 --> 00:39:07,300
for example, we already checked
that the status is a payment waiting

637
00:39:07,300 --> 00:39:09,330
or another status that we need.

638
00:39:09,700 --> 00:39:12,750
we are computing new state
by applying payment details.

639
00:39:13,040 --> 00:39:14,770
We are creating new side effects.

640
00:39:15,230 --> 00:39:19,890
for example, send checks command
to, transfer to move it further.

641
00:39:20,200 --> 00:39:24,320
And, we can produce new transfer
and, for analytic purposes and

642
00:39:24,340 --> 00:39:25,870
return just a new state and effect.

643
00:39:25,890 --> 00:39:26,290
That's all.

644
00:39:26,290 --> 00:39:30,990
So this is the whole, infrastructure
logic for, for your actor.

645
00:39:31,240 --> 00:39:31,790
That's all.

646
00:39:31,800 --> 00:39:34,760
So you can reach with your own logic.

647
00:39:35,420 --> 00:39:40,580
And, everything, all complexity
is not here, isn't Kafka.

648
00:39:40,590 --> 00:39:43,040
So your code is quite simple here.

649
00:39:44,380 --> 00:39:45,770
let's make a conclusion here.

650
00:39:46,260 --> 00:39:50,230
so by combining principles of actor model.

651
00:39:51,265 --> 00:39:56,205
And Kafka is a messaging backbone
with its own partners, of course, with

652
00:39:56,225 --> 00:40:01,645
additional, deployment unit, that's Kafka
servers and, things that you need, to,

653
00:40:02,055 --> 00:40:06,955
to set up Kafka correctly, actually,
for example, Amazon MSK does it pretty

654
00:40:06,955 --> 00:40:13,285
well, actually, and, all concurrency,
Issues can be handled by Kafka just

655
00:40:13,315 --> 00:40:18,865
because you're writing, imperative
code, you're doing things one by one.

656
00:40:18,865 --> 00:40:24,055
You don't need to think about,
Hey, here can be another product.

657
00:40:24,645 --> 00:40:26,855
So we need to lock it somehow.

658
00:40:27,245 --> 00:40:31,745
So we have a, just a key of messages
and a synchronous communication.

659
00:40:32,005 --> 00:40:37,155
So that means that we can just place a
comment for a transfer in order with Q.

660
00:40:37,545 --> 00:40:41,995
And, order topic in order to partition
in the topic and, your transfer

661
00:40:41,995 --> 00:40:44,445
consumer will handle it one by one.

662
00:40:45,115 --> 00:40:48,395
And that means that you just
evaded concurrency at all.

663
00:40:48,595 --> 00:40:49,915
So there is no locks.

664
00:40:49,965 --> 00:40:52,685
The, there is no concurrent,
And that the based on elections,

665
00:40:52,995 --> 00:40:57,565
you don't need to think about,
I, I need to go for some data.

666
00:40:57,735 --> 00:40:59,005
data can be inconsistent.

667
00:40:59,035 --> 00:41:02,955
If you need something, just
place a comment and the, another

668
00:41:02,985 --> 00:41:04,885
actor can tell the result back.

669
00:41:05,575 --> 00:41:09,205
So everything you need, just, finish
state machine for your entity, or even

670
00:41:09,205 --> 00:41:12,405
if you, maybe you don't need, FSM at all.

671
00:41:12,555 --> 00:41:16,645
So you have simple counter,
you have, dead letters check.

672
00:41:16,815 --> 00:41:19,105
actually it doesn't need, actors at all.

673
00:41:19,315 --> 00:41:24,545
So there is a lot of options how to,
how to manage concurrency, but this

674
00:41:24,545 --> 00:41:28,345
approach, help us just to avoid it at all.

675
00:41:29,475 --> 00:41:30,045
Thank you.

676
00:41:30,295 --> 00:41:32,055
And questions.

