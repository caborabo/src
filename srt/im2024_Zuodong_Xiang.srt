1
00:00:00,000 --> 00:00:03,770
Today's talk is, what
can possibly go wrong?

2
00:00:04,380 --> 00:00:06,950
Lessons in building resilient systems.

3
00:00:07,950 --> 00:00:11,790
The motivation of this talk is
that we all know the cost of

4
00:00:11,790 --> 00:00:13,900
maintenance and software is huge.

5
00:00:14,780 --> 00:00:18,920
and we all know that the outage
sucks for everyone, especially the

6
00:00:18,920 --> 00:00:23,520
on calls to wake up in the middle of
the night, solving sets, thinking,

7
00:00:23,640 --> 00:00:25,540
what am I doing with my life?

8
00:00:26,065 --> 00:00:30,545
You might be waking up your
neighbors, yourself, your spouse,

9
00:00:30,545 --> 00:00:33,905
your dogs, it's not a pretty scene.

10
00:00:34,894 --> 00:00:41,094
And the question I keep asking
myself is how can we prevent outage

11
00:00:41,105 --> 00:00:42,905
from happening in the first place?

12
00:00:43,945 --> 00:00:49,125
And this talk will talk about a few,
real life scenarios and the lessons we'll

13
00:00:49,125 --> 00:00:54,707
learn and we'll put it all together on
how do we build resilient, software.

14
00:00:55,977 --> 00:00:58,967
Let's first start with real
step happen and their lessons.

15
00:00:58,967 --> 00:01:00,897
The first one is flood of traffic.

16
00:01:01,457 --> 00:01:06,787
You get alert about CPU near 100 percent
of your host, followed by availability

17
00:01:06,797 --> 00:01:09,077
drop, high client side error rate.

18
00:01:10,497 --> 00:01:15,377
The background can be there is
unpredictable side wide event

19
00:01:15,427 --> 00:01:17,217
with just simply more traffic.

20
00:01:17,247 --> 00:01:20,337
We're sending traffic
to you, more traffic.

21
00:01:20,337 --> 00:01:25,912
And then you're seeing in auto scaling,
Panel, if you're using cloud provider

22
00:01:27,002 --> 00:01:30,982
that the host keep getting replaced
because they failed health check

23
00:01:31,542 --> 00:01:33,212
and you're like, what's going on?

24
00:01:33,212 --> 00:01:35,912
I'm just getting more host
to it, but they keep failing.

25
00:01:36,492 --> 00:01:37,772
The host stays down.

26
00:01:38,592 --> 00:01:43,032
and then you see that the host is down
because it failed the health check.

27
00:01:43,192 --> 00:01:49,842
So when the host has 100 percent CPU,
it cannot respond pin back to the.

28
00:01:50,367 --> 00:01:55,557
Out of scaling service and out of scaling,
console will think your host is done.

29
00:01:56,597 --> 00:02:00,357
So you really can't spin
up any host back on.

30
00:02:00,507 --> 00:02:01,577
What do you do then?

31
00:02:02,517 --> 00:02:06,537
you can, you might then realize that
you just have dropped traffic for

32
00:02:06,537 --> 00:02:11,757
it to do, for the host to spin back
on when host doesn't get traffic.

33
00:02:12,387 --> 00:02:20,637
It doesn't go down for 200 percent
CPU, so you can draw traffic and you

34
00:02:20,637 --> 00:02:25,997
can just a mass connection to the
host that instead of getting all these

35
00:02:26,027 --> 00:02:30,667
connection to your host that you cannot
handle, you can set a mass connection

36
00:02:30,687 --> 00:02:35,867
to handle only the connections you're
possible to have for your host.

37
00:02:36,912 --> 00:02:41,482
The lesson here is that, for the long
term, you should do a load testing on max

38
00:02:41,482 --> 00:02:43,982
connection a host can possibly handle.

39
00:02:44,412 --> 00:02:48,942
So you don't run into issues of a host
getting too much traffic and lead to

40
00:02:48,952 --> 00:02:54,162
100 percent CPU on your host, and you
cannot spin up any host because it keep

41
00:02:54,162 --> 00:02:56,232
going down because of the traffic, right?

42
00:02:56,492 --> 00:03:00,697
It's a Really, bad loop
of, of situation here.

43
00:03:01,977 --> 00:03:03,817
The next one is a retry storm.

44
00:03:03,897 --> 00:03:09,237
So this, you can get alert or something
like client bill ability below X percent.

45
00:03:10,097 --> 00:03:15,217
and then if you take a little closer,
you see the traffic goes up very high

46
00:03:15,277 --> 00:03:21,437
and, And the crux of the problem here
is that a lot of services like to see,

47
00:03:22,177 --> 00:03:24,357
when they see error, then they retry.

48
00:03:25,197 --> 00:03:28,857
So if each microservices
retries, the number of calls

49
00:03:28,867 --> 00:03:30,847
to database will then multiply.

50
00:03:31,517 --> 00:03:33,387
let's take this example on the right here.

51
00:03:33,387 --> 00:03:36,827
So you have, let's say, a
site calls the service once.

52
00:03:37,322 --> 00:03:42,032
Service 1 calls service 2, and
service 2 calls the database, and

53
00:03:42,032 --> 00:03:44,352
the database says it's not available.

54
00:03:44,692 --> 00:03:50,272
And then service 2 retries two
times, and it's still not available.

55
00:03:50,292 --> 00:03:53,012
So you go back to service 1
saying it's not available.

56
00:03:53,032 --> 00:03:54,107
Service 1 says it's not available.

57
00:03:54,427 --> 00:03:58,447
Let me retry two times, so it
goes back to service two for

58
00:03:58,477 --> 00:04:00,087
each of those service requests.

59
00:04:00,137 --> 00:04:04,577
It retries two times again, so the
total retry now becomes six times.

60
00:04:05,747 --> 00:04:09,717
And just imagine if you have
more services in between and more

61
00:04:09,717 --> 00:04:14,517
retries for those services, this
can easily multiply and multiply.

62
00:04:15,017 --> 00:04:19,877
And then the service at the bottom
layer will get overwhelmed, cannot

63
00:04:19,897 --> 00:04:21,777
handle any possible traffic.

64
00:04:22,647 --> 00:04:28,147
The mitigation here, happened along
the big shopping events that it was

65
00:04:28,167 --> 00:04:32,857
to drop majority of the traffic,
significant dropping traffic to

66
00:04:32,857 --> 00:04:37,377
mitigate this, and later added, retry
configuration changes and more hosts.

67
00:04:38,327 --> 00:04:42,587
So the lesson here is to really
standardize retry logic, such as

68
00:04:43,067 --> 00:04:48,637
exponential retry, respect to too
many requests, HTTP code, and also,

69
00:04:48,727 --> 00:04:50,557
add circuit breaker for dependency.

70
00:04:50,597 --> 00:04:57,067
The circuit breakers are ways to say,
hey, dependency getting too many errors.

71
00:04:57,147 --> 00:04:59,707
I just stopped calling that
dependency because I don't want to.

72
00:05:00,277 --> 00:05:06,217
Make matter even worse for that dependency
and being good citizens of not causing

73
00:05:06,227 --> 00:05:09,937
a retry storm to, services downstream.

74
00:05:10,937 --> 00:05:13,607
This one is about Plan B went poorly.

75
00:05:13,927 --> 00:05:18,347
So a letter you might get as dependency
availability below X percent.

76
00:05:19,227 --> 00:05:22,977
So sometimes there are more than one
dependencies to satisfy what we need.

77
00:05:23,607 --> 00:05:27,417
in the case of circuit breakers, if there
is a trouble dependency, we send back.

78
00:05:27,597 --> 00:05:32,927
Traffic to another dependency
now dependency be also service

79
00:05:32,927 --> 00:05:34,917
on traffic in many cases.

80
00:05:34,967 --> 00:05:39,727
So dependency be what you immediately
go down because it's unable to handle

81
00:05:39,727 --> 00:05:45,467
this newly, send traffic and cost
problem to services using dependency be.

82
00:05:45,467 --> 00:05:46,877
So it may matter worse.

83
00:05:46,927 --> 00:05:50,387
Not only your service not working,
you also make other services failing.

84
00:05:51,712 --> 00:05:57,362
So the mitigation here is to really focus
on fixing dependency A, and, try to fix

85
00:05:57,362 --> 00:06:03,002
what's issue in there, and, A lesson here
for the long term is to not introduce

86
00:06:03,002 --> 00:06:08,582
a backup dependency unless it's really
been prepared well enough, such as acting

87
00:06:08,582 --> 00:06:10,982
as a lever for high velocity events.

88
00:06:11,772 --> 00:06:17,572
And, in day to day operation,
focus on the plan A that will

89
00:06:18,372 --> 00:06:20,022
get your service up and running.

90
00:06:22,392 --> 00:06:24,572
The next one is a very common one.

91
00:06:24,572 --> 00:06:26,002
It's a back commit.

92
00:06:26,242 --> 00:06:29,862
An alert you might be getting is
server availability dropped 100%.

93
00:06:31,272 --> 00:06:34,802
A back commit that can go out of
production is not really the fault

94
00:06:34,822 --> 00:06:39,362
of the commit owner, but rather
lack of proper process to prevent it

95
00:06:39,362 --> 00:06:40,922
from happening in the first place.

96
00:06:40,922 --> 00:06:44,202
You think about lines of defenses, right?

97
00:06:44,222 --> 00:06:49,892
So you have code review, You have
testing that covers all aspects of the

98
00:06:49,892 --> 00:06:56,292
flow, such as unit testing, end to end
testing, menu testing, QA testing, right?

99
00:06:56,292 --> 00:06:57,372
And the list goes on.

100
00:06:57,392 --> 00:06:59,752
The testing did not catch any of this.

101
00:07:00,362 --> 00:07:03,722
And there are the gatings that
people typically add to make sure

102
00:07:03,722 --> 00:07:08,582
you can flip it back or flip it on
and monitor the feature releases.

103
00:07:09,367 --> 00:07:12,717
And there are things like change
management, risk assessments, and

104
00:07:12,717 --> 00:07:16,207
there are pipelines to get all
these changes, put it all together.

105
00:07:16,957 --> 00:07:19,387
So let me ask you this.

106
00:07:19,427 --> 00:07:24,767
If you have all of this and still cannot
catch the bad commit, is it really the

107
00:07:24,957 --> 00:07:27,127
fault of the person writing the commit?

108
00:07:28,407 --> 00:07:30,267
Especially that hasn't happened before?

109
00:07:30,947 --> 00:07:31,657
No, right?

110
00:07:31,977 --> 00:07:37,107
So every time there's a bad commit exposed
to something that's missing, is it?

111
00:07:37,747 --> 00:07:42,577
Did this commit somehow bypass a
code review that did not catch this?

112
00:07:42,607 --> 00:07:43,497
This can happen.

113
00:07:43,507 --> 00:07:43,867
Did it?

114
00:07:44,292 --> 00:07:49,512
Not have a proper testing or the, or
did not follow the guardrail of what's

115
00:07:49,562 --> 00:07:51,642
necessary to push those changes out.

116
00:07:52,782 --> 00:07:55,662
And somehow we miss all this.

117
00:07:56,112 --> 00:08:03,072
So every bad commit actually carries
some positive lesson for you to reflect

118
00:08:03,122 --> 00:08:07,382
what was missing in the release process
and how you, how can we catch this and

119
00:08:07,382 --> 00:08:09,842
prevent it from happening in the future.

120
00:08:12,002 --> 00:08:17,667
So in many ways bad commit is
actually a good way to improve your

121
00:08:17,977 --> 00:08:20,867
resilience, resiliency in your services.

122
00:08:21,867 --> 00:08:25,797
Now, the next one is lack
of sufficient ownerships.

123
00:08:26,727 --> 00:08:28,577
This one sometimes can be comical.

124
00:08:28,577 --> 00:08:33,247
You might receive an escalation of
people asking you, hey, user reported

125
00:08:33,257 --> 00:08:34,727
they are able to use this feature.

126
00:08:34,727 --> 00:08:36,847
And you're thinking, what is this feature?

127
00:08:36,847 --> 00:08:37,917
I never heard about it.

128
00:08:38,367 --> 00:08:42,737
And then after some back and forth,
You're actually the team owning this

129
00:08:42,737 --> 00:08:47,097
change and feature and, you have
to fix it and also be accountable

130
00:08:47,107 --> 00:08:49,917
for the, for the set of impact.

131
00:08:50,627 --> 00:08:55,207
So sometimes this can happen when
there is a team changes, the, some code

132
00:08:55,227 --> 00:08:59,217
don't have sufficient ownership, and
this can get to a very bad situations.

133
00:08:59,867 --> 00:09:03,277
so you don't, you want to wait
that, all right, you want to

134
00:09:03,327 --> 00:09:06,627
really make sure there's a process
of transferring co ownership.

135
00:09:07,227 --> 00:09:09,747
You want to make sure there
is sufficient annotations.

136
00:09:10,417 --> 00:09:14,047
Of co ownership, whether that's
in code, whether that's tooling,

137
00:09:14,477 --> 00:09:19,107
whether that's wiki, can all help
you to avoid this, from happening.

138
00:09:21,927 --> 00:09:27,327
S script is also very notorious,
some of the very best apps are

139
00:09:27,637 --> 00:09:31,497
because of people running scripts
and not testing as efficiently.

140
00:09:32,547 --> 00:09:36,477
Alright, you might get an alert saying
system's down at 0 percent availability,

141
00:09:37,207 --> 00:09:39,037
you're wondering what's going on.

142
00:09:39,782 --> 00:09:42,312
And it turned out to be a script
reference in the wrong book

143
00:09:42,312 --> 00:09:44,032
that was not tested thoroughly.

144
00:09:44,662 --> 00:09:49,152
And, the lesson here is to really treat
the production impacting scripts or

145
00:09:49,152 --> 00:09:54,412
changes the same rigor as production
commitments, such as sufficient

146
00:09:54,462 --> 00:09:58,422
testing, QAs, and proper code reviews.

147
00:09:59,062 --> 00:10:03,632
because remember, production changes
are all very risky, and if you

148
00:10:03,632 --> 00:10:05,302
don't test your script properly.

149
00:10:06,172 --> 00:10:11,032
Something bad can happen, especially
if it's, in the wrong book, that people

150
00:10:11,122 --> 00:10:13,172
are very likely to execute that script.

151
00:10:13,192 --> 00:10:16,452
So make sure you have the
script tested or any production

152
00:10:16,452 --> 00:10:18,692
impacting changes tested properly.

153
00:10:20,672 --> 00:10:26,142
Okay, now let's talk about the
next topic, which is, prevention.

154
00:10:26,842 --> 00:10:28,412
Let's put everything together.

155
00:10:31,272 --> 00:10:36,772
The first thing is to have A defensive
coding practices, a lot of the, that's

156
00:10:36,772 --> 00:10:42,372
can actually prevent it if you have,
a lot of defensive coding practices.

157
00:10:43,012 --> 00:10:48,082
But the 1st 1 is to pay your
changes in many companies, people

158
00:10:48,082 --> 00:10:50,572
typically get their changes inside.

159
00:10:51,437 --> 00:10:55,937
some gating so you can use
some service side knobs to

160
00:10:56,737 --> 00:10:58,607
increase allocation over time.

161
00:10:59,277 --> 00:11:00,527
This is a great way to

162
00:11:03,187 --> 00:11:06,137
quickly roll back and
also gradually roll out.

163
00:11:06,147 --> 00:11:09,857
So you don't impact as many users.

164
00:11:09,867 --> 00:11:12,227
Impact radius will be relatively small.

165
00:11:13,177 --> 00:11:15,767
And remember to log any exceptions.

166
00:11:16,447 --> 00:11:21,667
Corner cases and also remember to have
critical information in your login.

167
00:11:21,727 --> 00:11:25,447
If you don't have critical information,
sometimes people have to actually

168
00:11:25,457 --> 00:11:27,157
make that code changes right away.

169
00:11:27,247 --> 00:11:30,497
And with deployment to finish,
you don't want that to happen

170
00:11:32,817 --> 00:11:35,387
and do not overly rely on backup options.

171
00:11:35,447 --> 00:11:41,577
Like the example I mentioned earlier, it's
better to fail fast and focus on plan A.

172
00:11:43,627 --> 00:11:47,437
I've seen this many times, like
some of the steps are just because.

173
00:11:48,077 --> 00:11:49,997
It doesn't have proper error handling.

174
00:11:50,027 --> 00:11:52,377
This can be like no pointer exceptions.

175
00:11:52,397 --> 00:11:53,937
In Java that happens a lot.

176
00:11:54,497 --> 00:11:58,057
So make sure you have, no
safe, coding practices.

177
00:11:58,107 --> 00:12:01,727
if you're using a more, no
safe languages like Kotlin,

178
00:12:01,787 --> 00:12:02,817
you wouldn't have this problem.

179
00:12:02,827 --> 00:12:05,817
A lot of modern code, programming
languages have this nowadays.

180
00:12:06,717 --> 00:12:08,587
remember send a timeout.

181
00:12:08,607 --> 00:12:12,007
So if you're a service calling a
dependency and you don't have a

182
00:12:12,007 --> 00:12:15,917
timeout and a dependency doesn't
have a timeout either, then you got

183
00:12:15,917 --> 00:12:18,247
a situation where it hanks, right?

184
00:12:18,267 --> 00:12:20,537
So you want to away that from happening.

185
00:12:22,262 --> 00:12:27,452
And last thing I want to remind is
having a good retry practice, use

186
00:12:27,492 --> 00:12:34,132
exponential retry, and then, use it
very sparingly to avoid a retry storm.

187
00:12:35,132 --> 00:12:36,472
The next session we'll
talk about is detections.

188
00:12:37,552 --> 00:12:38,812
This has three parts.

189
00:12:39,042 --> 00:12:40,442
The first part is alerts.

190
00:12:41,092 --> 00:12:44,802
So for alerts, having an operational goal
is very important when setting alerts.

191
00:12:44,802 --> 00:12:50,092
So why you're setting alerts is
to make sure, is it availability

192
00:12:50,092 --> 00:12:51,162
wise, you want to meet a 99.

193
00:12:51,202 --> 00:12:56,622
99 percent availability, or higher or
lower, or is it about latency, your

194
00:12:56,622 --> 00:13:03,007
service is committed to, what's the
service, Level agreements, SLA you have,

195
00:13:03,627 --> 00:13:07,997
and based on these operational goal,
you can set a reasonable thresholds

196
00:13:08,717 --> 00:13:13,327
and you want to really find the balance
between noisy alerts and the misalerts.

197
00:13:13,357 --> 00:13:18,477
If you have a very strict threshold,
you will have very noisy alerts.

198
00:13:18,577 --> 00:13:21,497
If you have a relaxed one,
you will have misalerts.

199
00:13:22,807 --> 00:13:26,457
And you can also use the historical
data to inform the alert threshold.

200
00:13:27,037 --> 00:13:28,647
What was it before?

201
00:13:28,657 --> 00:13:30,177
What do we generally need?

202
00:13:30,217 --> 00:13:33,457
What's a reasonable threshold so we
don't get too many alerts or misalerts?

203
00:13:33,457 --> 00:13:38,407
Alerts should have a round table
to cover common steps on what to

204
00:13:38,407 --> 00:13:40,667
do and lessons from past scenarios.

205
00:13:41,467 --> 00:13:45,407
when you're getting alerts, on calls,
you should not be thinking what to do.

206
00:13:45,437 --> 00:13:46,107
You should have a plan.

207
00:13:46,497 --> 00:13:50,007
Concrete run books with the
links and steps what to do.

208
00:13:50,497 --> 00:13:52,317
It should not be thinking too much.

209
00:13:52,837 --> 00:13:56,467
3am stuff shouldn't be
needing any thinking.

210
00:13:56,477 --> 00:13:59,117
Neither are you capable of
doing something like that.

211
00:14:01,787 --> 00:14:05,747
And dashboard offers a different,
benefits and the compliments

212
00:14:05,747 --> 00:14:10,947
alerts a lot, in my opinion,
inform people about general trends.

213
00:14:11,007 --> 00:14:12,857
This can be a seasonal patterns.

214
00:14:13,362 --> 00:14:17,312
It can help to debug and find
correlations of what's happening.

215
00:14:18,332 --> 00:14:21,572
Also, in your dashboard, you should
have a clear name, time period,

216
00:14:21,572 --> 00:14:23,472
and useful references or markers.

217
00:14:23,472 --> 00:14:28,592
This can be alert set criterias,
week over week changes.

218
00:14:30,232 --> 00:14:35,382
And as part of your operational goal, you
should also know your system limitations.

219
00:14:35,977 --> 00:14:39,107
If you don't know your system limitation,
you're awaiting a set to happen.

220
00:14:39,537 --> 00:14:43,097
So you want to know what's
the maximum you can support.

221
00:14:43,447 --> 00:14:47,447
If you're online shopping, you want to
know how many orders you can support.

222
00:14:48,067 --> 00:14:52,187
If you're running a social media
company, you want to know how many

223
00:14:52,197 --> 00:14:53,987
users you can concurrently support.

224
00:14:54,577 --> 00:14:58,817
And you can know this limitation by
doing load testing and prepare yourself

225
00:14:58,827 --> 00:15:03,732
for dependency failures or Other type
of failures by doing chaos testing.

226
00:15:04,732 --> 00:15:08,952
So mitigation, when you get
alerts, a mitigation is like

227
00:15:08,952 --> 00:15:10,872
saving life in emergency room.

228
00:15:11,672 --> 00:15:13,162
Time is essence here.

229
00:15:13,172 --> 00:15:15,582
You're running against time.

230
00:15:16,352 --> 00:15:17,742
So follow the wrong book.

231
00:15:18,692 --> 00:15:21,972
Do not have your random
thoughts to a brainstorming.

232
00:15:22,032 --> 00:15:22,852
What's happening?

233
00:15:22,912 --> 00:15:24,062
Follow the wrong book.

234
00:15:24,502 --> 00:15:26,112
Look at your alerts and dashboard.

235
00:15:26,802 --> 00:15:30,772
A few questions you can think about
is it happening at a specific time?

236
00:15:30,782 --> 00:15:34,852
Does it line up with a specific
commit, aversions, or gating changes?

237
00:15:35,412 --> 00:15:39,392
Is there any exceptional logs or
metrics to pinpoint specific files?

238
00:15:39,892 --> 00:15:43,642
Are there related issues identified
by other teams at the same time?

239
00:15:44,012 --> 00:15:45,602
Is it company wide events?

240
00:15:46,247 --> 00:15:48,217
Do you need escalation?

241
00:15:50,867 --> 00:15:53,457
and there are also times where
you can prepare for, you need to

242
00:15:53,457 --> 00:15:56,147
prepare for high velocity events.

243
00:15:57,057 --> 00:15:59,407
So what are the high velocity events?

244
00:15:59,427 --> 00:16:02,797
So high velocity events
are dangerous times.

245
00:16:03,367 --> 00:16:09,817
you can think about it as, you know,
of, something you want to prepare for.

246
00:16:09,967 --> 00:16:14,297
So this can be an anticipated
large increase of traffic.

247
00:16:14,987 --> 00:16:22,502
Let's say It's prime day, or if
it is a feature release of a major

248
00:16:22,512 --> 00:16:26,172
feature release, you have a lot
of traffic going to your service.

249
00:16:28,282 --> 00:16:31,802
In those cases, you will have
a risk mitigation plan based on

250
00:16:31,842 --> 00:16:33,472
factors such as traffic estimations.

251
00:16:35,042 --> 00:16:40,262
the risk sometimes it can be beyond just
operational, it can be about integrity,

252
00:16:40,272 --> 00:16:43,472
it can be about media risk, PR risk.

253
00:16:44,192 --> 00:16:47,782
in any cases, you need to set up
dashboards and learn specifically

254
00:16:47,782 --> 00:16:52,082
and pay very close attention to
critical metrics to monitors and the

255
00:16:52,092 --> 00:16:54,252
deltas that this advance will bring.

256
00:16:56,252 --> 00:17:00,042
you should also have levers
for the worst case scenarios.

257
00:17:00,627 --> 00:17:04,657
so the levers can be, hey, if
traffic is two times my expectation,

258
00:17:04,677 --> 00:17:06,207
what would I do in that case?

259
00:17:06,847 --> 00:17:10,197
How to, how am I able to
accommodate all that traffic?

260
00:17:11,787 --> 00:17:15,537
And this is also a great
time to audit your services.

261
00:17:16,307 --> 00:17:21,657
so rerun your load test, chaos
testing, reassess your levers, think

262
00:17:21,657 --> 00:17:26,207
about what things can go wrong, do
a check on your, service health.

263
00:17:27,207 --> 00:17:31,527
Now, when Seth is finished.

264
00:17:31,632 --> 00:17:35,812
self review or sometimes in some
companies it can be called other names.

265
00:17:36,292 --> 00:17:39,942
But the essence is that you
will review the self, as a

266
00:17:39,942 --> 00:17:41,792
person who went for on call.

267
00:17:42,382 --> 00:17:46,152
The self review is intended
to be a learning experience.

268
00:17:46,222 --> 00:17:47,932
It's supposed to be blameless.

269
00:17:48,812 --> 00:17:53,102
after the self, you want to,
write a report that includes

270
00:17:53,172 --> 00:17:54,682
the timeline of the events.

271
00:17:54,692 --> 00:17:55,962
When did it happen?

272
00:17:56,242 --> 00:17:57,892
When did we detect it?

273
00:17:57,912 --> 00:17:58,912
When did we start?

274
00:17:59,227 --> 00:18:03,687
root causing it and, mitigate this,
and all these timelines so we can

275
00:18:03,697 --> 00:18:09,077
match and find out how we can improve
on these timelines, such as quicker

276
00:18:09,087 --> 00:18:13,697
time to detect and quicker time
to root cause, mitigate the set.

277
00:18:15,287 --> 00:18:20,837
for the root cause 1, Useful
exercises, think in terms of why, so

278
00:18:20,867 --> 00:18:23,637
for example, your service is down.

279
00:18:23,647 --> 00:18:24,947
Why is your service down?

280
00:18:24,967 --> 00:18:27,277
Because there's a memory
leak in your host.

281
00:18:28,067 --> 00:18:29,697
Why is there a memory leak?

282
00:18:29,747 --> 00:18:34,777
because there's a bad commit that
introduced a memory leak, in productions.

283
00:18:34,817 --> 00:18:38,587
And why did this memory
end up in production?

284
00:18:38,607 --> 00:18:43,327
Because we did not have proper load
testing to catch memory leak issues.

285
00:18:44,167 --> 00:18:45,262
Why did we not have that?

286
00:18:45,262 --> 00:18:48,612
the load has, and then that comes to
your action item to add the load has,

287
00:18:49,102 --> 00:18:53,892
and the proper guidelines and action item
prevented from happening in the future.

288
00:18:56,342 --> 00:19:00,962
in general, you want to focus on, the
next steps and action items, making

289
00:19:00,962 --> 00:19:04,952
sure those action items are tracked,
making sure there's a timeline for when

290
00:19:05,492 --> 00:19:10,207
those action items will be finished,
because if there's no action items, All

291
00:19:10,207 --> 00:19:12,177
this learning is for nothing, right?

292
00:19:12,557 --> 00:19:14,967
You need to find ways to
prevent it from happening.

293
00:19:15,447 --> 00:19:19,427
very rare cases is that
this is act of God.

294
00:19:19,497 --> 00:19:23,587
There is no possible
action items you can take.

295
00:19:24,587 --> 00:19:31,477
And at the end of the day, self review is
really a place to foster, this continuous

296
00:19:31,537 --> 00:19:37,047
improvements to make sure that people
are aware, we can build a resilient

297
00:19:37,047 --> 00:19:42,207
systems and we're able to, make it
better, make our own cause like better.

298
00:19:42,867 --> 00:19:44,887
yeah, that's end of the talk.

299
00:19:45,427 --> 00:19:47,817
I want to thank you for
attending this talk.

300
00:19:48,317 --> 00:19:50,857
And hopefully there's
some takeaways, you get.

301
00:19:51,287 --> 00:19:54,907
Out of this talk, and, hopefully you
have fewer steps next time you're

302
00:19:54,907 --> 00:20:00,167
going on call, following some of the,
lessons we discussed in this sessions.

303
00:20:00,747 --> 00:20:01,737
yeah, thank you.

