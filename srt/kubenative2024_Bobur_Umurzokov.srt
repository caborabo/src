1
00:00:14,180 --> 00:00:14,930
Hi, everyone.

2
00:00:14,940 --> 00:00:18,840
Welcome to Conf42 kubernetes event.

3
00:00:19,070 --> 00:00:21,650
I am glad to provide this session.

4
00:00:21,910 --> 00:00:25,670
This talk demonstrates how to
build a real time data processing

5
00:00:25,670 --> 00:00:30,009
pipeline with serverless
infrastructure from your day one.

6
00:00:30,390 --> 00:00:34,640
I hope you will like the session and
it will be helpful to, for you to

7
00:00:34,640 --> 00:00:39,519
understand the different technologies
and tools for real time data processing

8
00:00:39,519 --> 00:00:43,060
nowadays in modern streaming environment.

9
00:00:43,330 --> 00:00:45,030
So with that, let's get started.

10
00:00:45,040 --> 00:00:47,010
Let me introduce first myself.

11
00:00:47,059 --> 00:00:48,290
My name is Babur.

12
00:00:48,290 --> 00:00:51,960
I am a developer advocate at
Glassflow, and also I am a

13
00:00:52,010 --> 00:00:55,619
Microsoft MVP for Azure AI services.

14
00:00:55,619 --> 00:00:59,140
I like to build recent AI
applications, and I am also an

15
00:00:59,140 --> 00:01:02,249
evangelist for data streaming tools.

16
00:01:02,420 --> 00:01:06,900
If you would like to connect with me,
you can always request on LinkedIn.

17
00:01:07,120 --> 00:01:10,630
Please, if you have any questions
about this session or after this

18
00:01:10,640 --> 00:01:14,389
session, I would be more than
happy to address your questions.

19
00:01:14,759 --> 00:01:17,449
So let's start with our story.

20
00:01:17,880 --> 00:01:21,029
Our story begins with
a data engineer, Bob.

21
00:01:21,470 --> 00:01:27,190
Dreamer who works at a medium sized
company called dream together with more

22
00:01:27,190 --> 00:01:32,630
than 50 his colleagues and they are
building an innovative car selling online

23
00:01:32,640 --> 00:01:40,795
platform where I like you can register
and announce your vehicles to buy or sell.

24
00:01:41,425 --> 00:01:46,535
Bob was actually highly motivated
by the reason AI hype, since

25
00:01:46,535 --> 00:01:48,985
everybody around him talks about AI.

26
00:01:49,274 --> 00:01:54,355
He comes up with a brilliant idea
of creating a real time price

27
00:01:54,355 --> 00:01:59,769
recommender solution for their existing
website using modern data tools.

28
00:02:00,799 --> 00:02:02,199
Let me explain how that works.

29
00:02:02,249 --> 00:02:07,000
As new cars are registered in their
primary database, such as it can

30
00:02:07,000 --> 00:02:12,769
be NoSQL database, the app should
suggest the price for the vehicle

31
00:02:13,050 --> 00:02:18,320
using the AI and this application
need to show in real time recommended

32
00:02:18,370 --> 00:02:24,019
price on their website so users can
quickly receive appropriate feedback.

33
00:02:25,115 --> 00:02:27,454
Bob presents his idea to the team.

34
00:02:27,464 --> 00:02:33,165
And the team was curious about the
outcome and the team that gives

35
00:02:33,165 --> 00:02:37,565
him to prove some time to Bob
to provide the proof of concept.

36
00:02:38,515 --> 00:02:43,605
And Bob starts his investigation by
understanding what is batch processing and

37
00:02:43,644 --> 00:02:48,684
real time data processing because with his
background in using the tools for batch

38
00:02:48,984 --> 00:02:55,415
processing, he discovers that in practice
bots collect, they collect and process

39
00:02:55,424 --> 00:02:58,375
data for using downstream applications.

40
00:02:58,825 --> 00:03:02,935
The only real distinction between
batch and stream processing is

41
00:03:02,935 --> 00:03:06,265
that they, how they operate at
slightly different time intervals.

42
00:03:06,850 --> 00:03:11,250
For a long time, like in flexibility
of batch orchestrators and schedulers,

43
00:03:12,010 --> 00:03:15,250
keep the data teams to maintain
a separate set of systems, like

44
00:03:15,260 --> 00:03:19,709
one system for batch processing,
another system for stream processing.

45
00:03:20,029 --> 00:03:21,790
But it doesn't have to be that way.

46
00:03:21,980 --> 00:03:26,940
Nowadays, a real time data streaming
pipelines can solve this problem and

47
00:03:26,959 --> 00:03:31,790
combine for both batch processing and
stream processing into one workload

48
00:03:31,839 --> 00:03:35,070
so that you can use the same tools
and technologies infrastructure.

49
00:03:35,485 --> 00:03:37,785
to process and transfer your data.

50
00:03:38,195 --> 00:03:41,124
Here, as you can see in the
diagram, how the real time

51
00:03:41,124 --> 00:03:42,834
data streaming pipeline works.

52
00:03:43,964 --> 00:03:47,974
Actually, this solution is
essential for some applications.

53
00:03:47,974 --> 00:03:51,554
They need to up to date, bring
some up to date information,

54
00:03:51,784 --> 00:03:55,424
like monitoring live events or
updating dashboards in real time.

55
00:03:55,734 --> 00:03:59,234
As you can see, we have the data source
and we have the data destination.

56
00:03:59,515 --> 00:04:05,545
And in the middle layer, we Where all the
transformation happens as data changes,

57
00:04:05,835 --> 00:04:07,705
change happens on the source side.

58
00:04:07,975 --> 00:04:11,924
These changes are in the real time
captured from the transformation

59
00:04:11,925 --> 00:04:15,675
part and then you, in the
transformation, you can do aggregation.

60
00:04:15,885 --> 00:04:23,254
You can do filtering, enrichment, maybe
introduce ML or AI models to train

61
00:04:23,254 --> 00:04:28,744
your model, then send the output to
intended destinations like data lakes

62
00:04:28,754 --> 00:04:34,314
that can be data warehouses or any
analytical services in even a real

63
00:04:34,314 --> 00:04:36,564
time application and microservices.

64
00:04:37,164 --> 00:04:40,744
And then Bob once understanding the about
the string data streaming, pipelines.

65
00:04:41,124 --> 00:04:46,604
He thinks of his first solution for
the real time price recommendation

66
00:04:46,604 --> 00:04:51,094
system where he uses Debezium to
capture data changes, as you can see

67
00:04:51,104 --> 00:04:56,624
in the diagram, and stream those events
via Kafka to the prediction service.

68
00:04:56,904 --> 00:05:00,734
And the prediction service, actually
just a Python application that connects

69
00:05:00,734 --> 00:05:06,254
to the OpenAI API to calculate the
suggested prices and sends and reaches

70
00:05:06,334 --> 00:05:13,664
output to the web application or to
the analytics tool to see how the price

71
00:05:13,694 --> 00:05:18,574
is changing, how often and understand
Which users are buying or selling and

72
00:05:18,574 --> 00:05:24,894
so on, but however Bob face it with
several solution in his first solution,

73
00:05:25,334 --> 00:05:27,074
let's say he lacked the experience.

74
00:05:27,074 --> 00:05:30,634
First of all, with Kafka, because
he has a Python engineer, a Python

75
00:05:30,664 --> 00:05:34,804
background, and he didn't want to do
with the infrastructure complexities

76
00:05:35,084 --> 00:05:40,384
because he doesn't have enough experience
in DevOps or learning Kubernetes.

77
00:05:40,729 --> 00:05:44,339
Due to time concentrates to build
this a simple proof of concept

78
00:05:44,339 --> 00:05:45,909
and bring it to the production.

79
00:05:46,529 --> 00:05:49,739
His goal also to implement everything
through Python, obviously, because

80
00:05:50,009 --> 00:05:55,289
there might be some data scientific
approach to call some ML models

81
00:05:55,289 --> 00:05:56,709
to calculate predicted price.

82
00:05:57,019 --> 00:06:01,249
And he also make sure that multiple
data engineers in his team could

83
00:06:01,249 --> 00:06:03,499
collaborate on one pipeline.

84
00:06:03,900 --> 00:06:07,160
And in addition to this, he needed
the product service to notify

85
00:06:07,160 --> 00:06:11,430
the web application or their
other systems at the same time.

86
00:06:11,929 --> 00:06:17,469
And Bob continues his discovering
by searching on Google or asking

87
00:06:17,469 --> 00:06:19,769
GPT, also asking his friends.

88
00:06:20,084 --> 00:06:25,114
about their experiences with Kafka or
any real time data processing tools.

89
00:06:25,114 --> 00:06:29,874
And Bob hears some stories from Kafka
users about taking nine months to

90
00:06:29,944 --> 00:06:35,524
implement production ready Kafka based
data pipelines if they are self managed.

91
00:06:35,534 --> 00:06:40,904
They see the customers having 50 teams
relying on a single Kafka cluster.

92
00:06:41,214 --> 00:06:45,224
Memory is only by one person and
sometimes data engineers cannot

93
00:06:45,224 --> 00:06:50,124
simulate easily production environment
without initial complex setup.

94
00:06:50,474 --> 00:06:55,614
And also we see that data scientists
struggle with data integration, like

95
00:06:55,654 --> 00:07:01,334
building offline ML pipelines, experiment
and reproduce some models without

96
00:07:01,334 --> 00:07:03,964
pushing it to prod or debug them locally.

97
00:07:04,264 --> 00:07:09,584
There are many problems using this
Kafka related infrastructure, and then

98
00:07:09,674 --> 00:07:16,634
he also determines that there are some
problems with self managing Kafka that

99
00:07:16,634 --> 00:07:21,554
presents, let's say also it's difficult
to determine which team or individual

100
00:07:21,594 --> 00:07:26,124
actually responsible for Kafka operations,
Bob or DevOps team or software engineering

101
00:07:26,124 --> 00:07:31,104
team, or configuring Kafka correctly
from the scratch is also problematic.

102
00:07:31,104 --> 00:07:33,134
I think you need, you
don't know like how many.

103
00:07:33,509 --> 00:07:37,379
Clusters you need for your
organization or you need single

104
00:07:37,429 --> 00:07:39,209
block cluster or multiple clusters.

105
00:07:39,589 --> 00:07:45,419
Also deploying changes continuously to
Kubernetes to or to other environments

106
00:07:45,759 --> 00:07:48,319
and bring some also additional problems.

107
00:07:48,709 --> 00:07:52,939
Also upgrading Kafka brokers
always is challenging.

108
00:07:53,239 --> 00:07:56,330
Sometimes you don't know how
much you want to scale your

109
00:07:56,360 --> 00:07:58,290
Kafka broker or how much storage.

110
00:07:58,630 --> 00:08:02,200
You would like to give
to the topics and so on.

111
00:08:02,220 --> 00:08:05,810
And another big topic is, of course,
monitoring Kafka performance and health

112
00:08:06,170 --> 00:08:10,720
is quite challenging when you are,
have a self managing Kafka environment.

113
00:08:11,220 --> 00:08:15,580
Managing Kafka clusters, as we said,
can be expensive, both in terms of

114
00:08:15,620 --> 00:08:17,819
infrastructure and operational costs.

115
00:08:17,970 --> 00:08:22,380
At this point, you start to consider
using some managed Kafka providers.

116
00:08:22,400 --> 00:08:26,490
As you can see in the screen, I
highlighted some well known managed

117
00:08:26,530 --> 00:08:28,750
Kafka providers like Red Panda.

118
00:08:29,030 --> 00:08:32,270
We have the Ivan for Apache Kafka
Confluent, one of the biggest

119
00:08:32,270 --> 00:08:35,500
one, and we have a new startups
coming up like Warpstream.

120
00:08:35,910 --> 00:08:40,220
But however it does so it doesn't solve
the data engineers like both problem

121
00:08:40,220 --> 00:08:45,310
because they need still dedicated engineer
who understand this, his physical use

122
00:08:45,310 --> 00:08:50,930
cases and business needs and spin up
the Kafka environment in the cloud

123
00:08:51,090 --> 00:08:53,320
providers or that's in the confluent.

124
00:08:54,450 --> 00:08:59,100
Also, choosing providers should
be also secure enough, you need to

125
00:08:59,100 --> 00:09:01,120
make sure that everything is secure.

126
00:09:01,320 --> 00:09:07,120
Also, you need to additionally watch out
over provisioning or fluctuating costs,

127
00:09:07,150 --> 00:09:12,590
because If you don't manage this cost you
made you may end up with a high costs.

128
00:09:12,960 --> 00:09:16,010
You need to understand cost
of model that you are managing

129
00:09:16,010 --> 00:09:18,490
inside your Kafka providers.

130
00:09:18,530 --> 00:09:22,560
One of those drawbacks once you start
to use you need to bring some data

131
00:09:22,560 --> 00:09:24,490
transformation logics dynamically.

132
00:09:25,105 --> 00:09:28,765
But to use it in data transformation,
you need to use a separate

133
00:09:28,765 --> 00:09:32,275
environment in your Kafka provider.

134
00:09:32,275 --> 00:09:38,765
Let's say Amazon MSK also offers quite a
good useful tool for managing your Kafkas.

135
00:09:38,805 --> 00:09:42,410
But if you would like to do
some transformation, You need

136
00:09:42,410 --> 00:09:48,550
to use and also the pay for
additionally using Lambda functions.

137
00:09:49,340 --> 00:09:54,620
So we said he starts also understands like
data engineers or often depends on other

138
00:09:54,620 --> 00:09:56,600
teams to maintain data infrastructure.

139
00:09:57,065 --> 00:10:01,225
Sometimes data engineers need to ask
DevOps to assist the provision cloud

140
00:10:01,225 --> 00:10:07,745
resources to manage Kafka or deploying new
data pipelines for just sake of a proof

141
00:10:07,745 --> 00:10:10,095
of concept they are all creating delays.

142
00:10:10,395 --> 00:10:15,005
So commonly Bob wants some self
efficiency, self sufficiency in Python.

143
00:10:15,285 --> 00:10:20,505
He can just take care of his logic for
the data pipeline and orchestration.

144
00:10:20,790 --> 00:10:24,490
But the rest, when it comes to
infrastructure, do do it while,

145
00:10:24,500 --> 00:10:26,650
by maybe any other technologies.

146
00:10:27,100 --> 00:10:32,190
Bob combines all the problems and I said,
the data engineers and scientists are

147
00:10:32,200 --> 00:10:35,110
facing in 2024 and he writes a blog post.

148
00:10:35,110 --> 00:10:40,190
If you scan this quote, you can understand
more about other problems data scientists

149
00:10:40,190 --> 00:10:41,920
and engineers are facing nowadays.

150
00:10:42,530 --> 00:10:46,810
My advice is actually, you can
also start by discovering options.

151
00:10:47,000 --> 00:10:49,880
First of all, without
using any Kafka providers.

152
00:10:49,960 --> 00:10:53,450
Nowadays there are other tools
from the cloud providers like

153
00:10:53,460 --> 00:10:56,730
Azure or Google or the Amazon.

154
00:10:57,055 --> 00:11:00,465
Or instead of some other message
brokers coming up like a nuts which

155
00:11:00,465 --> 00:11:05,065
is open source used by many people now
a days gaining a lot of popularity.

156
00:11:05,465 --> 00:11:11,525
And Bob, as he's a dreamer, one of his
dreams, he so said, there's also some

157
00:11:11,655 --> 00:11:13,975
stream processing frameworks in Python.

158
00:11:14,320 --> 00:11:17,460
That can solve his existing problem.

159
00:11:17,820 --> 00:11:21,770
So he starts to investigate this
stream processing frameworks in Python.

160
00:11:22,180 --> 00:11:25,120
Let's focus on when and why
we can use a stream data

161
00:11:25,120 --> 00:11:26,690
processing frameworks for Python.

162
00:11:26,960 --> 00:11:30,490
First of all, we know that
Python is go to language for data

163
00:11:30,490 --> 00:11:31,720
science and machine learning.

164
00:11:32,210 --> 00:11:33,550
As you can see in the diagram.

165
00:11:33,785 --> 00:11:38,605
Python based framework can unify the
streaming data platform and stream

166
00:11:38,635 --> 00:11:42,535
processor components in the given
streaming data pipeline architecture.

167
00:11:42,975 --> 00:11:45,845
You can connect directly
to any data source.

168
00:11:46,275 --> 00:11:50,975
Using built in connectors that provides
these frameworks within your application

169
00:11:51,005 --> 00:11:56,575
code without knowing like how Kafka
or other Message brokers work and you

170
00:11:56,575 --> 00:11:59,705
can focus only on your business logic.

171
00:12:00,055 --> 00:12:05,535
In other words, stream processing
frameworks Combines both Kafka and Flink

172
00:12:05,535 --> 00:12:10,885
environment for let's say stateful data
stream processing operations and then

173
00:12:10,885 --> 00:12:15,795
Bob asks himself, okay, why used to put
Python frameworks for data streaming?

174
00:12:15,815 --> 00:12:17,505
There are other several reasons.

175
00:12:17,585 --> 00:12:21,745
If you are a Python developer you
can use frameworks out of the box

176
00:12:21,765 --> 00:12:23,445
with any existing Python library.

177
00:12:23,465 --> 00:12:27,475
No JVM, no wrapper, no orchestrator
or server side engine you need.

178
00:12:27,815 --> 00:12:33,005
You can bring pandas or any libraries
or you get used to deal with.

179
00:12:33,400 --> 00:12:36,690
And if you are a data engineer, you
don't need to deal with infrastructure.

180
00:12:37,140 --> 00:12:40,810
You can start this framework
without any complex initial

181
00:12:40,820 --> 00:12:43,080
setup, such as creating clusters.

182
00:12:43,340 --> 00:12:47,910
And Python frameworks offer zero
infrastructure environment for you.

183
00:12:48,360 --> 00:12:50,930
If you're a data scientist,
no need to learn Kubernetes.

184
00:12:50,990 --> 00:12:57,170
You can run your local code right from the
Jupyter notebook to try out your pipeline.

185
00:12:57,485 --> 00:13:01,515
And if you're worried about your data,
your original data also stays where it

186
00:13:01,515 --> 00:13:06,795
is in your primary database, and then
the framework just runs a transformation

187
00:13:06,805 --> 00:13:11,245
layer on the top of it and sends
data to your output destinations.

188
00:13:11,765 --> 00:13:16,285
Like you can use change data capture
settings like Debezium by directly

189
00:13:16,305 --> 00:13:20,825
connecting to the primary database and
doing the computational work, and save the

190
00:13:20,825 --> 00:13:25,135
result back to the your primary database
or send the data to output streams.

191
00:13:25,415 --> 00:13:31,335
So with that, our team also, we build this
stream processing framework to do with

192
00:13:31,375 --> 00:13:34,105
real time data or even the driven data.

193
00:13:34,525 --> 00:13:36,425
It's called it's called Glassflow.

194
00:13:36,515 --> 00:13:39,935
Glassflow is also serverless,
Python centric real time

195
00:13:39,935 --> 00:13:41,955
data processing pipeline.

196
00:13:42,270 --> 00:13:46,770
You can build your data pipelines
within the minutes without

197
00:13:46,860 --> 00:13:48,720
setting up any infrastructure.

198
00:13:48,920 --> 00:13:55,240
With just simple clicks or using the CLI,
you can bring your first data pipeline,

199
00:13:55,260 --> 00:13:57,470
let's say, in the first 15 minutes.

200
00:13:57,480 --> 00:14:03,160
And actually, Glassware excels in real
time data transformation of your events.

201
00:14:03,310 --> 00:14:07,930
So that application can immediately
react to the new information.

202
00:14:08,460 --> 00:14:13,060
In other words, data, real time
data pipelines creation process with

203
00:14:13,060 --> 00:14:18,280
Glassflow is as simple and casual
as Yusuf shoots at the Olympics.

204
00:14:18,690 --> 00:14:22,830
No infra, no Kafka, or
the flink is required.

205
00:14:23,380 --> 00:14:26,550
How we build data
pipelines is very simple.

206
00:14:26,730 --> 00:14:33,750
You just connect to your data sources,
which you send some real time data

207
00:14:33,750 --> 00:14:35,930
information using built in integrations.

208
00:14:36,150 --> 00:14:39,570
Without writing any code in a low
code environment, or if there is no

209
00:14:39,570 --> 00:14:43,840
any built in integration, you can
build your own integration without

210
00:14:43,840 --> 00:14:48,350
spending much time by implementing
custom connector using GlassLove SDK.

211
00:14:48,350 --> 00:14:52,110
And you write the code to implement
in Python data transformation logic.

212
00:14:52,410 --> 00:14:56,760
In Python, once you deploy this
transformation function to your

213
00:14:56,770 --> 00:15:03,410
pipeline, and this transformation
function runs in autoscalable serverless

214
00:15:03,410 --> 00:15:09,250
infrastructure, You can easily deal with
the billions of log records in real time.

215
00:15:09,950 --> 00:15:15,160
And also, Glassdoor built using the
modern technology, or the technology

216
00:15:15,160 --> 00:15:20,560
you get to use like Docker, NANTS,
and Kubernetes under the hood.

217
00:15:21,190 --> 00:15:26,500
That just abstracts for you to
make it easy not to manage this

218
00:15:26,500 --> 00:15:28,050
infrastructure under the hood.

219
00:15:28,550 --> 00:15:32,470
And let me show you some real
time examples of pipeline.

220
00:15:32,550 --> 00:15:35,380
I built myself using the log glass flow.

221
00:15:35,650 --> 00:15:40,100
For example, you can build a real
time clickstream analytics dashboard.

222
00:15:40,440 --> 00:15:43,920
You can use, let's say some data
analytics API, like a Google

223
00:15:43,920 --> 00:15:48,830
analytics in Python to collect some
clickstream data from your website

224
00:15:48,860 --> 00:15:50,900
and sends them to glass flow pipeline.

225
00:15:51,290 --> 00:15:55,960
And your transformation function might
analyze data to calculate additional

226
00:15:55,990 --> 00:16:01,890
metrics and you can use this metrics
or energy data to visualize something

227
00:16:01,900 --> 00:16:06,240
using some visualizing tools like a
stream lead or a plot lead in Python.

228
00:16:07,025 --> 00:16:11,695
Or another example is I build a
data processing pipeline to enrich

229
00:16:11,705 --> 00:16:14,235
some classified ads in real time.

230
00:16:14,505 --> 00:16:19,305
You can use Glassflow to process like
website ads and reach them with additional

231
00:16:19,305 --> 00:16:25,395
information or categorize them using
a long chain or open AI and store this

232
00:16:25,415 --> 00:16:32,805
enriched ads in some A quick and advanced
search database like Redis so that the

233
00:16:33,035 --> 00:16:39,635
website is always aware of real time
ads as they are coming into your system.

234
00:16:40,405 --> 00:16:44,535
Or what about like processing,
for example, Airbnb listing

235
00:16:44,535 --> 00:16:47,615
data in some sub base, database?

236
00:16:48,975 --> 00:16:53,075
And you can enrich this data with
your AI and continuously update the

237
00:16:53,085 --> 00:16:55,695
vector databases, such as we will wait.

238
00:16:56,185 --> 00:17:00,805
So this example pipeline can be used also
to create some personalized recommendation

239
00:17:00,805 --> 00:17:06,465
solutions or generate some targeted ads
based on the real time data information.

240
00:17:07,095 --> 00:17:10,245
Last example I would like to
show is a data transformation

241
00:17:10,245 --> 00:17:14,255
pipeline that demonstrates some
log data anomaly detection.

242
00:17:14,520 --> 00:17:21,380
With AI and ML models to monitor your
server logs to detect some unusual

243
00:17:21,430 --> 00:17:26,640
patterns or the activities and send
some notification to the notification

244
00:17:26,640 --> 00:17:30,540
systems or stores in long term storages.

245
00:17:30,890 --> 00:17:32,700
To understand these logs in the future.

246
00:17:32,800 --> 00:17:34,990
So let's just scan this QR code.

247
00:17:35,040 --> 00:17:40,550
You can see all the other use cases
with code examples, tutorials, and

248
00:17:40,550 --> 00:17:45,560
architecture explanation in which use
cases you can apply, or you can use them

249
00:17:45,590 --> 00:17:47,380
as a template for your future projects.

250
00:17:47,985 --> 00:17:52,045
So let's back into our topic
of Bob's about building real

251
00:17:52,045 --> 00:17:53,725
time data price recommendation.

252
00:17:54,165 --> 00:17:58,365
Here's a second example of real time price
recommendation data pipeline architecture.

253
00:17:58,425 --> 00:18:04,385
He As he provided and he developed, but
it is you can see this there are some

254
00:18:04,385 --> 00:18:06,795
components to process car price data.

255
00:18:07,265 --> 00:18:12,925
He ends up with using the super base
because of it is simplicity, open

256
00:18:12,925 --> 00:18:18,655
source then also he would like to,
that when every vehicle is registered

257
00:18:18,655 --> 00:18:23,215
in the PostgreSQL, or you can say, or
Support Base Database they would like

258
00:18:23,215 --> 00:18:29,005
to send the output to to Glassflow
Pipeline using a webhook connector, so

259
00:18:29,005 --> 00:18:34,965
that we are, Pipeline can capture every
change, every insert, every delete, or

260
00:18:34,965 --> 00:18:36,655
every update operation on Support Base.

261
00:18:37,215 --> 00:18:40,815
In, within a milliseconds, this data
will be arrive to the pipeline and the

262
00:18:40,815 --> 00:18:47,735
pipeline can connect to the open AI API
and to to provide some insights from ai.

263
00:18:47,735 --> 00:18:52,835
And then predicted car prices can
be sent to the output destinations.

264
00:18:53,185 --> 00:18:58,315
As you can see tools we use is first
of all starting from super base is

265
00:18:58,315 --> 00:19:01,375
pretty much like a fire base or Heroku.

266
00:19:01,835 --> 00:19:03,575
But it's open source SQL database.

267
00:19:03,585 --> 00:19:08,275
It's 100 percent portable
with your PostgreSQL as well.

268
00:19:09,095 --> 00:19:13,015
You can easily bring your existing
PostgreSQL database or build real

269
00:19:13,015 --> 00:19:15,235
time data applications on top of it.

270
00:19:16,165 --> 00:19:20,145
SuperBase in our architecture
triggers event whenever new entry

271
00:19:20,175 --> 00:19:24,495
with the car data added in the
car price database, let's say.

272
00:19:25,555 --> 00:19:30,175
Then it sends directly to that Glassflow
pipeline using a webhook data source

273
00:19:30,175 --> 00:19:31,875
connector, as I mentioned earlier.

274
00:19:32,340 --> 00:19:37,070
Then we use a Glasswalk web app,
which is one way to, you build a

275
00:19:37,070 --> 00:19:41,260
pipeline Glassflow without writing
any code, just a UI interface.

276
00:19:41,260 --> 00:19:46,170
You put the specified data source
transformation and data destination.

277
00:19:46,180 --> 00:19:47,910
You're going to see it soon in my demo.

278
00:19:48,260 --> 00:19:50,090
Then your pipeline is ready.

279
00:19:50,600 --> 00:19:56,610
Then we are gonna use open AI
obviously to use AI models like GPT

280
00:19:56,980 --> 00:20:03,150
to predict future car price based on
some information we got from superb.

281
00:20:03,920 --> 00:20:09,970
So once the use case is clear, let me
show you demonstrate how that works and

282
00:20:09,980 --> 00:20:15,310
how you can build your first pipeline
within the your five or six minutes.

283
00:20:16,020 --> 00:20:19,800
If you're scanning this QR code, you
can also find the project on GitHub.

284
00:20:20,300 --> 00:20:21,490
You can give a try.

285
00:20:21,520 --> 00:20:25,780
There are some samples code for
transformation and also sample

286
00:20:25,780 --> 00:20:31,050
codes for you to set up some queries
on the SOPA base site and so on.

287
00:20:31,740 --> 00:20:33,500
So let me dive into the demo.

288
00:20:34,535 --> 00:20:38,255
Assume that you have a primary database
like a super base and you have project

289
00:20:38,265 --> 00:20:43,455
inside the project some databases and
we have also car price table with some

290
00:20:44,135 --> 00:20:48,905
attributes belong to the car announced
for the selling or buying and then also

291
00:20:48,905 --> 00:20:52,435
you have sample data for some car prices.

292
00:20:53,105 --> 00:20:55,605
Let me dive into the demo
of creating the pipeline.

293
00:20:55,955 --> 00:20:59,014
Navigate to GlassLaw webapp.

294
00:20:59,195 --> 00:21:03,955
Using the GlassLaw webapp you can create
a pipeline with easy step within a minute.

295
00:21:04,240 --> 00:21:06,220
Start by creating new pipeline

296
00:21:08,460 --> 00:21:09,710
using the Glassflow.

297
00:21:09,760 --> 00:21:16,740
I will provide my pipeline name for the
new pipeline and like a cut price data.

298
00:21:16,770 --> 00:21:17,580
Click next.

299
00:21:17,600 --> 00:21:20,050
And for as a data source,
I'm going to use a webhook.

300
00:21:20,530 --> 00:21:24,230
Because a webhook, we're going to
set up for server based to push some

301
00:21:24,510 --> 00:21:26,710
every data changes to the Glassflow.

302
00:21:27,240 --> 00:21:30,500
Then next step third step, I need
to define my transformer function.

303
00:21:30,510 --> 00:21:34,380
You can use it existing templates
or write Python code to define your.

304
00:21:34,630 --> 00:21:36,740
Transformer function.

305
00:21:37,040 --> 00:21:41,400
I have my already defined transformer
function that you use OpenAI, and the

306
00:21:41,450 --> 00:21:47,255
call makes the call to Chat completion
endpoint to predict the car price for

307
00:21:47,255 --> 00:21:49,745
the upcoming car product event changes.

308
00:21:50,425 --> 00:21:53,325
So I'm going to copy this
transformation function and put

309
00:21:53,325 --> 00:21:54,905
into the editor of the web app.

310
00:21:55,225 --> 00:21:59,255
You can also find the transformation
function code on our GitHub repository.

311
00:21:59,665 --> 00:22:02,254
So we don't have to also
use our own OpenAI API.

312
00:22:02,255 --> 00:22:06,795
We provide the free OpenAI API
account to try out OpenAI use case.

313
00:22:06,795 --> 00:22:10,705
Next step, I'm going to set up
the destination as a webhook.

314
00:22:10,905 --> 00:22:15,005
That can be your real website or any
external website to build a custom

315
00:22:15,005 --> 00:22:17,385
workflows for the demos cases.

316
00:22:17,385 --> 00:22:21,645
I'm going to use webhook site
to create some random webhook so

317
00:22:21,645 --> 00:22:24,395
that the Glassware Pipeline can
send the output to the webhook

318
00:22:26,495 --> 00:22:30,315
once we data is enriched
in the transformer stage.

319
00:22:30,735 --> 00:22:36,165
I click next and finally check my
transformation function pipeline setup,

320
00:22:36,205 --> 00:22:38,125
then click next to create my pipeline.

321
00:22:38,145 --> 00:22:38,565
Now.

322
00:22:38,785 --> 00:22:39,495
Everything's good.

323
00:22:39,495 --> 00:22:40,605
My pipeline is created.

324
00:22:40,685 --> 00:22:45,905
Now I will need only my access
token and pipeline ID to access

325
00:22:45,935 --> 00:22:51,195
push some events from the database
or from your primary database.

326
00:22:51,225 --> 00:22:56,945
In that case, you can also create some
webhook connector for the super base.

327
00:22:57,115 --> 00:23:01,025
You can create a click put
some webhook connector details.

328
00:23:01,440 --> 00:23:06,110
And in this case like it's a core
quality, the car price data and change.

329
00:23:06,160 --> 00:23:12,840
Super base detects every change when
we do in sorts, then push the event to

330
00:23:12,840 --> 00:23:16,340
the Glassdoor pipeline automatically,
you can also push your events.

331
00:23:17,025 --> 00:23:21,385
To the Glassflow pipeline using Python
SDK and write some custom code to push

332
00:23:21,495 --> 00:23:23,825
from any data source to the Glassflow.

333
00:23:24,345 --> 00:23:30,775
So let's define also some HTTP type
and I put also HTTP request header to

334
00:23:30,775 --> 00:23:33,615
define some access token for my pipeline.

335
00:23:34,095 --> 00:23:37,275
I'm going to copy and add
the access token header.

336
00:23:38,010 --> 00:23:43,190
For the pipeline, then put the value
to make sure that we are securely

337
00:23:43,200 --> 00:23:45,590
pushing the events to the pipeline.

338
00:23:45,590 --> 00:23:47,110
Now, the webhook is ready.

339
00:23:47,380 --> 00:23:51,740
Once the webhook is starting to push
events, you can immediately see a

340
00:23:51,750 --> 00:23:55,360
transformed output on the Glassflow side.

341
00:23:55,410 --> 00:23:56,355
Let's go to the next step.

342
00:23:56,575 --> 00:24:01,735
Once again to some sample data in source,
and I'm going to insert some sample

343
00:24:01,755 --> 00:24:05,205
data into the, my primary database.

344
00:24:05,435 --> 00:24:09,575
If I run this SQL query command, I
will gonna add four more in source

345
00:24:09,595 --> 00:24:11,705
with some simple color prizes.

346
00:24:12,205 --> 00:24:17,690
Then, as you can see, as output,
we will have Some logs are coming

347
00:24:17,960 --> 00:24:21,310
from the price collected on API.

348
00:24:21,310 --> 00:24:22,940
Wow, here is, you can see some failures.

349
00:24:23,150 --> 00:24:28,300
In that case, you can also show with
a web app requirements, dependency

350
00:24:28,300 --> 00:24:29,490
requirements for the Python.

351
00:24:29,740 --> 00:24:33,730
I'm going to specify OpenAI and
using inside my transformer function.

352
00:24:34,625 --> 00:24:40,655
So now, if I try once again, as you
can see in the logs, I should get all

353
00:24:40,655 --> 00:24:46,865
of the transformers and processes that
events from the, our primary database,

354
00:24:47,005 --> 00:24:51,385
like server base, as you can see, one
hour, four events out for transformers

355
00:24:51,625 --> 00:24:56,265
and class will immediately send the
transformer data as output to the

356
00:24:56,315 --> 00:25:00,395
webhook we defined it when we create
the pipeline this is the magic of the

357
00:25:00,395 --> 00:25:02,725
pipeline that reacts on real time data.

358
00:25:03,225 --> 00:25:07,415
As you can see, creating pipeline
is quite easy when you're

359
00:25:07,415 --> 00:25:10,555
using tools for for your need.

360
00:25:10,685 --> 00:25:17,365
As you can, Bob made his Python code
that runs in real time mode with a

361
00:25:17,365 --> 00:25:19,815
serverless infrastructure from day one.

362
00:25:20,175 --> 00:25:24,855
Compared to challenges he faced,
he now Have a simple solution.

363
00:25:25,305 --> 00:25:31,395
So a few words about glass flow itself
when people, other people use a glass

364
00:25:31,395 --> 00:25:36,305
flow for, let's say, if you'd like
to have a blood jobs batch jobs Might

365
00:25:36,305 --> 00:25:37,865
take a minutes or hours to execute.

366
00:25:37,865 --> 00:25:42,755
Glassflow enables you processing
your data in milliseconds to seconds,

367
00:25:42,755 --> 00:25:47,105
making the real time analysis
for reality from your day one.

368
00:25:47,495 --> 00:25:52,315
And also if you would like to reduce
the costs connected with running

369
00:25:52,315 --> 00:25:54,365
a spark, like a similar jobs.

370
00:25:54,765 --> 00:25:58,355
That consume your big amount
of competing resources.

371
00:25:58,355 --> 00:26:00,715
Glassdoor offers more
cost efficient solution.

372
00:26:01,145 --> 00:26:06,045
As earlier, I mentioned that like
you can both combine data streaming

373
00:26:06,045 --> 00:26:09,695
part and data transformation part
in the side of the same price.

374
00:26:09,695 --> 00:26:15,935
You don't have to run separate Lambda
or in Azure or functions to run.

375
00:26:16,920 --> 00:26:20,210
Transformation part and launching
new data project with Glassflow as

376
00:26:20,210 --> 00:26:24,710
you can see easy Also integrating
with real time data sources.

377
00:26:24,720 --> 00:26:30,140
So this nation is also quite fast
Glassflow excels in real time data

378
00:26:30,160 --> 00:26:37,400
ingestion and processing of your events
and so on You can have a look Our

379
00:26:37,400 --> 00:26:43,220
documentation and blog to learn more
about this in summary You here few inside

380
00:26:43,230 --> 00:26:45,310
to what we have learned along the way.

381
00:26:45,640 --> 00:26:50,230
So Bob understand that we understand
that engineers face a problems

382
00:26:50,260 --> 00:26:54,280
nowadays with JVM, basic real
time processing tools like Kafka.

383
00:26:54,780 --> 00:26:59,980
And Bob as a data engineer or a
scientist maybe his team, or he

384
00:26:59,980 --> 00:27:04,750
needs to maybe data analysts they
want to self sufficiency in Python.

385
00:27:05,195 --> 00:27:09,375
And then there are some stream processing
frameworks in Python nowadays so that you

386
00:27:09,375 --> 00:27:12,065
don't have to learn some GVM environment.

387
00:27:12,415 --> 00:27:18,655
And so Bob also find out some serverless
stream processing solutions and tools.

388
00:27:18,975 --> 00:27:21,945
To apply to his first pipeline.

389
00:27:22,495 --> 00:27:25,995
So with that we are
about to finish the demo.

390
00:27:26,065 --> 00:27:30,465
Now, if you have any questions
after the session, I will be

391
00:27:30,475 --> 00:27:32,725
more than happy to address them.

392
00:27:33,045 --> 00:27:35,865
So thank you for attending my session.

393
00:27:35,985 --> 00:27:36,925
See you soon.

394
00:27:37,045 --> 00:27:37,555
Thank you.

