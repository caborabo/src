1
00:00:14,410 --> 00:00:14,990
All right.

2
00:00:15,079 --> 00:00:17,330
Welcome everyone to our session on Amazon.

3
00:00:17,390 --> 00:00:17,540
E.

4
00:00:17,540 --> 00:00:17,849
K.

5
00:00:17,850 --> 00:00:18,290
S.

6
00:00:18,780 --> 00:00:20,970
Are also known as Amazon Elastic Compute.

7
00:00:20,970 --> 00:00:23,900
Kubernetes services cost optimization.

8
00:00:24,320 --> 00:00:26,860
I'm Dariush and with me
is my colleague Piyush.

9
00:00:28,499 --> 00:00:29,189
Hello everyone.

10
00:00:29,470 --> 00:00:30,550
We're very excited.

11
00:00:30,570 --> 00:00:36,030
To share some insights on how you
all can optimize costs while running

12
00:00:36,030 --> 00:00:38,910
Kubernetes workloads on Amazon EKS.

13
00:00:39,530 --> 00:00:41,430
My name is p and I'm joining Dru.

14
00:00:41,760 --> 00:00:44,470
We both are dialing in
here today from SoCal.

15
00:00:46,140 --> 00:00:48,990
I'm, yeah, as well here
in the office today.

16
00:00:49,490 --> 00:00:52,730
Zach, so before we dive into cost
optimization strategies, let's

17
00:00:52,730 --> 00:00:54,470
start with a brief overview of.

18
00:00:54,820 --> 00:00:56,050
Amazon EKS.

19
00:00:56,620 --> 00:01:01,260
Piyush, could you give us a
quick overview of Amazon EKS?

20
00:01:01,650 --> 00:01:02,660
Absolutely.

21
00:01:02,660 --> 00:01:05,410
Do we, if we have a slide,
yeah, let's point to that.

22
00:01:05,470 --> 00:01:10,550
So Amazon Elastic Kubernetes
service and you see the high level

23
00:01:10,640 --> 00:01:15,480
architecture diagram here Amazon EKS,
it's our managed Kubernetes offering.

24
00:01:15,900 --> 00:01:20,130
It makes it easy to run Kubernetes on AWS.

25
00:01:20,854 --> 00:01:26,345
without needing to install,
operate and maintain your own

26
00:01:26,384 --> 00:01:28,794
Kubernetes control plane or nodes.

27
00:01:30,275 --> 00:01:30,825
Absolutely.

28
00:01:30,825 --> 00:01:31,385
That's right.

29
00:01:31,445 --> 00:01:36,615
So with Amazon EKS, AWS manages
the control plane components

30
00:01:37,135 --> 00:01:40,225
and you can focus on deploying
and managing your applications.

31
00:01:41,135 --> 00:01:46,365
It integrates seamlessly with
other Amazon AWS with AWS services,

32
00:01:46,785 --> 00:01:50,915
giving you a secure and scalable
environment for your workloads.

33
00:01:52,525 --> 00:01:59,525
Yes, and as organizations adopt Kubernetes
for container orchestration, Amazon

34
00:01:59,595 --> 00:02:05,335
EKS offers a reliable and efficient
way to run Kubernetes at scale.

35
00:02:06,830 --> 00:02:07,160
Absolutely.

36
00:02:07,710 --> 00:02:10,630
But with great power comes
great responsibility.

37
00:02:11,130 --> 00:02:13,660
So in this case, potential for high cost.

38
00:02:13,910 --> 00:02:18,999
If not managed properly, I would say
Kubernetes abstracts away much of

39
00:02:18,999 --> 00:02:23,885
the underlying infrastructure, which
can sometimes make Cost management,

40
00:02:24,225 --> 00:02:25,815
a challenge, would you agree?

41
00:02:26,865 --> 00:02:31,085
Yes, of course, and that's where
a cost optimization strategy comes

42
00:02:31,085 --> 00:02:36,245
into play Understanding where costs
are incurred and how to manage them.

43
00:02:36,275 --> 00:02:40,190
It's crucial for running
efficient operations Exactly.

44
00:02:40,530 --> 00:02:44,960
Now, in today's session, we'll
explore tools like CubeCost for cost

45
00:02:44,960 --> 00:02:51,120
visibility, and we'll discuss scaling
solutions like Cluster Autoscaler and

46
00:02:51,150 --> 00:02:53,930
Carpenter to optimize resource usage.

47
00:02:55,029 --> 00:02:58,520
There was a keynote back
in reInvent 2023 by Dr.

48
00:02:58,520 --> 00:03:04,200
Werner Vogel where where he emphasized
the importance of building cost aware

49
00:03:04,230 --> 00:03:10,279
workloads, particularly relevant
for Kubernetes cluster on AWS cloud.

50
00:03:10,779 --> 00:03:13,139
Yes, I remember watching that ocean.

51
00:03:13,449 --> 00:03:15,679
That's what inspired our session as well.

52
00:03:16,229 --> 00:03:21,069
When it comes to kubernetes cost, maybe
if you can transition to the next slide,

53
00:03:21,179 --> 00:03:27,879
that is where we showcase the overall cost
and The control plane cost it's fixed at

54
00:03:27,999 --> 00:03:34,259
10 cents an hour So even when you scale
up your amazon eks cluster to hundreds and

55
00:03:34,259 --> 00:03:39,849
thousands of node, there's no additional
control plane cost Which is great the

56
00:03:39,849 --> 00:03:44,649
main cost component is on the data plane
which comprises of Workload cost which

57
00:03:44,649 --> 00:03:49,639
includes your pods your containers your
auto scaling parameters and we'll cover

58
00:03:49,649 --> 00:03:56,084
some of that In this session next within
the data plane for some workloads, you

59
00:03:56,114 --> 00:04:01,814
incurred network cost, which includes
for data transfer, your elastic container

60
00:04:01,844 --> 00:04:08,664
registry cost, your load balancers and
costs doing your NAT gateway deployments.

61
00:04:09,744 --> 00:04:13,054
And then finally, you incur logging
and monitoring cost on the data plane.

62
00:04:14,584 --> 00:04:15,174
Good points.

63
00:04:15,464 --> 00:04:20,634
Maybe I should, maybe we should go back to
the slide that we had Just the back here.

64
00:04:21,154 --> 00:04:25,504
So when you mentioned the control
plane, I see two boxes here.

65
00:04:26,674 --> 00:04:31,224
Would you say the control plane is the box
on the left hand side and the data plane

66
00:04:31,224 --> 00:04:33,024
is on the one on the right hand side.

67
00:04:33,024 --> 00:04:36,194
So which part is really managed by AWS?

68
00:04:36,744 --> 00:04:41,304
I would say the other one that the
control plane runs on the AWS cloud,

69
00:04:41,584 --> 00:04:43,984
whereas data plane runs in your VPC.

70
00:04:45,759 --> 00:04:49,709
So over here, what we showcase is a
couple of options within data plane.

71
00:04:49,779 --> 00:04:53,159
You have self managed options,
there is a manage options, and

72
00:04:53,159 --> 00:04:54,609
then there is Fargate as well.

73
00:04:55,239 --> 00:05:01,139
So yeah, so it's, so AWS is managing
the the brain of Kubernetes, right?

74
00:05:01,139 --> 00:05:03,989
So to speak the control
plane, the orchestration, and

75
00:05:03,989 --> 00:05:05,079
it's on the right hand side.

76
00:05:05,789 --> 00:05:07,399
And that's where we have the label.

77
00:05:08,189 --> 00:05:11,529
If you can see customer VPC that's
what we mean by the control plane.

78
00:05:12,029 --> 00:05:13,629
Other way around AWS cloud.

79
00:05:15,574 --> 00:05:16,564
Oh, there you go.

80
00:05:16,634 --> 00:05:16,874
Yes.

81
00:05:17,054 --> 00:05:17,984
AWS Cloud.

82
00:05:18,314 --> 00:05:18,944
Absolutely.

83
00:05:18,944 --> 00:05:20,444
I was looking at the left hand side.

84
00:05:20,764 --> 00:05:25,714
So managed self-managed note groups and
managed note groups on my left by customer

85
00:05:25,714 --> 00:05:28,534
VPC and on the right is AWS Cloud.

86
00:05:29,144 --> 00:05:34,064
Alright, so as you can see, this video
this recording is not really scripted.

87
00:05:34,594 --> 00:05:35,134
Alright.

88
00:05:35,139 --> 00:05:40,124
So if you look at the recent CNCF
studies study on the Kubernetes

89
00:05:40,124 --> 00:05:44,389
cost back in December of 2023,
that's something that we looked at.

90
00:05:44,409 --> 00:05:44,629
Yes.

91
00:05:45,279 --> 00:05:50,079
Yeah, if you can move to that
study on the deck, yeah, perfect.

92
00:05:50,419 --> 00:05:55,819
Yeah, the study confirmed that
Kubernetes cost are indeed increasingly

93
00:05:55,829 --> 00:05:59,859
for increasing for many of the
organizations that we work with.

94
00:06:00,849 --> 00:06:04,549
Further, this survey revealed that
a number of human and technology

95
00:06:04,549 --> 00:06:09,859
factors were blamed for the increase
in both spent and unwanted and

96
00:06:09,919 --> 00:06:12,609
unexpected cost in cloud environments.

97
00:06:13,129 --> 00:06:18,469
Over provisioning is by far the most
common issue, likely due to difficulties

98
00:06:18,799 --> 00:06:23,629
in accurately estimating resource needs
in dynamic Kubernetes environments.

99
00:06:24,599 --> 00:06:30,899
Failure to deactivate unused resources was
cited as another issue and the presence

100
00:06:31,039 --> 00:06:33,649
of the technical debt, not to forget.

101
00:06:33,809 --> 00:06:38,609
So the workloads which have not been
optimized for cloud native environments

102
00:06:39,199 --> 00:06:41,389
really leads to inefficiencies.

103
00:06:41,779 --> 00:06:42,549
Piyush, would you agree?

104
00:06:43,049 --> 00:06:47,169
Yes no, I think this is a
very useful survey from a.

105
00:06:48,174 --> 00:06:53,254
Reputed organization cncf I have
some more surveys if you can

106
00:06:53,254 --> 00:06:57,974
go to the next slide I think we
covered this reasons for overspend.

107
00:06:58,004 --> 00:07:02,154
We covered the over provisioning
and the presence of technical debt.

108
00:07:02,154 --> 00:07:07,119
So if you go to the next one where
we look at the tooling survey I'd

109
00:07:07,129 --> 00:07:11,429
say if you look here, cloud native
default tooling came in first, like

110
00:07:11,439 --> 00:07:16,029
AWS cost Explorer in case of AWS cloud
in terms of purpose built tooling for

111
00:07:16,029 --> 00:07:18,789
Kubernetes cost cube cost came in first.

112
00:07:19,614 --> 00:07:25,394
What we see many customers using
KubeCost along with AWS Cost Explorer or

113
00:07:25,444 --> 00:07:27,674
traditional cloud cost management tooling.

114
00:07:28,434 --> 00:07:33,574
So while AWS Cost Explorer will
perform overall cloud cost and

115
00:07:33,574 --> 00:07:39,504
optimization, KubeCost would be
hyper focused on Kubernetes or Amazon

116
00:07:39,694 --> 00:07:43,794
Elastic Container Service, EKS, our
managed offering for Kubernetes.

117
00:07:45,804 --> 00:07:46,414
Absolutely.

118
00:07:46,414 --> 00:07:47,364
Great points here.

119
00:07:48,034 --> 00:07:51,494
All right we talked about the cube
cost, and that's something that

120
00:07:51,494 --> 00:07:54,994
probably we can do a quick overview.

121
00:07:55,884 --> 00:07:57,314
Is that something that
you want to start with?

122
00:07:57,314 --> 00:07:59,754
Or do you want to go into
a demo on a terminal?

123
00:08:00,724 --> 00:08:01,444
Yes.

124
00:08:01,444 --> 00:08:03,684
No, let's bring up the dashboard.

125
00:08:03,684 --> 00:08:05,364
And now that we've set the stage.

126
00:08:05,364 --> 00:08:05,554
Yeah.

127
00:08:05,554 --> 00:08:11,452
If you can discuss more about cost
visibility and why do we need it?

128
00:08:11,452 --> 00:08:13,246
And what's the sense of it?

129
00:08:13,246 --> 00:08:13,545
Yep.

130
00:08:13,545 --> 00:08:13,844
Awesome.

131
00:08:13,844 --> 00:08:14,824
Can you see my screen?

132
00:08:15,394 --> 00:08:15,874
Yes, I do.

133
00:08:15,924 --> 00:08:16,774
Okay, perfect.

134
00:08:16,774 --> 00:08:22,784
So what you see here it's a view
of KubeCast that's been deployed or

135
00:08:22,784 --> 00:08:25,754
rather installed on our EKS cluster.

136
00:08:25,804 --> 00:08:29,724
Now that's a demo version that
we're using, but you can also deploy

137
00:08:29,724 --> 00:08:31,954
this on your own EKS platform.

138
00:08:31,974 --> 00:08:35,434
The reason I'm using the demo,
it's got a lot more features than

139
00:08:36,074 --> 00:08:37,684
I would have with the free version.

140
00:08:38,144 --> 00:08:39,234
Comes into two tiers.

141
00:08:39,254 --> 00:08:41,304
One is the free version
was the enterprise version.

142
00:08:42,394 --> 00:08:44,084
Here, I'm looking at the at this cost.

143
00:08:44,124 --> 00:08:45,324
Yes, you're going to mention something.

144
00:08:46,084 --> 00:08:46,384
Yeah.

145
00:08:46,384 --> 00:08:46,714
No.

146
00:08:46,714 --> 00:08:51,134
So maybe so Darush, what's the challenge
here that KubeCost is addressing?

147
00:08:51,214 --> 00:08:55,764
Is it the busy getting the cost
visibility, understanding where cost is

148
00:08:55,774 --> 00:08:58,644
coming from within the Amazon EKS cluster?

149
00:08:58,644 --> 00:09:00,604
Is that what it is or something else?

150
00:09:01,144 --> 00:09:01,814
Absolutely.

151
00:09:01,844 --> 00:09:05,064
So it's really within EKS or Kubernetes.

152
00:09:05,619 --> 00:09:09,969
It breaks down the cost
by namespace, pod, teams.

153
00:09:10,139 --> 00:09:13,239
If you have set up labels, for
example, for your deployments.

154
00:09:13,519 --> 00:09:17,349
It really, that, and that breaks,
that breakdown is what what some

155
00:09:17,349 --> 00:09:18,959
of our customers are looking for.

156
00:09:19,709 --> 00:09:24,889
Yes, and it's particularly important
because kubernetes resources as some

157
00:09:24,889 --> 00:09:29,159
of our viewers will know they're
very dynamic They can scale rapidly.

158
00:09:29,169 --> 00:09:33,729
So it makes it difficult to track costs
associated with let's say specific

159
00:09:33,729 --> 00:09:38,769
applications specific teams And that's
where that's the space kube cost fills

160
00:09:38,769 --> 00:09:41,039
in or that's why kube cost exists, right?

161
00:09:41,589 --> 00:09:46,919
Yeah, go ahead And moreover, with the
integration of AWS split cost allocation

162
00:09:46,929 --> 00:09:51,769
data, Q cost can provide even more
insights into cost attribution in EKS.

163
00:09:53,924 --> 00:09:54,104
Good.

164
00:09:54,104 --> 00:09:58,704
And for those who may be unfamiliar
with this new term that Darush used,

165
00:09:59,044 --> 00:10:04,464
AWS split cost allocation data, what
it allows for is the distribution of

166
00:10:04,464 --> 00:10:10,554
shared costs like data transfer and EBS
volumes across various AWS resources

167
00:10:10,964 --> 00:10:13,714
giving a more precise cost allocation.

168
00:10:15,224 --> 00:10:15,724
Exactly.

169
00:10:15,774 --> 00:10:20,094
So maybe we should talk a
little bit about AWS split cost.

170
00:10:20,759 --> 00:10:21,659
Allocation data.

171
00:10:23,549 --> 00:10:23,709
Good.

172
00:10:23,759 --> 00:10:24,019
Yes.

173
00:10:24,069 --> 00:10:24,319
Yeah.

174
00:10:24,379 --> 00:10:24,829
Happy to.

175
00:10:25,189 --> 00:10:30,419
So I would say AWS has introduced
some amazing features to help enhance

176
00:10:30,519 --> 00:10:32,919
cost visibility for your Amazon.

177
00:10:33,284 --> 00:10:37,874
Eks cluster including the split
cost allocation data feature.

178
00:10:38,244 --> 00:10:42,254
So this feature, it allows you to break
down the cost of shared resource, like

179
00:10:42,264 --> 00:10:47,874
EC2 instances, networking and storage
into individual Kubernetes workloads.

180
00:10:50,794 --> 00:10:52,564
What does that really mean for you?

181
00:10:52,744 --> 00:10:56,834
Traditionally, if multiple pods
are running on a single EC2

182
00:10:56,834 --> 00:11:01,199
instance, You just see the overall
cost of that instance in a W.

183
00:11:01,199 --> 00:11:01,349
S.

184
00:11:01,399 --> 00:11:02,359
Cost Explorer.

185
00:11:03,159 --> 00:11:04,069
But with a W.

186
00:11:04,069 --> 00:11:04,229
S.

187
00:11:04,269 --> 00:11:09,019
Cost allocation, you can now break
down the cost of that instance and

188
00:11:09,039 --> 00:11:13,639
attribute portion of it, a portion
of it to each workload, if you will.

189
00:11:14,349 --> 00:11:20,189
Based on their actual resource usage,
this makes it easier, a lot easier, I

190
00:11:20,189 --> 00:11:26,519
would say, to identify which workloads are
contributing most to your overall costs.

191
00:11:28,159 --> 00:11:30,259
Yes, I think that's a good point.

192
00:11:30,259 --> 00:11:35,559
And if you were to combine tube
costs, real time visibility.

193
00:11:36,449 --> 00:11:39,729
Where the AWS is split cost allocation.

194
00:11:40,299 --> 00:11:44,749
You get a powerful way to track
and optimize your cost, right?

195
00:11:44,759 --> 00:11:48,209
You'll be able to precisely see
where your spend is going, whether

196
00:11:48,269 --> 00:11:54,119
it's what CPU memory storage network
and it allows you to take the right

197
00:11:54,179 --> 00:11:56,469
actions to optimize your cloud spend.

198
00:11:57,019 --> 00:12:00,109
So together, these tools
give you the level of detail.

199
00:12:00,854 --> 00:12:04,124
You need to make informed
decision about your infrastructure

200
00:12:04,124 --> 00:12:05,284
and resource management.

201
00:12:06,444 --> 00:12:06,764
Great.

202
00:12:06,804 --> 00:12:07,704
Yeah, absolutely.

203
00:12:07,714 --> 00:12:12,414
So next we're going to look at going
back to cube costs and see how it really

204
00:12:12,414 --> 00:12:14,814
helps identify underutilized resources.

205
00:12:15,204 --> 00:12:18,044
Look at some of the recommendations
that you can see on the dashboard.

206
00:12:18,414 --> 00:12:21,274
We'll walk through some of how to
spot some of these opportunities

207
00:12:21,654 --> 00:12:26,964
for optimization and review the
actionable insight cube cost provides.

208
00:12:27,649 --> 00:12:31,349
So let's jump into a quick
demo and see how it works.

209
00:12:33,299 --> 00:12:33,729
All right.

210
00:12:34,249 --> 00:12:39,149
So we can see that we have the
overview for the cube cost dashboard.

211
00:12:39,639 --> 00:12:44,679
Under monitor, you have a number of
filters, and those filters could be

212
00:12:44,989 --> 00:12:50,969
by namespace, nodes, pods, controllers
and containers, or in some cases,

213
00:12:50,979 --> 00:12:52,619
environments and deployments.

214
00:12:53,279 --> 00:12:56,049
We have here some examples of those.

215
00:12:56,559 --> 00:13:00,189
And the more environments you have or the
more departments you have with the labels

216
00:13:00,189 --> 00:13:02,799
deployed, you'll get to see it here.

217
00:13:03,379 --> 00:13:08,619
One area that I really like is
the reports where you can have a

218
00:13:08,619 --> 00:13:12,889
different number of reports created
based on what matters to you most.

219
00:13:13,279 --> 00:13:17,369
And, or, last but not least, the
most important one to me right now

220
00:13:17,399 --> 00:13:23,999
is the savings, where I can see,
as you can see, number of options.

221
00:13:24,059 --> 00:13:27,089
For example, here, right
sizing the cluster nodes.

222
00:13:27,824 --> 00:13:31,854
And if you click on it, it
comes back and there's a lot of

223
00:13:31,854 --> 00:13:33,294
details where we can get into.

224
00:13:34,114 --> 00:13:41,654
But at a high level you can see that, yes,
we are running a series of CPU families

225
00:13:41,934 --> 00:13:46,304
instance families, I should say, and the
recommendations which are most important.

226
00:13:48,144 --> 00:13:48,894
Going back to instance.

227
00:13:50,254 --> 00:13:50,634
Yes.

228
00:13:50,644 --> 00:13:56,034
And I'm just curious, what is cube cost
basing all these recommendations on?

229
00:13:56,074 --> 00:13:58,194
So what data does it have access to?

230
00:13:58,244 --> 00:14:04,754
Is it some specific data within EKS
cluster that it is tapping into and

231
00:14:04,784 --> 00:14:06,374
coming up with this recommendation?

232
00:14:06,959 --> 00:14:07,789
Good questions.

233
00:14:07,829 --> 00:14:11,519
I don't have the insight of how
it really delves into the details

234
00:14:11,519 --> 00:14:16,179
of putting the data but over time,
this is an example of a demo.

235
00:14:16,179 --> 00:14:17,639
If you had installed it on our own, E.

236
00:14:17,639 --> 00:14:17,859
K.

237
00:14:17,859 --> 00:14:18,169
S.

238
00:14:18,249 --> 00:14:20,529
Could be probably looking
at a lot more insight.

239
00:14:21,199 --> 00:14:25,469
I would say it looks at the load and
the load that is running on the E.

240
00:14:25,469 --> 00:14:25,749
K.

241
00:14:25,749 --> 00:14:26,009
S.

242
00:14:26,009 --> 00:14:28,009
And make some intelligent decision.

243
00:14:28,639 --> 00:14:34,769
In one case, we are running, for example,
carpenter carpenter auto scaler, where

244
00:14:34,769 --> 00:14:40,674
it decides the What is the best family
type of easy to instances or compute?

245
00:14:41,274 --> 00:14:43,094
That's really suited for your workload.

246
00:14:43,704 --> 00:14:44,784
But that's a good question.

247
00:14:45,364 --> 00:14:46,654
That's something that you should.

248
00:14:47,614 --> 00:14:48,074
Yes.

249
00:14:48,074 --> 00:14:48,334
Yes.

250
00:14:48,334 --> 00:14:50,964
No, what I see in the, I was just
speaking at the documentation.

251
00:14:50,964 --> 00:14:53,714
What I see is that there are
a couple of different options.

252
00:14:53,754 --> 00:14:59,244
One it queries metrics from Prometheus, as
some of you may know, it's our open source

253
00:14:59,784 --> 00:15:01,664
monitoring and time series database.

254
00:15:01,665 --> 00:15:06,100
It's installed by default when KubeCost
is installed on an Amazon EKS cluster.

255
00:15:06,320 --> 00:15:09,189
So KubeCost will query
metrics from Prometheus.

256
00:15:09,510 --> 00:15:10,380
In addition.

257
00:15:10,605 --> 00:15:15,704
It uses API calls to retrieve some
public pricing data for AWS services.

258
00:15:16,174 --> 00:15:21,324
And then there are some other integrations
like AWS cost and usage reports to

259
00:15:21,324 --> 00:15:25,764
further enhance the accuracy of pricing
information specific to a given account.

260
00:15:26,184 --> 00:15:30,954
So it relies on all of these
information and then I'm sure

261
00:15:30,994 --> 00:15:34,074
there's some intelligence built in
and that's how it's coming up with

262
00:15:34,074 --> 00:15:36,064
this recommended savings for you.

263
00:15:37,374 --> 00:15:38,904
And You mentioned Prometheus.

264
00:15:38,934 --> 00:15:42,444
I think we also have a managed, I
believe we have a managed service

265
00:15:42,444 --> 00:15:47,394
of Prometheus and Grafano on AWS,
where you can also take advantage of.

266
00:15:51,634 --> 00:15:52,124
All right.

267
00:15:52,304 --> 00:15:53,284
This was a great demo.

268
00:15:53,354 --> 00:15:54,364
Yeah, this was a great demo.

269
00:15:55,114 --> 00:15:57,684
You should have more time to get
into a lot more details, but I

270
00:15:57,694 --> 00:16:01,504
just want to introduce some of the
capabilities that cube cost gives you.

271
00:16:01,594 --> 00:16:06,064
And it's a free there is a free
version that users can deploy on their

272
00:16:06,094 --> 00:16:09,704
EKS platform and really give it a
spin and see how it works for them.

273
00:16:12,734 --> 00:16:13,164
All right.

274
00:16:14,839 --> 00:16:19,259
Next topic we want to talk to is
about auto scaling strategies.

275
00:16:19,829 --> 00:16:25,159
Let me move that away and go back
to our auto scaling strategies.

276
00:16:25,199 --> 00:16:29,519
And we are introducing carpenter
as one option, but we talked about

277
00:16:29,519 --> 00:16:33,679
the cost optimization and, but I
think while visibility is crucial.

278
00:16:35,004 --> 00:16:39,224
Optimizing resource usage is equally
important for cost management.

279
00:16:39,834 --> 00:16:44,384
Let's discuss scaling
solutions in Amazon EKS.

280
00:16:44,544 --> 00:16:44,754
You

281
00:16:46,794 --> 00:16:47,044
should.

282
00:16:47,104 --> 00:16:48,494
Yeah, I can take that.

283
00:16:49,074 --> 00:16:52,384
A lot of folks are a lot
of customers we talk to.

284
00:16:52,394 --> 00:16:54,384
They're familiar with cluster autoscaler.

285
00:16:55,534 --> 00:16:56,694
That's how we started.

286
00:16:56,694 --> 00:17:01,094
And so cluster autoscaler
automatically adjusts the number

287
00:17:01,094 --> 00:17:05,354
of nodes in your cluster based on
the scheduling needs of your pods.

288
00:17:05,354 --> 00:17:05,404
Thanks.

289
00:17:06,919 --> 00:17:11,349
Now, that's helpful, but I
think it has its own use cases.

290
00:17:11,629 --> 00:17:17,889
But in some instances, I see some
feedback where it's somewhat slow in

291
00:17:17,909 --> 00:17:22,889
scaling times and less flexible with
instance type selection, if you will.

292
00:17:23,839 --> 00:17:25,999
Yeah, and we don't make it easy.

293
00:17:26,059 --> 00:17:29,799
We have hundreds of instances, and
they're always launching new instances.

294
00:17:29,799 --> 00:17:34,659
So just amplifies that
limitation quite a bit.

295
00:17:35,199 --> 00:17:37,759
And that's where Carpenter
comes into picture.

296
00:17:37,769 --> 00:17:43,369
It's one of our newer, open source,
efficient node provisioning tool.

297
00:17:44,389 --> 00:17:46,019
It was built by AWS.

298
00:17:46,049 --> 00:17:48,544
And we donated Carpenter to the team.

299
00:17:48,544 --> 00:17:53,674
as part of Kubernetes auto scaling
special interest group SIG.

300
00:17:54,184 --> 00:17:57,394
So Carpenter aims to improve
the efficiency and speed

301
00:17:57,394 --> 00:17:58,914
of scaling operations.

302
00:17:58,964 --> 00:18:00,324
It's Kubernetes native.

303
00:18:00,324 --> 00:18:04,404
And and what I mean by that is sometimes
your Kubernetes workloads, they may

304
00:18:04,404 --> 00:18:06,044
be required to run in certain AZ.

305
00:18:07,094 --> 00:18:11,674
Certain instance types part on
demand so how do you do that?

306
00:18:11,984 --> 00:18:15,754
You need mechanisms like node
selector node affinity gains and

307
00:18:15,784 --> 00:18:19,874
toleration topology spreads Right,
all of these are kubernetes concepts

308
00:18:20,154 --> 00:18:24,414
So carpenter works with your part
scheduling constraints and that's what

309
00:18:24,434 --> 00:18:27,664
makes it kubernetes native Exactly.

310
00:18:28,004 --> 00:18:33,564
Carpenter can provision new nodes
in response to unschedulable pods

311
00:18:33,914 --> 00:18:38,964
within seconds, optimizing for
cost and performance really by

312
00:18:38,984 --> 00:18:40,904
selecting the right instance types.

313
00:18:41,399 --> 00:18:42,159
On the fly.

314
00:18:44,189 --> 00:18:44,559
Yes.

315
00:18:44,609 --> 00:18:49,089
And it also supports features like
consolidation and bin packing which

316
00:18:49,109 --> 00:18:53,519
can significantly reduce costs by
making better use of resources.

317
00:18:55,159 --> 00:19:00,059
So let's jump into a quick Devos
Carpenter in action and see how it's

318
00:19:00,069 --> 00:19:05,589
really operates when we try to scale
up and down some of the resources.

319
00:19:07,129 --> 00:19:10,369
Is that a good time to go
into the terminal perhaps?

320
00:19:11,509 --> 00:19:13,069
Yeah, absolutely.

321
00:19:13,189 --> 00:19:13,559
Yeah.

322
00:19:14,599 --> 00:19:18,709
Maybe if you could start with what's
your starting state, I see two windows.

323
00:19:18,809 --> 00:19:22,089
You can provide some more
background, what it is.

324
00:19:22,449 --> 00:19:29,419
This is essentially the, I have an EKS
cluster deployed on AWS and I've logged

325
00:19:29,419 --> 00:19:35,359
into the, I've set the context for the
cluster that we're using the top window.

326
00:19:35,419 --> 00:19:37,289
It's an EKS node viewer.

327
00:19:37,289 --> 00:19:40,419
It's an open source tool that
you can download and install

328
00:19:40,419 --> 00:19:42,574
on your Kubernetes On your E.

329
00:19:42,584 --> 00:19:42,774
K.

330
00:19:42,774 --> 00:19:42,964
S.

331
00:19:42,964 --> 00:19:47,584
Cluster, and it really gives
you a live view near real time.

332
00:19:47,614 --> 00:19:49,264
I would say view into your E.

333
00:19:49,264 --> 00:19:49,444
K.

334
00:19:49,444 --> 00:19:49,634
S.

335
00:19:49,634 --> 00:19:51,664
Cluster on that first line.

336
00:19:51,664 --> 00:19:53,544
It shows three nodes in the space.

337
00:19:53,544 --> 00:19:58,784
I'm running three nodes and 51
parts on the right hand side.

338
00:19:59,204 --> 00:20:04,634
It gives you an estimated cost off
per nodes per hour and or per month,

339
00:20:04,854 --> 00:20:06,324
which is to some extent helpful.

340
00:20:06,724 --> 00:20:07,944
It's not an exact answer.

341
00:20:07,944 --> 00:20:07,994
Yeah.

342
00:20:08,214 --> 00:20:11,274
Price, but it gives you
an estimate below that.

343
00:20:11,284 --> 00:20:16,444
It also shows you the instance
families that are used both by E.

344
00:20:16,444 --> 00:20:16,704
K.

345
00:20:16,704 --> 00:20:17,064
S.

346
00:20:17,264 --> 00:20:21,224
For the main cluster and other resources
that you need for your deployment.

347
00:20:21,724 --> 00:20:25,714
The screen below just a terminal
screen where and take a look at

348
00:20:25,714 --> 00:20:30,384
You get deploy, and you can see
here I have three deployments.

349
00:20:30,394 --> 00:20:37,034
One is CPU Memory Hungry app, named after
Angry app and next is Inflate and Nginx.

350
00:20:38,114 --> 00:20:40,954
Each one of them, starting
from the top, we have seven

351
00:20:40,954 --> 00:20:43,154
replicas, 13 replicas in one.

352
00:20:44,084 --> 00:20:50,784
So in this case you can scale up the
replicas on the Hungry app, from seven to

353
00:20:50,784 --> 00:20:53,199
something else, So that we can scale up.

354
00:20:54,049 --> 00:20:57,719
So let's say scale up
in late or rather CPU

355
00:20:58,279 --> 00:21:07,169
memory from seven replicas to 125
replicas on while, or I click.

356
00:21:07,799 --> 00:21:10,589
Yeah, before you hit
enter, I'm just curious.

357
00:21:11,069 --> 00:21:14,059
So up in the top window, we see.

358
00:21:14,829 --> 00:21:20,569
Like 48 parts running and then at
the bottom we have three applications

359
00:21:20,579 --> 00:21:23,959
So looks like there are more parts I
think that's an interesting one and

360
00:21:24,519 --> 00:21:28,629
part of it is because carpenter has
visibility into the control plane

361
00:21:28,629 --> 00:21:36,949
and carpenter recognizes kubernetes
resources Which are on the Kubernetes

362
00:21:36,959 --> 00:21:41,369
control plane, and that's where those
additional resources comes into play.

363
00:21:41,859 --> 00:21:46,369
Exactly, because within the within EKS
or Kubernetes in general, they have a

364
00:21:46,369 --> 00:21:50,769
number of different namespaces based
on what applications you deploy and the

365
00:21:50,769 --> 00:21:53,279
namespaces that you as a user you create.

366
00:21:53,869 --> 00:21:58,139
Now within the cube system
namespace on Kubernetes or on EKS.

367
00:21:59,044 --> 00:22:03,074
There are a number of pods
that are spun up to support the

368
00:22:03,074 --> 00:22:07,574
Kubernetes infrastructure, be it
the control plane, the scheduler,

369
00:22:07,784 --> 00:22:10,664
the proxies, the core DNS and so on.

370
00:22:10,844 --> 00:22:13,794
That was a great point you
brought up because looking at the

371
00:22:13,794 --> 00:22:17,624
number of pods and looking at the
screen below, it doesn't add up.

372
00:22:17,834 --> 00:22:20,739
But that's why those are
pos that are not showing.

373
00:22:20,799 --> 00:22:22,329
'cause I'm only looking ads.

374
00:22:22,844 --> 00:22:27,194
A deployment, the number of
deployments that I have got it.

375
00:22:27,254 --> 00:22:27,464
Yep.

376
00:22:28,594 --> 00:22:30,714
So let's scale up your hungry app.

377
00:22:31,034 --> 00:22:31,614
All right.

378
00:22:31,734 --> 00:22:37,534
And as you can see on the top dynamically,
it's creating the parts and wait.

379
00:22:37,674 --> 00:22:43,304
So pending running within a
few seconds or pretty fast.

380
00:22:43,544 --> 00:22:45,074
Yeah, it is pretty fast.

381
00:22:45,074 --> 00:22:45,124
Yeah.

382
00:22:45,589 --> 00:22:52,189
It's pretty fast in terms of response, and
I really took a big number here, Piyush.

383
00:22:52,849 --> 00:22:53,109
Yes.

384
00:22:53,389 --> 00:22:56,569
The demo gods are with us 25 pending.

385
00:22:57,069 --> 00:23:00,949
It's really a simple app, but I can,
we can take a look at some of the

386
00:23:01,369 --> 00:23:03,089
some of the one interesting thing.

387
00:23:03,099 --> 00:23:09,212
The other one is, so of the three nodes,
I see two are on demand and one is spot.

388
00:23:09,212 --> 00:23:11,122
That's interesting to me.

389
00:23:11,162 --> 00:23:16,582
And perhaps if we can Take a look at the
configuration, node pool configuration

390
00:23:17,122 --> 00:23:20,922
because there was something in the node
pool configuration that made pot pointer

391
00:23:20,922 --> 00:23:23,732
decide to spin on demand and spot.

392
00:23:23,732 --> 00:23:27,012
And yeah, for the benefit of our
viewers, if you can bring it up.

393
00:23:27,477 --> 00:23:29,697
I have a couple of deployments.

394
00:23:29,717 --> 00:23:35,157
One is the high priority yaml file,
which essentially shows the deployment.

395
00:23:35,157 --> 00:23:38,157
The other one is the hungry app.

396
00:23:38,267 --> 00:23:39,657
Let's see hungry app.

397
00:23:39,777 --> 00:23:43,927
And so when you say couple of,
you mean you have multiple node

398
00:23:43,957 --> 00:23:48,837
pools and then there is a way to
run them in parallel, assign some.

399
00:23:49,512 --> 00:23:53,362
Wait to it and then, okay, let's
take a look at one, if you want

400
00:23:53,362 --> 00:23:59,172
to hi, just be mindful of time.

401
00:23:59,342 --> 00:24:04,492
I'm not sure how much time we have left,
but in this case we have a note pools.

402
00:24:04,867 --> 00:24:09,867
We have selected specified different
families of instances that are

403
00:24:09,867 --> 00:24:15,987
available to Carpenter and one
on line 40, the operator, the

404
00:24:15,987 --> 00:24:18,037
values are spot and on demand.

405
00:24:19,177 --> 00:24:24,877
So Carpenter can pick and select depending
on the workload that's on Carpenter.

406
00:24:25,507 --> 00:24:26,027
Got it.

407
00:24:26,537 --> 00:24:31,617
And what happens is there is this trade
off for spot where if you just choose

408
00:24:31,617 --> 00:24:36,402
the lowest price, sometimes, You get
a very small capacity pool, a pool

409
00:24:36,402 --> 00:24:40,512
that's almost unavailable, and it's
at a very high risk of interruption.

410
00:24:41,372 --> 00:24:45,982
So what Carpenter does is it uses EC2
fleet API feature, which relies on

411
00:24:46,022 --> 00:24:48,522
price capacity optimized strategy.

412
00:24:48,992 --> 00:24:49,812
So it gives us.

413
00:24:50,212 --> 00:24:53,712
It gives you the cheapest instance
type, but then it doesn't give you the

414
00:24:53,712 --> 00:24:55,322
one which is about to be interrupted.

415
00:24:56,052 --> 00:24:59,332
So there's a bit of balance in
there and Carpenter relies on

416
00:24:59,342 --> 00:25:01,412
fleet API to make that trade off.

417
00:25:01,952 --> 00:25:02,572
Great point.

418
00:25:02,672 --> 00:25:04,842
Yeah that's a great point to bring up.

419
00:25:05,052 --> 00:25:09,282
And we have also a parameter
called weight within Carpenter.

420
00:25:09,432 --> 00:25:14,412
Here I have set the weight for 20 for
high priority workloads versus a lower

421
00:25:14,412 --> 00:25:16,682
number for lower priority workloads.

422
00:25:16,732 --> 00:25:19,022
All right, so let me scale

423
00:25:21,422 --> 00:25:30,732
down in this case and see, okay, scale
down, going back to example five replicas.

424
00:25:31,468 --> 00:25:32,887
And in this case,

425
00:25:33,467 --> 00:25:38,257
if the demand on the application that we
have deployed, the hungry app, it doesn't

426
00:25:38,257 --> 00:25:42,057
really consume a lot of resources, but
if you had an application that would

427
00:25:42,057 --> 00:25:46,167
consume a lot more resources, it would
spin up additional instance family type

428
00:25:46,168 --> 00:25:52,297
like spot instance and if required on
demand instances in the interest of time.

429
00:25:52,847 --> 00:25:58,517
Probably it's a good time to talk
about a few takeaways from our

430
00:25:58,517 --> 00:26:03,797
discussion as we have seen effective
cost or cost optimization in KS.

431
00:26:04,087 --> 00:26:08,307
Involves both visibility and action use.

432
00:26:08,427 --> 00:26:11,887
Yes, and there are tools like
cube cost that you should

433
00:26:12,587 --> 00:26:13,747
give a presentation on that.

434
00:26:13,757 --> 00:26:17,607
And it gets you visibility to
understand where your spend is going.

435
00:26:17,747 --> 00:26:22,097
Very granular level parts,
namespaces and whatnot.

436
00:26:22,177 --> 00:26:22,367
Yep.

437
00:26:23,197 --> 00:26:28,267
And scaling solutions like Carpenter
allows you to act on that information

438
00:26:28,317 --> 00:26:33,467
by optimizing resource allocation in
somewhat near real time, if you will.

439
00:26:34,297 --> 00:26:34,737
Yes.

440
00:26:34,777 --> 00:26:42,572
And what we recommend at Amazon is don't
treat this cost optimization as a one off

441
00:26:42,572 --> 00:26:47,447
thing, something that you do only at the
time of migration or when you cut over.

442
00:26:49,212 --> 00:26:53,622
Continuously monitor your cost,
regularly review your resource

443
00:26:53,622 --> 00:26:58,312
optimization or your resource
utilization and then adopted dynamic

444
00:26:58,312 --> 00:27:00,292
scaling solutions we saw carpenter.

445
00:27:00,412 --> 00:27:05,752
Yeah, and also don't forget the
importance of right sizing your workloads

446
00:27:06,072 --> 00:27:09,012
and cleaning up unused resources.

447
00:27:09,522 --> 00:27:13,062
Small adjustments can lead to significant.

448
00:27:13,432 --> 00:27:14,522
savings over time.

449
00:27:15,922 --> 00:27:16,272
Yes.

450
00:27:16,272 --> 00:27:19,872
And like we discussed cost
optimization, it's an ongoing process.

451
00:27:19,872 --> 00:27:24,762
So stay proactive and leverage
the tools at your disposal.

452
00:27:25,022 --> 00:27:28,842
We showcased cube cost carpenter, but
then, yeah, there are more out there.

453
00:27:29,862 --> 00:27:30,232
All right.

454
00:27:30,352 --> 00:27:32,342
So thank you all for joining us today.

455
00:27:32,702 --> 00:27:36,432
We hope you found this session
valuable and that you can apply these

456
00:27:36,442 --> 00:27:38,712
strategies in your own environments.

457
00:27:40,382 --> 00:27:44,042
Yes thank you everyone and feel
free to reach out to us with any

458
00:27:44,042 --> 00:27:46,462
questions or any further discussions.

459
00:27:46,922 --> 00:27:50,982
And perhaps we can, before we wrap up
and let's do a quick recap if we go to

460
00:27:50,982 --> 00:27:53,042
this slide, do we have a slide on that?

461
00:27:53,072 --> 00:27:53,512
Yes.

462
00:27:53,552 --> 00:27:53,742
Yep.

463
00:27:54,212 --> 00:27:55,762
Do you want me to take it or okay.

464
00:27:55,762 --> 00:27:56,532
Yeah, I can do that.

465
00:27:56,552 --> 00:28:00,692
So as a recap, what we did today
was we we started out looking at the

466
00:28:00,702 --> 00:28:02,992
breakdown of your Amazon EKS cost.

467
00:28:03,382 --> 00:28:07,472
We positioned cube cost as a way
to get granular cost visibility

468
00:28:07,472 --> 00:28:09,762
within your Amazon EKS clusters.

469
00:28:09,892 --> 00:28:14,032
And then we highlighted different
auto scaling strategies and Did a

470
00:28:14,332 --> 00:28:18,392
sort of deep dive demo on carpenter,
which is an efficient cluster

471
00:28:18,392 --> 00:28:20,662
node autoscaler for Kubernetes.

472
00:28:20,942 --> 00:28:24,572
In between, we highlighted how
important it is to ensure that

473
00:28:24,582 --> 00:28:29,972
these cost optimization strategies
are sustainable over a long period,

474
00:28:30,632 --> 00:28:34,692
which requires you to establish
the cost governance, so to speak.

475
00:28:34,693 --> 00:28:40,112
Yeah, that's all we had.

476
00:28:40,302 --> 00:28:44,112
Once again, thank you very much
for joining and see you next time.

477
00:28:44,592 --> 00:28:45,152
Thank you all.

