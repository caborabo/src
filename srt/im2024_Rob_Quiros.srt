1
00:00:00,110 --> 00:00:00,620
Hi, everyone.

2
00:00:00,620 --> 00:00:01,600
I'm Rob Kuros.

3
00:00:01,750 --> 00:00:06,880
I'm CEO and co founder of Caber Systems,
and I'm here to talk to you today about

4
00:00:06,890 --> 00:00:11,889
how we can fix the high false positive
rate you all are dealing with every day.

5
00:00:12,379 --> 00:00:16,530
I hope you'll find this talk valuable, and
I invite you to connect with me via email.

6
00:00:16,905 --> 00:00:22,535
Or LinkedIn if you do I spent 30 years
on the vendor side of IT and security

7
00:00:22,535 --> 00:00:25,205
at the likes of Cisco Riverbed Akamai.

8
00:00:26,325 --> 00:00:30,525
And after helping build one of the
first startups in the SAE or Secure

9
00:00:30,525 --> 00:00:35,165
Access Service Edge space, I realized
our approach to cybersecurity

10
00:00:35,165 --> 00:00:37,085
today is completely broken.

11
00:00:38,085 --> 00:00:43,935
In 2023, the false positive rate,
according to the Sands Institute hit 95%.

12
00:00:44,575 --> 00:00:47,605
That was up 20% since 2019.

13
00:00:49,175 --> 00:00:55,205
And also me, we missed 20 percent
of all incidents despite that.

14
00:00:56,975 --> 00:01:00,715
And it can sometimes take years
before we could detect a compromise.

15
00:01:02,265 --> 00:01:03,895
So what's the problem here?

16
00:01:05,145 --> 00:01:09,635
one of the issues is collectively,
companies spend 60 billion on tools

17
00:01:09,635 --> 00:01:14,435
to find and alert us on indicators
that we might be compromised.

18
00:01:15,285 --> 00:01:18,315
And you already know false positives
are when we see these alerts

19
00:01:18,745 --> 00:01:23,085
that are indicators that turn
out not to be actual compromise.

20
00:01:24,274 --> 00:01:29,795
So how do we tell the difference between
an indicator and a true false or a

21
00:01:29,795 --> 00:01:33,964
true positive, an actual compromise?

22
00:01:35,760 --> 00:01:37,650
This goes back to security 101.

23
00:01:39,460 --> 00:01:44,400
It's when the indicator leads us to an
actual compromise to the confidentiality,

24
00:01:44,410 --> 00:01:47,000
integrity, or availability of data.

25
00:01:47,970 --> 00:01:51,580
As information security professionals,
our job is to protect data.

26
00:01:52,300 --> 00:01:57,550
So why aren't we detecting the
compromises to data directly

27
00:01:57,609 --> 00:01:59,736
instead of, Looking for indicators,

28
00:02:02,076 --> 00:02:05,246
the newest version of this cyber
security framework calls out the same

29
00:02:05,246 --> 00:02:07,516
protections for all states of data.

30
00:02:08,246 --> 00:02:14,106
But the only 1 of those states where
we can directly protect data is data

31
00:02:14,106 --> 00:02:16,386
at rest when it's on a storage system.

32
00:02:16,806 --> 00:02:19,716
In fact, when we talk about data
security products and tools.

33
00:02:20,166 --> 00:02:24,436
We're only talking about securing
data addressed, and that's because

34
00:02:24,436 --> 00:02:29,236
the on storage systems, that's
the only place where data and its

35
00:02:29,236 --> 00:02:33,286
ownership and permissions can be
directly linked for access control.

36
00:02:34,626 --> 00:02:38,146
So what do we do for access control
when it comes to our cloud native

37
00:02:38,146 --> 00:02:43,476
applications for data in transit and
data in use inside those applications?

38
00:02:44,076 --> 00:02:46,236
We don't even know what
the permissions are.

39
00:02:46,506 --> 00:02:49,296
We don't know who owns
the data inside the APIs.

40
00:02:49,736 --> 00:02:54,276
Because we don't copy that
information when those APIs copy

41
00:02:54,276 --> 00:02:55,916
the data from storage systems.

42
00:02:56,896 --> 00:03:04,476
Instead, we rely on controlling access
to the APIs, thinking that somehow that's

43
00:03:04,476 --> 00:03:10,796
going to be able to stand in as a proxy
for controlling access to the data.

44
00:03:12,076 --> 00:03:19,556
It's like we're arbitrating access
to bags of sand or bags that are

45
00:03:19,556 --> 00:03:23,816
labeled sand and thinking that
because it says sand on the outside

46
00:03:23,816 --> 00:03:25,596
that it contains sand on the inside.

47
00:03:28,216 --> 00:03:30,826
But we have no idea what the bags contain.

48
00:03:30,826 --> 00:03:37,016
There's no correlation between API
parameters and what the data is inside,

49
00:03:37,166 --> 00:03:40,206
who owns that data, and who can access it.

50
00:03:41,036 --> 00:03:45,176
In fact, we might as well be calling
zero trust network access API.

51
00:03:45,446 --> 00:03:48,276
It's infinite trust zero access,
because that's what we're doing.

52
00:03:48,526 --> 00:03:53,816
We're completely trusting the, these
header parameters and the labels on

53
00:03:53,816 --> 00:03:57,376
those bags to tell us what that data is.

54
00:03:58,906 --> 00:04:02,446
And our so called data leakage
prevention tools aren't any better.

55
00:04:02,836 --> 00:04:07,976
They tell us what data looks like,
not who owns it or the permissions

56
00:04:07,986 --> 00:04:09,966
on who can access that data.

57
00:04:10,296 --> 00:04:15,706
They can't tell you if it's your
data or if it's my data, and that's

58
00:04:15,706 --> 00:04:17,056
where we're getting into trouble.

59
00:04:17,701 --> 00:04:25,101
We have no way of effectively
doing authorization and access

60
00:04:25,101 --> 00:04:31,431
control on data in APIs and the
top security vulnerabilities.

61
00:04:31,431 --> 00:04:32,441
No surprise.

62
00:04:33,351 --> 00:04:35,181
are authorization failures.

63
00:04:37,261 --> 00:04:42,811
So in order to keep you from getting
access to my data, I need to know

64
00:04:42,821 --> 00:04:45,181
what is your data, what is my data.

65
00:04:46,051 --> 00:04:50,701
It can't be just a determination
of, hey, this data contains PII,

66
00:04:50,701 --> 00:04:55,091
or it looks like it's health care
information, because my health care

67
00:04:55,091 --> 00:04:58,161
information can't be distinguished
from your health care information.

68
00:04:59,341 --> 00:05:02,771
My PII could be yours,
could be somebody else's.

69
00:05:03,221 --> 00:05:07,691
What we have to have is
deterministic controls.

70
00:05:07,971 --> 00:05:10,201
Access control has to be deterministic.

71
00:05:10,661 --> 00:05:16,651
We need to know the identity of the
data, its ownership and permissions,

72
00:05:16,661 --> 00:05:20,791
like we know the identity of the
services and users that access it.

73
00:05:21,851 --> 00:05:26,541
That's what we need to
control data in use.

74
00:05:26,956 --> 00:05:32,336
And data, in transit, and that's going to
help us solve this false positive problem.

75
00:05:34,906 --> 00:05:41,126
A lot of people have called this
kind of direct control over data

76
00:05:41,136 --> 00:05:43,656
in use the holy grail of security.

77
00:05:44,831 --> 00:05:50,571
Because with it, we can
solve so many problems today.

78
00:05:51,131 --> 00:05:56,491
We can address broken object level
authorization, privileged articulation,

79
00:05:56,501 --> 00:06:03,381
SQL injections, and other, vulnerabilities
in our applications just by checking

80
00:06:03,381 --> 00:06:08,851
authorization rather than having
to look for chains of indicators

81
00:06:09,201 --> 00:06:10,631
that these things may be happening.

82
00:06:12,071 --> 00:06:18,336
And moreover, instead of it taking
us to Months, or in fact years, like

83
00:06:18,336 --> 00:06:22,536
it was in the case of First American,
to find a compromise in our systems,

84
00:06:22,866 --> 00:06:24,356
we can detect them forthright.

85
00:06:24,356 --> 00:06:28,626
We can detect them as they're
happening because they become

86
00:06:28,656 --> 00:06:30,116
authorization failures.

87
00:06:31,756 --> 00:06:33,446
So why hasn't this happened before?

88
00:06:33,446 --> 00:06:35,936
Why haven't we been
able to do this before?

89
00:06:36,926 --> 00:06:40,981
there's three Particularly hard
problems that have to be solved.

90
00:06:42,221 --> 00:06:46,611
The first I've already mentioned
before, that our APIs don't

91
00:06:46,621 --> 00:06:48,011
send permissions with data.

92
00:06:48,631 --> 00:06:53,081
We read the data, we copy it into
an API or a service, we move it to

93
00:06:53,081 --> 00:06:56,701
another service, and we leave the
ownership and permissions behind.

94
00:06:57,911 --> 00:07:03,386
We're expecting that our knowledge of how
the software works It's going to allow

95
00:07:03,386 --> 00:07:09,556
us to tell whether or not these APIs
are authorized to move that data or not,

96
00:07:11,766 --> 00:07:13,646
but that gets us to the second problem.

97
00:07:14,636 --> 00:07:15,706
service accounts.

98
00:07:15,886 --> 00:07:19,716
These services themselves
hide the end user identity.

99
00:07:20,276 --> 00:07:25,376
If you have an end user that often
that authenticates to a front end.

100
00:07:26,406 --> 00:07:31,116
That front end doesn't use the user's
identity to make a request to the

101
00:07:31,116 --> 00:07:33,456
back end to get data from storage.

102
00:07:33,836 --> 00:07:37,686
So the storage system never
knows who ultimately is going

103
00:07:37,686 --> 00:07:38,986
to get access to the data.

104
00:07:39,516 --> 00:07:44,406
It only knows the back end with
broad access rights read some data

105
00:07:44,506 --> 00:07:45,896
and passed it to the front end.

106
00:07:46,266 --> 00:07:49,546
The front end doesn't know what the
permissions are and passes it to the user.

107
00:07:50,346 --> 00:07:57,631
There's no place within our applications
where the user permissions and the

108
00:07:57,631 --> 00:08:03,406
data permissions and the user identity
come together for authorization.

109
00:08:05,046 --> 00:08:07,276
And the third problem is
a little bit more subtle.

110
00:08:08,326 --> 00:08:10,946
It comes down to data deduplication.

111
00:08:11,866 --> 00:08:18,966
Common chunks of data, byte sequences that
exist in multiple different documents, in

112
00:08:18,966 --> 00:08:21,416
multiple different records and databases.

113
00:08:21,416 --> 00:08:27,691
It that form the data that
we move in these APIs.

114
00:08:28,621 --> 00:08:33,421
We can't actually tell, looking at
the data itself, whether it came

115
00:08:33,421 --> 00:08:38,021
from, say, document A with policy
A, or document B with policy B.

116
00:08:40,006 --> 00:08:47,216
So we need a way to be able to figure
out not just what the object was that it

117
00:08:47,216 --> 00:08:54,906
came from, but the total lineage of this
data so we can rationalize and determine

118
00:08:55,036 --> 00:08:59,456
what those permissions should be, whether
they should be policy A or policy B.

119
00:08:59,746 --> 00:09:04,686
Policy B, and this is the critical
problem that we're actually

120
00:09:04,686 --> 00:09:06,946
facing with generative AI.

121
00:09:07,856 --> 00:09:12,146
We take documents and we ingest them
without permissions and put them in a

122
00:09:12,146 --> 00:09:18,026
vector database and then we expect that
we can do some sort of access control

123
00:09:18,026 --> 00:09:21,826
as those chunks of data are pulled
out of, Out of that vector database

124
00:09:22,006 --> 00:09:27,676
in RAG systems, but we can't and even
copying the permissions, as I mentioned

125
00:09:27,676 --> 00:09:31,666
before, because of data deduplication
means that we're not going to get

126
00:09:31,676 --> 00:09:39,596
the permissions on each chunk as it
should be, we get the permissions on

127
00:09:39,596 --> 00:09:41,756
the last document that chunk was in.

128
00:09:42,456 --> 00:09:49,016
So we may be redacting information from
all of our users that they shouldn't,

129
00:09:49,106 --> 00:09:50,596
that they should have access to.

130
00:09:52,756 --> 00:09:56,436
Moreover, we can mix these chunks
together in a single API call.

131
00:09:57,046 --> 00:10:00,186
And then how do we get to the permissions?

132
00:10:00,576 --> 00:10:03,376
Of the data in aggregate
should actually be.

133
00:10:04,376 --> 00:10:06,966
So we need a new architecture
to fix these problems.

134
00:10:07,126 --> 00:10:12,586
We need to be able to identify data
as it's moving in API payloads,

135
00:10:13,266 --> 00:10:14,976
and we've tried this in the past.

136
00:10:15,006 --> 00:10:18,196
We've gone down the road of
enterprise digital rights management.

137
00:10:18,676 --> 00:10:21,516
Google's come up with their
own solution for this.

138
00:10:22,416 --> 00:10:24,666
My company, cber is
proposing a new solution.

139
00:10:25,606 --> 00:10:28,986
But let's look at the, let's
look at the previous ones first.

140
00:10:29,496 --> 00:10:30,246
EDRM.

141
00:10:31,626 --> 00:10:35,356
So EDRM is a crypto,
cryptography solution.

142
00:10:35,986 --> 00:10:41,026
Basically, we're encrypting the data that
we're moving in these API payloads, and as

143
00:10:41,026 --> 00:10:43,186
they were constituted on the other side.

144
00:10:44,206 --> 00:10:48,076
The users that have the keys
that we've passed out to decrypt

145
00:10:48,076 --> 00:10:49,746
that data can access them.

146
00:10:50,446 --> 00:10:52,216
The ones that don't can't access them.

147
00:10:52,706 --> 00:10:58,156
And it's a pretty effective way of doing
access control on a per object basis.

148
00:10:59,566 --> 00:11:03,776
But now let's shift our perspective
and look at what happens when an API

149
00:11:04,766 --> 00:11:09,611
pulls a chunk of data out of a document
and then moves that chunk To another

150
00:11:09,611 --> 00:11:11,881
API that ultimately gets to a user.

151
00:11:12,881 --> 00:11:14,951
It starts to get pretty messy from there.

152
00:11:15,911 --> 00:11:21,201
You'd have to, the service that,
that extracts the chunk wants to

153
00:11:21,201 --> 00:11:22,651
send it securely to an end user.

154
00:11:22,651 --> 00:11:27,521
it's got to send it back to some
central crypto authority to have

155
00:11:27,521 --> 00:11:29,901
it re encrypted with a private key.

156
00:11:31,151 --> 00:11:34,641
We get that chunk back, then we have
to send the public key to the right

157
00:11:34,641 --> 00:11:36,851
people that have access to those chunks.

158
00:11:37,251 --> 00:11:40,991
Which may not necessarily be the same
people with access to the entire document.

159
00:11:42,521 --> 00:11:45,401
And as the number of chunks
grows and the number of services,

160
00:11:45,401 --> 00:11:47,471
this becomes simply untenable.

161
00:11:48,311 --> 00:11:51,301
So imagine how many
encryptions, decryptions, and

162
00:11:51,301 --> 00:11:52,921
re encryptions we'd have to do.

163
00:11:55,761 --> 00:11:58,781
So Google came up with a
different solution to this.

164
00:11:58,881 --> 00:12:04,931
As they're putting together Google
Cloud, they were thinking about data

165
00:12:04,931 --> 00:12:11,076
security, in their BeyondProd project,
and coupled with Zanzibar, which was

166
00:12:11,076 --> 00:12:16,706
their central authorization engine, they
came up with A new way of doing data

167
00:12:16,706 --> 00:12:19,346
security between their microservices.

168
00:12:20,186 --> 00:12:23,866
So what they've done is effectively
what you would do if you

169
00:12:23,866 --> 00:12:26,596
decided to build data security.

170
00:12:26,966 --> 00:12:31,826
Correct from the first point is you pass
that you actually do pass the permissions

171
00:12:32,196 --> 00:12:37,746
of all the data that you are carrying
in your APIs all the way down to the.

172
00:12:38,636 --> 00:12:43,456
final point where those APIs are accessed
by an end user, and you pass the user

173
00:12:43,456 --> 00:12:48,536
credentials all the way up so that
every point where the data and the user

174
00:12:48,536 --> 00:12:52,066
identity meet, you can authorize access.

175
00:12:53,016 --> 00:12:58,116
Be it the API access or the
user access to that data.

176
00:12:59,256 --> 00:13:00,876
Now, that's a great solution.

177
00:13:02,806 --> 00:13:03,956
The problem is,

178
00:13:04,956 --> 00:13:08,676
the problem is that they
had to re engineer their

179
00:13:08,676 --> 00:13:10,446
service to be able to do this.

180
00:13:10,796 --> 00:13:15,016
And if you think about modern applications
and enterprises that use hundreds, if

181
00:13:15,016 --> 00:13:22,256
not thousands of services, many of which
are open source or third party sourced

182
00:13:22,256 --> 00:13:28,111
services, How are you going to reengineer
them all to pass the credentials of the

183
00:13:28,121 --> 00:13:30,441
users or pass the permissions of the data

184
00:13:32,771 --> 00:13:35,471
for most companies other than Google?

185
00:13:36,371 --> 00:13:38,061
that's a complete barrier to entry.

186
00:13:38,671 --> 00:13:40,631
They won't be able to
make use of this solution.

187
00:13:41,711 --> 00:13:45,581
So that's why at Caber we came
up with a third option based

188
00:13:45,581 --> 00:13:47,201
upon data lineage tracing.

189
00:13:48,551 --> 00:13:54,161
Effectively, what we're doing is we're
looking at the byte sequences in every

190
00:13:54,161 --> 00:13:59,491
API payload around an application,
tracing them back to all the source

191
00:13:59,491 --> 00:14:05,651
objects that could possibly come from,
and then building a map that allows

192
00:14:05,651 --> 00:14:13,041
us to determine which source of that
data should those sequences inherit.

193
00:14:14,041 --> 00:14:21,466
We're leveraging some, Basic technologies
like content defined chunking to make

194
00:14:21,466 --> 00:14:27,116
sure that the sequences that we see
in one API payload is the same as the

195
00:14:27,136 --> 00:14:29,496
chunks that we see in other API payloads.

196
00:14:31,426 --> 00:14:35,756
And that allows us to connect the dots
between all of these APIs to build

197
00:14:35,756 --> 00:14:37,836
those relationships I mentioned before.

198
00:14:38,436 --> 00:14:41,416
And that map is extensive.

199
00:14:42,646 --> 00:14:48,081
This is an example of How we look
at data within an application, the

200
00:14:48,121 --> 00:14:52,511
objects that those chunks of data
themselves, which are the small blue

201
00:14:52,511 --> 00:14:59,401
dots come from the objects that hold
them and the APIs that move them.

202
00:15:01,011 --> 00:15:07,401
And from this, we can
eliminate the junk data that.

203
00:15:07,931 --> 00:15:12,911
It doesn't carry permissions, it doesn't
show us what, or doesn't contribute

204
00:15:12,911 --> 00:15:15,421
to our ability to authorize access.

205
00:15:15,891 --> 00:15:22,161
And we can narrow down to just the
data that we know what the permissions

206
00:15:22,171 --> 00:15:26,251
are and we can authorize access at any
given point within the application.

207
00:15:27,251 --> 00:15:32,411
So based upon doing that sort of
time series analysis or graphical

208
00:15:32,411 --> 00:15:36,731
analysis and coupling that with time
series analysis, looking at event

209
00:15:36,741 --> 00:15:42,941
ordering and eliminating impossible
paths, we can effectively do this.

210
00:15:43,741 --> 00:15:49,931
And the result, if we're looking at, the
management console of Kaber's product,

211
00:15:50,941 --> 00:15:57,851
we can see that we're detecting access to
sequences of data that come from objects.

212
00:15:57,871 --> 00:15:58,571
Even if.

213
00:15:59,551 --> 00:16:05,751
Those sequences are only a small
fraction of the content that came

214
00:16:05,751 --> 00:16:07,271
from the object in the first place.

215
00:16:08,611 --> 00:16:15,071
in this case, we have Bob as a
user, and Bob is accessing some

216
00:16:15,071 --> 00:16:17,051
data from an object owned by Amy.

217
00:16:17,671 --> 00:16:22,851
And it's through an API where they
got 7 byte sequences out of about

218
00:16:22,861 --> 00:16:25,271
10, 000 that came from her document.

219
00:16:26,381 --> 00:16:27,221
this is very powerful.

220
00:16:27,701 --> 00:16:30,441
fine grained access control for data.

221
00:16:30,731 --> 00:16:36,131
We don't need to see hundreds
of kilobytes of data to be able

222
00:16:36,131 --> 00:16:37,731
to provide access control to it.

223
00:16:37,761 --> 00:16:40,621
We can do it on a very fine grained basis.

224
00:16:41,241 --> 00:16:42,751
Even down to

225
00:16:45,291 --> 00:16:53,021
30, 40, 50 bytes is in the case of
this demonstration that I'm showing

226
00:16:53,021 --> 00:16:58,611
here where We're providing access
control directly on the chunks that

227
00:16:58,611 --> 00:17:00,211
are coming out of a RAG system.

228
00:17:01,371 --> 00:17:09,411
And in this slide, what I'm showing is
the user on the top has access to, we're

229
00:17:09,411 --> 00:17:15,593
using Apple 10 Q statements, financial
filings that Apple has made with the SEC.

230
00:17:15,593 --> 00:17:21,776
And the user on top has access to
Every year's financial statements,

231
00:17:21,776 --> 00:17:26,486
but let's just consider 2024 is
material nonpublic information.

232
00:17:26,486 --> 00:17:32,746
That's only accessible by the CFO on the
top and the regular users on the bottom.

233
00:17:32,746 --> 00:17:35,356
They can be asked questions about 2024.

234
00:17:35,906 --> 00:17:36,846
But as you can see.

235
00:17:37,566 --> 00:17:42,106
we can tell the difference between the
data that exists in the 2024 statements

236
00:17:42,106 --> 00:17:44,226
and what exists in the 2023 statements.

237
00:17:44,706 --> 00:17:48,376
Even though those two statements, if
you put them side by side, you see

238
00:17:49,776 --> 00:17:52,676
they have a lot of data in common.

239
00:17:53,396 --> 00:17:57,746
The boilerplate management
discussions, it's really only the

240
00:17:57,746 --> 00:18:00,916
distinguishing characteristics
of these documents that matter.

241
00:18:01,246 --> 00:18:07,536
And that's the sequences that we're
redacting from this user and holding back.

242
00:18:07,611 --> 00:18:11,871
from the LLM that's about to give
them knowledge based upon that.

243
00:18:13,631 --> 00:18:17,361
And this access control is deterministic.

244
00:18:18,051 --> 00:18:25,111
We know for a fact we have a one to one
exact match of those byte sequences to

245
00:18:25,111 --> 00:18:30,581
the data that they contain And the data
and the source where they came from.

246
00:18:32,921 --> 00:18:38,421
So this determinism, we need
two or more observation points.

247
00:18:39,081 --> 00:18:41,791
We need to be able to see the
data in two different places.

248
00:18:41,791 --> 00:18:46,291
And this is unlike what you would have
with DLP, where you're just looking at

249
00:18:46,291 --> 00:18:51,121
the data itself and trying to decide
on the spot, what does it look like?

250
00:18:51,881 --> 00:18:56,291
We need to have multiple sources where
we can connect those sources together

251
00:18:56,341 --> 00:18:58,631
to determine the identity of that data.

252
00:18:59,091 --> 00:19:01,131
But that's the secret sauce for this.

253
00:19:02,131 --> 00:19:02,531
Now,

254
00:19:05,081 --> 00:19:10,001
why, again, we're doing this, is
to be able to detect incidents

255
00:19:10,001 --> 00:19:11,121
without false positives.

256
00:19:11,731 --> 00:19:14,781
Now, I'm going to be honest right here.

257
00:19:16,381 --> 00:19:20,761
These are hashes, and we're
still subject to hash collisions

258
00:19:20,761 --> 00:19:22,231
within a given environment.

259
00:19:23,391 --> 00:19:27,971
But the probability of
a hash collision is 0.

260
00:19:28,031 --> 00:19:30,601
00001 percent or so.

261
00:19:31,186 --> 00:19:36,266
Compare that to the 95 percent false
positive that we have today, and you

262
00:19:36,266 --> 00:19:38,206
see this completely flips the model.

263
00:19:39,806 --> 00:19:41,336
So what can we do with this?

264
00:19:41,736 --> 00:19:49,026
Beyond just being able to detect access
problems, Bob accessing Amy's data, what

265
00:19:49,026 --> 00:19:54,066
we need to be able to do, or what we want
to be able to do, Is to use this data

266
00:19:54,076 --> 00:19:59,746
effectively to help us not just detect
problems, but solve those problems.

267
00:20:01,356 --> 00:20:05,756
And that's what we can do by
tracing the path and the movement

268
00:20:05,766 --> 00:20:09,986
of these byte sequences through
every API call that carries them.

269
00:20:11,026 --> 00:20:17,491
Here I'm showing a call graph of
what happened in that alert that we

270
00:20:17,491 --> 00:20:19,781
generated with Bob accessing Amy's data.

271
00:20:20,171 --> 00:20:22,061
And we can see exactly what happened.

272
00:20:22,601 --> 00:20:27,471
From starting with Amy uploading a
document that gets passed through

273
00:20:27,471 --> 00:20:33,411
this NuCo Next Cloud service stored
in an S3 bucket, we can then see that

274
00:20:33,411 --> 00:20:40,351
there's this service NuCo Disrupt at
the bottom that accessed that data,

275
00:20:41,241 --> 00:20:45,171
extracted some content out of it, and
wrote it back to a different place.

276
00:20:45,171 --> 00:20:51,256
In fact, a shared bucket where Bob can
get access to it and see a preview of

277
00:20:51,256 --> 00:20:53,776
the text of the document she uploaded.

278
00:20:55,186 --> 00:20:59,356
But by being able to trace this
path, we have the full and complete

279
00:20:59,356 --> 00:21:01,276
picture of how the incident happened.

280
00:21:02,006 --> 00:21:04,126
But not only that, we know how to stop it.

281
00:21:04,876 --> 00:21:11,206
We can prevent this just knowing
that the mechanism that he was able

282
00:21:11,206 --> 00:21:13,976
to get access to that data was this.

283
00:21:14,341 --> 00:21:15,891
NUCO disrupt service.

284
00:21:16,591 --> 00:21:21,001
Now think about that disrupt service
as a piece of malware in your system,

285
00:21:22,581 --> 00:21:28,361
or as a bad actor that was able to
escalate their privileges, gain access

286
00:21:28,361 --> 00:21:33,091
to an instance, and then read the data
from one bucket and write it to another.

287
00:21:34,341 --> 00:21:39,816
These sorts of errors go undetected
because they're all authorized access

288
00:21:40,126 --> 00:21:44,886
when you look at them individually, but
they're unauthorized when you look at

289
00:21:44,886 --> 00:21:50,896
them from an end to end data perspective
and know that the data that Bob got

290
00:21:51,666 --> 00:21:53,316
came from an object owned by Amy.

291
00:21:55,546 --> 00:22:02,766
But this ability to trace data
through a series of API calls gives

292
00:22:02,766 --> 00:22:09,671
us an even better more interesting
window into the application behavior.

293
00:22:10,211 --> 00:22:16,061
Beyond just detecting how an incident
happened, think about black box testing,

294
00:22:17,081 --> 00:22:23,446
where you look at the inputs and outputs
of a given software package, To be able

295
00:22:23,446 --> 00:22:30,376
to determine what that software package is
doing, what its function is, and by being

296
00:22:30,376 --> 00:22:36,236
able to trace how data flows and being
able to identify what that data is as it

297
00:22:36,246 --> 00:22:40,726
moves through these different services
gives us the ability to do that as well.

298
00:22:42,356 --> 00:22:43,656
And so what does this mean?

299
00:22:44,371 --> 00:22:49,231
It means that as we're building our
applications and building our policies,

300
00:22:49,821 --> 00:22:54,191
we know exactly what's happening
across all of our services, or we can

301
00:22:54,201 --> 00:22:58,531
know what's happening across all of
our services in terms of actual data

302
00:22:58,531 --> 00:23:08,731
flow, and then use that knowledge to
build the policies that we need to

303
00:23:08,731 --> 00:23:10,471
be able to control that data flow.

304
00:23:11,221 --> 00:23:15,911
In fact, we're able to take
this data along with some high

305
00:23:15,911 --> 00:23:22,401
level policies Prevent Bob from
gaining access to Amy's data.

306
00:23:23,371 --> 00:23:26,281
Feed that to a generative AI.

307
00:23:27,611 --> 00:23:33,261
And get back actual policy implementations
and where to put those policies to

308
00:23:33,601 --> 00:23:36,451
enforce that compliance requirement.

309
00:23:39,101 --> 00:23:45,831
We think the direction that we can take
with this level of data identification,

310
00:23:46,021 --> 00:23:51,661
the granularity that we can provide, And
the rationalization of the authorization

311
00:23:52,201 --> 00:23:56,311
on access to that data everywhere
within an application is going to

312
00:23:56,321 --> 00:24:02,261
open up a wide new arena for security.

313
00:24:02,761 --> 00:24:08,351
We think this is the key to
eliminating false positives

314
00:24:08,811 --> 00:24:11,171
in applications going forward.

315
00:24:11,981 --> 00:24:13,631
Thank you very much for your time.

316
00:24:14,501 --> 00:24:15,771
I really appreciate it.

317
00:24:15,801 --> 00:24:17,361
And I look forward to hearing from you.

318
00:24:17,431 --> 00:24:18,861
If this is of interest.

319
00:24:19,621 --> 00:24:23,101
If you think that this will
solve a problem within your

320
00:24:23,101 --> 00:24:24,731
environments, let me know.

321
00:24:25,041 --> 00:24:27,041
I'd be really interested
in hearing from you.

322
00:24:27,701 --> 00:24:28,331
Thanks again.

