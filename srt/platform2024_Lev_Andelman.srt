1
00:00:14,209 --> 00:00:16,789
So let's talk what happens
when clusters collide.

2
00:00:16,790 --> 00:00:22,410
If you search in any search engine,
the R for Kubernetes or the R for

3
00:00:22,599 --> 00:00:26,200
AKS, you will feel like in the
song, but life could be a dream.

4
00:00:26,209 --> 00:00:26,979
Everything is simple.

5
00:00:26,979 --> 00:00:31,939
You just deploy to Kubernetes
clusters in two different regions.

6
00:00:32,530 --> 00:00:38,269
You set up some load balancer
or a DNS in front of them.

7
00:00:38,640 --> 00:00:42,139
You may be copy some data from
one cluster to another man.

8
00:00:42,200 --> 00:00:45,670
Everything is super simple,
but from working in the last

9
00:00:45,670 --> 00:00:50,159
years with many companies, from
startups to large enterprises.

10
00:00:50,899 --> 00:00:56,330
I experienced that nothing is
simple in doing a DR for Kubernetes

11
00:00:56,330 --> 00:00:57,599
and it's actually pretty tricky.

12
00:00:58,459 --> 00:00:59,739
And hi everyone.

13
00:00:59,769 --> 00:01:04,670
I'm Lev, CTO at TerraSky and TerraSky
is a global solution integrator.

14
00:01:05,620 --> 00:01:11,689
We are helping our customers to make
sure their Kubernetes is secured,

15
00:01:11,869 --> 00:01:14,029
monitored, resilient, and maintainable.

16
00:01:16,920 --> 00:01:20,869
What we see is that Kubernetes
provide us a lot of power, right?

17
00:01:20,869 --> 00:01:23,199
It's very self service focused.

18
00:01:24,595 --> 00:01:29,634
And it's super convenient for developers,
but it comes at a price of how easy it is

19
00:01:29,654 --> 00:01:31,505
to set up DR, and there's a reason for it.

20
00:01:32,195 --> 00:01:36,095
If you take a look how things
were in the past, someone who was

21
00:01:36,095 --> 00:01:39,944
responsible for the infrastructure
would probably create the compute.

22
00:01:40,235 --> 00:01:44,455
The servers, the disks, the
load balancers, configure DNS,

23
00:01:45,045 --> 00:01:47,304
configure, issue the certificates.

24
00:01:47,695 --> 00:01:52,945
So everything was provided for
the developer and it was maybe

25
00:01:52,945 --> 00:01:57,804
more manual and slower, but the DR
process was more straightforward.

26
00:01:58,125 --> 00:02:03,474
Whoever set up this environment knew
how to actually do the entire failover

27
00:02:03,554 --> 00:02:05,499
and what is required to do that.

28
00:02:06,920 --> 00:02:10,820
Today, when the developers can
actually ask for many of those

29
00:02:10,820 --> 00:02:15,920
things by themselves, they can get
load balancers through Ingress.

30
00:02:15,950 --> 00:02:18,839
They can get disks through pvnpvc.

31
00:02:18,850 --> 00:02:23,599
They can issue certificates in the
investor and we'll see how things become

32
00:02:23,600 --> 00:02:25,439
more complex when we're talking about DR.

33
00:02:26,930 --> 00:02:29,440
So let's talk let's set the stage, right?

34
00:02:30,340 --> 00:02:32,620
We have two AWS region.

35
00:02:33,150 --> 00:02:37,390
I will have two AWS region in this
talk, EKS cluster, one EKS cluster

36
00:02:37,410 --> 00:02:40,780
in each of the regions, and we
want to have disaster recovery.

37
00:02:40,780 --> 00:02:46,450
And I'm not going into the philosophy and
the RTO, RPO, you decide what it means for

38
00:02:46,450 --> 00:02:49,355
you and what are you looking for, but I
will just show you a couple of examples.

39
00:02:50,395 --> 00:02:53,295
So we will start with a simple
one and a naive one that probably

40
00:02:53,295 --> 00:02:55,855
rarely happens in the reality.

41
00:02:56,415 --> 00:03:02,185
Let's say we have a simple stateless
deployment, and this deployment is

42
00:03:02,185 --> 00:03:04,155
receiving no traffic from outside.

43
00:03:04,195 --> 00:03:06,735
It just runs, maybe processes something.

44
00:03:07,225 --> 00:03:07,775
Easy stuff.

45
00:03:08,795 --> 00:03:14,115
So I take this YAML of deployment
of sample app, two replicas.

46
00:03:14,175 --> 00:03:15,245
This is our image.

47
00:03:15,995 --> 00:03:18,725
I will just deploy it
to different regions.

48
00:03:18,765 --> 00:03:18,905
U.

49
00:03:18,905 --> 00:03:19,035
S.

50
00:03:19,045 --> 00:03:19,785
is one, U.

51
00:03:19,785 --> 00:03:19,925
S.

52
00:03:19,955 --> 00:03:20,445
is two.

53
00:03:21,295 --> 00:03:23,645
Two deployments, easy, right?

54
00:03:24,105 --> 00:03:27,075
we deploy the first cluster,
we deploy the second cluster.

55
00:03:27,295 --> 00:03:32,675
In case of emergency, we can switch
the traffic or deploy on demand, or we

56
00:03:32,675 --> 00:03:35,275
can have them active in both clusters.

57
00:03:35,895 --> 00:03:36,255
Nothing.

58
00:03:38,495 --> 00:03:40,615
Let's do a little more.

59
00:03:41,605 --> 00:03:44,885
Let's say we have a simple
stateless application or

60
00:03:44,885 --> 00:03:47,545
software with incoming traffic.

61
00:03:48,485 --> 00:03:51,375
No encryption at this point,
that probably never happens in

62
00:03:51,375 --> 00:03:52,645
real production environment.

63
00:03:53,165 --> 00:03:57,245
And at this point, we're doing
manual DNS management, right?

64
00:03:57,305 --> 00:03:59,585
So what it will look like.

65
00:04:00,125 --> 00:04:02,035
So same deployment.

66
00:04:02,675 --> 00:04:05,365
We will have our container with port 80.

67
00:04:06,355 --> 00:04:11,575
We will have our service that is
forwarding the traffic to this port 80.

68
00:04:12,030 --> 00:04:18,320
And we will have the ingress that
will actually, listen to this, FQDN

69
00:04:18,380 --> 00:04:24,270
of Main Do Level Labs link, and
it'll forward everything that is

70
00:04:24,270 --> 00:04:26,840
going to slash into our service.

71
00:04:28,350 --> 00:04:31,535
Again, easy, we just deploy
to the first cluster.

72
00:04:33,165 --> 00:04:35,755
We can deploy it at the same
time to the second clusters.

73
00:04:36,095 --> 00:04:42,835
So in case of the emergency, we can
create some kind of a CNAME that will

74
00:04:42,835 --> 00:04:48,585
point to each of the clusters and or we
can have it as active and we can have

75
00:04:48,585 --> 00:04:51,335
round robin or some kind of weighted DNS.

76
00:04:51,335 --> 00:04:53,585
We just have to adjust DNS.

77
00:04:54,550 --> 00:04:55,140
Not complicated.

78
00:04:56,440 --> 00:04:59,530
Now, if we're talking about
persistent volumes, there are

79
00:04:59,530 --> 00:05:01,040
many ways to achieve that.

80
00:05:01,250 --> 00:05:03,940
And this is actually when you're
searching, in the search engines,

81
00:05:03,990 --> 00:05:05,070
this is what you will find.

82
00:05:05,380 --> 00:05:07,880
So here's an example of how
you can do it with Valero.

83
00:05:08,410 --> 00:05:12,070
You will just have a cluster,
you will create a backup, put

84
00:05:12,070 --> 00:05:13,830
it in your object storage.

85
00:05:13,870 --> 00:05:16,020
You can have another
cluster from which you can.

86
00:05:16,500 --> 00:05:20,550
Take it from the object storage and
restore it into another cluster.

87
00:05:20,850 --> 00:05:23,160
Here's a link if you want
to check how to do that.

88
00:05:23,920 --> 00:05:28,800
Another option, you can do it with
commercial products like Portworx.

89
00:05:28,880 --> 00:05:31,650
So Portworx will be running continuously.

90
00:05:31,710 --> 00:05:35,090
This is a synchronic
replication, disaster recovery.

91
00:05:35,645 --> 00:05:41,125
Portworx will take the data, periodically
put it in object storage, then another

92
00:05:41,125 --> 00:05:44,515
cluster will pull it from the object
storage and continuously restore it.

93
00:05:44,725 --> 00:05:48,525
And then you can execute commands
to failover to another cluster

94
00:05:48,525 --> 00:05:49,945
and just launch the workload.

95
00:05:50,675 --> 00:05:53,175
Very convenient, very
easy in terms of data.

96
00:05:55,095 --> 00:05:56,705
But that's not why we're here.

97
00:05:56,705 --> 00:06:00,725
We want to talk about Kubernetes
and self service, right?

98
00:06:00,725 --> 00:06:02,175
And there are two great projects.

99
00:06:02,675 --> 00:06:06,055
One of them called external DNS
and the other one is CertManager.

100
00:06:06,685 --> 00:06:07,405
So what do they do?

101
00:06:08,445 --> 00:06:16,185
Imagine a world where you can create
DNS records automatically and you

102
00:06:16,205 --> 00:06:19,645
can generate certificates on demand.

103
00:06:20,555 --> 00:06:23,395
Now stop imagining and
just create this ingress.

104
00:06:23,655 --> 00:06:28,605
And this ingress will actually, because
of this annotation and CertManager

105
00:06:28,635 --> 00:06:33,460
that is installed, We'll go and
issue the certificate for main.

106
00:06:34,680 --> 00:06:35,060
levlabs.

107
00:06:35,060 --> 00:06:41,210
link because of this TLS, and
he will save the certificate in

108
00:06:41,210 --> 00:06:49,460
this secret, and he will also
generate the A record in, route 53.

109
00:06:50,450 --> 00:06:51,460
Very convenient.

110
00:06:52,080 --> 00:06:55,380
Great self-service for any developer
that just wants to deploy his software.

111
00:06:57,230 --> 00:06:59,600
This is what it'll look like.

112
00:07:00,080 --> 00:07:02,360
so you will create an ingress.

113
00:07:02,450 --> 00:07:06,530
This ingress will cause to go to
route 53 and update the record.

114
00:07:06,740 --> 00:07:12,890
It'll go to less encrypt and generate
the certificate, and this is great, but.

115
00:07:13,455 --> 00:07:16,525
Great power comes great
responsibility, as we know.

116
00:07:17,915 --> 00:07:19,735
So let's talk about the
problems that it creates.

117
00:07:19,735 --> 00:07:22,015
The first one is the
routing traffic problem.

118
00:07:23,015 --> 00:07:27,725
I can't deploy the same ingress twice
because of what I just showed you.

119
00:07:28,265 --> 00:07:34,255
If I deploy the same ingress
twice, external DNS will now fight

120
00:07:34,755 --> 00:07:36,655
over who the record belongs to.

121
00:07:37,235 --> 00:07:38,165
So that's a problem.

122
00:07:38,165 --> 00:07:44,535
So I need an ingress for the active
cluster and I need the ingress for

123
00:07:44,535 --> 00:07:51,930
the ingress And I need a CNAME for a
main record that will actually forward

124
00:07:51,940 --> 00:07:54,250
the traffic to the relevant cluster.

125
00:07:55,630 --> 00:07:56,500
But there's a problem.

126
00:07:57,410 --> 00:08:02,520
If I create this CNAME for main and
I will actually access my clusters

127
00:08:02,560 --> 00:08:08,070
where I have only ingress for active or
ingress for the arm, no one will listen.

128
00:08:08,390 --> 00:08:09,690
to the main host header.

129
00:08:10,230 --> 00:08:12,830
So what I actually have to do is
something that looks like that.

130
00:08:14,020 --> 00:08:19,650
I will have two clusters with
ingress for primary levelups.

131
00:08:19,650 --> 00:08:20,700
link and dr.

132
00:08:21,650 --> 00:08:21,990
levelups.

133
00:08:21,990 --> 00:08:26,040
link that will auto
configure the route 53.

134
00:08:26,700 --> 00:08:30,780
And I will have a separate record
that's called main that I control.

135
00:08:31,230 --> 00:08:35,230
That will point to those
ingresses that are actually

136
00:08:35,620 --> 00:08:37,030
forwarding to the same service.

137
00:08:37,030 --> 00:08:38,730
So this way I can control

138
00:08:41,010 --> 00:08:43,760
which cluster I want to access and when.

139
00:08:44,380 --> 00:08:47,710
Let's see how that looks
in terms of configuration.

140
00:08:48,810 --> 00:08:53,740
So here we have an
example of the DR, right?

141
00:08:53,760 --> 00:08:59,480
This is an ingress for the app DR
that listens on the DR lablabs.

142
00:08:59,540 --> 00:09:02,424
link and forwards to
the application of DR.

143
00:09:02,635 --> 00:09:06,465
And here we have an ingress
for the primary, so it will

144
00:09:06,465 --> 00:09:08,095
listen on a host of primary.

145
00:09:08,385 --> 00:09:13,005
So those will create the route
53 records automatically.

146
00:09:13,555 --> 00:09:16,795
And then I will have the
ingress for the main.

147
00:09:17,055 --> 00:09:20,365
As you can see what I'm doing here
is I'm saying that the annotation

148
00:09:20,905 --> 00:09:24,215
for this one, for external DNS, is
actually the host name is empty.

149
00:09:24,790 --> 00:09:25,990
This way, main.

150
00:09:28,000 --> 00:09:28,100
levlabs.

151
00:09:28,160 --> 00:09:33,620
link will not try to create
any records in Route 53.

152
00:09:33,960 --> 00:09:38,820
And this is how I can create
my CNAME and manually control

153
00:09:39,020 --> 00:09:40,460
who is the active cluster.

154
00:09:40,950 --> 00:09:44,680
And then in each cluster, it will
point to the relevant server, whether

155
00:09:44,680 --> 00:09:48,900
it's active, whether it's DR, whether
it's the same name in both of them.

156
00:09:50,950 --> 00:09:55,030
And this is the CNAME
in my, Route 53, right?

157
00:09:55,030 --> 00:10:00,860
So you will see that it's a C name,
and it's actually a forwarding

158
00:10:00,910 --> 00:10:03,920
now to the primary lab, levlab.

159
00:10:03,960 --> 00:10:04,210
link.

160
00:10:04,330 --> 00:10:05,820
So this is the record.

161
00:10:06,020 --> 00:10:07,330
This is what it points to.

162
00:10:07,630 --> 00:10:12,470
And I have to have very short TTL so I can
adjust it and failover traffic quickly.

163
00:10:15,390 --> 00:10:18,990
Let's talk about the second
problem, the certificates.

164
00:10:20,290 --> 00:10:22,030
Now we have an FQDN.

165
00:10:22,040 --> 00:10:24,650
Those, this is how our
customers will access it, right?

166
00:10:24,650 --> 00:10:27,000
So we have FQDN is called main.

167
00:10:27,170 --> 00:10:28,390
levlabs.

168
00:10:28,390 --> 00:10:31,600
link, but it doesn't match dr.

169
00:10:31,600 --> 00:10:32,600
levlabs.

170
00:10:32,770 --> 00:10:34,210
link or primary.

171
00:10:34,490 --> 00:10:35,240
levlabs.

172
00:10:35,240 --> 00:10:38,680
link that are auto generated
from our ingresses.

173
00:10:39,900 --> 00:10:42,370
So this is what it looks like.

174
00:10:42,710 --> 00:10:48,580
And those are the ingresses that
are creating this certificate.

175
00:10:49,080 --> 00:10:52,690
And we have to figure some
way out how to solve it.

176
00:10:53,170 --> 00:10:54,510
So here's how I'm doing it.

177
00:10:55,305 --> 00:11:01,225
I will have ingress of our sample app,
let's say in the primary, but inside

178
00:11:01,225 --> 00:11:03,165
of the TLS, I will have multiple hosts.

179
00:11:03,375 --> 00:11:08,535
This is how I will create the SAN,
the service alternative name, right?

180
00:11:08,555 --> 00:11:12,055
So I will have multiple FQDNs here.

181
00:11:12,895 --> 00:11:17,715
This will cause cert manager to
go and issue the certificate.

182
00:11:18,265 --> 00:11:22,155
One certificate with both FQDNs
and it will place it in this

183
00:11:22,165 --> 00:11:24,125
secret name in the ingress cert.

184
00:11:24,845 --> 00:11:32,405
Then I will actually create my main
ingress for main level labs, but it's not

185
00:11:32,465 --> 00:11:35,025
actually going to Let's Encrypt at all.

186
00:11:35,345 --> 00:11:38,245
But instead, I have the same TLS hosts.

187
00:11:38,740 --> 00:11:42,370
And I'm leveraging the same
secret that I issued here.

188
00:11:42,680 --> 00:11:48,400
So this way, I will have Ingress that is
listening to me, that actually will have

189
00:11:48,430 --> 00:11:55,370
the certificate that matches its FQDN, and
it will be able to receive the traffic.

190
00:11:55,560 --> 00:12:00,880
But it's not trying to generate the, Route
52 record or the certificate for itself.

191
00:12:02,490 --> 00:12:03,810
And this is what it will look like.

192
00:12:04,215 --> 00:12:08,755
Here's my safari that was able
to successfully access, the

193
00:12:08,755 --> 00:12:11,475
site main level lobster link.

194
00:12:11,715 --> 00:12:16,425
And you can see that I actually have
DNS name for Main and DNS for primary.

195
00:12:16,455 --> 00:12:20,295
And if I will fail over, I will
actually have main and the dr.

196
00:12:22,585 --> 00:12:26,305
So what this solution actually
looks like, if I take everything

197
00:12:26,305 --> 00:12:28,015
and I just show it in one picture.

198
00:12:28,045 --> 00:12:32,515
So I will have two AWS region,
one IKEAs cluster in one.

199
00:12:32,795 --> 00:12:37,125
In one region, I will have self
service for certificates, self service

200
00:12:37,135 --> 00:12:42,065
for DNS and functional DR that I can
control where the traffic is going to.

201
00:12:42,625 --> 00:12:43,715
And this is how, right?

202
00:12:44,015 --> 00:12:50,685
So I will have my ingresses that will
access Amazon route 53 and let's encrypt.

203
00:12:51,675 --> 00:12:53,025
And I will have, my.

204
00:12:54,775 --> 00:12:58,555
Ingresses that are forwarding to
service that is forwarding into my

205
00:12:58,605 --> 00:13:03,575
deployment and potentially I can have
persistent volume with data replication

206
00:13:03,585 --> 00:13:07,955
that I can copy, let's say, with
Portworx or periodically with Valero

207
00:13:08,835 --> 00:13:10,225
commercial products like a step.

208
00:13:10,875 --> 00:13:14,815
It's important to remember that we
are, it's a true Pandora box, right?

209
00:13:16,665 --> 00:13:19,055
For example, not everything
has to be replicated.

210
00:13:19,055 --> 00:13:19,615
There are a couple of.

211
00:13:19,695 --> 00:13:21,795
gotchas.

212
00:13:22,255 --> 00:13:27,435
One example is that you cannot replicate
or copy the same ingress into both places,

213
00:13:27,435 --> 00:13:33,405
especially if you're doing stuff like self
service with self manager or external DNS.

214
00:13:33,875 --> 00:13:39,485
Another example is that you probably want
to change some of the labels that are you,

215
00:13:39,485 --> 00:13:42,815
that you're restoring in another cluster.

216
00:13:42,855 --> 00:13:45,915
Maybe you need to change them,
adjust them, something like this.

217
00:13:45,915 --> 00:13:47,235
So you can't just copy paste.

218
00:13:49,900 --> 00:13:53,190
A lot of people or a lot of companies
forget about the dependencies.

219
00:13:53,460 --> 00:13:57,430
They're really focused on just
making sure they can switch stuff

220
00:13:57,440 --> 00:13:58,640
from one cluster to another.

221
00:13:58,640 --> 00:14:02,650
But there are things that potentially
close to the cluster that can fail.

222
00:14:02,790 --> 00:14:09,600
For example, the manifest repository,
where you pulling all your yamls, all

223
00:14:09,620 --> 00:14:13,910
your helm charts, all your manifests.

224
00:14:14,830 --> 00:14:17,910
Second thing obviously
is the image repository.

225
00:14:17,910 --> 00:14:21,360
It has to be accessible and available
in the second site or from the second

226
00:14:21,360 --> 00:14:25,970
site, when you're doing the failover,
obviously you have to address the

227
00:14:26,240 --> 00:14:29,340
monitoring, the ICD and authentication.

228
00:14:30,150 --> 00:14:33,880
Now, in addition, again, as you can see.

229
00:14:34,550 --> 00:14:38,700
Many of the companies
really focused on the data.

230
00:14:39,120 --> 00:14:43,610
They copying data and from their
perspective, this is all they had to do.

231
00:14:44,050 --> 00:14:48,740
But the reality is the connectivity
aspects are usually ignored and they're

232
00:14:49,260 --> 00:14:52,500
super important to take in consideration.

233
00:14:52,540 --> 00:14:57,660
And you have to drill your DR, do your
chaos engineering and just test this.

234
00:14:59,330 --> 00:15:00,080
Thanks a lot.

235
00:15:00,090 --> 00:15:01,360
It was helpful.

236
00:15:01,360 --> 00:15:04,120
If you have any questions,
please talk to me.

237
00:15:04,120 --> 00:15:05,770
I will be available in the chat.

238
00:15:06,265 --> 00:15:07,385
Or email me.

239
00:15:08,875 --> 00:15:09,479
Thanks a lot.

