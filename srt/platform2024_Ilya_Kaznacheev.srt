1
00:00:14,360 --> 00:00:19,979
I want to share with you some stories,
some findings, some experience

2
00:00:19,979 --> 00:00:26,080
of mine about how to make the
application, the system more available.

3
00:00:26,554 --> 00:00:33,095
How to improve its characteristics on
several layers and why it is important

4
00:00:33,495 --> 00:00:39,464
to know about each of these layers
and know how to incorporate them into

5
00:00:39,464 --> 00:00:42,925
your work as a platform engineer.

6
00:00:43,534 --> 00:00:45,494
Let me introduce you my talk.

7
00:00:46,294 --> 00:00:47,144
Many layers.

8
00:00:47,634 --> 00:00:53,394
Of availability or how to achieve
so many nines in practice.

9
00:00:53,674 --> 00:00:58,704
And as we are platform engineers,
the only thing we care about.

10
00:00:59,529 --> 00:01:00,239
Is nines.

11
00:01:02,539 --> 00:01:03,149
Let me start.

12
00:01:04,639 --> 00:01:10,649
Many people, while talking about
achieving nines, talk about microservices.

13
00:01:11,059 --> 00:01:14,869
Let's migrate to microservices,
let's start with microservices.

14
00:01:15,829 --> 00:01:19,600
And they solve this issue often not.

15
00:01:19,970 --> 00:01:24,910
Because it's, sometimes, it's a little
bit more compli complicated than that.

16
00:01:25,710 --> 00:01:33,320
It is like Saga, works well in theory,
but often works poorly in practice.

17
00:01:33,360 --> 00:01:38,424
Because world, real world,
is just more complex game.

18
00:01:38,695 --> 00:01:40,115
Place and books.

19
00:01:41,055 --> 00:01:46,665
So I want to share what I found
during my work in some very

20
00:01:46,675 --> 00:01:50,145
interesting projects And who am I?

21
00:01:51,165 --> 00:01:52,705
My name is Ilya Kozachev.

22
00:01:52,945 --> 00:01:58,265
I'm a cloud native application architect
at Capgemini and previously I worked

23
00:01:58,335 --> 00:02:05,315
at MTS cloud, Deutsche Telekom video
and some other interesting companies.

24
00:02:06,095 --> 00:02:10,785
I'm a Google developer expert on
cloud and I work for some time

25
00:02:10,805 --> 00:02:14,455
in software engineering, software
architecture, platform engineering

26
00:02:14,595 --> 00:02:17,365
and public cloud building.

27
00:02:18,955 --> 00:02:24,335
And one of my recent projects was building
a public cloud provider from scratch.

28
00:02:24,335 --> 00:02:30,532
For example, a Kubernetes as a service
is a core part of this project.

29
00:02:31,447 --> 00:02:38,347
And I learned a lot about availability
and reliability and many layers that

30
00:02:38,347 --> 00:02:43,037
we often don't see because we don't
have a direct access to them, but

31
00:02:43,067 --> 00:02:46,987
it's very important to know them,
to understand them and understand

32
00:02:46,997 --> 00:02:51,527
how to design your applications,
thus that they will be successful.

33
00:02:51,937 --> 00:02:56,537
include features or rely on
features of those layers.

34
00:02:57,037 --> 00:02:59,927
So let me share it with you.

35
00:03:01,197 --> 00:03:02,277
But what about NINES?

36
00:03:02,927 --> 00:03:03,827
What about NINES?

37
00:03:04,537 --> 00:03:09,527
Let me start with application then
and tell you what about NINES.

38
00:03:10,457 --> 00:03:16,247
When we're talking about applications we
always talk about patterns, approaches,

39
00:03:16,787 --> 00:03:20,037
architecture, standards, et cetera.

40
00:03:20,737 --> 00:03:24,187
So let's assume we have poorly designed.

41
00:03:24,547 --> 00:03:29,877
monolithic spaghetti code application
and we want to make it highly available.

42
00:03:30,427 --> 00:03:36,767
We will start with re architecting this
application and moving parts of this

43
00:03:36,767 --> 00:03:47,072
spaghetti code to separated spaghetti
balls and each one We'll only fail and

44
00:03:47,082 --> 00:03:49,302
not bring down the whole application.

45
00:03:49,712 --> 00:03:55,502
On the other hand, if something
fails in spaghetti code monolith,

46
00:03:56,472 --> 00:04:01,382
it is the single point of failure
and the whole application will fail.

47
00:04:01,602 --> 00:04:07,002
So we want to avoid a single point of
failure first and design the application

48
00:04:07,032 --> 00:04:09,802
that way that if something breaks.

49
00:04:10,037 --> 00:04:19,017
The number of stuff broken is very
limited, and we want to have this

50
00:04:19,897 --> 00:04:22,197
number of broken stuff very low.

51
00:04:23,477 --> 00:04:24,977
Next is redundancy.

52
00:04:25,167 --> 00:04:30,127
This is our main tool, achieving higher
availability and higher reliability.

53
00:04:30,977 --> 00:04:32,017
It's that simple.

54
00:04:33,027 --> 00:04:35,317
Humanity still not
invented something better.

55
00:04:35,927 --> 00:04:38,627
So we have to run more replicas together.

56
00:04:39,097 --> 00:04:44,267
More pieces of the same application
so that if one of these replicas

57
00:04:44,417 --> 00:04:48,697
fail, we don't have a big issue.

58
00:04:49,157 --> 00:04:55,187
Okay, we can redistribute the load to the
rest of the replicas and it will be fine.

59
00:04:55,917 --> 00:04:59,927
Second, it will help to process requests.

60
00:05:00,967 --> 00:05:05,577
So one application can be
overloaded, but when we have

61
00:05:05,617 --> 00:05:09,807
multiple similar applications, we
can distribute load among them.

62
00:05:10,642 --> 00:05:16,042
And be sure that it's enough
capacity to process such a load.

63
00:05:17,002 --> 00:05:20,252
And if it's not enough
capacity, we can scale.

64
00:05:20,632 --> 00:05:21,932
We can add more replicas.

65
00:05:21,982 --> 00:05:29,252
And while our system already can work
with several replicas, we can put

66
00:05:29,252 --> 00:05:33,072
more replicas if needed, or remove
replicas if we don't need them.

67
00:05:33,332 --> 00:05:37,907
If you have Black Friday, we You can
increase number of replicas, but if you

68
00:05:37,907 --> 00:05:43,477
have I don't know, some summer vacation
time, you can remove replicas because

69
00:05:43,717 --> 00:05:48,827
people maybe don't buy from your store
and you don't have to pay your AWS bills.

70
00:05:49,327 --> 00:05:53,907
But to make it possible, you
also have to deliver requests.

71
00:05:54,542 --> 00:05:59,112
To this replica somehow, and it
is done by means of load balancer.

72
00:05:59,972 --> 00:06:05,912
So this tool distributes requests
between different replicas.

73
00:06:06,652 --> 00:06:12,742
It can just circle through the list
of addresses, or it can implement some

74
00:06:12,852 --> 00:06:18,542
smart balancing algorithms, like to
distribute more or less requests based

75
00:06:18,572 --> 00:06:24,492
on the load of CPU load, for example, of
each of the replicas, or something else.

76
00:06:25,722 --> 00:06:33,362
And it also will allow you to implement
dynamic scaling, so you can put more

77
00:06:33,362 --> 00:06:39,052
replicas or remove replicas dynamically
again based on CPU load, for example,

78
00:06:39,642 --> 00:06:44,252
add load balancer will automatically
add addresses of new replicas.

79
00:06:44,772 --> 00:06:49,252
to the load balancing list or remove
addresses of removed replicas to

80
00:06:49,372 --> 00:06:52,272
avoid sending requests to nowhere.

81
00:06:53,542 --> 00:06:58,652
Next, we want to reduce
dependencies between applications,

82
00:06:58,712 --> 00:07:00,662
between different modules.

83
00:07:01,392 --> 00:07:06,042
And thus they interact only
via APIs and not via shared

84
00:07:06,042 --> 00:07:08,852
database, not via something else.

85
00:07:09,432 --> 00:07:16,467
So that if we change something, We can
do that transparently, and we are sure

86
00:07:16,467 --> 00:07:23,857
that our change will not break other
systems by surprise, because we know

87
00:07:23,887 --> 00:07:26,077
the dependency between two systems.

88
00:07:26,117 --> 00:07:28,107
They are clearly defined as an API.

89
00:07:28,677 --> 00:07:34,317
Second, we want to eliminate
implicit dependencies I don't know,

90
00:07:34,317 --> 00:07:40,537
something, strange logic in several
applications, or using of libraries.

91
00:07:41,047 --> 00:07:47,457
With our some internal variables, internal
magic variables or something like that.

92
00:07:47,747 --> 00:07:53,757
Sometimes it's useful, but it has to be
limited and you have to understand that

93
00:07:53,807 --> 00:08:00,747
if it's variable thing and you just want
to put your standard values to somewhere

94
00:08:00,747 --> 00:08:07,237
it's okay but if you want to share code
between applications share logic between

95
00:08:07,237 --> 00:08:16,307
them be very careful because when this
shared code will evolve it may bring some

96
00:08:16,867 --> 00:08:20,152
unexpected problems to your applications.

97
00:08:20,992 --> 00:08:25,012
And next, try to consolidate
features together.

98
00:08:25,512 --> 00:08:31,152
If features belong to one topic,
one domain, put them together,

99
00:08:31,192 --> 00:08:34,592
put them into one microservice,
put them into one module.

100
00:08:34,902 --> 00:08:37,392
It also helps to avoid problems.

101
00:08:37,542 --> 00:08:43,962
problems, avoid miscommunication between
modules, avoid evolutionary problems when

102
00:08:43,962 --> 00:08:50,542
your modules grow, when your applications
grow, changing, and if you have features

103
00:08:50,812 --> 00:08:58,232
of one domain into several applications,
they will grow closer and bring you

104
00:08:58,242 --> 00:09:03,352
a thing called distributed monolith,
which will decrease the availability.

105
00:09:04,167 --> 00:09:09,737
Because bringing more errors and
problems, so try to avoid that.

106
00:09:10,177 --> 00:09:12,367
Try to use domain driven
design, for example.

107
00:09:13,777 --> 00:09:18,192
And the last but not
least, Synchronous VPC.

108
00:09:18,192 --> 00:09:22,887
Asynchronous like HTTP requests
are okay, nothing bad with them.

109
00:09:23,197 --> 00:09:28,947
But if you have chain of requests,
one error can break the whole chain,

110
00:09:29,357 --> 00:09:32,907
and you can have a cascading error.

111
00:09:33,577 --> 00:09:34,447
That's not very good.

112
00:09:34,827 --> 00:09:39,057
And thus people use asynchronous
communication via message brokers,

113
00:09:39,077 --> 00:09:45,327
for example, they can be message
based when you pretty similar to HTTP

114
00:09:45,327 --> 00:09:49,977
request, you send a message, you send a
request, and you expect to receive it.

115
00:09:50,177 --> 00:09:55,167
And answer, but you don't wait for
that request stopping your work.

116
00:09:55,217 --> 00:10:00,537
You continue working, do something
else, but, and then when the request is

117
00:10:00,547 --> 00:10:02,657
there, you can start processing that.

118
00:10:03,227 --> 00:10:07,857
The application can even restart
during that time between, and

119
00:10:07,937 --> 00:10:09,307
there is no problem at all.

120
00:10:09,567 --> 00:10:10,697
Second is even best.

121
00:10:10,827 --> 00:10:14,127
The difference is it's one
directional, it's fire and forget.

122
00:10:14,472 --> 00:10:19,552
Forget, you only send the
message and you as a sender

123
00:10:19,652 --> 00:10:22,062
don't care what will happen next.

124
00:10:22,562 --> 00:10:26,122
Maybe it will be processed, maybe
it will be processed by multiple

125
00:10:26,162 --> 00:10:33,232
applications, or maybe no one cares and
it doesn't matter for you as a sender.

126
00:10:34,752 --> 00:10:41,312
So this is the first layer
of our cake, the application.

127
00:10:42,392 --> 00:10:44,672
Let's see what we have next.

128
00:10:45,537 --> 00:10:52,507
The platform for every application is
very important where it is running, and

129
00:10:52,527 --> 00:10:59,737
it's always running within some kind of
infrastructure based on platform, virtual

130
00:10:59,767 --> 00:11:03,117
machines, hardware, et cetera, et cetera.

131
00:11:03,772 --> 00:11:12,092
And without stable and reliable platform,
your whole number of efforts toward

132
00:11:12,252 --> 00:11:17,472
higher availability on the application
layer will cost nothing because the

133
00:11:17,472 --> 00:11:19,922
unreliable platform will fail you.

134
00:11:20,372 --> 00:11:21,302
Let's take a look.

135
00:11:22,612 --> 00:11:24,752
First, cluster architecture.

136
00:11:25,272 --> 00:11:29,102
A lot of platforms are built
in the cluster manner like

137
00:11:29,522 --> 00:11:32,552
Kubernetes, Kafka, Hadoop, etc.

138
00:11:33,202 --> 00:11:35,907
Because It helps to prevent outage.

139
00:11:36,147 --> 00:11:42,937
If one node in cluster will fail, cluster
can very easily red redistribute the load

140
00:11:43,287 --> 00:11:50,037
between other nodes while it restarts
or recreates the failed node, and there

141
00:11:50,037 --> 00:11:55,947
will be no, or almost no problem for the
avail availability of your applications.

142
00:11:56,127 --> 00:12:00,957
If your application is running in
several replicas on top of the Kubernetes

143
00:12:01,137 --> 00:12:03,837
cluster, for example, there will be.

144
00:12:04,017 --> 00:12:10,597
No issue for your application if
one node with one replica fail,

145
00:12:11,397 --> 00:12:12,947
if you use autoscaling, of course.

146
00:12:13,997 --> 00:12:15,627
Second is load distribution.

147
00:12:15,747 --> 00:12:21,897
Cluster also schedules applications
that way to evenly distribute

148
00:12:21,977 --> 00:12:28,537
load between the nodes, so there
will be no outages when load.

149
00:12:28,737 --> 00:12:38,057
Our over overflows with tasks is
work, and it also helps to avoid

150
00:12:38,157 --> 00:12:45,337
some not very pleasant surprises in
production and next network replication.

151
00:12:45,367 --> 00:12:51,397
Again, Kubernetes example, each worker
node is a gateway and network gateway

152
00:12:51,397 --> 00:12:54,827
to internal Kubernetes cluster network.

153
00:12:55,717 --> 00:12:57,367
So when you send a request.

154
00:12:57,752 --> 00:13:03,272
Node 1 may take this request
and then redirect it to the node

155
00:13:03,752 --> 00:13:06,132
2 where your port is running.

156
00:13:06,872 --> 00:13:12,052
And when node 1 fails, there is
no issue because node 2 can also

157
00:13:12,432 --> 00:13:15,662
be a gateway and it is a gateway.

158
00:13:16,152 --> 00:13:20,762
To the network and node three is
a gateway and you can load balance

159
00:13:21,202 --> 00:13:26,912
the request to any node and that's
what's happening in any environment.

160
00:13:28,292 --> 00:13:29,522
Next orchestration.

161
00:13:30,062 --> 00:13:33,052
So we started talking about orchestrators.

162
00:13:33,192 --> 00:13:34,822
Let's go a little bit further.

163
00:13:35,477 --> 00:13:39,797
Orchestrations are very nice
because they help to control the

164
00:13:39,797 --> 00:13:41,407
life cycle of the applications.

165
00:13:42,197 --> 00:13:47,587
Yeah, you basically can put the
application into the empty virtual machine

166
00:13:48,057 --> 00:13:50,207
with systemd or something like that.

167
00:13:50,547 --> 00:13:58,127
It will even restart the application
when it crashes, but Orchestrator

168
00:13:58,197 --> 00:14:00,487
helps to do a lot of stuff.

169
00:14:00,537 --> 00:14:04,917
For example, controlling readiness
of the application auto scaling

170
00:14:04,917 --> 00:14:10,777
application, restarting when it
fails, of course, and more and more.

171
00:14:11,297 --> 00:14:12,557
Next, auto scaling.

172
00:14:13,337 --> 00:14:17,967
As we said a couple of times, it's
a very important thing when based on

173
00:14:17,977 --> 00:14:23,707
metrics of your application, for example,
CPU or number of requests and common

174
00:14:23,717 --> 00:14:31,097
requests or number of consumed messages,
the orchestrator can also scale your

175
00:14:31,307 --> 00:14:35,907
application by putting more replicas
or removing some replicas if there are

176
00:14:35,907 --> 00:14:41,557
no need no need for such a capacity,
which is very useful if you have spikes

177
00:14:41,597 --> 00:14:47,977
in your load and you don't want to
lose clients, for example, if you can't

178
00:14:47,987 --> 00:14:56,837
process their requests and One more
very nice thing is anti affinity, when

179
00:14:56,877 --> 00:15:04,687
a cluster cares that your replicas are,
if possible, running on different nodes.

180
00:15:05,237 --> 00:15:11,987
If one node fails, there is less likely
that several replicas of your application

181
00:15:11,987 --> 00:15:17,237
run on this node, and this will have a big
impact on your application availability.

182
00:15:18,337 --> 00:15:21,777
This is also not so much
achievable without the

183
00:15:23,807 --> 00:15:24,427
orchestration.

184
00:15:24,427 --> 00:15:28,557
And the last very nice
thing here is self healing.

185
00:15:29,357 --> 00:15:36,087
When the machine, virtual machine if
something goes wrong and it goes down,

186
00:15:36,817 --> 00:15:42,897
It can be restarted on the platform
layer or on the infrastructure layer

187
00:15:43,517 --> 00:15:50,367
because the infrastructure monitors its
health by means of health checks and load

188
00:15:50,367 --> 00:15:56,707
balancer for example does health checks
to understand is your node running, is

189
00:15:56,707 --> 00:16:02,747
it healthy, can it send requests to that
node, or maybe avoid sending applications

190
00:16:02,747 --> 00:16:05,837
to a dead node so they will not be lost.

191
00:16:06,507 --> 00:16:13,557
And there are also health monitoring
inside to see how node is going

192
00:16:14,147 --> 00:16:19,287
and is it okay or something wrong
or maybe it's time to restart the

193
00:16:19,287 --> 00:16:21,817
node or stop receiving the requests.

194
00:16:22,627 --> 00:16:27,577
And it's very useful because we always
work in dynamic environment and even

195
00:16:27,577 --> 00:16:34,962
like Cloud infrastructure is not 100
percent invulnerable for failures and

196
00:16:34,972 --> 00:16:39,502
it fails from time to time because
underlying hardware fails and software

197
00:16:39,502 --> 00:16:44,312
on top of that hardware fails and
thus it works in some dynamics.

198
00:16:45,562 --> 00:16:50,202
Something breaks, something restarts,
something being recreated and

199
00:16:50,602 --> 00:16:53,252
such tools help to do this a lot.

200
00:16:54,602 --> 00:16:55,712
That's the second layer.

201
00:16:56,292 --> 00:16:57,082
Third layer.

202
00:16:57,482 --> 00:17:02,882
Is the data we often forget
about the data when we're talking

203
00:17:02,882 --> 00:17:04,272
about availability and stuff.

204
00:17:04,302 --> 00:17:06,412
We only talk about the application.

205
00:17:07,152 --> 00:17:09,092
It's not 100 percent correct.

206
00:17:09,182 --> 00:17:14,752
In fact, data is a key
for the application.

207
00:17:14,752 --> 00:17:18,522
Data is a key for almost everything.

208
00:17:18,712 --> 00:17:22,882
Our work as a software engineers
as a platform engineers.

209
00:17:23,652 --> 00:17:26,672
And let's talk a little bit about data.

210
00:17:26,672 --> 00:17:26,742
Data.

211
00:17:27,292 --> 00:17:33,942
Again, Redundancy is the most
beloved tool also in the data area.

212
00:17:34,672 --> 00:17:36,102
We do database replicas.

213
00:17:36,512 --> 00:17:42,972
So we run several applications, several
instances of the database, and we in

214
00:17:42,972 --> 00:17:48,722
the runtime, in the real time, share the
changes between these replicas so that

215
00:17:48,742 --> 00:17:57,172
if one replica fails, we still able to,
uh, redistribute the request to the next

216
00:17:57,172 --> 00:17:59,652
replica and everything will be okay.

217
00:18:00,892 --> 00:18:01,202
Second.

218
00:18:01,797 --> 00:18:07,117
Message broker replicas work similar,
but a little bit differently, but the

219
00:18:07,157 --> 00:18:13,057
thing is, we have multiple brokers, for
example, in Kafka cluster, and if one

220
00:18:13,067 --> 00:18:17,917
broker fails, cluster will continue to
work, your topics will continue to work,

221
00:18:18,587 --> 00:18:21,627
and, yeah, it's achieved by redundancy.

222
00:18:22,607 --> 00:18:31,477
Backups, so we have Real time replicas,
we also have archived replicas, so that if

223
00:18:31,527 --> 00:18:36,747
the whole data center will burn, we still
can restore the data from this backup.

224
00:18:38,057 --> 00:18:39,537
And the second thing is caching.

225
00:18:41,787 --> 00:18:45,917
Sometimes services you
rely on are not available.

226
00:18:46,232 --> 00:18:50,202
By some reason, maybe they are
down, maybe the network connection

227
00:18:50,202 --> 00:18:53,072
to them is unstable, so what to do?

228
00:18:53,472 --> 00:19:00,042
We have to save some data for later
to avoid this kind of cascade outages.

229
00:19:01,062 --> 00:19:03,562
And first thing is application caching.

230
00:19:04,082 --> 00:19:09,792
You can just save the data in the
application memory and use it next

231
00:19:09,792 --> 00:19:16,102
for faster access or if the underlying
service is not available anymore.

232
00:19:16,702 --> 00:19:18,212
Second is database caching.

233
00:19:18,682 --> 00:19:23,782
For many database engines you can specify
caching on the database layer, again

234
00:19:23,802 --> 00:19:28,792
for either faster access or maybe to
collecting data from several instances

235
00:19:29,502 --> 00:19:32,082
and avoiding single point of failure.

236
00:19:33,862 --> 00:19:37,312
Standalone caching, like having
a Redis instance for example.

237
00:19:37,832 --> 00:19:44,832
It also helps To avoid one
service failing and you are not

238
00:19:44,882 --> 00:19:47,172
able to get the data from there.

239
00:19:47,982 --> 00:19:52,562
There are many more options,
but I want to cover the basics.

240
00:19:52,902 --> 00:19:58,992
And of course we have to talk about
content delivery networks, CDN.

241
00:19:59,492 --> 00:20:03,042
The replicas where you
hold backend is not needed.

242
00:20:03,062 --> 00:20:06,692
You can get data from their network.

243
00:20:07,127 --> 00:20:08,987
Of caches around the globe.

244
00:20:09,097 --> 00:20:15,647
Normally it's often more about front
end and data media content, but

245
00:20:15,647 --> 00:20:20,377
it's still very useful technique
and it makes sense to keep in

246
00:20:20,387 --> 00:20:23,777
mind that it can save your system.

247
00:20:24,177 --> 00:20:25,247
from big outage.

248
00:20:25,257 --> 00:20:30,907
For example, if your startup starts
in your campaign, you may expect

249
00:20:30,907 --> 00:20:39,057
maybe a thousand of people hitting
your index page after some successful

250
00:20:39,127 --> 00:20:43,937
post in social media or something and
your application may not be posted.

251
00:20:43,992 --> 00:20:50,412
And CDN can actually save you by
redirection requests to save copies

252
00:20:50,412 --> 00:20:52,792
of your index page basically.

253
00:20:53,582 --> 00:20:55,042
And that was the third layer.

254
00:20:55,532 --> 00:20:57,762
The next one is infrastructure.

255
00:20:58,612 --> 00:20:59,712
The virtual machines.

256
00:21:00,212 --> 00:21:06,442
The networks and the disks, the beloved
trio basis of everything that we have.

257
00:21:07,362 --> 00:21:10,902
So why is it important?

258
00:21:11,402 --> 00:21:17,492
Because you also have to know the
features and what they can bring

259
00:21:17,492 --> 00:21:19,322
you, or maybe what it can mean.

260
00:21:19,352 --> 00:21:24,697
For example, for compute in many
platforms, in many cloud providers like

261
00:21:24,737 --> 00:21:29,777
public or private cloud providers, like
VMware, there is a hot migration option.

262
00:21:30,107 --> 00:21:34,027
That means that if the host where
your virtual machine is running fails,

263
00:21:34,447 --> 00:21:40,697
your virtual machine will, with its
memory, its state, will be restarted,

264
00:21:40,757 --> 00:21:46,472
will be started on another host
from the same or almost same host.

265
00:21:46,832 --> 00:21:51,922
Place almost same operation
position in memory and will continue

266
00:21:51,922 --> 00:21:54,922
to work almost without change.

267
00:21:55,522 --> 00:22:01,012
It's also a very nice feature,
but it may bring some surprises

268
00:22:01,082 --> 00:22:02,442
if you don't know about it.

269
00:22:03,322 --> 00:22:04,302
And instance groups.

270
00:22:04,872 --> 00:22:09,102
It's also a feature for managing
multiple virtual machines with

271
00:22:09,112 --> 00:22:12,922
the same configuration, like
auto scaling them, updating them,

272
00:22:13,372 --> 00:22:15,572
managing their network, etc.

273
00:22:16,032 --> 00:22:20,172
And it also helps us to
improve availability.

274
00:22:20,462 --> 00:22:27,382
For example, by auto scaling the
fleet of machines where you run your

275
00:22:27,382 --> 00:22:29,052
application or platform or something.

276
00:22:29,992 --> 00:22:31,132
Second is networking.

277
00:22:33,602 --> 00:22:34,672
Almost always.

278
00:22:35,172 --> 00:22:40,452
Software defined network is used
over commodity machines instead

279
00:22:40,452 --> 00:22:46,652
of some hardware switches,
hardware, something it's used

280
00:22:46,672 --> 00:22:48,712
also, but not for these purposes.

281
00:22:49,697 --> 00:22:56,527
And it also has a lot of built in
availability features inside for

282
00:22:56,527 --> 00:22:59,677
example, Gameware and Asics tools.

283
00:23:00,367 --> 00:23:03,217
They have a lot of very useful techniques.

284
00:23:03,677 --> 00:23:10,207
that you have maybe no idea about,
but they help to, they help overland

285
00:23:10,867 --> 00:23:16,207
platforms, for example, to work
smoothly and to virtual machines to

286
00:23:16,207 --> 00:23:19,957
communicate without interruptions,
even if something goes wrong.

287
00:23:20,787 --> 00:23:21,812
Load balancers.

288
00:23:22,052 --> 00:23:29,532
Also, we almost never use metal load
balancers, we use software defined load

289
00:23:30,512 --> 00:23:39,382
balancers, and as any other application,
load balancer can be a single point

290
00:23:39,382 --> 00:23:42,172
of failure, and we want to avoid that.

291
00:23:42,402 --> 00:23:47,972
We always have multiple load balancers,
balancing the load, distributing the load.

292
00:23:48,502 --> 00:23:50,492
Yeah, know about that, think about that.

293
00:23:50,572 --> 00:23:58,092
It also may bring some complexity, but
it normally brings us a lot of benefits.

294
00:23:58,502 --> 00:24:00,942
Reliability to our system.

295
00:24:02,262 --> 00:24:04,302
And next, the disk, the storage.

296
00:24:05,802 --> 00:24:07,702
We always replicate stuff.

297
00:24:07,732 --> 00:24:09,002
We do redundant things.

298
00:24:09,192 --> 00:24:10,062
We like that.

299
00:24:10,772 --> 00:24:15,922
And, yeah, we have to replicate storage.

300
00:24:16,907 --> 00:24:23,137
For example, if you save some binary
file, this file being spread into

301
00:24:23,207 --> 00:24:29,207
several smaller chunks and each chunk
will be copied into several machines

302
00:24:29,607 --> 00:24:35,137
to avoid that one machine will fail
and you will lost this chunk and

303
00:24:35,177 --> 00:24:37,867
you cannot Collect this file again.

304
00:24:38,267 --> 00:24:40,947
So for example, this is how theft works.

305
00:24:41,447 --> 00:24:44,367
It puts different chunks
into different machines.

306
00:24:44,427 --> 00:24:47,917
Each chunk is copied two or three times.

307
00:24:48,407 --> 00:24:55,527
And if machine fails, you still, you
can still recreate the initial data.

308
00:24:56,012 --> 00:24:57,452
From the rest of the machines.

309
00:24:57,512 --> 00:24:59,972
And there will be no issue with that.

310
00:25:00,902 --> 00:25:10,072
And network file system is used to
attach disk to a machine without direct

311
00:25:10,082 --> 00:25:11,632
physical connection between them.

312
00:25:11,872 --> 00:25:17,202
And this, for example, how Kubernetes
mounts, mounts the disks to nodes.

313
00:25:17,562 --> 00:25:21,622
And if node fails, you won't
lose the data on the disk.

314
00:25:22,122 --> 00:25:25,722
It Kubernetes will just
remount it to another node.

315
00:25:25,722 --> 00:25:25,742
Okay.

316
00:25:26,072 --> 00:25:30,852
and run the pod there and you
can continue using this data.

317
00:25:31,402 --> 00:25:37,622
This was fourth level of
our cake, our ability cake.

318
00:25:38,282 --> 00:25:42,172
Let's go to harder level, hardware.

319
00:25:43,082 --> 00:25:49,227
And we don't have so many Fancy
stuff on the hardware level.

320
00:25:49,667 --> 00:25:54,757
It's definitely a lot of stuff there,
but I'm not going to go deeper.

321
00:25:55,137 --> 00:25:57,687
I just want to mention a couple of things.

322
00:25:58,007 --> 00:26:01,727
First rate again replication.

323
00:26:02,227 --> 00:26:06,297
Unfortunately, these hard
drives are not reliable.

324
00:26:06,782 --> 00:26:11,412
And any other drives are not
reliable and we cannot trust them.

325
00:26:11,882 --> 00:26:17,002
So we have to put several disks
together and again, split data into

326
00:26:17,052 --> 00:26:23,202
tiny chunks and redistribute copies
of these chunks between these disks.

327
00:26:23,362 --> 00:26:24,642
If one disk fail.

328
00:26:24,697 --> 00:26:30,197
We still can reconstruct the initial
data from the rest of the disks.

329
00:26:32,237 --> 00:26:34,087
Second, rack distribution.

330
00:26:34,877 --> 00:26:35,197
Sorry.

331
00:26:35,747 --> 00:26:41,937
We can expect that if we have
all of our servers in one

332
00:26:41,957 --> 00:26:43,987
rack, something can go wrong.

333
00:26:44,087 --> 00:26:49,997
It can burn it can be electricity
outage, it can be fan outage,

334
00:26:50,607 --> 00:26:52,557
and it will stop working.

335
00:26:52,757 --> 00:26:58,567
So we want to have our servers in
different racks and some applications

336
00:26:58,607 --> 00:27:04,117
like Kafka, for example, has rack
awareness feature when you can

337
00:27:04,117 --> 00:27:11,237
specify which Brokers run on rich
racks, other run together or not.

338
00:27:11,557 --> 00:27:18,327
So Kafka will manage the data replicas
that way to minimize the possibility of

339
00:27:18,327 --> 00:27:22,617
the replicas to host on some same rack.

340
00:27:22,897 --> 00:27:30,427
So if the whole rack goes down, Kafka
will be able to still continue operating.

341
00:27:31,307 --> 00:27:35,947
And that means that different
levels are interconnected.

342
00:27:36,462 --> 00:27:42,962
And interdependent and you also have
to understand this dependency to not

343
00:27:43,022 --> 00:27:49,102
only use underlying infrastructure,
but also leverage its features.

344
00:27:50,122 --> 00:27:53,142
And next is internet channel backup.

345
00:27:53,652 --> 00:28:00,642
So in data center, it's always more than
one internet provider because one internet

346
00:28:00,762 --> 00:28:02,982
provider is a single point of failure.

347
00:28:04,152 --> 00:28:10,882
A single physical cable is a single
point of failure, that can be an angry

348
00:28:10,882 --> 00:28:12,712
excavator or something like that.

349
00:28:12,962 --> 00:28:15,942
And it happens again
and again, surprisingly.

350
00:28:16,932 --> 00:28:26,102
And here we have our whole cake, but
there is still a room for an icing on

351
00:28:26,102 --> 00:28:29,892
top, which is a global availability.

352
00:28:30,442 --> 00:28:37,607
Basically, all of our applications
running in pretty stable environment, but

353
00:28:37,627 --> 00:28:40,447
something may happen with the data center.

354
00:28:40,457 --> 00:28:45,947
For example, it can burn, it can
crash, like I don't know, meteorite

355
00:28:45,947 --> 00:28:51,087
can crash into data center, or
just a tsunami, or something else.

356
00:28:51,787 --> 00:28:54,122
And if It's important.

357
00:28:54,172 --> 00:28:57,852
And in many cases, it is you can
have a several data center with a

358
00:28:57,852 --> 00:29:03,512
standby disaster recovery replica of
your whole architectural landscape.

359
00:29:03,812 --> 00:29:10,172
And if something goes wrong with your main
data center, immediately you switch to the

360
00:29:10,232 --> 00:29:12,222
second data center and continue working.

361
00:29:12,242 --> 00:29:16,732
It requires replicating all
data constantly and doing.

362
00:29:17,132 --> 00:29:26,292
A lot of training, but it works in all the
cases and more interesting case is a multi

363
00:29:26,292 --> 00:29:33,332
zone regions in cloud providers when if
one region have has more than three, three

364
00:29:33,332 --> 00:29:39,572
or more zones, it is disaster tolerant
because It's a dangerous situation.

365
00:29:40,282 --> 00:29:48,022
If one zone fails, it's the region and
its services will continue to work.

366
00:29:48,032 --> 00:29:53,732
For example, Kubernetes having
nodes in three zones will lose

367
00:29:53,762 --> 00:29:55,782
nothing, only some capacity.

368
00:29:56,392 --> 00:29:58,692
But the applications
will continue to work.

369
00:29:59,432 --> 00:30:01,257
The databases will continue to work.

370
00:30:01,257 --> 00:30:04,737
The Kafka brokers will
continue to work and so on.

371
00:30:05,637 --> 00:30:13,177
And one more interesting thing is
global server load balancer or GSLB.

372
00:30:14,177 --> 00:30:22,677
Sometimes you even rely on
a single, the single region.

373
00:30:23,122 --> 00:30:26,152
Or a single provider or a single city.

374
00:30:26,642 --> 00:30:29,952
Something may happen with a
city or even with a country.

375
00:30:30,762 --> 00:30:36,532
And for some businesses, it's crucial
to be available, like, all of the time.

376
00:30:36,882 --> 00:30:41,692
In that case, you may want to have
your application running in different

377
00:30:41,702 --> 00:30:45,222
regions, maybe in different countries,
maybe on different continents.

378
00:30:45,752 --> 00:30:48,732
How to distribute traffic
between those regions?

379
00:30:49,307 --> 00:30:54,867
That's very difficult, but there is
a solution, JSLB, it works based on

380
00:30:55,127 --> 00:31:03,127
BGP Anycast protocol giving you an
ability to access the nearest endpoint

381
00:31:03,137 --> 00:31:04,637
to you, the nearest host to you.

382
00:31:04,897 --> 00:31:11,302
For example, if you are in Japan, and
there are, Several regions regional

383
00:31:11,302 --> 00:31:16,092
installations, one in China, one
US, the nearest will be in China.

384
00:31:16,412 --> 00:31:22,252
But if you are in Mexico, the nearest will
be, In US, and it works automatically.

385
00:31:22,782 --> 00:31:27,862
And it also gives the ability to
do multi region load balancing.

386
00:31:28,362 --> 00:31:33,982
And so for example, if your application is
very load heavy, you can also spread the

387
00:31:33,982 --> 00:31:40,207
load between different regions, different
locations, and Again, in turn, it will

388
00:31:40,247 --> 00:31:44,537
improve your application performance
and your application availability.

389
00:31:45,227 --> 00:31:48,707
One example of such an
application is a YouTube.

390
00:31:50,857 --> 00:31:51,297
So

391
00:31:53,467 --> 00:31:58,277
we have our whole cake,
we have an icing on top.

392
00:31:59,277 --> 00:32:01,927
Let me sum it up.

393
00:32:04,037 --> 00:32:09,337
First, availability is made
up of many layers, levels.

394
00:32:10,392 --> 00:32:15,542
Second, each level must cooperate
with other levels, as we've seen.

395
00:32:17,382 --> 00:32:22,912
And third, we need to consider
all levels when developing highly

396
00:32:23,032 --> 00:32:28,282
available applications, so they
can leverage different availability

397
00:32:28,282 --> 00:32:30,072
features of underlying levels.

398
00:32:31,607 --> 00:32:39,207
And it's all thanks for your
attention and feel free to find me

399
00:32:39,227 --> 00:32:45,967
on LinkedIn, or if it's happening,
that will be in the same city, I'd

400
00:32:45,967 --> 00:32:47,697
be happy to drink a coffee with you.

401
00:32:48,917 --> 00:32:49,447
Thank you.

