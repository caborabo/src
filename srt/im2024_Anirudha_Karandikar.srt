1
00:00:00,290 --> 00:00:04,930
Hello, a warm welcome at Con
42 Incident Management 2024.

2
00:00:05,680 --> 00:00:08,979
Today, I will be speaking about
using generative AI to tackle

3
00:00:08,979 --> 00:00:10,600
API sprawl in enterprises.

4
00:00:11,190 --> 00:00:13,190
But before that, a brief
background about myself.

5
00:00:13,190 --> 00:00:18,160
I'm Anirudh Karandikar, Principal
Software Engineer at Fortune 500 Company.

6
00:00:18,905 --> 00:00:22,815
I'm a full stack engineer with a
niche skill set in API management.

7
00:00:23,665 --> 00:00:26,995
As part of my work profile, I
deal with the entire API lifecycle

8
00:00:27,494 --> 00:00:29,314
from ideation to governance.

9
00:00:30,375 --> 00:00:34,624
The observations, ideas expressed
here are strictly my own.

10
00:00:36,105 --> 00:00:37,705
Now let's take a look at the agenda.

11
00:00:38,464 --> 00:00:41,975
First, we'll discuss about some
history to build a context.

12
00:00:42,465 --> 00:00:45,709
Then we'll talk about the problem
statement, which is API sprawl.

13
00:00:46,700 --> 00:00:51,120
Then we'll take a look at the proposed
construct to deal this problem statement.

14
00:00:52,230 --> 00:00:54,730
Then we'll take a deeper
look at the governance model.

15
00:00:55,660 --> 00:00:57,710
Then we'll explore the API discovery tool.

16
00:00:58,650 --> 00:01:02,020
Then we'll talk about API
documentation and then some strategies

17
00:01:02,020 --> 00:01:03,820
to control existing API sprawl.

18
00:01:05,240 --> 00:01:06,110
So let's dive in.

19
00:01:07,799 --> 00:01:12,440
So roughly about a decade ago, most
of the enterprises started their

20
00:01:12,449 --> 00:01:15,179
move from monolith to microservices.

21
00:01:15,860 --> 00:01:22,880
And for those dealing in APIs, No, what
advantages or benefits microservices

22
00:01:22,949 --> 00:01:28,490
offer, but just to recap microservices
architecture is a software design

23
00:01:28,490 --> 00:01:33,709
approach where application is built as a
collection of small independent services.

24
00:01:34,759 --> 00:01:37,139
Each services focus on
specific business function.

25
00:01:37,169 --> 00:01:41,399
So this is usually known as single
responsibility model, and they offer

26
00:01:41,470 --> 00:01:48,190
wide range of benefits, scalability,
like each individual microservices can

27
00:01:48,190 --> 00:01:51,739
be scaled independently irrespective
of others, depending upon demand.

28
00:01:52,495 --> 00:01:53,455
Faster development.

29
00:01:53,975 --> 00:01:57,635
The modularity leads to faster
development, as in different teams can

30
00:01:57,645 --> 00:01:59,854
work on different set of microservices.

31
00:02:00,135 --> 00:02:00,845
Resilience.

32
00:02:01,225 --> 00:02:06,585
This modular structure offers fault
tolerance, and even if one service goes

33
00:02:06,595 --> 00:02:11,084
down, rest of the services can still
serve other business functionalities.

34
00:02:12,114 --> 00:02:13,135
Easier maintenance.

35
00:02:13,895 --> 00:02:18,765
Again, this modular structure helps
to make maintenance easy, and mostly

36
00:02:18,765 --> 00:02:21,275
there are different teams managing
different set of microservices.

37
00:02:22,470 --> 00:02:26,676
Agility, the faster development,
scalability, resilience, make

38
00:02:26,676 --> 00:02:30,213
microservices agile and the time
to market is greatly reduced.

39
00:02:31,273 --> 00:02:32,473
Technological diversity.

40
00:02:32,473 --> 00:02:34,763
I think this is the key advantage.

41
00:02:35,093 --> 00:02:37,933
Unlike monolith, technology
is not a limiting factor.

42
00:02:38,453 --> 00:02:42,193
We can very well have a set of
microservices developed in Java and

43
00:02:42,213 --> 00:02:47,123
other set of microservices developed
in other language, say Python, coexist

44
00:02:47,123 --> 00:02:48,683
in the same deployment architecture.

45
00:02:49,608 --> 00:02:51,618
And finally, cost efficiency.

46
00:02:52,248 --> 00:02:57,588
I think the whole microservices
architecture is built or geared towards

47
00:02:57,678 --> 00:03:02,158
resource optimization, thus lowering
the infrastructure and operation costs.

48
00:03:03,928 --> 00:03:06,388
If we take a look at some key milestones.

49
00:03:06,478 --> 00:03:11,008
So between 2011 to 2013, companies
like Netflix and Amazon pioneered

50
00:03:11,068 --> 00:03:11,888
the adoption of microservices.

51
00:03:11,888 --> 00:03:15,693
2014, 2015, this term.

52
00:03:16,013 --> 00:03:18,513
became widely recognized
in software community.

53
00:03:19,293 --> 00:03:24,143
And then from 2016 to present, with
the rise of cloud native technologies,

54
00:03:24,533 --> 00:03:29,203
containerization, mostly Docker, and
container orchestration tools, read

55
00:03:29,593 --> 00:03:34,453
Kubernetes, microservice adoption
has grown by leaps and bounds.

56
00:03:35,423 --> 00:03:37,313
That brings to our problem statement.

57
00:03:38,233 --> 00:03:42,933
With the widespread adoption of REST
APIs and microservices, it has led to

58
00:03:42,933 --> 00:03:45,303
an explosion in the number of APIs.

59
00:03:45,943 --> 00:03:48,568
And I'm talking about
tens of thousands of APIs.

60
00:03:49,288 --> 00:03:53,148
Many enterprises adopted distributed
development as orgs within

61
00:03:53,148 --> 00:03:54,908
enterprises moved at different pace.

62
00:03:55,388 --> 00:03:59,588
And this setup provided team
autonomy and reduced conflicts.

63
00:04:00,228 --> 00:04:03,628
So talking about this, like each
individual team maintaining their own

64
00:04:03,628 --> 00:04:08,418
set of repositories, their own standards,
their own guidelines, their own timelines

65
00:04:08,428 --> 00:04:10,338
to deploy these services into production.

66
00:04:11,498 --> 00:04:15,778
However, this setup created
difficulties as enterprises scaled.

67
00:04:16,908 --> 00:04:19,088
And following were the
main contributing factors.

68
00:04:19,688 --> 00:04:21,938
First and foremost of them is duplication.

69
00:04:22,858 --> 00:04:26,048
Different teams creating APIs
that serve the same function.

70
00:04:26,678 --> 00:04:29,898
So at any given point in
time, all the teams needed to

71
00:04:29,898 --> 00:04:31,408
solve for some common issues.

72
00:04:32,153 --> 00:04:36,583
example, location services or address
management or user management.

73
00:04:37,283 --> 00:04:42,243
And to solve for these, everyone
created a flavor of the same solution,

74
00:04:42,473 --> 00:04:44,213
thus resulting in duplication.

75
00:04:45,393 --> 00:04:48,983
Second problem is harder to
exercise security control and major.

76
00:04:49,953 --> 00:04:55,663
So here the problem is twofold
non standard APIs expose different

77
00:04:56,223 --> 00:05:01,413
attack vectors and a large number of
APIs expose greater attack surface.

78
00:05:02,138 --> 00:05:06,798
So think of this from, from the point
of view of horizontal organizations

79
00:05:06,808 --> 00:05:08,488
like cybersecurity or security.

80
00:05:08,848 --> 00:05:14,298
If they want to enforce governance,
it becomes very difficult if the

81
00:05:14,298 --> 00:05:16,688
APIs are scattered and non standard.

82
00:05:17,718 --> 00:05:19,678
Third are operational inefficiencies.

83
00:05:20,238 --> 00:05:24,998
Again, since it's hard to enforce
standards, each, each team can

84
00:05:24,998 --> 00:05:26,598
have different operational needs.

85
00:05:27,398 --> 00:05:32,308
thus creating overheads, operational
overheads and leading to maintenance

86
00:05:32,308 --> 00:05:37,178
and performance issues and fourth,
which is not directly related to the

87
00:05:37,178 --> 00:05:42,028
distributed architecture, but often
found is inconsistent documentation.

88
00:05:42,848 --> 00:05:45,928
So incomplete or outdated API
documentation makes it hard to

89
00:05:45,928 --> 00:05:47,698
find and reuse existing APIs.

90
00:05:48,148 --> 00:05:52,598
So even if the two internal teams were
to collaborate and they are using the

91
00:05:53,638 --> 00:05:56,428
design, contract driven development.

92
00:05:57,063 --> 00:06:00,633
Then, the gap between the
documentation and the actual API

93
00:06:01,083 --> 00:06:03,113
causes a lot of, a lot of issues.

94
00:06:05,253 --> 00:06:06,953
So what's the proposed construct?

95
00:06:07,053 --> 00:06:09,583
Like, how can we solve
this particular problem?

96
00:06:10,193 --> 00:06:12,663
So before we look at the
construct, we need to understand

97
00:06:12,663 --> 00:06:14,083
why centralization matters.

98
00:06:14,958 --> 00:06:18,498
A decentralized approach leads
to scattered APIs, inconsistent

99
00:06:18,498 --> 00:06:22,418
standards, difficulty in tracking
changes or enforcing governance.

100
00:06:22,878 --> 00:06:29,028
By moving to a centralized monorepo
model, we can have a single repository

101
00:06:29,058 --> 00:06:32,588
that maintains all the API contracts,
which offers not only transparency,

102
00:06:32,778 --> 00:06:34,468
but it also improves visibility.

103
00:06:35,728 --> 00:06:39,148
Teams can access, a unified
source of API definition.

104
00:06:39,618 --> 00:06:43,478
This in turn increases collaboration
and accelerates development cycles.

105
00:06:44,668 --> 00:06:48,618
So let's take a look at, this is like
a blueprint of the proposed construct.

106
00:06:49,568 --> 00:06:51,848
There are different parts of the
solution and we'll talk about them.

107
00:06:52,773 --> 00:06:54,623
So the first and foremost
is centralization.

108
00:06:54,673 --> 00:06:59,713
And by centralization, I don't mean
that, like each individual teams

109
00:06:59,713 --> 00:07:03,483
can still retain the implementation
in their individual repositories.

110
00:07:04,043 --> 00:07:07,453
When centralization of the API
definitions or the API contracts.

111
00:07:08,123 --> 00:07:12,173
This certainly needs to be centralized
so that it becomes a single source

112
00:07:12,173 --> 00:07:16,833
of truth for all the APIs within an
organization or within an enterprise.

113
00:07:17,753 --> 00:07:21,673
The second and the foremost step is
defining business rules and standards.

114
00:07:22,493 --> 00:07:27,083
A few examples of this are naming
nomenclature, like, how do you want

115
00:07:27,093 --> 00:07:32,143
to name your APIs, or how do you
want to name the parts of your APIs,

116
00:07:32,203 --> 00:07:35,133
like base path or the actual path.

117
00:07:37,823 --> 00:07:41,803
Most of, what standards do you,
does an enterprise want to use?

118
00:07:41,833 --> 00:07:45,393
Like most of the APIs today,
or most of the companies today

119
00:07:45,753 --> 00:07:47,823
use Open API specification.

120
00:07:48,898 --> 00:07:49,688
beyond this.

121
00:07:50,218 --> 00:07:54,718
Every enterprise can define
certain custom standards.

122
00:07:55,608 --> 00:07:59,638
For example, in OpenAPI specifications,
you can extend the tags.

123
00:08:00,168 --> 00:08:05,148
So enterprises today can have
categorization to identify their domains.

124
00:08:05,408 --> 00:08:08,858
There can be domains like payment,
there can be domains like supply chain.

125
00:08:09,288 --> 00:08:15,008
So you can create separate tags to
classify and further categorize the APIs.

126
00:08:17,218 --> 00:08:21,488
The third is programmatic enforcement, and
we'll see this in the governance model.

127
00:08:22,728 --> 00:08:26,978
But once you define the rules and
standards, there needs to be a

128
00:08:27,028 --> 00:08:32,308
programmatic enforcement of these
rules so that all the API contracts

129
00:08:32,368 --> 00:08:36,088
adhere to these standards and
there is no bypassing of them.

130
00:08:37,728 --> 00:08:39,998
The fourth point is about starter kits.

131
00:08:40,238 --> 00:08:44,728
So let's say we have a unified
definition, like all the APIs are

132
00:08:44,728 --> 00:08:46,528
defined in a certain standard.

133
00:08:46,528 --> 00:08:46,918
Okay.

134
00:08:47,568 --> 00:08:52,428
Now, if we want to extend this to
code, there are libraries today

135
00:08:52,618 --> 00:08:59,838
which can work on your API contracts
and spit out a skeleton of code.

136
00:09:02,398 --> 00:09:07,533
What one team, what the team
should do is, Go one level down.

137
00:09:08,193 --> 00:09:11,743
Like every API today needs
some cross cutting concerns.

138
00:09:12,383 --> 00:09:13,223
Be it logging.

139
00:09:13,483 --> 00:09:15,693
it needs a standard logging format.

140
00:09:16,913 --> 00:09:18,863
It needs standards for observability.

141
00:09:19,093 --> 00:09:20,493
It needs standards for security.

142
00:09:20,493 --> 00:09:22,273
Your auth and auth z mechanisms.

143
00:09:22,913 --> 00:09:26,733
And it needs a standard way of
defining errors and exceptions.

144
00:09:27,703 --> 00:09:30,243
These are some of the major
cross cutting concerns.

145
00:09:30,683 --> 00:09:31,643
There can be others.

146
00:09:32,053 --> 00:09:36,328
In addition to this, if you're
deploying using containers in a

147
00:09:36,328 --> 00:09:42,853
Kubernetes environment, you also
need images and you can, this is a

148
00:09:42,873 --> 00:09:44,763
place to standardize those images.

149
00:09:45,713 --> 00:09:49,083
So the advantages of
this are again twofold.

150
00:09:49,773 --> 00:09:54,483
As an enterprise, you can exercise
controls or set standards on

151
00:09:55,003 --> 00:09:58,603
how, these crosscutting concerns
can be leveraged for it.

152
00:09:58,693 --> 00:10:02,823
For instance, if you want to change
your logging standards, this would

153
00:10:02,833 --> 00:10:04,403
be one place where you change that.

154
00:10:04,413 --> 00:10:05,173
Or if you want.

155
00:10:05,868 --> 00:10:08,948
To add additional policies, on security.

156
00:10:09,138 --> 00:10:13,518
This is the place you do so that
everyone using the starter kits will

157
00:10:13,588 --> 00:10:15,848
automatically inherit, these standards.

158
00:10:16,448 --> 00:10:19,048
And the second point is from
development perspective.

159
00:10:19,308 --> 00:10:22,548
So as a developer, I don't have to
worry about this cross cutting concerns

160
00:10:22,578 --> 00:10:29,178
because I know that the deployment unit,
generated out of the starter kit will

161
00:10:29,188 --> 00:10:31,668
have all these cross cutting concerns.

162
00:10:32,003 --> 00:10:36,913
Standardized cross cutting concerns baked
into it, and all that I have, I would,

163
00:10:37,013 --> 00:10:39,183
I need to care is the functional code.

164
00:10:40,813 --> 00:10:46,053
Okay, so once, let's say we have
established all of these, the next in line

165
00:10:46,153 --> 00:10:50,983
is how do we harness AI on top of this?

166
00:10:51,973 --> 00:10:56,303
So I would feed all of
this rich data into an LLM.

167
00:10:58,833 --> 00:11:03,103
Apart from this API definition, we
can also pair it with the actual

168
00:11:03,143 --> 00:11:05,693
deployment or lives live services data.

169
00:11:05,763 --> 00:11:08,173
Like these are the APIs
that are actually deployed.

170
00:11:08,723 --> 00:11:13,613
And from production, we can get a rich
set of data like on usage patterns,

171
00:11:15,023 --> 00:11:17,253
and other, Other observability metrics.

172
00:11:18,043 --> 00:11:22,963
So this combined with the source
data provides very rich insights.

173
00:11:23,233 --> 00:11:28,713
And we will look into a few use cases,
but then based on that, we can develop

174
00:11:28,863 --> 00:11:34,983
the API discovery to, also, like most
of the enterprises today use dashboards.

175
00:11:35,033 --> 00:11:39,213
so there are two types of dashboards, one
primarily used for troubleshooting and

176
00:11:39,213 --> 00:11:40,303
the other are the executive dashboards.

177
00:11:40,993 --> 00:11:41,298
Thank you for listening.

178
00:11:41,558 --> 00:11:43,748
That provide the qualitative data.

179
00:11:45,028 --> 00:11:47,778
So imagine, the A.

180
00:11:47,798 --> 00:11:47,948
I.

181
00:11:47,948 --> 00:11:51,408
Insights derived from this rich set.

182
00:11:51,638 --> 00:11:56,888
They can provide a rich context to
the already existing dashboards.

183
00:11:58,058 --> 00:11:59,428
Those are the use cases.

184
00:12:00,228 --> 00:12:05,588
Now let's talk or double click
a bit on the governance model,

185
00:12:06,068 --> 00:12:08,728
which we talked earlier again.

186
00:12:08,818 --> 00:12:11,018
There are many ways in which
governance can be applied.

187
00:12:11,823 --> 00:12:14,703
Below is just a sample
based on CICD paradigm.

188
00:12:15,373 --> 00:12:18,313
So let's say we have defined a
centralized repo wherein we are

189
00:12:18,363 --> 00:12:19,943
maintaining our API contracts.

190
00:12:20,493 --> 00:12:26,943
And if I were to check in a new API
contract or, if I were to check in

191
00:12:27,463 --> 00:12:32,333
modifications to an existing API
contract, first and foremost, my

192
00:12:32,343 --> 00:12:35,693
pipeline would check for compliance
policies, like whether my API is

193
00:12:35,723 --> 00:12:37,403
compliant with all the policies.

194
00:12:38,008 --> 00:12:39,008
That have defined.

195
00:12:39,688 --> 00:12:45,678
Secondly, it will run a linter on top
of it and can generate maybe a score.

196
00:12:46,498 --> 00:12:50,798
but basically linter, it will run
linter to check for standards.

197
00:12:51,468 --> 00:12:53,038
The third is, quality gate.

198
00:12:53,418 --> 00:12:56,618
once you have run for standard
and compliance policies,

199
00:12:57,198 --> 00:12:58,618
you can define a threshold.

200
00:12:59,158 --> 00:13:01,588
is this API matching a certain threshold?

201
00:13:02,158 --> 00:13:04,228
If not, you can fail and give feedback.

202
00:13:04,593 --> 00:13:08,913
To the developers and only if and if
it crosses that certain threshold,

203
00:13:09,263 --> 00:13:13,403
then you can allow that contract
to move ahead in the API lifecycle.

204
00:13:14,903 --> 00:13:17,013
Next is the similarity check.

205
00:13:17,623 --> 00:13:21,443
So if you remember earlier, we talked
about API sprawl and duplication

206
00:13:21,453 --> 00:13:23,143
being the number one reason.

207
00:13:23,643 --> 00:13:25,023
Contribute into this problem.

208
00:13:25,713 --> 00:13:28,503
So how can we prevent it
right at the inception stage?

209
00:13:29,263 --> 00:13:33,903
So this is where we check when an
API contract is being committed.

210
00:13:35,413 --> 00:13:40,653
We can do a similarity check
using the LLM to see whether it is

211
00:13:40,753 --> 00:13:42,643
similar to any of the existing APIs.

212
00:13:43,053 --> 00:13:45,753
Now there are multiple criterias
to do this similarity check.

213
00:13:46,003 --> 00:13:48,253
It can be done on your request response.

214
00:13:48,303 --> 00:13:50,103
It can be done on your descriptions.

215
00:13:50,103 --> 00:13:54,343
It can be done on the downstream
domains that you're calling.

216
00:13:54,908 --> 00:13:59,168
But the idea is to check whether,
to check whether you really need a

217
00:13:59,168 --> 00:14:04,798
net new API or what are what you are
trying to achieve can be achieved by

218
00:14:04,798 --> 00:14:06,708
extending any of the existing APIs.

219
00:14:09,308 --> 00:14:14,478
So this presents a simple idea
on how governance, programmatic

220
00:14:14,668 --> 00:14:19,648
governance can be enforced so that
we have a way of enforcing standards.

221
00:14:20,158 --> 00:14:21,738
And also reducing duplication.

222
00:14:24,308 --> 00:14:28,908
Now let's take a look about if
we were to build an API discovery

223
00:14:28,948 --> 00:14:32,378
tool, using AI, how would I do that?

224
00:14:33,683 --> 00:14:36,453
Before that, let's take a
look at the building blocks.

225
00:14:36,473 --> 00:14:40,953
If I, if we were to build this tool,
so first and foremost is the LLM.

226
00:14:41,023 --> 00:14:46,913
This would be your, chat GPT or
cloud or llama or any other model.

227
00:14:48,133 --> 00:14:50,203
we basically need a
natural language processor.

228
00:14:51,238 --> 00:14:53,048
Secondly, we need a vector database.

229
00:14:53,088 --> 00:14:58,358
This can be a Pinecone or FAS, but
basically something where indexed

230
00:14:58,388 --> 00:15:02,798
data can reside and LLMs can do
a semantic search on top of it.

231
00:15:02,798 --> 00:15:05,938
Third, we need an API
documentation parser.

232
00:15:05,988 --> 00:15:10,578
There are many libraries available
in the market, which can parse the

233
00:15:11,258 --> 00:15:12,528
OpenAPI specification or Swagger.

234
00:15:13,488 --> 00:15:15,468
And fourth, we would need a web interface.

235
00:15:15,973 --> 00:15:22,853
This is basically wherein the users would
type in your natural language query and,

236
00:15:22,953 --> 00:15:24,913
would receive the API recommendations.

237
00:15:25,803 --> 00:15:30,533
So if you had this building blocks in
place, These are the high level steps.

238
00:15:31,033 --> 00:15:32,533
So first is the data preparation.

239
00:15:33,053 --> 00:15:36,003
so like I said, we have our
common repo containing all the API

240
00:15:36,063 --> 00:15:38,793
contracts and it's a rich data set.

241
00:15:39,123 --> 00:15:40,933
It has all the endpoint URLs.

242
00:15:40,933 --> 00:15:42,773
It has your request methods.

243
00:15:43,183 --> 00:15:44,583
It has responses.

244
00:15:44,633 --> 00:15:45,493
It has parameters.

245
00:15:45,493 --> 00:15:46,493
It has descriptions.

246
00:15:47,153 --> 00:15:50,853
All that is needed is to organize
this in a structured format,

247
00:15:51,453 --> 00:15:52,733
which is very easy to feed.

248
00:15:53,583 --> 00:15:56,543
The most commonly used format is JSON.

249
00:15:56,983 --> 00:15:58,213
but there is no such restriction.

250
00:15:59,093 --> 00:16:01,013
Second is indexing and context building.

251
00:16:01,993 --> 00:16:05,383
The metadata that is prepared in step
one needs to be indexed and it needs

252
00:16:05,383 --> 00:16:09,273
to be stored as something called
as embeddings in a vector database.

253
00:16:09,773 --> 00:16:14,983
So in AI terms, embedding
represents a real world object.

254
00:16:15,133 --> 00:16:19,013
So whatever So think of this if
you have some nouns in the real

255
00:16:19,013 --> 00:16:22,893
world, those are represented as
embeddings in the vector database.

256
00:16:23,723 --> 00:16:28,933
Secondly, we also need to build a mapping
system that links API functionalities

257
00:16:29,183 --> 00:16:30,503
to the natural language queries.

258
00:16:30,543 --> 00:16:31,733
And what do I mean by that?

259
00:16:32,373 --> 00:16:37,013
So for example, if someone queries or
someone asks, how do I get user details?

260
00:16:38,093 --> 00:16:44,103
Then, the LLM needs to know which
API endpoints to point it to in order

261
00:16:44,103 --> 00:16:45,383
to answer this particular query.

262
00:16:45,383 --> 00:16:49,198
Transcribed Third is
building the query interface.

263
00:16:50,338 --> 00:16:54,058
this is basically creating an
interface for natural language parsing.

264
00:16:54,878 --> 00:16:59,598
And the other part of this is
to craft prompts for LLM based

265
00:16:59,598 --> 00:17:00,838
on the natural language queries.

266
00:17:03,508 --> 00:17:05,988
And the fourth part is
the response generation.

267
00:17:06,168 --> 00:17:09,088
this is where the relevant
information is identified.

268
00:17:09,358 --> 00:17:10,938
LLM will generate a detailed response.

269
00:17:11,658 --> 00:17:16,733
Now, here, These steps, this step needs
to be done in an iterative manner.

270
00:17:17,343 --> 00:17:22,183
Like the first response that is
generated may not be detailed enough

271
00:17:22,183 --> 00:17:25,963
or may not be, may not have all
the information, that is needed.

272
00:17:26,313 --> 00:17:29,831
So we need to ask for more
prompts, ask for more questions,

273
00:17:29,831 --> 00:17:34,763
do it in an iterative fashion and
enhance that particular response.

274
00:17:35,443 --> 00:17:39,143
The other part of it is, like whenever
this response is generated, it.

275
00:17:39,583 --> 00:17:44,923
It can give snippets from the actual
sliders or actual API definitions.

276
00:17:45,693 --> 00:17:48,123
And, this is known as
contextual augmentation.

277
00:17:49,283 --> 00:17:53,718
So if you were to build, or follow
all these steps, The simplified

278
00:17:53,718 --> 00:17:55,138
flow would look something like that.

279
00:17:55,648 --> 00:18:00,028
A user would go to a web interface,
would type in a natural language

280
00:18:00,028 --> 00:18:02,318
query like, give me user details.

281
00:18:03,078 --> 00:18:06,738
This will be converted into a
prompt, which would do a semantic

282
00:18:06,738 --> 00:18:08,358
search on the indexed data.

283
00:18:09,088 --> 00:18:11,378
That's the processing on
the vector DB and LLM.

284
00:18:11,378 --> 00:18:17,288
And the output is the response, which
we could see in the user interface.

285
00:18:18,023 --> 00:18:21,703
With all the necessary details
and the contextual augmentation

286
00:18:22,703 --> 00:18:25,793
going, moving to the next
topic about API documentation.

287
00:18:26,103 --> 00:18:29,903
So let's say we have at
the setup build already.

288
00:18:30,433 --> 00:18:34,313
How can we use leverage the
setup to do API documentation?

289
00:18:35,023 --> 00:18:36,443
The steps are more or less similar.

290
00:18:36,663 --> 00:18:39,423
So the first step is data preparation,
which we have already done as

291
00:18:39,453 --> 00:18:42,193
we have fed our common source.

292
00:18:43,868 --> 00:18:45,098
Common repository data.

293
00:18:45,918 --> 00:18:49,268
The second is we have to define a
document, documentation structure.

294
00:18:49,308 --> 00:18:53,028
So let's say if we want to create
a document, then, what are the

295
00:18:53,028 --> 00:18:54,528
sections that document will have?

296
00:18:54,608 --> 00:18:55,708
It will have an overview.

297
00:18:55,838 --> 00:18:58,378
Let's say it will have endpoint details.

298
00:18:58,438 --> 00:19:01,308
It will have your input
or output parameters.

299
00:19:01,378 --> 00:19:02,718
It would have some examples.

300
00:19:03,368 --> 00:19:07,578
and then it will have an error
handling section, which will describe

301
00:19:07,658 --> 00:19:09,938
what exceptions or errors to expect.

302
00:19:10,988 --> 00:19:16,333
The third part of this is, Prompt
engineering for document generation.

303
00:19:17,283 --> 00:19:23,393
similar to what we saw earlier, we will
have to build prompts for LLM in order

304
00:19:23,473 --> 00:19:27,693
to provide the output needed in the
needed for the documentation structure.

305
00:19:28,473 --> 00:19:30,423
And the fourth is response generation.

306
00:19:33,168 --> 00:19:37,548
The key unlock of this
documentation is with automation.

307
00:19:38,358 --> 00:19:40,838
Again, here there are two
scenarios to consider.

308
00:19:40,938 --> 00:19:43,628
One, it could be like a mass generation.

309
00:19:43,638 --> 00:19:49,448
You already have a centralized
repo or an API documentation

310
00:19:49,538 --> 00:19:51,988
for let's say 1000, 1000 APIs.

311
00:19:52,508 --> 00:19:54,818
And you want to do, and
you want to do a bulk.

312
00:19:55,563 --> 00:20:00,253
processing, and develop and generate
documentation for all these thousand APIs.

313
00:20:00,753 --> 00:20:06,193
And the other use case is if you already
have documentation, how can you augment

314
00:20:06,503 --> 00:20:10,573
or keep it up to date as and when
you make changes to an existing API?

315
00:20:12,193 --> 00:20:15,133
Again, there can be several
ways, but I have provided an

316
00:20:15,133 --> 00:20:16,913
example using CSV pipeline.

317
00:20:17,883 --> 00:20:22,893
So first and foremost is again, like if
I were to do a check in on one of the API

318
00:20:22,953 --> 00:20:28,363
contracts, I would query, that repository
for all these, endpoints and gather

319
00:20:28,363 --> 00:20:30,223
all the information, in that contract.

320
00:20:30,823 --> 00:20:32,403
Secondly, I will create a prompt.

321
00:20:33,953 --> 00:20:38,353
this is a programmatic creation of
prompt and it's, it would be, pretty

322
00:20:38,643 --> 00:20:43,648
easy because the construct for the
prompt is pretty much locked in.

323
00:20:43,998 --> 00:20:47,628
All I have to change, each
time are the endpoint details.

324
00:20:48,708 --> 00:20:52,778
The third important step in the
pipeline would be compiling a response.

325
00:20:53,128 --> 00:20:57,358
So the output that you get from
LLM, it may not be formatted,

326
00:20:57,558 --> 00:21:00,688
like most of the developer
portals that the enterprises have.

327
00:21:01,033 --> 00:21:04,693
Would store the documentation
in, in, let's say, Markdown

328
00:21:04,763 --> 00:21:07,063
or HTML or PDF formats.

329
00:21:07,143 --> 00:21:11,703
So you need that output to be styled,
let's say if someone is using Markdown.

330
00:21:12,323 --> 00:21:17,653
So the third step talks about that,
that the needed styling to be applied

331
00:21:17,783 --> 00:21:21,143
on the data or on the response
that is generated from the LLM.

332
00:21:21,983 --> 00:21:26,843
And the fourth step is to,
actually integrate This generated

333
00:21:26,843 --> 00:21:28,533
document into a developer portal.

334
00:21:29,523 --> 00:21:34,523
So if the developer portal is a Git
repo, the fourth step would, would

335
00:21:34,523 --> 00:21:36,633
raise, an MR to the developer portal.

336
00:21:37,643 --> 00:21:40,723
also, this is the step where
developers can review the

337
00:21:40,723 --> 00:21:44,433
documentation request because the
ones that is generated may not be.

338
00:21:44,733 --> 00:21:48,393
Perfect or may not have all the detailed
information and that would be the

339
00:21:48,393 --> 00:21:51,823
stage where developers can provide
even more context or they can provide

340
00:21:52,143 --> 00:21:55,863
some additional information like links
to existing documentation or links to

341
00:21:55,863 --> 00:21:59,143
existing sites, so on and so forth.

342
00:22:01,553 --> 00:22:07,483
This approach will always ensure that your
documentation stays true to the code or

343
00:22:07,493 --> 00:22:10,543
to your API contract that you have defined

344
00:22:13,113 --> 00:22:13,833
moving ahead.

345
00:22:15,443 --> 00:22:18,033
Strategies to address and
control existing API sprawl.

346
00:22:18,063 --> 00:22:20,703
So above strategies are.

347
00:22:21,098 --> 00:22:24,248
Are good if you're starting new
right when you're starting new You

348
00:22:24,248 --> 00:22:29,258
have all the majors that, that we
talked about, you are controlling the

349
00:22:29,328 --> 00:22:31,158
API sprawl right at the beginning.

350
00:22:32,138 --> 00:22:36,418
But how about the enterprises
or companies that already have

351
00:22:36,458 --> 00:22:37,918
tens and thousands of APIs?

352
00:22:39,738 --> 00:22:43,278
so I think the steps to solve
the first step is still the same.

353
00:22:43,298 --> 00:22:48,958
Like you, you have to centralize your API
definition and establish common standards.

354
00:22:49,583 --> 00:22:51,683
And it can be a gradual move.

355
00:22:51,703 --> 00:22:53,473
It need not be a big bang.

356
00:22:54,103 --> 00:22:58,173
Small organizations, small teams can
start moving towards the centralized repo.

357
00:22:58,863 --> 00:23:05,793
And the more teams adopt this, the
more standardization and better

358
00:23:05,793 --> 00:23:08,073
visibility the enterprises gain.

359
00:23:09,543 --> 00:23:14,833
The second use case is, or the
second important step is identify

360
00:23:14,883 --> 00:23:16,053
unused or redundant APIs.

361
00:23:17,403 --> 00:23:22,723
So this is where that step, which
we discussed earlier, of feeding

362
00:23:22,733 --> 00:23:28,803
the live deployment data into the,
into the LLM would come in handy.

363
00:23:29,483 --> 00:23:31,703
So we can utilize the AI driven insights.

364
00:23:32,388 --> 00:23:37,408
to analyze API usage patterns and
then identify APIs that are outdated,

365
00:23:38,008 --> 00:23:41,628
underutilized, or have multiple
versions serving the same function.

366
00:23:42,708 --> 00:23:46,818
This will help streamline the API
portfolio by either deprecating

367
00:23:46,858 --> 00:23:48,698
or phasing out unnecessary APIs.

368
00:23:49,338 --> 00:23:56,868
So what I mean by that is In production,
if we find that certain APIs are not

369
00:23:56,898 --> 00:24:02,208
used at all, or certain versions of
existing APIs are not used at all, then

370
00:24:02,608 --> 00:24:08,958
the easiest way to reduce API sprawl is
to decommission unused or duplicate APIs.

371
00:24:10,418 --> 00:24:14,063
The third is Cluster similar
APIs for consolidation.

372
00:24:14,553 --> 00:24:19,223
Again, earlier we saw an example
wherein, as in when I tried to check

373
00:24:19,223 --> 00:24:22,363
in an API, it did a similarity check.

374
00:24:22,913 --> 00:24:28,523
That was for one API, but we can extend
that same logic to the whole set of APIs

375
00:24:28,823 --> 00:24:30,523
and then form cohorts of similar API.

376
00:24:31,343 --> 00:24:36,333
Once these cohorts are formed, these
can be then manually reviewed and

377
00:24:36,403 --> 00:24:40,998
we can generate one main API out of
them and decommissioned rest others.

378
00:24:42,428 --> 00:24:48,058
this will effectively help to
control or curb, the duplication

379
00:24:48,638 --> 00:24:50,028
among the existing APIs.

380
00:24:51,228 --> 00:24:54,888
Fourth is implement custom
tagging for better management.

381
00:24:55,678 --> 00:24:59,615
so many a times, in the existing
production, scenario, we teams

382
00:24:59,615 --> 00:25:04,772
or enterprises need to target
one particular set of APIs.

383
00:25:04,822 --> 00:25:05,752
What do I mean by that?

384
00:25:05,832 --> 00:25:11,631
many enterprises can have public
facing APIs and internal APIs.

385
00:25:11,632 --> 00:25:15,862
Let's say, there is a new security
protocol that needs to be applied.

386
00:25:16,897 --> 00:25:21,377
Since the risk is greater for the
public facing APIs, I want to, I want

387
00:25:21,387 --> 00:25:23,247
to apply this to the public facing APIs.

388
00:25:23,607 --> 00:25:27,227
That's where I will use the API
discovery tool, write a natural

389
00:25:27,247 --> 00:25:32,947
language query, that hey, give me a
list of all the public facing APIs.

390
00:25:33,017 --> 00:25:37,947
And if we already have tags for the
segmentation, then you should receive

391
00:25:38,007 --> 00:25:39,777
this list in a matter of seconds.

392
00:25:40,937 --> 00:25:42,877
The other use case is
deployment platforms.

393
00:25:42,947 --> 00:25:46,367
Let's say we have multiple deployment
platforms and we want to retire one.

394
00:25:46,857 --> 00:25:48,917
Migrate all services
from one to the other.

395
00:25:49,497 --> 00:25:53,997
Then again, the API discovery
tool can be used to get this list.

396
00:25:55,327 --> 00:25:57,137
The use cases are endless.

397
00:25:57,617 --> 00:25:58,517
but the idea here is.

398
00:25:59,472 --> 00:26:05,142
Once you have an API construct, once
you have an API discovery tool based on

399
00:26:05,142 --> 00:26:13,452
the AI construct, you can then slice and
dice the API sets to suit your particular

400
00:26:13,482 --> 00:26:15,482
need, to suit your particular needs.

401
00:26:16,482 --> 00:26:24,422
So effectively to conclude or the TLDR
version of this talk is generative AI

402
00:26:24,822 --> 00:26:29,732
can help transform the API landscape
by enabling intelligent API discovery.

403
00:26:30,082 --> 00:26:33,522
Similarity detection and
automated documentation.

404
00:26:34,652 --> 00:26:40,212
Secondly, developers can prevent the
creation of duplicate APIs, effortlessly

405
00:26:40,242 --> 00:26:44,712
find existing solution and receive
real time feedback or suggestion

406
00:26:45,112 --> 00:26:48,572
that can enhance the productivity
and accelerate time to market.

407
00:26:50,022 --> 00:26:54,212
Third, for production or live
systems, integrating AI with API

408
00:26:54,242 --> 00:26:58,682
gateway facilitated continuous
monitoring of API usage patterns.

409
00:26:59,737 --> 00:27:03,417
providing data to optimize,
consolidate, or retire API

410
00:27:03,767 --> 00:27:05,127
based on actionable insights.

411
00:27:05,687 --> 00:27:10,707
Not only this, but if we can go
one step beyond, based on API usage

412
00:27:10,707 --> 00:27:15,957
patterns, we can even drive resource
optimization, further lowering the

413
00:27:15,967 --> 00:27:18,267
operational and infrastructure costs.

414
00:27:20,157 --> 00:27:26,252
So to, so in essence, right use
of AI becomes the catalyst That

415
00:27:26,312 --> 00:27:31,822
empowers enterprises to navigate
API sprawl, turning complexity

416
00:27:31,912 --> 00:27:34,912
into a managed strategic asset.

417
00:27:35,902 --> 00:27:36,382
That's it.

418
00:27:37,902 --> 00:27:41,352
Thank you Con42 for providing
the platform to present my ideas.

419
00:27:42,122 --> 00:27:45,372
And thank you for the viewers
who have made it thus far.

420
00:27:46,312 --> 00:27:53,452
I sincerely hope you found a couple of
ideas that might resonate with you and

421
00:27:53,452 --> 00:27:55,502
which you can apply in your everyday work.

422
00:27:56,682 --> 00:27:58,232
Enjoy the rest of the conference.

