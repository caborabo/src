1
00:00:14,020 --> 00:00:18,949
Hi, my name is Omer Farooq, and today I'll
talk about how to secure applications that

2
00:00:18,950 --> 00:00:25,840
are built around Gen AI or integrate Gen
AI capability into existing applications.

3
00:00:26,440 --> 00:00:29,179
But before we get started, I'd
like to talk about a little bit

4
00:00:29,180 --> 00:00:31,110
about myself and introduce myself.

5
00:00:32,205 --> 00:00:37,845
I am a founder and security engineer
at Oxford Security based in Maryland.

6
00:00:38,415 --> 00:00:43,425
We've been around since 2018 and we've
been able, we have been able to help over

7
00:00:43,425 --> 00:00:48,435
50 different customers build out their
digital transformation general journey

8
00:00:48,705 --> 00:00:51,405
from on-prem to hybrid to multi-cloud.

9
00:00:52,709 --> 00:00:57,340
We've been able to help them integrate
DevSecOps, including DAST, SAST,

10
00:00:57,940 --> 00:01:02,410
and interactive testing and security
validation of the networks, and then

11
00:01:02,599 --> 00:01:08,439
their phishing attacks as part of
their large scale security initiatives.

12
00:01:10,309 --> 00:01:15,420
We are experts and we take a lot of
pride in presenting data as technically

13
00:01:16,539 --> 00:01:19,899
close to as, with engineering
rigor and engineering practices.

14
00:01:20,780 --> 00:01:25,540
We make sure that our data is
transparently available to not

15
00:01:25,540 --> 00:01:29,390
only our customers, but when
possible to our larger audience.

16
00:01:29,630 --> 00:01:33,380
That's why we take a lot of our time
and we make sure that we present

17
00:01:33,410 --> 00:01:34,850
and give back to the community.

18
00:01:35,800 --> 00:01:41,960
So today, in that spirit, we'll talk
about how to secure Gen AI applications

19
00:01:42,270 --> 00:01:46,130
or applications that exist and
are integrating Gen AI capability.

20
00:01:46,725 --> 00:01:53,025
And what that mean and how to do that
rapidly and do it in a way that scales up.

21
00:01:53,655 --> 00:01:56,095
So we will start off by a little
bit of short introductions

22
00:01:56,095 --> 00:01:58,045
about LLM applications.

23
00:01:58,355 --> 00:02:02,055
We'll go into how LLM applications
are built and what are the challenges.

24
00:02:03,685 --> 00:02:07,165
This will include application
frameworks and include an

25
00:02:07,175 --> 00:02:09,075
application development methodology.

26
00:02:10,245 --> 00:02:14,515
And then we'll dive into more into how
to automate LLM application deployment.

27
00:02:14,960 --> 00:02:19,700
either as part of existing tooling's
tool set or to build new tool sets.

28
00:02:21,250 --> 00:02:24,450
And finally, we'll cover LLM
security, how to make sure that you

29
00:02:24,450 --> 00:02:29,990
be strategically built LLM application
secure, and how do we tactically do

30
00:02:30,030 --> 00:02:33,920
things to make sure that they are
actually built securely by engineers.

31
00:02:35,490 --> 00:02:39,220
One of the key underlying themes we would
like to make sure that we follow along

32
00:02:39,230 --> 00:02:46,235
with this Is that we understand that
LLAMP applications are no different from

33
00:02:46,235 --> 00:02:52,045
existing applications, but the trick is
to make sure that we do not increase the

34
00:02:52,105 --> 00:02:59,755
OpEx or the operating costs to expand the
application sets or the functionality.

35
00:03:00,460 --> 00:03:04,890
We should be able to integrate these
new function exciting technology into

36
00:03:04,890 --> 00:03:10,830
our existing applications without having
to redesign or reengineer our delivery,

37
00:03:11,330 --> 00:03:13,420
our integration and our security.

38
00:03:14,500 --> 00:03:17,950
So let's get started and go
and talk about LM applications.

39
00:03:19,910 --> 00:03:23,130
So the talk title is LM SecOps.

40
00:03:24,010 --> 00:03:27,860
And I like to type to put that
together and used and put that

41
00:03:27,860 --> 00:03:31,310
together to a, to the Venn diagram
that we have on the screen.

42
00:03:32,160 --> 00:03:35,860
LLM applications usually
are on the form of chatbots.

43
00:03:36,560 --> 00:03:39,080
That do summarization to translation.

44
00:03:39,760 --> 00:03:44,000
Basically, there are things that are
either used by machines or users.

45
00:03:44,480 --> 00:03:48,580
that are based on the principle
of a question or query.

46
00:03:49,210 --> 00:03:50,210
And there is a response.

47
00:03:51,020 --> 00:03:52,440
But there's much more than that.

48
00:03:53,190 --> 00:03:56,450
LLM applications usually have
built in semantic search.

49
00:03:56,500 --> 00:04:00,530
They have built in complicated
data rights management platforms.

50
00:04:00,900 --> 00:04:03,190
They have a lot of data life cycles.

51
00:04:04,900 --> 00:04:08,200
So we'll talk about that
as part of our LLM Ops.

52
00:04:08,630 --> 00:04:12,070
So when we are integrating and we are
building out applications with LLM

53
00:04:12,250 --> 00:04:18,430
capability, we need to understand how to
make sure the data life cycle policies The

54
00:04:18,430 --> 00:04:24,945
data ownership and classification policies
are extended for the current existing

55
00:04:24,945 --> 00:04:27,045
applications into LLM applications.

56
00:04:28,485 --> 00:04:30,265
And this overlaps with the security.

57
00:04:30,955 --> 00:04:37,645
As it, as the organizations shift left,
we need to make sure the LLM security

58
00:04:37,990 --> 00:04:43,000
Shift left at the same time, and thus,
we'll talk about how to secure the

59
00:04:43,000 --> 00:04:47,300
models, how to secure data, how to
ensure data privacy, how to build the

60
00:04:47,300 --> 00:04:52,230
data rights management, how to ensure
the data is only in granularly available

61
00:04:52,230 --> 00:04:57,760
to the right set of people when they're
asking a question on a large data sets.

62
00:04:59,470 --> 00:05:03,890
So once again, LLM SecOps is a
combination of LLM applications,

63
00:05:04,160 --> 00:05:08,260
LLM CI CD automation, LLM security.

64
00:05:08,860 --> 00:05:11,480
I'd like to, I'd like to start
out with a high level picture.

65
00:05:11,500 --> 00:05:15,420
This is a one view of the world,
but it's not, and it should not be

66
00:05:15,420 --> 00:05:17,560
considered all reference models.

67
00:05:17,660 --> 00:05:20,800
But one common reference
model for LM applications is

68
00:05:20,800 --> 00:05:22,510
the model around how to build

69
00:05:24,860 --> 00:05:29,760
a chat bot or a machine to machine
interface that essentially has a

70
00:05:29,760 --> 00:05:31,950
query and then gets back a response.

71
00:05:33,275 --> 00:05:38,335
And essentially, usually a query
is translated to our embedding.

72
00:05:38,955 --> 00:05:43,455
Those embeddings are looked up in a
vector database to find the closest

73
00:05:43,455 --> 00:05:45,835
data set that is related to that.

74
00:05:46,375 --> 00:05:52,605
Sometimes this is also used, AI is
used to build the right level of query

75
00:05:52,975 --> 00:05:55,455
by something called semantic index.

76
00:05:57,205 --> 00:06:00,655
the data itself is injected
into initial data prompt.

77
00:06:01,390 --> 00:06:07,670
and the data prompt is optimized by
either building a bigger context or

78
00:06:07,670 --> 00:06:13,915
using L-M-A-P-I to ensure the prompt
itself is functional and optimized,

79
00:06:14,455 --> 00:06:18,535
and then the you, the prompt is
routed through the LM ap, LME, through

80
00:06:18,535 --> 00:06:25,255
the l to the LM through the API or
sometimes you have LM caching the data.

81
00:06:25,675 --> 00:06:27,475
The response is content filtered.

82
00:06:28,240 --> 00:06:32,180
And the content filter response is
provided back to the user of the machine.

83
00:06:32,980 --> 00:06:37,950
This high level architecture presents,
is not super difficult to understand.

84
00:06:37,960 --> 00:06:40,980
But one of the things that I like
to point out at this point is that

85
00:06:41,680 --> 00:06:46,070
it is important to maybe possibly
build a reference application or

86
00:06:46,080 --> 00:06:49,560
build LM applications in two ways.

87
00:06:49,620 --> 00:06:53,360
One, keep the front end, the
UI or the machine interface

88
00:06:53,370 --> 00:06:54,670
separate from the back end.

89
00:06:56,870 --> 00:06:59,720
And this can help organizations
in multiple different fashions.

90
00:06:59,730 --> 00:07:04,920
From OPEX point of view, building a
single large multi tenant LLM backend

91
00:07:04,930 --> 00:07:13,240
can help you build out one, one, one,
one backend that has a detailed rights

92
00:07:13,240 --> 00:07:19,430
management for data and assets that
can be front to multiple different

93
00:07:19,430 --> 00:07:24,985
application types, either bots, Other
translation services, data summarization

94
00:07:24,985 --> 00:07:30,865
services, middleware that use the
LAI, or even other AI services.

95
00:07:31,905 --> 00:07:36,375
This segregate, this decoupling of
the frontend to the backend will

96
00:07:36,375 --> 00:07:40,035
help also with security and the
development and deployment of it.

97
00:07:40,515 --> 00:07:45,275
We'll show that how decoupling of this
application at the backend and the

98
00:07:45,275 --> 00:07:50,505
frontend is critical in actually making
sure security scales up at the same time.

99
00:07:51,090 --> 00:07:55,980
Not all frontend applications are going
to be equally complex, but they will

100
00:07:55,980 --> 00:07:57,640
have their own set of requirements.

101
00:07:58,400 --> 00:08:05,180
It makes sense to secure the frontend
applications using current existing non

102
00:08:05,180 --> 00:08:12,710
LM security models, and then ensure that
all the new future LM security provisions

103
00:08:13,040 --> 00:08:17,520
only have to be done once in the large
one singular backend that could be

104
00:08:17,530 --> 00:08:19,340
frontend for multiple different frontends.

105
00:08:20,005 --> 00:08:25,545
what are some of the high level
challenges that are usually part

106
00:08:25,545 --> 00:08:28,125
of LM applications development?

107
00:08:28,795 --> 00:08:31,135
One, there's a lot of integrations.

108
00:08:31,885 --> 00:08:35,015
LM applications are data routing
applications where a certain

109
00:08:35,015 --> 00:08:39,195
amount of data is pulled, a certain
amount of data is acquired from

110
00:08:39,235 --> 00:08:40,385
sometimes real applications.

111
00:08:41,255 --> 00:08:42,685
Near real time systems.

112
00:08:42,805 --> 00:08:44,735
It could be coming from OT equipment.

113
00:08:44,745 --> 00:08:46,225
It could come from IOT.

114
00:08:46,735 --> 00:08:48,385
It could come from Bing search.

115
00:08:48,475 --> 00:08:53,235
It could come from search engines
from the, from specific catalogs.

116
00:08:53,765 --> 00:08:57,105
It, the data could cut, the data
could be egressed and ingress

117
00:08:57,115 --> 00:08:58,885
from all types of data sources.

118
00:08:59,185 --> 00:09:02,475
That's what makes LM
applications very powerful.

119
00:09:03,125 --> 00:09:09,745
Being able to build and, Chains of events
or change of data interactions that makes

120
00:09:09,795 --> 00:09:13,355
the query and the response very powerful.

121
00:09:14,705 --> 00:09:19,655
Data integrations represents
few critical security issues.

122
00:09:20,445 --> 00:09:24,425
All integrations require you to have
secure network, all require you to

123
00:09:24,475 --> 00:09:26,695
ensure that you have delegated access.

124
00:09:27,450 --> 00:09:31,570
And furthermore, it ensures that
you, the data that's being accessed

125
00:09:31,600 --> 00:09:36,810
on the base, based on the initial
request user has entitlement to that

126
00:09:36,810 --> 00:09:40,830
data that's going to be used for his
semantic search is going to be used

127
00:09:40,860 --> 00:09:43,870
for his AI search or his AI query.

128
00:09:45,310 --> 00:09:49,160
Furthermore, as the data is being
embedded and pulled into the embedding

129
00:09:50,090 --> 00:09:54,100
databases, the LM applications,
that data needs to be equally.

130
00:09:55,070 --> 00:09:59,010
Part of your data lifecycle
process, but the retention policy,

131
00:09:59,040 --> 00:10:03,980
the classification policies and
data catalog and tags and labels.

132
00:10:06,650 --> 00:10:10,200
This becomes a difficult challenge
when you're dealing with a lot of data.

133
00:10:11,290 --> 00:10:14,380
But at scale for different
front end systems.

134
00:10:15,250 --> 00:10:18,530
We'll talk about how to use
specific tools and technologies

135
00:10:18,530 --> 00:10:20,700
and concepts to build that out.

136
00:10:21,480 --> 00:10:23,330
That's part of our LM Ops.

137
00:10:24,735 --> 00:10:29,055
And finally, if not equally
important is the AI services.

138
00:10:30,545 --> 00:10:36,285
It is possible an AI application or
AI back end that is LM application

139
00:10:36,295 --> 00:10:40,045
back end will interact with
multiple models, if not one or more.

140
00:10:41,275 --> 00:10:45,125
We need to be very careful about how
those model integrations and models

141
00:10:45,155 --> 00:10:48,875
APIs are protected, not just for cost.

142
00:10:49,335 --> 00:10:54,505
But also for availability, ensuring
those meter services are logged, making

143
00:10:54,505 --> 00:10:58,585
sure that those model communications
are protected over the public

144
00:10:58,585 --> 00:11:02,975
Internet with at least with some
level of data encryption at rest.

145
00:11:04,985 --> 00:11:09,645
The data itself, when you're inquiring
from the user to all the way to the

146
00:11:09,645 --> 00:11:15,135
model, can potentially include a lot
of proprietary, if not sensitive data.

147
00:11:16,160 --> 00:11:20,510
including personal information, including
things that could be considered privilege.

148
00:11:21,430 --> 00:11:26,880
Thus, AI API model security is equally
important and needs to be considered

149
00:11:26,880 --> 00:11:29,140
as part of the LM application security.

150
00:11:30,320 --> 00:11:34,260
We'll cover this and we'll talk
about how that can be secured as part

151
00:11:34,260 --> 00:11:39,330
of our LM SecOps, which is in the
latter part of this conversation.

152
00:11:40,015 --> 00:11:44,545
So before we get into a little
bit of how and what LM application

153
00:11:44,555 --> 00:11:49,195
look like, I like to start out with
saying, let's put a developer hat on.

154
00:11:49,225 --> 00:11:53,385
And how do you build an LM application or
how do you actually add an LM application

155
00:11:53,385 --> 00:11:55,875
capability into existing application?

156
00:11:56,115 --> 00:11:57,685
you have two major components.

157
00:11:58,205 --> 00:12:02,245
You have the actual logic and of data
integration, and then you have the model.

158
00:12:02,245 --> 00:12:06,315
And Understanding the use of the
right model is very important.

159
00:12:06,975 --> 00:12:10,945
That also is equally important because
not all models offer the same type

160
00:12:10,945 --> 00:12:15,875
of security and responsible ethical
AI capability as other models.

161
00:12:16,415 --> 00:12:20,695
Pulling an open source model from public
internet versus a model that's managed

162
00:12:20,695 --> 00:12:24,995
and curated has different set of side
effects and different consequences.

163
00:12:25,275 --> 00:12:27,125
Be careful when you
choose the right model.

164
00:12:28,185 --> 00:12:30,095
Also, like as we mentioned prior.

165
00:12:30,850 --> 00:12:35,160
Keep the front end of the application,
the bot, the user, or the machine to

166
00:12:35,160 --> 00:12:39,170
machine interfaces separate and try
not to consider those as part of the

167
00:12:39,170 --> 00:12:44,160
LM applications as much because those
could be factored out and developed

168
00:12:44,170 --> 00:12:50,440
using existing application interfaces
or with low code, no code solutions and

169
00:12:50,460 --> 00:12:57,190
keep a focus on building one time an LM
backend that could be secured with data

170
00:12:57,190 --> 00:12:59,600
rights management and other features.

171
00:13:00,395 --> 00:13:04,075
And does not have to be redeveloped.

172
00:13:05,155 --> 00:13:08,545
Another feature of LLM applications,
this is very important to cover because

173
00:13:08,545 --> 00:13:13,305
this dictates how security and how
development works is the fact that LLM

174
00:13:13,305 --> 00:13:15,305
applications are cloud native artifact.

175
00:13:15,495 --> 00:13:16,815
They were built in the cloud.

176
00:13:16,815 --> 00:13:19,535
They've come from most of the
development is in the cloud.

177
00:13:20,165 --> 00:13:24,105
And that means is that a lot of
the components, part of the rapid

178
00:13:24,105 --> 00:13:29,925
prototype and fast results methodology
for LM application required to use

179
00:13:29,925 --> 00:13:33,805
a lot of open source data router
frameworks like slang chain, a

180
00:13:33,805 --> 00:13:36,845
semantics kernel, and llama index.

181
00:13:37,565 --> 00:13:39,835
You will have a lot of
components of all resource.

182
00:13:39,965 --> 00:13:43,135
If not, the entire model
could be open source.

183
00:13:43,935 --> 00:13:46,665
You will be using a lot of low
code, no code for your front end.

184
00:13:47,350 --> 00:13:50,570
You will be using, leveraging a
lot of cloud services, including

185
00:13:50,600 --> 00:13:57,030
serverless and managed services for
AI and AI related compute and storage.

186
00:13:57,730 --> 00:14:01,840
And most of the times, if not all,
you will be utilizing some kind of a

187
00:14:01,840 --> 00:14:05,890
vector database that will be sitting
in the cloud or could be on prem

188
00:14:06,400 --> 00:14:10,650
that needs to be protected equally,
just like the way we traditionally

189
00:14:10,780 --> 00:14:12,450
protect traditional databases.

190
00:14:13,000 --> 00:14:16,390
Though the purpose of the vector
database is an It's a bit different,

191
00:14:16,430 --> 00:14:19,580
and the capability is different,
and the data context is different.

192
00:14:20,030 --> 00:14:24,820
We still need to protect the vector
database equally as the data, how

193
00:14:24,820 --> 00:14:29,620
we protect traditional relational
and non relational databases.

194
00:14:32,280 --> 00:14:36,180
before we get further, I'd like to point
out three large principles for data

195
00:14:36,190 --> 00:14:39,670
application security for LM applications.

196
00:14:39,720 --> 00:14:41,560
One, decouple the architecture.

197
00:14:41,820 --> 00:14:44,450
Applications should be cloud
native whenever possible.

198
00:14:44,845 --> 00:14:50,395
Applications should be managed services
as much as possible and vector databases

199
00:14:50,445 --> 00:14:53,375
should be, should, per storage and
performance should be considered.

200
00:14:54,025 --> 00:14:59,085
Now, these principles are not entirely for
LLM application development for security

201
00:14:59,085 --> 00:15:03,415
development, but they're part of a general
principles that you should consider.

202
00:15:04,265 --> 00:15:07,755
Decoupled architecture will provide
you better return on your investment.

203
00:15:08,865 --> 00:15:12,805
Applications in cloud native will allow
you to do rapid prototype, which is part

204
00:15:12,805 --> 00:15:16,355
of what all applications are built like.

205
00:15:17,045 --> 00:15:22,035
And vector databases that have higher
performance of storage functionality

206
00:15:22,035 --> 00:15:26,865
will allow you to make sure that you have
the data rights management at scale, so

207
00:15:26,905 --> 00:15:30,755
you can build a decoupled architecture
that we want to build for security.

208
00:15:31,310 --> 00:15:34,420
So the next concept that we want
to talk about is now that we know

209
00:15:34,420 --> 00:15:38,140
what the, what LM applications look
like, and we understand the design

210
00:15:38,160 --> 00:15:43,040
principles and how the methodology for
LM application is, which means fast

211
00:15:43,050 --> 00:15:45,660
prototypes, build fast, fail fast.

212
00:15:45,990 --> 00:15:46,920
How do we do that?

213
00:15:47,420 --> 00:15:51,640
And the one and that is basically, and
how do we do that with existing tools?

214
00:15:51,640 --> 00:15:52,620
And how do we do that?

215
00:15:52,960 --> 00:15:57,150
with, without learning new tricks
and how do we make sure that we do it

216
00:15:57,260 --> 00:16:00,950
quickly in a way that we don't slow
down the prototypes and the development.

217
00:16:01,800 --> 00:16:06,220
going from Jupyter Notebook, for
example, to an application can be done

218
00:16:06,830 --> 00:16:10,210
quite literally over, over a few hours.

219
00:16:10,250 --> 00:16:14,000
But when you do it at a scale and do it
from an organizational point of view, how

220
00:16:14,000 --> 00:16:15,770
do you bring all those teams together?

221
00:16:16,220 --> 00:16:18,160
you can use your existing pipelines.

222
00:16:18,730 --> 00:16:24,350
The good news is if the additional of a
few stages and steps in your cloud native

223
00:16:24,350 --> 00:16:29,860
or your on prem automation, you should
be able to support LLM application.

224
00:16:30,440 --> 00:16:31,370
How do we do that?

225
00:16:31,950 --> 00:16:33,430
let's start with
automation and deployment.

226
00:16:34,090 --> 00:16:37,940
The backend applications that are part
of the LM applications are no different.

227
00:16:37,950 --> 00:16:42,130
There are still cloud native, but
yet, but the minute differences are

228
00:16:42,130 --> 00:16:46,640
about the configurations of the model,
how to integrate the keys and how to

229
00:16:46,640 --> 00:16:49,260
integrate this, the service accounts.

230
00:16:50,010 --> 00:16:52,810
All of those should be
deployed and automated in the

231
00:16:52,820 --> 00:16:54,310
similar fashions we do today.

232
00:16:55,555 --> 00:17:00,575
Integrations of secrets, integrations
of tokens should be done in a similar

233
00:17:00,575 --> 00:17:05,885
fashion as we do today, except with
the caveat that, remember, a lot of the

234
00:17:05,935 --> 00:17:09,175
applications for LM are meter services.

235
00:17:09,435 --> 00:17:14,565
In that scenario, we do suggest to pick
up the speed and use delegated access.

236
00:17:14,585 --> 00:17:20,125
Whenever possible, make sure that you
build LM apps within your application.

237
00:17:20,585 --> 00:17:26,035
To utilize the end users credentials
and users, identity for meter service

238
00:17:26,535 --> 00:17:31,545
past that responsibility out of
the model and don't manage it at a

239
00:17:31,545 --> 00:17:35,385
large level manager, the user, the
organizational department level.

240
00:17:36,735 --> 00:17:41,255
Build new indexes and new detections
and new threat detections for you

241
00:17:41,265 --> 00:17:43,705
in your SecOp tooling for LLM ops.

242
00:17:43,815 --> 00:17:47,365
There, there won't be that different
for your existing, but they will require

243
00:17:47,365 --> 00:17:54,475
to have separate LLM For example, for
prompt injection and data and prompt

244
00:17:54,755 --> 00:17:58,815
poisoning, you will need to build
that in your, in the threat detection.

245
00:17:58,885 --> 00:18:01,005
For example, if you see a user.

246
00:18:01,820 --> 00:18:05,400
Continuously trying to do prop injection.

247
00:18:05,640 --> 00:18:13,490
you need to build out new threat detection
scenarios in your same tool or your threat

248
00:18:13,520 --> 00:18:18,330
intelligence, threat planning tool, keep
it, you need to pull in the fees for the

249
00:18:18,330 --> 00:18:23,290
threat intelligence to make sure that
they include the LLM application types.

250
00:18:24,230 --> 00:18:27,180
And if not the last and the most
informed data management, you do

251
00:18:27,180 --> 00:18:28,900
need to enable encryption at rest.

252
00:18:29,290 --> 00:18:35,260
Encryption in transit, encryption in use,
we need to make sure the embeddings, the

253
00:18:35,260 --> 00:18:39,090
question and the answer, the responses
that are logged into the applicant,

254
00:18:39,300 --> 00:18:43,170
that are logging the data, the questions
that are asked by the users of services

255
00:18:43,460 --> 00:18:45,570
are considered privileged information.

256
00:18:46,640 --> 00:18:50,230
One of the key things we need to make
sure the part of the LMOPS is that you

257
00:18:50,250 --> 00:18:55,580
need to make sure there are column, if
not in column encryption to protect.

258
00:18:56,085 --> 00:18:59,745
The response and the requests
that the application will see.

259
00:19:00,545 --> 00:19:04,155
This can be considered equally
privileged as two people's conversation.

260
00:19:04,235 --> 00:19:09,835
Because people and users will ask
and inquire things that are either

261
00:19:09,835 --> 00:19:15,715
very personal or PII, include PII,
or do include company private,

262
00:19:15,745 --> 00:19:21,045
company sensitive, or general
sensitive data in their queries.

263
00:19:21,045 --> 00:19:21,155
Thank you for listening.

264
00:19:21,695 --> 00:19:26,935
the queries have to be protected, have
to be only available to a certain users.

265
00:19:27,285 --> 00:19:31,555
The honor trail and the investigative
and the forensic investigation should

266
00:19:31,585 --> 00:19:33,635
only be available to a certain users.

267
00:19:33,745 --> 00:19:39,165
The data rights management of the
log data is a critical component.

268
00:19:39,385 --> 00:19:42,515
And we suggest that, we suggest
using column encryption.

269
00:19:43,480 --> 00:19:47,540
We use suggesting using double encryption
to make sure those columns with the data

270
00:19:47,820 --> 00:19:50,370
requests and data prompts are protected.

271
00:19:50,940 --> 00:19:55,440
Before we get into LMSecOps, security
ops, we need to cover the Some of the what

272
00:19:55,440 --> 00:19:59,040
left high level privilege and what are
we really protecting and work and trying

273
00:19:59,040 --> 00:20:00,980
to gear towards with an LM application?

274
00:20:00,980 --> 00:20:03,940
And how is that different for
from a traditional application?

275
00:20:04,510 --> 00:20:08,330
Traditional applications are also equally
worried about data leakage, but they

276
00:20:08,350 --> 00:20:10,920
are less worried about prompt rejection.

277
00:20:10,930 --> 00:20:14,540
They're less worried about what model
poisoning to some extent because

278
00:20:14,540 --> 00:20:15,770
sometimes there are no models.

279
00:20:16,800 --> 00:20:20,310
adversarial attacks against LLM
applications are different than from

280
00:20:20,310 --> 00:20:24,540
the nature of the attacks that are
for existing non LLM applications.

281
00:20:24,540 --> 00:20:28,750
For example, adversarial attacks
on LLM application could be to

282
00:20:28,830 --> 00:20:31,820
introduce LLM hallucination.

283
00:20:31,820 --> 00:20:38,640
And finally, the ethical concerns are
our primary part of the LLM SecOps and

284
00:20:38,640 --> 00:20:40,150
how do we make sure we manage those.

285
00:20:41,705 --> 00:20:43,525
So let's start with ethical concerns.

286
00:20:43,595 --> 00:20:47,545
Ethical concerns are something that you,
that we should think about when we're

287
00:20:47,565 --> 00:20:51,535
building out applications, and those
could be tuned using content filtering.

288
00:20:52,185 --> 00:20:56,455
Data leakage can be addressed using
data rights management and double

289
00:20:56,455 --> 00:20:59,305
encryption or column encryption.

290
00:20:59,305 --> 00:21:03,535
Adversarial attacks can be protected
and detected using SecOps and threat

291
00:21:03,535 --> 00:21:05,665
detection and threat intelligence.

292
00:21:06,205 --> 00:21:10,795
Model poisoning and prompt injection
can be protected using the traditional

293
00:21:10,855 --> 00:21:16,905
input validation, metered throttle
responses, content filtering, and

294
00:21:16,905 --> 00:21:22,995
those type of built in cloud resources
for making sure that the model stays

295
00:21:23,005 --> 00:21:25,005
protected and inputs are checked.

296
00:21:26,995 --> 00:21:32,225
Next, but finally, we get to the
point where we can actually do things

297
00:21:32,225 --> 00:21:35,055
today to protect our LLM applications.

298
00:21:35,585 --> 00:21:41,585
I filed them under the Tactical LLM
SecOps and essentially these are the

299
00:21:41,595 --> 00:21:46,865
few steps at high level that we can
take today using existing tooling

300
00:21:46,895 --> 00:21:48,655
to protect our LLM applications.

301
00:21:49,555 --> 00:21:55,365
Number one, use your existing tooling for
DAST and SAST to ensure that all LLM open

302
00:21:55,365 --> 00:22:02,935
source components that include SCA, that
includes using SCA tools or using SAS

303
00:22:02,955 --> 00:22:07,770
tools, Make sure you all the code that
is written for your LM application is

304
00:22:07,880 --> 00:22:13,050
equally goes through your secure coding
practices as your other code, even though

305
00:22:13,050 --> 00:22:16,560
it might be cloud native, even though
it might be more configuration heavy,

306
00:22:16,910 --> 00:22:22,200
even though it might be something that
looks as an, as a serverless function.

307
00:22:22,870 --> 00:22:26,460
All of this code needs to
be protected and reviewed.

308
00:22:27,620 --> 00:22:30,050
we suggest using tools for.

309
00:22:30,610 --> 00:22:35,610
as existing pipeline steps to run
scans of this code, making sure that

310
00:22:35,620 --> 00:22:40,150
these particular code snippets of the
code libraries are able to protect

311
00:22:40,170 --> 00:22:44,940
against prompt injection are able
to protect against model poisoning.

312
00:22:45,340 --> 00:22:48,020
There are certain different
tools out there that are able to

313
00:22:48,030 --> 00:22:53,500
look for these kind of patterns
within the LM based type of code.

314
00:22:54,000 --> 00:22:59,330
So if you already have code that is able
to do build semantic index or is able to

315
00:22:59,430 --> 00:23:07,190
do response or input query caching, you
can build out and scan this particular

316
00:23:07,190 --> 00:23:12,470
piece of code with these scanners to
ensure you are focused and hyper focused

317
00:23:12,470 --> 00:23:13,920
on finding those vulnerabilities.

318
00:23:15,605 --> 00:23:20,085
Ensure that content filtering, ensure
that content filtering is both ways,

319
00:23:20,085 --> 00:23:24,285
that the input and the output, ensure
the model does not, is not, does not,

320
00:23:24,745 --> 00:23:31,035
is not able to accept PII, or is not
able to, or is able to accept PII if

321
00:23:31,035 --> 00:23:33,285
needed, but to a certain limitations.

322
00:23:33,355 --> 00:23:37,155
Make sure the data that gets
ingressed, ingressed into the model

323
00:23:37,465 --> 00:23:40,435
is classified, is labeled, is tagged.

324
00:23:41,020 --> 00:23:44,380
Those are the things that at the
code level you can check today.

325
00:23:44,820 --> 00:23:47,950
You can make sure that the rights
management with your back end

326
00:23:47,950 --> 00:23:53,590
logic is tested and using DAST
and other validation tools.

327
00:23:54,620 --> 00:23:57,920
And last if not least, ensure
all your logs for all your

328
00:23:58,520 --> 00:24:00,230
applications are end to end.

329
00:24:00,610 --> 00:24:06,770
And they're able to capture data capture
from user to machine and back to the user.

330
00:24:07,750 --> 00:24:13,530
In case you need to go back and
trace out LLM interactions to

331
00:24:13,960 --> 00:24:19,790
either find detection of abuse, or
detection of illegal data usage, or

332
00:24:19,790 --> 00:24:24,870
model hallucination, or model prompt
injection, those ones will be critical.

333
00:24:27,130 --> 00:24:31,950
Thank you for joining me today for this
conversation about LLM Security Ops.

334
00:24:32,765 --> 00:24:36,665
I hope you had, I hope you had enjoyed
this conversation and took away the

335
00:24:36,665 --> 00:24:40,535
key points about how to keep the data
front end and the back end separate,

336
00:24:40,615 --> 00:24:44,535
how to build up the automation
and the security from a shift left

337
00:24:44,535 --> 00:24:49,725
perspective from day one in your LLM
applications or adding the capability

338
00:24:49,735 --> 00:24:52,835
of LLM into existing applications.

339
00:24:53,420 --> 00:24:56,230
If you have any further questions
or you would like to have a dialogue

340
00:24:56,230 --> 00:25:00,080
or if you'd like to have questions,
feedback, feel free to contact us at www.

341
00:25:00,150 --> 00:25:01,150
auxin.

342
00:25:01,820 --> 00:25:02,450
io.

343
00:25:02,850 --> 00:25:05,230
My email is omarfarouq at auxin.

344
00:25:05,510 --> 00:25:11,580
io and I'm always happy to respond
back to constructive feedback.

345
00:25:12,510 --> 00:25:13,980
Thank you and have a great day.

