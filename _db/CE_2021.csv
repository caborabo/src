Featured,Track,Name1,JobTitle1,Company1,Name2,JobTitle2,Company2,Title,Abstract,LinkedIn1,Twitter1,LinkedIn2,Twitter2,Slides,Picture,YouTube,Keywords
Yes,,Mikolaj Pawlikowski,Engineering Lead,Bloomberg,,,,Chaos Engineering in 2021,"Chaos Engineering has come a long way in the recent years.

Starting with Chaos Monkey, through the distributed effort, it has evolved into a serious, scientific method of making your software better.

In this talk, Mikolaj Pawlikowski, the author of ""Chaos Engineering: Site reliability through controlled disruption"" will talk about the journey so far, and what lies ahead.

Oh, and we'll see whether sharks are really more dangerous than hamburgers.",https://www.linkedin.com/in/mikolajpawlikowski/,@mikopawlikowski,,,,ce_miko.png,https://youtu.be/g01ObyU3aEU,"chaos engineering,2021"
Yes,,Leonid Belkind,CTO,Stackpulse,,,,Chaos Engineering + Generic Mitigations: The Path to Self-Healing Systems,"Chaos Engineering is a powerful tool to test your resilience when the unexpected occurs. But you also need to build out the practice for what comes next - how you rectify issues caused by unexpected conditions. 

In this talk, Leonid Belkind, StackPulse CTO and Co-Founder, discusses how to use Chaos Engineering together with generic mitigations to deliver resilience that can't be accomplished with either alone. He'll share examples of how to fit these practices together, what's needed to get started, and how to use both together to begin building out self-healing systems.",https://www.linkedin.com/in/leonidbelkind/,@LBelkind,,,,ce_leonid.png,https://youtu.be/Zl0q_JJzCUQ,"generic mitigations,self healing systems"
No,security,Swapnil Deshmukh,CTO,Certus CyberSecurity,,,,Shift up: Continous Security and feedback loop in production,"DevOps engineering culture demands deploying code at lightning speeds. And speed equals carelessness. And carelessness may lead to breach. 

This talk is an introduction to shift up paradigm, think of it as an extension of shift left but a culture that only strives in production. Shift up enables an organization identify, and remediate insecure code and address any security gaps within infrastructural stack in seal-healing and iterative manner. To achieve this end state an organization needs to perform defensive dynamic security testing and test configuration, and system failures against A/B units. These exercises helps validate effectiveness of production's layered protection, which is responsible to protect application code and most importantly customer's data. And last but not the least, building capabilities to identify external-facing assets in continuous manner and monitor it through out its existence. Enabling an organization with a feedback loop between *AST tools (SAST, DAST, IAST, MAST) and layered defenses in production. Further arming them with a protective shield against ever-evolving attacks and ultimately gaining IT utopia!",https://www.linkedin.com/in/swapnil2/,@sec2_0,,,,ce_swapnil.png,https://youtu.be/azofjyUDgJw,"shift up,security,feedback loop"
No,security,Aaron Rinehart,CTO,Verica,David Lavezzo,Director of Cyber Chaos Engineering,Capital One,Cyber Chaos Engineering: How to Implement Security without a Blindfold,"The complex ordeal of delivering secure and reliable software will continue to become exponentially more difficult unless we begin approaching the craft differently.

Enter Chaos Engineering, but now also for security. Instead of a focus on resilience against service disruptions, the focus is to identify the truth behind our current state security and determine what “normal” operations actually look like when it's put to the test.

The speed, scale, and complex operations within modern systems make them tremendously difficult for humans to mentally model their behavior. Security Chaos Engineering is an emerging practice that is helping engineers and security professionals realign the actual state of operational security and build confidence that it works the way it was intended to.",https://www.linkedin.com/in/aaronsrinehart/,@aaronrinehart,https://www.linkedin.com/in/david-lavezzo-44977434/,,,ce_aarondavid.png,https://youtu.be/5-nlCxeQUF4,cyber chaos engineering
No,security,Thibault Koechlin,CTO,CrowdSec,,,,Leveraging the crowd power to regain faith in Internet’s zero trust architecture,"""Did you know that, every day across the Internet, each IP address is scanned hundreds of times? Or that more than 2,000 attacks are perpetrated, stealing 1.4 million personal records? That’s right, every single day! Today, there is a way to rebalance the odds and protect our resources through crowdsourced security and reputation. 

In 2020, our ways of living and working turned completely upside down in a matter of days. We all brought our companies home and our homes in our companies’ systems. Staying connected to our colleagues, friends and family became a critical necessity, which opened the door for hackers to cause disruption and we saw a huge increase of attacks all around the world.

Even though worldwide spending on cybersecurity is predicted to reach $1 trillion in 2021 according to Forbes, the game will still be asymmetrical and all companies will keep being hacked regardless of their security budgets. Expensive security doesn’t mean better security. A new approach is needed and this is why we created CrowdSec.

Join us for this talk so we can explore why a collaborative approach to security could contribute to solving the problem and how we could make the Internet safer together.""",https://www.linkedin.com/in/thibault-koechlin-3061a226/,@Crowd_Security,,,,ce_thibault.png,https://youtu.be/CpcJhMyCw3M,"crowd power,internets zero trust architecture"
No,security,Kennedy Torkura,Cyber Security Engineer,Mattermost,,,,Risk-Driven Fault Injection: Security Chaos Engineering for the Fast & Furious,"""The dynamic nature of cloud-native infrastructure requires continuous security mechanisms to effectively detect security threats, especially those with unknown patterns and behavior. This talk proposes Risk-driven Fault Injection (RDFI) techniques to address these
challenges. Essentially, RDFI applies the principles of chaos engineering to cloud security and leverages feedback
loops to execute, monitor, analyze and plan security fault injection campaigns, based on a knowledge-base.

The knowledge-base consists of fault models designed from secure baselines, cloud security best practices, and observations derived during iterative fault injection campaigns. These observations are helpful for
identifying vulnerabilities while verifying the correctness of security attributes (integrity, confidentiality, and
availability). Furthermore, RDFI proactively supports risk analysis and security hardening efforts by sharing
security information with security mechanisms.  We have designed and implemented the RDFI strategies
including various chaos engineering algorithms as a software tool: CloudStrike. 

Several evaluations have
been conducted with CloudStrike against infrastructure deployed on two major public cloud infrastructure:
Amazon Web Services and Google Cloud Platform. The time performance linearly increases, proportional
to increasing attack rates. Also, the analysis of vulnerabilities detected via security fault injection has been
used to harden the security of cloud resources to demonstrate the effectiveness of the security information
provided by CloudStrike. Therefore, we opine that our approaches are suitable for overcoming contemporary
cloud security issues.
""",https://www.linkedin.com/in/aondona/,@run2obtain,,,,ce_kennedy.png,https://youtu.be/B9_WBz4VJO8,fault injection
No,security,Yury Nino,DevOps Engineer,Aval Digital Labs,Jhonnatan Gil Chaves,DevOps Engineer,Aval Digital Labs,Securing the Cloud: Empowering Developers to practice Security Chaos Engineering,"Cloud platforms must face all kinds of security issues that are frequently a matter for security engineers, not for developers. As a result, security is treated as separate from development. Although sponsors have promoted the integration of security practices into all stages of software development, many developers think security is a topic for other engineering fields. Despite having tools such as Snyk and Blackduck, as result developers are missing the benefits they get from their cloud platforms.  

This talk will show the benefits of practicing security chaos engineering [SCE] by empowering developers to leverage the power of security topics directly. [SCE] offers many advantages that include a reduction in remediation costs, disruption to end-users, and improvement of confidence in production systems. In this talk, we are going to show how this practice has helped us to develop a culture based on security between software developers.

Methodology:
* Present the foundation of the software development life cycle.
* Explore the integration of SDLC, resilience, and security using tools such as Snyk and Blackduck.
* Analyze why developers do not include the security topics in their activities.
* Present a novel practice titled Security Chaos Engineering.
* Show how democratizing security between software developers has shown us the benefits from the distributed, immutable and ephemeral, or DIE, model.
* Show some of the experiments that we are trying in ADL for promoting a culture based on security using SCE.""",https://www.linkedin.com/in/yury-ni%C3%B1o-roa-91752113/,@yurynino,https://www.linkedin.com/in/jhonnatan-gil-chaves-57773919/,@jthan24,,ce_yuryjhon.png,https://youtu.be/W_wazXG70-Y,"cloud,security chaos engineering"
No,security,Romansh Yadav,Senior Security Consultant,Aptiva Corp,,,,Attacking/Defending Mobile Apps Training,"The talk aims to teach attendees Android & iOS application security from basic level to advanced.
​
I will cover architecture, file system, security model, application components, OWASP mobile top 10, Mitigation, toolset, frameworks, techniques used to identify, analyse and exploit vulnerabilities.",https://www.linkedin.com/in/romansh-yadav/,@romanshyadav,,,,ce_romansh.png,https://youtu.be/4WdMWhGrIeQ,"mobile apps,attack,defend"
No,deep dive,Tammy Bryant (Butow),Principal SRE,Gremlin,,,,Chaos Engineering: When The Network Breaks,"Chaos Engineering is a disciplined approach to identifying failures before they become outages. By proactively testing how a system responds under stress, you can identify and fix failures before they end up in the news. Chaos Engineering lets you compare what you think will happen to what actually happens in your systems. You literally “break things on purpose” to learn how to build more resilient systems.

Tammy Butow leads a walkthrough of network chaos engineering, covering the tools and practices you need to implement chaos engineering in your organization. Even if you’re already using chaos engineering, you’ll learn to identify new ways to use chaos engineering to improve the resilience of your network and services. You will also discover how other companies are using chaos engineering—and the positive results they have had using chaos to create reliable distributed systems.

This talk will share how you can accelerate your understanding of how your network can break (packet loss, blackhole attacks, latency injection, and packet corruption) and impact your services.",https://www.linkedin.com/in/tammybutow/,@tammyxbryant,,,,ce_tammy.png,https://youtu.be/vslg-W-ox-k,breaking network
No,deep dive,Paweł Skrzypek,CTO,7bulls.com,Alicja Reniewicz,Full Stack Engineer,7bulls.com,Forecasting based proactive optimization of cloud resources,"Novel concept of advanced adaptation of cloud resource using predicted demand for Cloud resources. Presented approach is using advanced forecasting methods combined with machine learning based solvers, which can dynamically adapt to changed workload. The benefits of that approach will be shown.

The presentation, with the practical examples of the novel approach to proactive optimization of cloud resources based on dynamical and anticipated use of resources. The prediction of application workload is provided as input to the advanced, machine learning based solvers which calculate the optimal deployment plan for the application to anticipate the future needs. The latest state of the art methods are used for forecasting, like ES-Hybrid and advanced Monte Carlo Tree Search based solvers are used to find the optimal solution.",https://www.linkedin.com/in/pawel-skrzypek-836344/,,https://www.linkedin.com/in/alicja-reniewicz-834b321bb/,,,ce_pawel_alicja.png,https://youtu.be/LrPYtefkHJ4,"proactive optimization,cloud resources"
No,deep dive,Lisa Karlin Curtis,Tech Lead,GoCardless,,,,How to avoid breaking other people's things,"Unless you know all the assumptions the consumers of your API have made, it’s impossible to reliably avoid breaking their software. Assuming you, like me, can’t read minds, what can we do to try and keep the number of sad developers to a minimum?

Breaking changes are sad. We’ve all been there; someone else changes their API in a way you weren’t expecting, and now you have a live-ops incident you need to fix urgently to get your software working again. Of course, many of us are on the other side too: we build APIs that other people’s software relies on.
There are many strategies to mitigate breaking changes: think SemVer or API versioning, but all rely on the API developer identifying which changes are breaking.

That’s the bit we’ll focus on in this talk: how do you
(a) Get really good at identifying what changes *might* break someone’s integration
(b) Help your API consumers to build integrations that are resilient to these kinds of changes
(c) Release potentially breaking changes as safely as possible",https://www.linkedin.com/in/lisa-karlin-curtis-a4563920/,@paprikati_eng,,,,ce_lisa.png,https://youtu.be/PPoQSa_6Dko,breaking things
No,deep dive,Uma Mukkara,CEO,Chaos Native,,,,Increasing Kubernetes Resilience for an SRE,"SREs main task is to keep the operations up and running. An SRE dealing with Kubernetes has many challenges to keep resilience is at the desired level and improving over time. In this talk we will go through techniques to measure and improve resilience of Kubernetes platforms in a Cloud-Native way.

The number of micro services in a Kubernetes environment can grow into hundreds easily. Continuous upgrade of these micro services and the Kubernetes platform itself will need a system to measure resilience of the deployment. SREs need to practice chaos engineering in a cloud native way in such a way that it is easily manageable, reuses as many as chaos experiments and workflows. This talk is intended for those SREs that would like to practice or are already practising chaos engineering in their environments. In this talk, we will introduce chaos hub for SRE and discuss how to construct complex chaos workflows using Litmus and Argo projects. A live demo will take the audience through the construction of a end-to-end chaos workflow involving Kubernetes node failure, a CPU hog, a network slowness in a ecommerce application and how resilience is measured and monitored during this process. ",https://www.linkedin.com/in/uma-mukkara/,@Uma_Mukkara,,,,ce_uma.png,https://youtu.be/yQiISxndExA,"cloud native,at scale"
No,deep dive,Long Zhang,PhD Student in Computer Science,KTH Royal Institute of Technology,,,,Maximizing Error Injection Realism for Chaos Engineering with System Calls,"Some of the perturbation models for chaos engineering are based on a random strategy such as ChaosMonkey. However, realistic perturbations could also come from errors that have naturally happened in production. I would like to share more about how to improve the realism for CE experiments.

During the talk, I would like to share our research work on chaos engineering for system call invocations. I will present a novel fault injection framework for system call invocation errors,  called Phoebe. Phoebe is unique as follows: First, Phoebe enables developers to have full observability of system call invocations. Second, Phoebe generates error models that are realistic in the sense that they mimic errors that naturally happen in production. Third, Phoebe is able to automatically conduct experiments to systematically assess the reliability of applications with respect to system call invocation errors in production. We evaluate the effectiveness and runtime overhead of Phoebe on two real-world applications in a production environment. The results show that Phoebe successfully generates realistic error models and is able to detect important reliability weaknesses with respect to system call invocation errors.

The corresponding research paper could be found here: https://arxiv.org/abs/2006.04444",https://www.linkedin.com/in/gluckzhang/,@gluckzhang,,,,ce_long.png,https://youtu.be/NbAUmoifzdQ,"error injection,system calls"
No,deep dive,Ryan Guest,Software Architect,Salesforce,,,,Normalizing Chaos: A Cloud Architecture for Embracing Failover,"What if instead of designing cloud architectures where failover is an exceptional case, we embraced failover as a normal part of running and system and failed over all the time?
Let's deep dive into an architecture currently in production doing just that and share lessons learned along the away.
This talk will use production examples and real-world experiences to showcase an architecture where failover is now the norm instead of something that happens in exceptional situations.

Early in my career, I envied those who answered calls at 2am to jump in and heroically save mission-critical systems. I saw the late nights as a badge of honor. After participating in my fair share of on-call events, I started to think about if we could optimize for events happening at 2pm instead of 2am. This evolved into thinking what if failover handled as part of the normal running of the system and not only in exceptional situations.

I'll dig into an architecture where we are able to artificially inject chaos as part of the normal running of the system and discuss tradeoffs of where an approach like this  makes sense.",https://www.linkedin.com/in/ryanguest/,@ryanguest,,,,ce_ryang.png,https://youtu.be/gxV9dtM6FgE,"cloud architecture,embracing failover"
No,deep dive,Quintessence Anx,Developer Advocate,PagerDuty,,,,Sensory Friendly Monitoring: Keeping the Noise Down,"As infrastructure increases in complexity and monitoring increases in granularity, engineering teams can be notified about each and every hiccup in each and every server, container, or process. In this talk, I’ll be discussing how we can stay in tune with our systems without tuning out.

The ability to monitor infrastructure has been exploding with new tools on the market and new integrations, so the tools can speak to one another, leading to even more tools, and to a hypothetically very loud monitoring environment with various members of the engineering team finding themselves muting channels, individual alerts, or even alert sources so they can focus long enough to complete other tasks. There has to be a better way - a way to configure comprehensive alerts that send out notifications with the appropriate level of urgency to the appropriate persons at the appropriate time. And in fact there is: during this talk I’ll be walking through different alert patterns and discussing: what we need to know, who needs to know it, as well as how soon and how often do they need to know.",https://www.linkedin.com/in/quintessenceanx/,@QuintessenceAnx,,,,ce_quintessence.png,https://youtu.be/F6czQRpCjiw,"monitoring,sensory friendly"
No,deep dive,Mahesh Venkataraman,Managing Director,Accenture,,,,Taming the spatio-temporal-causal uncertainty in Chaos Engineering and Observability,"There are 2 challenges in observability. Uncertainty in prognosis decisions (false+ and false- in failure predictions) and discovering causal connections in diagnosis. We address this by modeling spatio-temporal uncertainty for prognosis& knowledge representation/ graph database for causal diagnosis

Distributed systems are complex and are prone to failures. With more and more enterprises migrating their on-prem systems to cloud, there is also increased risk of failures. These failures often happen owing to unpredictability of production system workload and usage patterns and the consequent emergent response of distributed systems which cannot be easily envisaged during design and implementation. Principles of observability built on three pillars namely, logging, monitoring and metrics attempt to observe the internal state of the system in order to perform prognosis of failure modes and post event failure diagnosis. Observability & chaos engineering techniques are usually combined by conducting planned and thoughtful experiments (inject chaos) and uncovering weaknesses in the systems by analyzing the runtime data of the system (observability).

There are multiple challenges in proactively discovering failure modes during these experiments. First, the logging and monitoring data and their visualization from observability tools is often too overwhelming and voluminous for them to be ‘actionable’. There is data deluge leading to 'data fatigue'. Secondly, there is significant uncertainty in decisions to classify an observed response behavior as either normal or anomalous. Both false positives and false negatives have impact. During a course of series of experiments this uncertainty manifests itself in both temporal and spatial dimensions. The earlier the decision point in time (before the actual expected failure), the more possibility of false alarms and possible ‘alert fatigue’; The later the decision point (i.e closer to the expected failure in time), the less the usefulness of the decision since it is most likely too late to take action. Moreover, the longer the causal chain (spatial separation of original cause and its later effect) the more the uncertainty. We propose to use spatio-temporal models to address this uncertainty in prognosis

Another challenge in prognosis and diagnosis is determining causality and connections between events. Often, with huge amount of observability data, the causal connections between various connected events are not very clear. We propose to use knowledge representation and graph databases to automate discovery of such causal connections

",https://www.linkedin.com/in/maheshvenkataraman/,@maheshv713,,,,ce_mahesh.png,https://youtu.be/QrdhysXUiJQ,"spatio temporal causal uncertainty,observability"
No,lessons learned,Derris Boomer,Founder,BoomerTechGroup,,,,5-Technology Trends and Opportunities for Start-ups & Fortune 500 Companies,"Talk will review hot jobs and skills to acquire. Lastly talk will cover how start-ups and corporate workers can use these technologies.

Technology such as Artificial Intelligence (AI), Internet of Things(IoT), Machine Learning, Software-as-a-Service(SAAS) and Big Data. Talk will review hot jobs and skills to acquire. Lastly talk will cover how start-ups and corporate workers can use these technologies.",https://www.linkedin.com/in/derrisboomer/,@BoomerTechGroup,,,,ce_derris.png,https://youtu.be/NBA8hDkoBfM,"technology trends,startups,fortune 500 companies"
No,lessons learned,Maik Figura,IT Consultant,codecentric AG,Oliver Kracht,Implementation Lead,Deutsche Bahn,Clear the ring for Chaos Engineering at Vertrieb Deutsche Bahn! One year sensations and attractions!,"Let's talk about some experiences, learnings and failures. Your asking about our learning environment? Well there you go: 300+ microservices, 100+ developers, 100+ Gamedays, countless experiments with various outcomes :)

Have you ever been to the circus?
As software developers, we have much more in common with circus trainers than we think. On the one hand, we have to tame and maintain a steadily growing zoo of technologies and on the other hand, the undesirable audience expects us to show more and more astonishing features within a short period of time. On top of that, we’re also shooting right into the circus ring with our big CI/CD cannon. That can lead to interesting effects, because we’re shooting while a show is running.
In this session you will learn about our top 5 excuses not to do Chaos Engineering and how even the most hostile environment can be used to do Chaos Engineering.
Whatever your role is, product owner or developer, we will share our experience in establishing a chaos engineering centered culture at DB Vertrieb for future ringmasters to withstand turbulent conditions in production and ultimately to satisfy your customer's expectations.",https://www.linkedin.com/in/maik-figura/,@maiksensi,,,,ce_maikoliver.png,https://youtu.be/aTs7Rspyb4I,"clear the ring,deutsche bahn"
No,lessons learned,Ana Margarita Medina,Senior Chaos Engineer,Gremlin,,,,In the kitchen: A sprinkle of fire and chaos,"Learn by fire! Chaos and Learning!

How do you ensure your food tastes good? Or maybe you don't and everyone hates your cooking. How do you learn to avoid burning or cutting yourself while cooking? Is the kitchen on fire or is the smoke alarm just complaining that it needs a new battery? =
What does all of this have to do with the cloud? Chaos and Learning!  She will talk about her favorite ingredient for shipping resilient cloud-native applications.",https://www.linkedin.com/in/anammedina/,@ana_m_medina,,,,ce_ana.png,https://youtu.be/EEDE7YzWjCU,"fire,chaos"
No,lessons learned,Bart Enkelaar,Lead SRE,bol.com,,,,One year of SRE failures,"Last year we pitched SRE to our management team and got the OK to get cracking.
We've achieved a lot, but failed even more.
This talk is a front-row seat to a blameless postmortem on the first year of SRE at bol.com, the largest online retailer in The Netherlands and Belgium.

Getting SRE right is hard.  We tried in 2017, we failed.  We tried in 2019, we failed.
We picked ourselves up, dusted off, took the learnings and tried again in 2020. This time we're here to stay.
bol.com is the largest online retailing platform in the Netherlands and Belgium.  We have about 10 million active daily users and innovate with about 700 software engineers.

This is the story of a scrappy team of Site Reliability Engineers trying to shift a large enterprise (about 3000 employees) to think SRE.  Our successes, but far more interestingly, our failures and learnings. ",https://www.linkedin.com/in/bart-enkelaar-02242710/,@BartEnkelaar,,,,ce_bart.png,https://youtu.be/MjGRhsBfrUo,"sre,failures"
No,lessons learned,Joey Parsons,CEO,effx.com,,,,Organizational Chaos and recipes for Service Ownership,"Service Ownership can mean a lot of things in a growing engineering organization.  The advent of microservices has made it more critical to get right.  In this talk, we'll talk through all the way your organizational can cause operational chaos before you get Service Ownership correct.

Before you begin failure injection or chaos engineering, it's important to have clear Service Ownership defined within your organization ... else an even greater level of chaos can ensue in your infrastructure and between your teams.  In this talk, Joey Parsons from effx will share recipes for defining Service Ownership within your engineering organization and why it's critically important for chaos engineering and your overall engineering success.
With stories from his new startup effx, and previous companies Airbnb, Flipboard, and others, he'll share strategies and stories from what's worked well and what hasn't.",https://www.linkedin.com/in/joeyparsons/,@joeyparsons,,,,ce_joey.png,https://youtu.be/iUYdO2ZNgms,"service ownership,organizational chaos"
No,lessons learned,Reuben Rajan George,Cloud Reliability Architect,Accenture,,,,Role of Quality Engineers in SRE,"The role of traditional testers has evolved to validate the resilience of modern applications and infrastructure. During this session, the speaker will share some insights/lessons learned while working helping customers make their quality engineering transformation journey.

This talk would cover the following points:
 - Applying observability in testing processes (functional testing, performance testing etc.)
 - Automating resilience (chaos) tests along with performance tests",https://www.linkedin.com/in/reuben-rajan-george-01946326/,@reubenrajan,,,,ce_reuben.png,https://youtu.be/urCJ53iqGSM,"sre,quality engineers"
No,lessons learned,Quintin Balsdon,Expert Software Engineer,Zuhlke Group,,,,Sleeping with one eye open: Experiences with production support,"I will talk about my experience as a software production support engineer. For over 7 years I have been supporting software in various degrees and have developed some insights I feel would be helpful to others.
I will cover the most vital aspects of software production support and include some of the more memorable stories, with the lessons learned.

> Put on the right hat: Assuming the role of a support engineer
>> Ask questions: Getting to know the problem
>>> Have your tools ready: Being ready for analysis
>>>> Trust your team: Code reviews, quality assurance and best practice are your friends
>>>>> Take downtime: Ensure you recover
",https://www.linkedin.com/in/qbalsdon/,@qbalsdon,,,,ce_quintin.png,https://youtu.be/Hb8rRfi7_b0,"production support,experiences"
No,lessons learned,Piyush Verma,CTO,last9.io,,,,Software won't make a noise when it breaks,"Systems fail, but the real failures are the ones from those we learn nothing. This talk is a tale of few such failures that went right under our noses and what we did to prevent those. The techniques covered range from Heterogenous systems, unordered events, missing correlations, and human errors.

Every time there is a failure, there is a root cause analysis, and there is a vow not to repeat the mistake. I will take some curious shortcomings that I have dealt with in the past decade of my work with Infrastructure systems and the steps we had to undertake to:

1. Isolate
2. Limit the spread
3. Prevent from happening again

## Failure 1 ##
An un-replicated consul configuration results in data loss 25 hours before a countrywide launch. Took a staggering five engineers and 20 hours to find one single line of change.

## Failure 2 ##
A failed, etcD, lock forced us to re-write the whole storage on Consul and hours of migration, only to find out later that it was a clock Issue.

The above Isolation and immediate fixes were painfully long yet doable. 
The real ambition was to prevent *similar* such Incidents from repeating. I will share a sample of some of our RCAs and what was missing with each one of those versions. This section touches briefly upon blameless RCA, but the real focus is the action-ability of an RCA.

In this section, I will showcase some of the in-house frameworks and technologies (easy to replicate) built to turn the prevention/alert section of RCAs into lines of code rather than lines of the blurb of text. This section aims to advertise and advocate the need to build/adopt toolchains that promise early-detection and not just faster-resolution.  ",https://www.linkedin.com/in/meson10/,@realmeson10,,,,ce_piyush.png,https://youtu.be/WHc_9QRIpL4,"breaking,software"
No,culture,Karolina Rachwał,Chaos Engineering Practice Lead,Nuaware,,,,Onboarding Chaos Engineering,"Now that stakeholder approval for something as outrageous sounding as ""chaos engineering"" has been granted, a new challenge arises. This talk is a high-level guide on how to prepare and what to expect from the first months of (deliberate) chaos in your organisation. In other words, the “before, during and after” of making chaos successful.",https://www.linkedin.com/in/karolinarachwal/,,,,,ce_karolina.png,https://youtu.be/Dxt1SjUtCDk,"onboarding,chaos engineering"
No,culture,Julie Gunderson,Advocating DevOps,PagerDuty,,,,"Postmortems, Continuous Learning and Enabling Blameless Culture","So you’ve had an incident. Restoring service is just the first step—your team should also be prepared to learn from incidents and outages. In this talk you will learn some best practices around postmortems/post incident reviews to help your team and organization see incidents as a learning opportunity and not just a disruption in service. In this workshop, attendees will:

* Get an overview of blameless postmortems
* Learn techniques for effective information sharing
* Learn how to run a postmortem meeting effectively
* Understand the difference between “blame” and “accountability”",https://www.linkedin.com/in/juliegunderson/,@julie_gund,,,,ce_julie.png,https://youtu.be/Gdm96CtmrL0,"postmortem,continuous learning,blameless culture"
No,culture,Humaira Ahmed,CEO,Locelle,,,,Embracing the Fear of Failure,"The fear of failure and rejection holds so many of us back from achieving our ambitions in life. Especially for women, the fear of failure and rejection becomes ever more so prevalent as we struggle not only against our personal insecurities but also against the pressure put onto us by society.
 
Humaira Ahmed shares her remarkable background in overcoming barriers in the tech industry. Growing up in Pakistan, Humaira is familiar with the struggles she, and many other women, faced in achieving success in choosing to pursue a career in the tech industry, which remains to be very much male-dominated. Humaira excelled in mathematics and was able to secure a position in studying abroad at York University, where she specialized in marketing and communications, and dedicated her passion and experience in starting Locelle to motivate, inspire and empower other women to pursue their ambitions in tech. 

Humaira’s unique experience from growing up in Pakistan to becoming the Founder and CEO of Locelle has taught her three core values to succeed and be empowered in working as a woman in the tech industry.

Top three themes to be covered in this presentation include:

> Fear of Failure/Rejection
>> The Power of Mentorship
>>> Bootstrapping My Tech Startup
",https://www.linkedin.com/in/humaira-ahmed-95b2348/,,,,,ce_humaira.png,https://youtu.be/6-MOtJPpves,"fear,failure"
No,culture,Robert Ross,CEO,FireHydrant,,,,Incident Ready: How to Chaos Engineer Your Incident Response Process,"I started to build FireHydrant, because the solutions for incident management were just bad (to put it nicely). My goal is to build tech for engineers, by engineers. So, I incorporated my experiences, and feedback from the community, to implement researched and vetted recommended practices into an incident management platform. The chaos engineering I'm speaking to is from actual experiences that I and my team have had and we want to make sure other engineers can focus on building without BS.

We’re pretty sure using a real incident to test a new response process is not the best idea. So, how do you test your process ahead of time?

This talk will share how to leverage best practices to break, mitigate, resolve, and fireproof incident processes. We’ll show you how to use chaos engineering philosophies to stress test 3 critical parts of a great process:

1. When and how you declare an incident
2. How you communicate an incident - internally and externally
3. When and if you should escalate an incident to your stakeholders",https://www.linkedin.com/in/bobby-tables/,@bobbytables,,,,ce_robert.png,https://youtu.be/GohmewqmwqM,incident response process
No,culture,Pranjal Deo,Engineering Program Manager,Google,,,,Blameless Postmortem Culture,"Psychological safety has been identified as the topmost feature of a successful and innovative organization. At the same time, we need to learn from failure and prevent recurrence of mistakes. 
These two practices seem to contradict each other, but is there a way to achieve them both?

## Introduction to Postmortem Culture
* Goal: System shouldn't break in the same way again.
* Growth mindset.
* Learn from mistakes.
* Document everything for posterity!

## What does a postmortem look like?
* Various sections.
* All sections for a reason.
* Best practices for each section.
* When does it make sense to write one?

## Psychological safety & Blamelessness
* Postmortems, if not written well can be counterproductive to your culture.
* Why is blamelessness important? The most important?
* Leadership and promoting this culture.
* Celebrating failure!
* Going beyond technology.

## Ok, I wrote one, what next?
* Going above and beyond just writing some documents.
* Making things run like a clockwork, while self sustaining!",https://www.linkedin.com/in/pranjal-deo-56840230/,,,,,ce_pranjal.png,https://youtu.be/6bAqj6B4m-Q,"postmortem culture,blameless"
No,culture,Amir Shaked,Senior VP R&D,PerimeterX,,,,Creating a learning culture,"Building and marinating a five 9s system isn’t just about the tools and technologies. Development culture has a big part in how you keep a system available while scaling it up and supporting more features, users, and locations.

A healthy learning culture, supporting the development, not repairing mistakes, and identifying weak points is another tool in the engineering toolbox.
In this talk, we will discuss how to create a learning culture using debriefs, what to avoid, and how to instill change in an engineering organization.",https://www.linkedin.com/in/amirshaked/,@amirshaked,,,,ce_amir.png,https://youtu.be/7Sg3etadb4w,learning culture
No,culture,Fabricio Buzeto,CTO,bxblue,,,,Día de los Muertos - Postmortems that saves lives,"I've been developing software for the past 20 years and managing teams for over 15 of those. My current team has grown from 5 to 18 over the past two years whist our application grew from a few thousand to more than a million users. We had our fair share of issues and we always kept our heads in check on how to improve things. Our way of doing postmortems is something I'd like to share with others. Not only because I believe it's helpful, but also, because I love to receive feedback from outside our team.

Postmortems are a well-established way to document the history of a project, especially when things break or don't go as planned. Most teams have a hard time keeping up with it. Among those who do, to get value out of it is also another challenge.
Let me share how my team brought postmortems as part of our process. By not only making it a must-have when handling emergencies but also celebrating it we transformed it as a tool for team bonding. Also, bringing postmortems as an onboarding tool to newcomers. To finish, I'll share two occasions where our postmortems helped us avoid issues and paid themselves.",https://www.linkedin.com/in/fabriciobuzeto/,@nukdf,,,,ce_fabricio.png,https://youtu.be/tSnQoN6J9R0,"dia de los muertos,postmortems"