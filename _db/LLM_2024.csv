Title,Abstract,Name1,Company1,url
"You get an LLM, you get an LLM, everyone gets an LLM, but does it work?","Language models be it fine tuned or foundational models ahve proven to be a eureka moment for everyone. While at their core you could deep dive into attention mechanisms and the architecture overall, many are just using the models available out there without diving into if that is the right choice for them based on tasks at hand.

We'll begin by unpacking the key concepts of intrinsic and extrinsic evaluation. Intrinsic evaluation delves into the model's internal workings, examining its grasp of language structure and statistical properties. Extrinsic evaluation, on the other hand, focuses on how well the model performs on specific tasks, such as machine translation or question answering. Throughout the talk, we'll explore popular benchmarks used to evaluate language models. We'll discuss the challenges associated with creating these benchmarks, including potential biases and data limitations.
The talk goes beyond technical aspects to explore the broader considerations in language model evaluation. We'll address concerns about fairness, bias, and robustness.  

To summarize(irony if you see it):
- Why evaluate? 
- What to evaluate?
- Where to evaluate?
- How to evaluate?
- The illusion of choice in model selection based on tasks
- Off you go with more knowledge 

",Ashwin Phadke,,https://ashwin-phadke.github.io/
The Human Touch in AI-Driven Business Development: Striking the Right Balance,"In a world increasingly governed by artificial intelligence, the human touch is crucial. This speech examines the delicate balance of technology innovation and human connection in business progress. We'll discuss the need of maintaining a personal touch in the face of AI developments, emphasizing that successful business development methods use technology to augment rather than replace human engagement. Join me as we traverse the interface of humanity and technology in search of long-term business growth.",Marina Korobeynikova,,https://www.linkedin.com/in/marina-korobeynikova-6844ba38/
The Evolution of AI Models and LLMS in last 10 years,"Over the last decade, the landscape of Artificial Intelligence (AI) models and Language Models (LLMs) has shifted dramatically, altering businesses and revolutionizing how humans engage with technology. In this captivating address, we will take a journey through the last ten years, studying the transformative developments that have catapulted AI models to unprecedented heights. From the introduction of cutting-edge language models to the democratization of AI technology, we'll look at the important milestones that impacted the present and are poised to change the future. The narrative will look at the symbiotic relationship between technological progress and real-world applications, demonstrating how AI models have become not only more advanced but also more accessible to a wider audience. As we progress through this chronological journey, I will share insights from my experience as a seasoned marketing leader, weaving practical examples and industry trends to create a full grasp of the dynamic growth of AI models and LLMs.",Venkata Sai Chaitanya Gatreddi,,https://www.linkedin.com/in/gvs-chaitanya/
Self-host LLMs on Mac/ across devices Fast and Efficient,"This talk will show how to use several command to run LLM on your Mac or across device. It explores how the combination of Rust and WebAssembly revolutionizes AI inference,  in edge and cloud computing. We'll delve into a case study of running Llama2 models at native speed on heterogeneous hardware accelerators, focusing on the Rust+Wasm stack's advantages over Python in terms of size, speed, and security. The session will include a demonstration of a simple Rust program for Llama2 inference, emphasizing its portability and efficiency when compiled to Wasm, and its seamless integration with the WasmEdge runtime for secure and scalable cloud environments.",Michael Yuan,,https://wasmedge.org/
Product Management & Regulation: A Key Requirement or an MVP?,"In this pitch titled “Product Management & Regulation: A Key Requirement or an MVP?”, Parth Sonara explains the demands regulatory requirements place on Product Managers. He explains the importance of meeting regulations while also keeping key product principles such as user experience, industry direction, and scale top of mind. Finally, Parth will also help attendees calculate direct and indirect costs on a product if the minimum viable product (MVP) scope is limited solely to regulation. 
",Parth Prafulbhai Sonara,,https://www.linkedin.com/in/parthsonara
Operational excellence for your LLMs using Amazon Bedrock,"As more customers evaluate large language models (LLMs) for their use, we have seen an increasing interest in operational excellence. Like any other software, machine learning models need operational best practices for deployment, monitoring and maintenance. In this session we will cover the operational excellence recommendations for LLMs and how they compare to traditional ML models. We will discuss the architectural patterns, observability, governance and cost considerations for LLMs. 

We will use Amazon Bedrock to depict the interaction with LLMs via APIs, which allows individual developers and enterprise customers to build and scale generative AI applications with foundation models.",Suraj Muraleedharan,,
Measuring hallucinations in RAG,"RAG or retrieval Augmented Generation is emerging as the architecture pattern of choice for enterprise grade LLM applications, due to its ability to significantly reduce hallucinations and introduce citations, both of which increase the trust of the end user in the result. This is of crucial importance for healthcare applications, where the implications of hallucinations can be dire indeed. 
In this talk I will describe what RAG is, how it's implemented, and introduce Vectara's Hallucination Evaluation Model (HHEM) which allows to measure hallucinations in the response. I will demonstrate this using a sample RAG application.",Ofer Mendelevitch,,https://vectara.com
LLM-Models: Revolutionizing Customer Support or Ringing its Death Knell?,"As customer support undergoes a transformative shift with the advent of LLM (Large Language Models), the question arises: will these AI-powered tools revolutionize the industry or render traditional support obsolete? In this talk, I will explore the potential impacts of LLM models on customer support, delving into both the opportunities they present and the challenges they may pose. Key points will include an examination of the capabilities of LLM models in handling support queries, their potential to enhance efficiency and scalability, concerns surrounding privacy and data security, and the importance of human oversight in maintaining quality service. By weighing the pros and cons, I aim to shed light on the nuanced role of LLM models in the future of customer support.",Dmitry Kindrya,,https://www.linkedin.com/in/dmitry-kindrya-8a72a133/
Implementing GenAI in a banking context: A hands-on approach,"In the forthcoming presentation, the speaker will meticulously outline a comprehensive strategy for integrating large language models (LLMs) into business processes, with a specific focus on identifying up to five distinct use cases and making a critical decision between utilizing proprietary or external LLMs. The process begins with assembling a specialized data science team tasked with the development of a foundational LLM model. This step is followed by the careful selection of a tangible use case for a Proof of Concept (PoC), ensuring it has a clear business owner and aligns with organizational goals. The establishment of a dedicated product team is crucial, comprising backend and frontend developers, QA specialists, an analyst, data scientists, a product owner, and a Scrum master, all working together under the agile framework of weekly Scrum meetings. The speaker will delve into the nuances of developing the PoC, its deployment in a production environment, and the imperative of setting up a systematic feedback loop from customers to gather actionable insights. Emphasis will be placed on the iterative process of addressing and resolving issues identified through customer feedback, thereby enhancing the PoC. The final segment of the speech will explore strategies for scaling the successful aspects of the PoC across other identified use cases, ensuring a broad application of the learnings and successes to maximize business impact.",Denis Skokov,,https://www.linkedin.com/in/dansb
Getting AI to Do the Unexpected,"In an era where AI features in apps are no longer a novelty but a necessity, developers are creatively embedding Large Language Models (LLMs) into applications ranging from “dad joke generators” to critical healthcare tools like “automated EHR systems”. But amidst this innovative surge, a crucial question often lingers: ""What if a bad actor decides to toy with my LLM app, making it behave in ways it was never intended to?""

In October 2023, the OWASP foundation released their top 10 vulnerabilities in LLM apps. In the report, the top 3 vulnerabilities were Prompt Injections, Insecure Output Handling, and PII data leakage. Thus, in this session, through live demos, attendees will learn about these prompt hacking vulnerabilities, mitigation strategies, and the importance of 'secure by design' practices in app development. The goal is to equip attendees with the knowledge to build secure LLM apps.",Pranav Shikarpur,,https://pangea.cloud/blog
Topic modeling for text documents using NLP techniques ,"Talk will cover extraction of keywords from the large text documents using NLP and generative APIs. The demo will use the open source NLP library, Open AI and Python to extract keywords from the text documents and to create an automated cluster that helps understand text documents and categorise it for analytics / reporting purposes. I will be covering an example of a framework that is used at a UK government organisation to categorise applications text documents against industrial code and categories to identify new innovation trends in technology. ",Akshay Dineshkumar Jain,,https://www.linkedin.com/in/akshaydjain/
Make your LLM app sane again: forgetting incorrect data in real time.,"Thanks to LLMs and Retrieval-augmented generation (RAG), building a chatbot that answers questions about your own internal documentation is child's play. RAG extends the knowledge of the LLM by adding the relevant documents, dubbed the context, in the prompt. However, if your data contains outdated information or fake news, your results will be compromised too.
Fortunately, the context is lost at the end of the conversation: removing incorrect data from the documentation makes the LLM forget about it in future conversations. The challenge is maintaining a document index in real time to retrieve only up-to-date information.
In this talk, you will learn how to create an LLM-powered chatbot in Python from scratch with an always up-to-date RAG mechanism. The chatbot will answer questions about your own documents, updating in real-time its knowledge alongside the changes in the documentation.",Olivier Ruas,,
Automated Evaluation for your RAG Chatbot or Other Generative Tool,"It’s easy to build a chatbot using Retrieval-Augmented Generation (RAG). But how do you tell if it's working? In this talk, I’ll discuss why automated testing of your generative tools for your specific use case is important and why evaluations using an LLM are better than previous NLP methods. We'll then look at several ways to write automated LLM-led evaluations, including with a library that allows you to easily and with very little coding create complex grading rubrics for your tests.",Abigail Haddad,,
"Using Apache NiFi, Apache Kafka, RisingWave, and Apache Iceberg with Stock Data and LLM","In this talk, we will discuss how to use Apache NiFi, Apache Kafka, RisingWave, and Apache Iceberg to process and analyze stock data. We will start by discussing the different components of the stack and how they work together. We will then show how to use this stack to ingest, process, and analyze stock data. Finally, we will show how to use an LLM to generate predictions from the analyzed data.",Karin Wolok,,https://www.linkedin.com/in/karinwolok/
Leveraging open-source LLMs for production,"The talk will cover:
	1.	Pros and cons of open-source LLMs vs OpenAI and alike
	2.	Basic economics of hosting open-source LLMs
	3.	Open-source serving frameworks
	4.	State of cloud GPU
	5.	An overview of main open-source LLMs",Andrey Cheptsov,,https://www.linkedin.com/in/andrey-cheptsov/
Large Language Models and Observability ,"Large Language Models (LLMs) like GPT-4 have profoundly impacted the AI landscape, powering applications from customer service to automated content creation. However, these powerful models also come with their own set of challenges, including issues of output inconsistency and model drift. In traditional engineering, observability plays a crucial role in maintaining system reliability. How can we apply these principles to LLMs to ensure they are both effective and safe in production environments?

In this talk, we delve into the unique challenges that LLMs present, such as addressing issues related to output drift and model brittleness. We will explore a variety of tools and techniques that borrow from traditional engineering observability to provide real-time monitoring, analytics, and actionable insights for optimizing LLM performance.

I will demonstrate how implementing these observability techniques can mitigate the risks associated with LLMs, allowing for more robust, accountable, and safe AI-powered applications.

What You'll Learn:

    - The complexities and challenges associated with LLMs, particularly in terms of output inconsistency and model drift.
    - A survey of tools and techniques that adapt traditional engineering observability principles for monitoring and optimizing LLMs. (Tools: GCP, Vertex AI, LangSmith)",Aarushi Kansal,,
The future of search,"Open-source LLMs and vector databases allowed us to build a search technology that allows users to describe their search terms using common language, returning all results that fit the category of this language. For example, we can now search an entire e-commerce store with a query such as ""a slightly bitter beverage people drink to wake them up"" and our tool should return results related to coffee. In fact, the user can use any input language or even images as their query. ",Ben Fistein,,https://www.semantee.ai/
"Governance, Risk, and Compliance for Large Language Models","The talk explains how the explosion of Generative AI introduces new risks for organizations that use tools like ChatGPT in the workplace.

It's essential to manage risks to avoid cyber incidents, private data leaks, regulatory fines, or copyright infringement.

The presentation covers relevant governance, risks, and compliance controls for security, privacy, and legal professionals.

The examples include asset management and observability for LLMs, privacy for data protection and compliance, security against cyber risks, and safety for responsible AI adoption.",Eugene Neelou,,https://www.linkedin.com/in/eneelou/
Safe AI Implementation Strategies: Protecting Your Company's Future While Embracing Innovation,"""Safe AI Implementation Strategies: Protecting Your Company's Future While Embracing Innovation"" is a talk designed for executives who are interested in leveraging artificial intelligence (AI) to drive innovation and growth in their organizations. The talk focuses on the importance of implementing AI in a safe and responsible manner, considering the potential risks and challenges associated with this rapidly evolving technology.

During the talk, the following key points will be addressed:
- Understanding the potential benefits and risks of AI: Executives will learn about the various ways AI can benefit their organizations, such as optimizing and automating business processes, providing decision support, and creating new business opportunities. They will also gain insights into the potential risks and challenges associated with AI implementation, such as security concerns, ethical considerations, and the need for continuous monitoring and evaluation.
- Choosing the right AI platform and service providers: Executives will be guided on how to select AI platforms and service companies with the necessary security certifications to ensure the safety of their data. They will also learn about the importance of customization and ongoing management to meet their organization's specific requirements.
- Collaborative effort and diversity of perspectives: Embracing AI should be a collaborative effort that includes a diverse range of stakeholders, from technologists and ethicists to policymakers and the public. Executives will understand the value of diverse perspectives in identifying potential pitfalls and ensuring that AI benefits society.
- Adaptable regulations and practices: As AI is a rapidly evolving field, regulations and practices should be adaptable to accommodate future advancements while maintaining a focus on ethical and safety considerations. Executives will learn about the importance of flexibility in their AI implementation strategies.

By the end of the talk, executives will have a better understanding of how to implement AI in a safe and responsible manner, protecting their company's future while embracing innovation. They will be equipped with practical strategies and considerations to guide their AI implementation efforts and ensure the long-term success of their organizations.",Dan Savino,,
Large Language Models is What We Need to Create Jarvis from Ironman,"Prepare to embark on a captivating journey as Jason takes the stage to unveil the incredible world of Large Language Models (LLMs) and their pivotal role in bringing science fiction to life. In this thought-provoking talk, you will delve deep into the groundbreaking advancements in LLMs that are redefining the very fabric of software development. Join us as we explore the complexities and opportunities of crafting an AI assistant akin to the legendary Jarvis from Iron Man.

Jason will unravel the following key themes in this enthralling session:

- Advancements in LLMs: Discover how LLMs are pushing the boundaries of what was once considered science fiction and turning it into reality. 

- Changing the Software Landscape: We are closer than ever to witnessing a paradigm shift in the software industry, where LLMs play a central role in reimagining how humans interact with technology.

- The Challenges of Creating Jarvis: Delve into the intricacies and hurdles of creating a Jarvis-like AI using LLMs.

Don't miss this opportunity to gain invaluable insights into the future of AI and software development. ",Jason Tan,,https://linktr.ee/jpctan
Leveraging Large Language Models in a Communication Platform,"In the evolving landscape of technology, understanding the depth of context within communication platforms is crucial. With the advent of GenAI, we aim to explore how this technology can significantly enhance context awareness and productivity.

Our session delves into the transformative power of GenAI, spotlighting its ability to streamline intricate processes such as Site Reliability Engineering, Change Management, Production Readiness Reviews, and Incident Management. Moreover, GenAI addresses the pervasive issue of FOMO (Fear Of Missing Out) while prioritizing overall well-being, fostering a balanced work environment.

In this presentation, we don't shy away from the critical conversations surrounding security and ethics. We emphasize the importance of the role of Open Source AI Models, ethical AI integration and robust security measures, ensuring the responsible use of this technology. By participating in this session, attendees will gain valuable insights into harnessing the potential of AI while upholding ethical standards and data security.",Spiros Economakis,,https://www.linkedin.com/in/spirosoik/
How to use ChatGPT without getting caught,"Join us as we explore the world of **detecting AI-generated text**, such as what you get from ChatGPT. Have you ever wondered how experts uncover it? We'll spill the beans!

We'll start from scratch, briefly explaining how the generation of text using AI works. No previous background in the field is required!

Then, we'll discuss classifier systems for detection, black and white box models, watermarking, and manual techniques for detection — cool stuff to keep in your toolbox!

But here's the twist—these systems are not foolproof. There are ways to get around them, and we'll explore how. **Ready to uncover the mysteries? See you there!**",Aldan Creo,,https://acmc-website.web.app
Navigating the New Internet: LLMs as the Key to Web4 Transformation,"This talk delves into the transformative role of Large Language Models (LLMs) in shaping the future of the internet, particularly in the Web4 landscape. We'll examine the impact of LLMs on user interface design and environmental sustainability, using real-world applications like the Kizana AI fashion advisor as case studies. Insights will be drawn on GPU demand, the environmental footprint of LLMs, and the potential of Small Language Models (SLMs) for specialized tasks. Additionally, we'll explore the ethical considerations in data handling and privacy in the Web4 context, highlighting the synergy between LLMs and SLMs for an optimal user experience.",Sam Naji,,https://www.1001epochs.ch/knowledge-center
How to develop framework for Real-Time RAG in LLM Applications,"This talk will go over a real-life example/demo of how to keep the vector DB up to date using streaming pipelines, importance of RAGs and how they can be used to eliminate hallucinations, which can have a huge catastrophic impact on the outputs of LLMs. It would be a great session for data and machine learning engineers would want to learn through a deep dive session on how to fine tune LLMs using open source libraries.",Ankit Virmani,,https://www.linkedin.com/in/ankitvirmani/
Future of LLM's and Machine learning Productionization,"In this talk, we'll dive into the captivating realm of the Future of Large Language Models (LLMs) and the intricate process of Machine Learning Productionization. I will unravel the latest breakthroughs in language models, shedding light on their evolving capabilities and the transformative impact on various industries. Moreover, we will explore the art and science of deploying machine learning models into real-world applications, ensuring scalability, efficiency, and tangible business impact. From language understanding to production integration, this session promises an enlightening glimpse into the exciting future of AI.",Deepak Karunanidhi,,https://www.linkedin.com/in/akdeepak85
How well do LLMs detect anomalies in your data?,"Data quality issues can significantly impact revenue, with a reported average 12% loss for US companies ([Experian report](https://intelligent-ds.com/blog/the-real-cost-of-bad-data)). Addressing this starts with the identification of anomalies. This talk follows a journey into building an anomaly detector, iteratively expanding on the solution to boost its accuracy.

In this talk, we will begin with a simple anomaly detector using OpenAI’s API. We will then delve down multiple exploratory tracks including prompt engineering and the impact of input data types. To wrap things up, we will undertake a comparative analysis of tools, including BigQuery and Mistral, to highlight their unique strengths in anomaly detection.

By the end of this talk, you'll gain a solid foundation to build your own anomaly detection strategy. Whether utilising in-house solutions or custom-built tools, you'll be better equipped to boost your data quality.

Outline

- Using OpenAI to detect simple anomalies
- Effect of Prompt Engineering on the accuracy of an anomaly detector
- Impact of numerical data vs text-based data on performance
- Using BigQuery for anomaly detection in structured data
- Performance of OpenAI vs Mistral",Chloé Caron,,
Unleash GenAI while remaining cost-effective with serverless,"GenerativeAI is rapidly reshaping the tech landscape. Its implementation, especially in intricate scenarios, often involves a sophisticated dance among application code, users, and multiple Large Language Models (LLMs). The complexity of coordinating these elements extends beyond the capabilities of basic API calls. While some vendors offer means to access LLMs, they fall short in guiding developers to construct an LLM framework that effectively balances responsiveness, accuracy, and cost-efficiency. Consequently, many projects grapple with unforeseen costs, pushing them beyond budget.

Addressing this challenge, our proposal centers on a serverless architecture. This approach adeptly orchestrates various language models, from Foundational to fine-tuned variants. It supports developers and organizations in building forward-thinking projects with specific Cloud Development Kit (CDK) constructs, tailored for effective model monitoring and coordination.

This talk delves into the GenAI solutions arena, highlighting diverse model approaches, whether deployed via APIs or other means. We will spotlight serverless architectures using AWS Lambda, AWS StepFunctions, and Amazon Bedrock, citing real-world applications. To streamline complexity, we transform these architectural elements into CDK constructs, offering developers a rapid and efficient launching pad for their projects.",Luca Bianchi,,https://www.linkedin.com/in/lucabianchipavia/
Running an open source LLM,"Large language models (LLMs) like GPT-3 have demonstrated impressive capabilities in natural language processing.  These models can generate human-like text and power applications like chatbots, content generators, and code assistants. OpenAI, Bard, Bedrock are just a few services which have arisen in order to provide generative AI services. But different use cases might prevent you from using these ready to serve services, or you may have actually considered stepping away from these services and running your LLM model. However, these models are computationally intensive and require specialised hardware to deploy. In this talk, we will explore the options available for harnessing LLMs, including deploying your own service or using existing LLM APIs. 

For organisations with sufficient technical resources, deploying an internal LLM service can provide more control, customisation, and data privacy. We will outline hardware requirements, training considerations, and maintenance needs. Yet this approach also involves significant upfront investment and ongoing management overhead. The talk will also highlight some performance challenges faced with the models

Alternatively, existing LLM services offer convenient APIs for integration into applications. We will compare basic need to know of these services, capabilities, and limitations.

Attendees will learn best practices for leveraging LLMs effectively, whether by building or subscribing. The talk aims to demystify options for harnessing the power of large language models in real-world systems.",Bongani Shongwe,,https://www.linkedin.com/in/bonganishongwe/
Vectoring Into The Future: AWS Empowered RAG Systems for LLMs,"The power of Retrieval Augmented Generation (RAG) for LLMs is transformative, and AWS tools can help unlock this potential. This talk explores how to use vector databases, custom silicon with Inferentia, and LLMs on SageMaker and BedRock to augment your RAG systems. Learn how to maintain up-to-date reference documents, execute relevancy searches, and deliver accurate responses, all while optimizing cost and performance. Let's pioneer the future of AI together!",Samuel Baruffi,,
LLM for better developer learning of your product,"Developing a tech product is not just about coding and deployment. It's about the learning journey that goes into building and utilizing it as well. If you have a developer-oriented product, it is about ensuring that developers understand your product at a deep level through the docs, tutorials, and how-to guides, improving both their own skills and the quality of the work they produce. Nowadays AI can not only generate docs from code but also it makes easy to find specific information or answer questions about your product using a chatbot for a better developer experience. It is life-changing for these project docs maintainers.

This talk explores how LLMs(Large Language Models) and LLM apps such as Pathway can be leveraged for effective and efficient developer education, which can boost the utilization of your product.",Bobur Umurzokov,,https://www.iambobur.com/speaking
Generative AI Security — A Practical Guide to Securing Your AI Application ,"The pace of innovation in generative AI offers immense opportunities while also introducing new security challenges. Those include new threat vectors, explainability, governance, transparency, and privacy for large language models. As organizations seek to leverage generative AI for innovation, security leaders must take concrete steps to enable rapid experimentation without compromising security. 

We will begin our talk by understanding the scope of generative AI applications, based on their intended use and the potential risks associated with their deployment. We will then discuss key strategies for securing generative AI applications, including threat modeling, guardrails, observability, and evaluation of effectiveness of security measures.

Through case studies and practical examples, we will show how to apply these strategies in real-world scenarios, from ideation to production. Attendees will learn how to identify and mitigate potential risks in their generative AI applications, and how to evaluate the effectiveness of their security measures.

This talk is intended for security leaders, developers, and practitioners who are working with generative AI applications, and who want to ensure the security and integrity of their systems. By the end of the talk, attendees will have a deeper understanding of the security challenges associated with generative AI, and the practical steps they can take to address them.
",Manuel Heinkel,,https://www.linkedin.com/in/manuelheinkel/
Building Chatbots with Advanced Retrieval-Augmented Generation Techniques,"Chatbots are becoming increasingly popular for interacting with users, providing information, entertainment, and assistance. However, building chatbots that can handle diverse and complex user queries is still a challenging task. One of the main difficulties is finding relevant and reliable information from large and noisy data sources.

In this talk, I will present some of the latest advances in retrieval-augmented generation(RAG) techniques, which combine the strengths of both retrieval-based and generative approaches for chatbot development. Retrieval-based methods can leverage existing text documents to provide informative and coherent responses, while generative methods can produce novel and engaging conversations personalized to the user.

 I will cover the following topics:
Hybrid search with vector databases: How to use both keyword-based and semantic-based search methods to retrieve relevant documents from large-scale vector databases.
Query generation using LLMs: How to use large language models to generate natural and effective queries for document retrieval, based on the user input and the dialogue history.
Automatically excluding irrelevant search results: How to use various filtering and ranking techniques based on vector distance to exclude irrelevant search results.
Re-ranking: How to dynamically re-rank retrieved documents to further improve context relevance.
Chunking Techniques: How to use text segmentation and summarization methods to chunk long documents into shorter and more relevant passages.
 
I will demonstrate the effectiveness of these advanced techniques in the RAG workflow. I will also discuss the challenges and limitations of these techniques and the future directions for research and development.",Zain Hasan,,
Building our own LLM Vulnerability Scanner to audit and secure AI applications,"Over the next few years, we'll see more organizations building various AI-powered tools and systems. While most AI-powered tools can be built using 3rd-party services and APIs, we'll see more companies using their own LLMs and hosting it in their own private network environments. For one thing, having a self-hosted LLM would guarantee greater control over data privacy and security. In addition to this, companies would gain the much needed flexibility when customizing their LLMs to specific business needs and constraints.

At this point, most professionals are not aware of the security threats and potential security vulnerabilities when building AI-powered applications utilizing self-hosted Large Language Models (LLMs). Similarly, security teams and professionals have not yet adjusted to the new wave of security attacks due to the pace of innovation in the AI space.

One of the more practical techniques to secure these AI-powered applications involves building and using a vulnerability scanner that checks for common vulnerabilities such as prompt injection. In this session, we will use Python to build a custom scanner to help teams identify and mitigate security issues specific to their self-hosted LLMs right away. Finally, we'll also take a look at various strategies on how to mitigate the vulnerabilities found by our scanner.",Joshua Arvin Lat,,https://www.linkedin.com/in/joshualat
Applying LLMs in real PHP projects: Transform Your Code with Advanced AI,"In the speech ""Applying LLMs in Real PHP Projects: Transform Your Code with Advanced AI,"" we explore the revolutionary impact of Large Language Models on PHP development. This talk dives into how LLMs, like GPT and BERT, are reshaping the PHP landscape, offering new possibilities for code generation, optimization, and problem-solving. We will discuss the practical aspects of integrating these sophisticated AI models into real-world PHP projects, highlighting both the opportunities and challenges that come with this cutting-edge technology.

Attendees will learn about various use cases where LLMs can enhance PHP development, from automating routine coding tasks to providing advanced code suggestions and debugging assistance. The session will also address common challenges in integrating LLMs, such as dealing with model limitations, ensuring code quality, and managing the balance between AI assistance and developer control.

Furthermore, the talk will provide actionable insights and methodologies for overcoming these obstacles, ensuring a smooth integration of LLMs into your PHP projects. Whether you're a PHP veteran or new to the language, this talk will offer valuable perspectives on harnessing the power of AI to elevate your coding practices and project outcomes.",Filipp Shcherbanich,,https://www.linkedin.com/in/shcherbanich/ 
AI chats — what nobody told you: the conundrums of business integration.,"ChatGTP's growth in popularity is unmatched. Yet, only some companies are integrating it into their systems to make something more sophisticated than a single prompt. One of the reasons is that integration requires both specific knowledge and effort. However, it is not the only one. When one considers integrating their production system with a ChatGPT-like tool, the privacy of your data and the cost it will generate are two essential matters to consider. Careful analysis of what data one sends, where it's being processed, and how much and when one will have to pay for it are the questions to be addressed. These questions are crucial for defining a business case which has the potential to bring the expected return on investment. 
Marcin will go through all the tricky bits related to LLMs during the talk. Starting with the ways of direct integration with the ChatGPT API, then he will move into cost calculation and best practices in optimising it. He will also discuss privately hosted models and how to pragmatically tune them for your needs - because you probably don't have resources similar in capacity to the likes of Google or Microsoft.",Marcin Szymaniuk,,https://www.linkedin.com/in/marcin-szymaniuk/
Advanced API Design for Scalable and Fault-Tolerant Data-Intensive Distributed Systems,"This proposal delves into RESTful principles, asynchronous operations, security considerations, techniques for optimizing data retrieval through pagination and filtering, exploration of caching and rate limiting, importance of idempotent operations and error handling in maintaining data consistency and facilitating seamless integration with evolving systems.


Learning objectives:

This topic aims to provide practical insights and guidelines for modern software engineers, architects, and developers involved in the creation and evolution of APIs for data-intensive distributed systems. By adopting the proposed advanced API design principles, organizations can achieve a balance between scalability, performance optimization and fault tolerance, ultimately contributing to the success of their distributed computing initiatives.


Advanced API Design:

1. RESTful Principles:
RESTful principles form the foundation of well-structured APIs, emphasizing stateless communication, resource-oriented design, and standard HTTP methods.
Adhering to RESTful principles ensures a standardized, scalable, and easily maintainable API architecture. It facilitates clear communication between clients and servers, promoting a consistent and predictable user experience.

2. Asynchronous Operations with Webhooks:
Asynchronous operations allow decoupling of processing tasks from the main application flow, while webhooks enable real-time communication by pushing data to subscribed servers when specific events occur.
By incorporating asynchronous operations and webhooks, APIs can handle time-consuming tasks efficiently, minimize latency, and provide real-time updates. This contributes to improved responsiveness and scalability, particularly in systems with fluctuating workloads.

3. Security:
Authentication mechanisms (e.g., OAuth, API keys), authorization strategies, data encryption (HTTPS), and protection against common security threats.
Security measures are paramount to safeguard data integrity, confidentiality, and prevent unauthorized access or malicious attacks. Robust security practices instill trust in the API and protect sensitive information from potential breaches.

4. Pagination and Filtering:
Pagination involves breaking down large result sets into manageable pages, while filtering allows clients to request specific data based on criteria.
Pagination and filtering optimize data retrieval, reduce bandwidth consumption, and enhance API performance, especially when dealing with large datasets. This ensures efficient use of resources and minimizes response times.

5. Caching:
Caching involves storing copies of frequently accessed data to reduce redundant computations or database queries.
Caching enhances API performance by minimizing the need for repeated data retrieval. It reduces latency and conserves resources, making it particularly beneficial for read-heavy operations and improving overall system responsiveness.

6. Rate Limiting:
Rate limiting restricts the number of requests a client can make within a specified time period.
Rate limiting prevents abuse, ensures fair resource usage, and protects the API from being overwhelmed by excessive requests. It promotes stability, reliability, and fair access to resources.

7. Idempotent Operations:
Idempotent operations produce the same result regardless of how many times they are executed.
Idempotence ensures consistency in the face of network issues or retries. It reduces the risk of unintended consequences, making operations more predictable and resilient.

8. Error Handling and Resilience:
Clear, standardized error responses, along with mechanisms for graceful error handling and recovery.
Effective error handling and resilience mechanisms contribute to the overall reliability of the API. Well-designed error responses enable clients to understand and respond appropriately to unexpected situations, while resilience ensures continued functionality despite failures.",Santosh Nikhil Kumar,,https://www.linkedin.com/in/santoshadireddy/
Adding Generative AI to Real-Time Streaming Pipelines,"In this talk I walk through various use cases where bringing real-time data to LLM solves some interesting problems.

In one case we use NiFi to provide a live chat between a person in Slack and several LLM models all orchestrated via NiFi and Kafka. In another case NiFi ingests live travel data and feeds it to HuggingFace and WatsonX.AI LLM models for summarization. I also do live Q&A from hosted chat webpages. We also augment LLM prompts and results with live data streams.

https://github.com/tspannhw/FLaNK-HuggingFace-BLOOM-LLM
https://github.com/tspannhw/FLaNK-watsonx.ai",Timothy Spann,,https://github.com/tspannhw/SpeakerProfile