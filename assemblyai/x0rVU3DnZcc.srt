1
00:00:27,040 --> 00:00:30,518
Hello everyone, I'm Stephan Batifau. I'm a developer advocate at Ziliz

2
00:00:30,606 --> 00:00:34,798
and today I'm here to talk to you about vector databases 101

3
00:00:34,926 --> 00:00:38,526
vector databases. Everyone talks about them and

4
00:00:38,550 --> 00:00:42,006
today my objective is to explain to you how they work, in detail

5
00:00:42,110 --> 00:00:45,430
actually. So let's get started. First,

6
00:00:45,622 --> 00:00:49,166
what is Ziliz? Zilis is the maintainer of

7
00:00:49,310 --> 00:00:52,598
Melvis, which is an open source vector database. We are also

8
00:00:52,646 --> 00:00:56,422
maintaining nowhere, which is a vector search engine GPT cache which

9
00:00:56,438 --> 00:00:59,766
can be very useful if you want to do 17 cache for

10
00:00:59,790 --> 00:01:03,206
LLM queries. We have VDB benchmark, which is also like a benchmark

11
00:01:03,230 --> 00:01:06,630
tool for vector databases. We have Zliscloud,

12
00:01:06,782 --> 00:01:10,142
which is our cloud offering, but I'm not here to talk about that.

13
00:01:10,158 --> 00:01:14,414
Today we also partner with different industry leaders such as AWS,

14
00:01:14,454 --> 00:01:18,254
Google Cloud, Azure. We also partner with different Gene AI

15
00:01:18,294 --> 00:01:21,814
tooling, also partnered directly with some chip manufacturers

16
00:01:21,894 --> 00:01:25,490
such as Nvidia or Intel. But today

17
00:01:25,682 --> 00:01:29,610
what we're going to talk about is going to start with why vector

18
00:01:29,642 --> 00:01:32,106
databases, why you may need one.

19
00:01:32,250 --> 00:01:35,734
Also, where do vectors come from the different use cases?

20
00:01:36,194 --> 00:01:39,442
How do vector database work? What is similar to search

21
00:01:39,538 --> 00:01:43,146
different indexes? And then I'm going to explain to you and show you the build

22
00:01:43,170 --> 00:01:46,402
basic architecture and then hopefully after

23
00:01:46,458 --> 00:01:49,746
that you should be able to understand how database work and vector

24
00:01:49,770 --> 00:01:53,224
databases work in particular. So let's start with the why.

25
00:01:53,724 --> 00:01:56,412
Why do you think you need a vector database?

26
00:01:56,588 --> 00:02:00,660
Well, one reason is that unstructured data is

27
00:02:00,732 --> 00:02:04,492
about 80% of the data you have worldwide. And the

28
00:02:04,508 --> 00:02:08,156
way the vector databases is working is a bit different to usual

29
00:02:08,220 --> 00:02:11,508
SQL NonSQL database. So the search pattern or

30
00:02:11,516 --> 00:02:15,404
the search type in a traditional database is you go

31
00:02:15,524 --> 00:02:19,332
to find a very specific result. You go with select

32
00:02:19,388 --> 00:02:23,088
star from a specific table or select some different columns

33
00:02:23,136 --> 00:02:26,680
because you want a result that is exact. With vector database,

34
00:02:26,712 --> 00:02:30,004
it's a bit different. We're doing a lot of vector comparisons,

35
00:02:30,824 --> 00:02:33,864
which, and if you don't know, vectors are just

36
00:02:33,904 --> 00:02:37,192
representation of the data of the unstructured data. So they

37
00:02:37,208 --> 00:02:40,848
are basically a long series of numbers. And vector databases

38
00:02:40,896 --> 00:02:44,296
are specifically built to work

39
00:02:44,360 --> 00:02:47,904
with this kind of unstructured data, including text,

40
00:02:47,984 --> 00:02:51,800
images, videos and audio. And one thing

41
00:02:51,832 --> 00:02:55,484
that usually people tell me and they're always wondering is like,

42
00:02:56,144 --> 00:02:59,360
can I just use numpy canon? Could I just write

43
00:02:59,392 --> 00:03:02,848
my own and then make it work? The answer is that,

44
00:03:02,896 --> 00:03:06,392
well, I mean, technically yes you can, but you might encounter a

45
00:03:06,408 --> 00:03:09,600
problem once you go to scale at the beginning,

46
00:03:09,712 --> 00:03:13,616
it's fine, you only have 1000 embeddings it's very small. You don't

47
00:03:13,640 --> 00:03:16,968
really need more time. Once you go really high in the

48
00:03:16,976 --> 00:03:20,454
amount of embeddings then you can really struggle

49
00:03:20,494 --> 00:03:23,354
and can take a long time. And then basically it doesn't scale.

50
00:03:25,054 --> 00:03:28,510
And that's usually the reason why when people ask me about vector

51
00:03:28,542 --> 00:03:31,794
search libraries in general, I mean, yeah, they can work.

52
00:03:32,094 --> 00:03:34,678
I'm not going to say you always need a vector database,

53
00:03:34,846 --> 00:03:38,142
but you might encounter some problem with the

54
00:03:38,158 --> 00:03:41,278
search quality. What do you do if you

55
00:03:41,286 --> 00:03:45,358
need hybrid search or if you need filtering? Also scalability.

56
00:03:45,486 --> 00:03:49,696
What happens when you have billions of vectors? What about multi tenancy?

57
00:03:49,880 --> 00:03:53,424
What about the cost? Are you putting everything in memory? Are you putting

58
00:03:53,464 --> 00:03:56,792
everything on disk? Are you putting everything on s three? Security as

59
00:03:56,808 --> 00:03:59,568
well as a big concern. What about data safety and privacy?

60
00:03:59,736 --> 00:04:03,088
Also, what if you have to update your data? What if you

61
00:04:03,096 --> 00:04:05,684
have to update your embeddings? It can be a problem.

62
00:04:06,704 --> 00:04:10,736
TLDR, they usually like the infrastructure to

63
00:04:10,760 --> 00:04:14,562
help you scale and deploy and manage your apps in productions.

64
00:04:14,688 --> 00:04:18,454
They can be really cool for pocs, but once you go more

65
00:04:18,494 --> 00:04:22,014
into the scale and once you go more into production, then they can really

66
00:04:22,054 --> 00:04:26,430
struggle. Why not use a SQL or NoSql database?

67
00:04:26,582 --> 00:04:30,166
Well, there's also a problem here is that they're

68
00:04:30,190 --> 00:04:33,750
usually inefficient in high dimensional spaces, which is what you have

69
00:04:33,782 --> 00:04:38,158
actually for vector databases. They also have sub optimal indexes.

70
00:04:38,246 --> 00:04:41,662
They don't support different indexes that you might need for

71
00:04:41,758 --> 00:04:45,702
vector search. Inadequate query supports

72
00:04:45,838 --> 00:04:49,294
for scalability can be a problem. You have data conversion

73
00:04:49,334 --> 00:04:52,990
issues and then yeah, you have different problems like that. So usually

74
00:04:53,062 --> 00:04:56,914
vector search and vector operations in general are a bit too intensive

75
00:04:57,254 --> 00:05:00,590
from a compute point of view. For traditional

76
00:05:00,622 --> 00:05:04,222
databases and vector databases,

77
00:05:04,318 --> 00:05:08,046
and in particular milvis is really beautiful scale. So I won't go in the

78
00:05:08,070 --> 00:05:11,386
details over like everything, but thanks

79
00:05:11,410 --> 00:05:14,474
to vector databases you might have advanced filtering,

80
00:05:14,554 --> 00:05:18,802
hybrid search, multi vector search, you can also have automatic

81
00:05:18,858 --> 00:05:22,094
backups, replications, higher variabilities,

82
00:05:23,194 --> 00:05:26,594
aggregations. You have different things really

83
00:05:26,634 --> 00:05:30,650
good for billion plus scale, so you can really do those.

84
00:05:30,722 --> 00:05:34,458
If that's not a problem for us, then some

85
00:05:34,586 --> 00:05:38,154
vector erases. And the one in particular, like Mailvis, we have GPU index,

86
00:05:38,234 --> 00:05:42,154
which can be very useful in some cases. You might need

87
00:05:44,614 --> 00:05:47,910
a lot of embeddings, you need a very high accuracy and you need to be

88
00:05:47,942 --> 00:05:51,126
very quick. Then you may need to use a GPU index.

89
00:05:51,310 --> 00:05:55,062
So as a takeaway. Vector databases are purpose

90
00:05:55,118 --> 00:05:58,394
built to handle indexing, storing and querying vector data.

91
00:05:58,814 --> 00:06:03,150
And they usually design and we are usually designed for billion plus use

92
00:06:03,182 --> 00:06:06,518
cases. It doesn't mean you don't have to do, it doesn't mean you can't use

93
00:06:06,566 --> 00:06:10,142
us for smaller scale. It just means that basically

94
00:06:10,198 --> 00:06:14,004
you start from the ground up, you start with a couple of hundreds embeddings,

95
00:06:14,084 --> 00:06:16,864
and then we can help you go to the billion plus.

96
00:06:17,804 --> 00:06:21,404
But then where do vectors come from? It's a

97
00:06:21,484 --> 00:06:24,548
very good question. They just don't pop up like that

98
00:06:24,596 --> 00:06:28,384
out of the blue. So it's usually,

99
00:06:28,764 --> 00:06:31,916
imagine you have your knowledge base, so it can be text, pictures,

100
00:06:32,020 --> 00:06:35,740
internal documents, and the data gets passed into a deep learning

101
00:06:35,772 --> 00:06:39,836
model. And that's very important that this deep learning model is

102
00:06:39,860 --> 00:06:42,858
the right type of model. What I mean by that is that it has to

103
00:06:42,866 --> 00:06:47,090
be trained on the right of data. So if you're working with images,

104
00:06:47,282 --> 00:06:50,250
you gotta have a model that has been trained on image data. For example,

105
00:06:50,362 --> 00:06:53,934
if you're working on text, it has to be trained on text data,

106
00:06:54,274 --> 00:06:57,922
working on images. And you wanna identify the type

107
00:06:57,938 --> 00:07:01,218
of dogs you have, then, well, you gotta have that in

108
00:07:01,226 --> 00:07:04,454
the data. It has to be trained on that.

109
00:07:05,034 --> 00:07:08,242
And what happens then is that you

110
00:07:08,258 --> 00:07:11,748
take your image data, for example, you run it through a deep learning model,

111
00:07:11,946 --> 00:07:16,096
and then you cut off the last layer, because usually the last layer does

112
00:07:16,120 --> 00:07:19,232
a prediction and we're not interested in the prediction,

113
00:07:19,368 --> 00:07:22,284
we're interested in what the model has learned about the data.

114
00:07:22,624 --> 00:07:26,096
We really don't care about the prediction. So what we want to

115
00:07:26,120 --> 00:07:29,416
know is what it has learned. And we want to have the numerical

116
00:07:29,440 --> 00:07:32,616
representation so that later on, when we work

117
00:07:32,640 --> 00:07:36,136
with that kind of data, we have a way to work with it, we can

118
00:07:36,240 --> 00:07:38,854
understand it. And yeah,

119
00:07:40,034 --> 00:07:42,574
that's why we cut off before the last layer.

120
00:07:43,154 --> 00:07:46,226
And why not take the second layer from

121
00:07:46,250 --> 00:07:49,882
the beginning or something? Why is the last one? Why not one in the middle?

122
00:07:50,018 --> 00:07:53,010
It's also because as you pass through the model,

123
00:07:53,162 --> 00:07:57,122
each layer will learn something new. And the second to last layer contains all the

124
00:07:57,138 --> 00:08:00,738
semantic information without doing the prediction. And that's what

125
00:08:00,786 --> 00:08:04,614
vector is in our context. That's what we use all the time.

126
00:08:05,224 --> 00:08:08,080
So then, yeah, then you have your vectors, then you put it in a vector

127
00:08:08,112 --> 00:08:11,424
database like Milvis, and then you can query it.

128
00:08:11,544 --> 00:08:14,360
And how did it work in practice?

129
00:08:14,552 --> 00:08:17,992
Usually you have embedding models and you have a lot of

130
00:08:18,048 --> 00:08:21,164
different embedding models, and they are actually quite important.

131
00:08:21,664 --> 00:08:25,844
You have to keep in mind that embedding models will be key for you.

132
00:08:26,264 --> 00:08:29,760
They are like, you know, you have embedding models that have been trained

133
00:08:29,792 --> 00:08:33,448
on like very very specific things. So let's say you're working with

134
00:08:33,616 --> 00:08:36,952
english data and it's like nothing specific. Maybe the embeddings of

135
00:08:36,968 --> 00:08:40,064
OpenAI will work. But in my case,

136
00:08:40,144 --> 00:08:42,984
I live in Germany and we have german documents,

137
00:08:43,144 --> 00:08:46,576
so we need embedding models that have been trained on german data.

138
00:08:46,720 --> 00:08:50,072
And for example, if I do that, then I'm gonna use Gina AI, usually,

139
00:08:50,128 --> 00:08:53,768
because it's quite good. And I would suggest

140
00:08:53,856 --> 00:08:57,056
for you, if you're like, if you don't know where to start, go on hugging

141
00:08:57,080 --> 00:09:00,676
face, look at the leaderboards and check the different embeddings they

142
00:09:00,700 --> 00:09:04,068
have, because they have so many that it can be really good.

143
00:09:04,156 --> 00:09:08,076
And then hugging face makes it very easy for you as well to use embedding

144
00:09:08,100 --> 00:09:11,932
models. So I would say, yeah, maybe try at first with OpenAI,

145
00:09:12,028 --> 00:09:15,508
try with different models. But yeah,

146
00:09:15,636 --> 00:09:19,092
those are very important. You have to have a specific one. And how

147
00:09:19,108 --> 00:09:22,284
does this work? Because here we have a very simple example.

148
00:09:22,444 --> 00:09:25,642
We have a text which is happy dog wagging trail. Then you put it through

149
00:09:25,658 --> 00:09:28,614
an embedding model and then it generates a vector.

150
00:09:29,034 --> 00:09:32,218
And then this vector is, then you

151
00:09:32,226 --> 00:09:36,494
can imagine this is a vector space. So that's very simplified.

152
00:09:36,994 --> 00:09:40,570
But you can imagine you have vectors everywhere and it's in so many

153
00:09:40,602 --> 00:09:44,258
dimensions. And what happens is that everything that is similar,

154
00:09:44,346 --> 00:09:46,654
that would be close to each other in the vector space.

155
00:09:47,394 --> 00:09:51,146
So if you have a building with a small window, and then you have

156
00:09:51,170 --> 00:09:53,962
another one, which is a big building, once that transformed,

157
00:09:54,138 --> 00:09:57,534
then the vector will be very close in vector space,

158
00:09:58,114 --> 00:10:01,298
and that's the same for other text. And if you have something that

159
00:10:01,306 --> 00:10:04,334
is completely unrelated, then it will be far from the others.

160
00:10:05,194 --> 00:10:09,434
So what about the use cases we've

161
00:10:09,474 --> 00:10:12,882
been talking about? Maybe when you should use it or why you

162
00:10:12,898 --> 00:10:15,454
should use it. But it's more like when should you use it? Now,

163
00:10:15,914 --> 00:10:20,334
it's not all about the embeddings and everything. It's like, okay, the different use cases.

164
00:10:21,264 --> 00:10:24,736
Well, the first one and the main one is rag. So that's

165
00:10:24,760 --> 00:10:28,644
the one that has been the most popular recently, which means retrieval, augmented generation.

166
00:10:28,944 --> 00:10:32,520
It's basically, if I were to explain it, it's like you

167
00:10:32,592 --> 00:10:36,232
want to expand your LLM with knowledge that

168
00:10:36,248 --> 00:10:40,104
is external to the LLM. Let's say you work for a company and

169
00:10:40,144 --> 00:10:43,336
you want to add this data to the LLM. Usually you would use

170
00:10:43,360 --> 00:10:47,072
rack for that. So you put all your data, you process it, you put everything

171
00:10:47,128 --> 00:10:50,044
in a vector database, and then once you make the query,

172
00:10:50,894 --> 00:10:54,246
then it goes through the vector database to see if you have something that is

173
00:10:54,270 --> 00:10:57,702
similar and then it gives that back as a context to the

174
00:10:57,718 --> 00:11:00,886
LLM. And that's what a drag is. But that's only one of

175
00:11:00,910 --> 00:11:04,894
the nine usual use cases. You may use a vector database for,

176
00:11:05,014 --> 00:11:07,774
you have the good old one recommended system.

177
00:11:07,934 --> 00:11:11,734
So for product recommendation, it's actually one of the most common use

178
00:11:11,774 --> 00:11:14,274
cases for mailroom production.

179
00:11:15,094 --> 00:11:18,630
Then they're really able to compare products and users and then use

180
00:11:18,662 --> 00:11:22,030
a vector database to quantify that and compare that. Then you

181
00:11:22,062 --> 00:11:25,262
have text and semantic search, image similarity search,

182
00:11:25,318 --> 00:11:28,702
video search as well, for similarity, and audio as well.

183
00:11:28,878 --> 00:11:32,902
It can also have molecular similarity search, and it

184
00:11:32,918 --> 00:11:36,710
can be really useful for companies that are working with proteins, for example.

185
00:11:36,902 --> 00:11:40,134
Anomaly detection is also a common one. It's how

186
00:11:40,174 --> 00:11:43,966
different are two user actions, how similar are the actions of two users.

187
00:11:44,070 --> 00:11:47,036
And it's very important when it comes to photo detection, for example.

188
00:11:47,190 --> 00:11:50,688
And then you have multimodal similarity search.

189
00:11:50,856 --> 00:11:55,096
So it allows you to search over multiple types of data, text and images.

190
00:11:55,280 --> 00:11:58,800
And that can be really useful. When you do, then you can

191
00:11:58,832 --> 00:12:02,104
combine the two, you can combine multimodal similarity search with

192
00:12:02,144 --> 00:12:05,896
rag and then you have multimodal rag. That's what happens.

193
00:12:05,920 --> 00:12:08,564
Then that's also very useful.

194
00:12:09,504 --> 00:12:13,360
Instead of just giving text to your LLM or your rack system,

195
00:12:13,472 --> 00:12:16,778
you can give an image which is then transformed and then you

196
00:12:16,786 --> 00:12:20,194
can actually return an image. You can return text, you know, it can return whatever

197
00:12:20,234 --> 00:12:23,554
you want it to return. And that's like, it can be very, very useful,

198
00:12:23,594 --> 00:12:27,554
especially in the era of GPT four or Gemini

199
00:12:27,594 --> 00:12:31,146
1.5, you know, like with what was being announced, you can

200
00:12:31,170 --> 00:12:34,818
actually also do it on yourself. So yeah,

201
00:12:34,946 --> 00:12:37,890
those are the common use cases. So then how did it work? You know,

202
00:12:37,922 --> 00:12:40,930
how does it actually work? And now we're going to go a bit more into

203
00:12:40,962 --> 00:12:44,416
the details. So we're going to talk about the different,

204
00:12:44,560 --> 00:12:47,364
you know, the different similarity search.

205
00:12:48,424 --> 00:12:51,800
How do you do similarity search? So let's go.

206
00:12:51,992 --> 00:12:55,696
We have here an example entry and this is what you

207
00:12:55,720 --> 00:12:58,752
may store in a vector database. So on this one,

208
00:12:58,888 --> 00:13:02,368
the two most important pieces are the id

209
00:13:02,536 --> 00:13:06,216
and the embeddings. Id is the way the vector database is able to

210
00:13:06,240 --> 00:13:09,584
have a unique id on your entry and embedding is the vector

211
00:13:09,624 --> 00:13:13,568
embedding. And this is what gets compared when we go and search with a

212
00:13:13,576 --> 00:13:17,544
vector database. So they basically like

213
00:13:17,704 --> 00:13:21,752
mailbis and vector database in general. They are specific kind of database that are

214
00:13:21,888 --> 00:13:24,568
automate, making everything automatic for you.

215
00:13:24,736 --> 00:13:28,312
So you compare embeddings and then you gonna find the

216
00:13:28,328 --> 00:13:31,204
best one, the closest one to you, and that's what we do.

217
00:13:31,944 --> 00:13:35,720
Then here you also have a bunch of metadata fields where

218
00:13:35,752 --> 00:13:38,272
you may filter on those as well. If you want to, if you want to

219
00:13:38,288 --> 00:13:41,560
reduce your query, if you want to lose your search

220
00:13:41,592 --> 00:13:45,120
when you run a query. So let's say, I don't know. Here you only want

221
00:13:45,152 --> 00:13:48,536
publication from towards data science, then you can put a filter for

222
00:13:48,560 --> 00:13:52,392
that one. If you want publication from a different one,

223
00:13:52,448 --> 00:13:55,560
then you can also put a filter on that one and that can allow you

224
00:13:55,632 --> 00:13:58,976
to have better search results. And how does

225
00:13:59,000 --> 00:14:02,944
it work? How do you know that something is similar to something else?

226
00:14:03,104 --> 00:14:06,862
Well, that's the whole concept of similarity search.

227
00:14:07,048 --> 00:14:10,054
And so what is it, how does it work?

228
00:14:10,354 --> 00:14:13,866
Usually as a basic rule of thumb,

229
00:14:14,050 --> 00:14:17,466
you always have similarity metrics for your index.

230
00:14:17,570 --> 00:14:20,614
And people always ask me, oh, which one should I use?

231
00:14:21,194 --> 00:14:25,242
Usually match the one that was used to train your embedding model.

232
00:14:25,378 --> 00:14:28,938
It can be written on hugging face, for example,

233
00:14:28,986 --> 00:14:32,454
depending on the model you use or the research paper or something.

234
00:14:32,754 --> 00:14:36,374
So yeah, if it's been trained with cosine, go with cosine.

235
00:14:36,924 --> 00:14:39,860
So yeah, just try to match the one that has been trained for your embedding

236
00:14:39,892 --> 00:14:42,940
models. And so how does it work?

237
00:14:43,052 --> 00:14:46,892
So as an overview of the search, so you

238
00:14:46,908 --> 00:14:50,660
know, like here you have a text which is find the two most similar images,

239
00:14:50,732 --> 00:14:54,148
documents or video. Then that is processed through an

240
00:14:54,156 --> 00:14:57,588
embedding model. You get a vector and then you're going

241
00:14:57,596 --> 00:15:01,580
to find the most similar ones. So you can see like the most similar ones

242
00:15:01,772 --> 00:15:05,228
to our request is the first one in the database and the second to last

243
00:15:05,276 --> 00:15:09,864
one. And that's what basically is happening. Then it will give you results back.

244
00:15:10,764 --> 00:15:13,980
And yeah, that's in a nutshell, that's how it works with vector

245
00:15:14,012 --> 00:15:17,092
search. But then, you know, I've been telling you, oh, I'm going to tell you

246
00:15:17,108 --> 00:15:20,864
about the metrics. Let's go over the metrics. You have the first one,

247
00:15:21,444 --> 00:15:25,564
which is called l two or euclidean, and this

248
00:15:25,724 --> 00:15:28,908
measures the distance in space. So if you

249
00:15:28,916 --> 00:15:32,528
have the right triangle, it measures direct distance in space.

250
00:15:32,636 --> 00:15:35,856
So you can imagine this as like the amount of space between

251
00:15:35,920 --> 00:15:39,280
two objects. So let's say how far from the screen I am with my

252
00:15:39,312 --> 00:15:42,880
face. That's what it's measuring. And one

253
00:15:42,912 --> 00:15:46,728
problem though, that this metric is sensitive to scale as well

254
00:15:46,736 --> 00:15:50,936
as the vector relative location in space. So what it means is

255
00:15:50,960 --> 00:15:54,632
that vectors with large values will have a larger

256
00:15:54,728 --> 00:15:58,354
euclidean distance than vectors with smaller values.

257
00:15:58,504 --> 00:16:02,274
Even if the vectors are very similar. That doesn't matter.

258
00:16:02,614 --> 00:16:06,686
So that's, one is like, in most cases, you don't

259
00:16:06,710 --> 00:16:10,918
use it with a deep learning model, but you would rather use it with more

260
00:16:10,966 --> 00:16:13,990
basic vector encoding models like LSH,

261
00:16:14,062 --> 00:16:17,782
which means locality sensitive hashing, you very likely

262
00:16:17,838 --> 00:16:20,954
won't see that one. If you work with LLMs in general,

263
00:16:21,814 --> 00:16:24,598
then you have the inner product one.

264
00:16:24,766 --> 00:16:27,854
So it's a bit more complicated than the Euclidean,

265
00:16:28,014 --> 00:16:30,914
it measures the projection of one line into the other.

266
00:16:31,374 --> 00:16:34,910
And the bigger the angle between the two vectors, the smaller the inner product.

267
00:16:35,102 --> 00:16:38,830
And so what we do is that, you know, we project from the origin these

268
00:16:38,862 --> 00:16:42,574
two points, and the inner product is the projection of this point onto

269
00:16:42,614 --> 00:16:45,554
another. And so it's like,

270
00:16:45,894 --> 00:16:48,990
sorry, it's like how do you project that out of the right angle? And this

271
00:16:49,022 --> 00:16:52,804
measure only not the angle difference between the two vectors, but also the

272
00:16:52,844 --> 00:16:56,092
space distance between the two vectors. So it measures the

273
00:16:56,108 --> 00:16:59,644
difference in orientation and magnitude. And l two is only

274
00:16:59,684 --> 00:17:03,188
magnitude. And then the last one, which is the most famous

275
00:17:03,236 --> 00:17:07,524
one, very likely it's cosine. And this one measures orientation.

276
00:17:07,604 --> 00:17:10,744
So cosine is the difference in angles between two vectors.

277
00:17:11,204 --> 00:17:14,684
And you just measure how big the angle is between your two vectors, you know,

278
00:17:14,764 --> 00:17:17,946
and this one is not affected by the size of the vector,

279
00:17:18,050 --> 00:17:21,186
but only by the angle between them. So that's

280
00:17:21,210 --> 00:17:24,106
like, that's like a really good one, you know, like no matter, like how big

281
00:17:24,130 --> 00:17:27,586
your vector is, you know, it doesn't really matter. And this one is

282
00:17:27,610 --> 00:17:31,866
mostly used in NLP. So usually you see like formatting

283
00:17:31,890 --> 00:17:36,370
models, if you work with LLMs, they're going to use cosine and

284
00:17:36,402 --> 00:17:40,162
base also, if you, you may have noticed, but cosine is normalized in a

285
00:17:40,178 --> 00:17:43,490
product. So yeah, if the model was trained using cosine

286
00:17:43,522 --> 00:17:46,586
similarity, you can either use cosine similarity

287
00:17:46,690 --> 00:17:49,484
or you can normalize and you that our product.

288
00:17:50,184 --> 00:17:53,632
So it's usually not suitable though, when you have data where

289
00:17:53,648 --> 00:17:57,204
the magnitude of vector is very important. So you have to take that into account.

290
00:17:58,064 --> 00:18:01,684
For example, it's not really appropriate to,

291
00:18:02,064 --> 00:18:05,952
if you want to use it, to compare the similarity of image embeddings based

292
00:18:06,048 --> 00:18:09,564
on pixel intensities, for example. So yeah,

293
00:18:10,464 --> 00:18:14,796
you may like, you know, choose your different types for

294
00:18:14,860 --> 00:18:18,132
the embeddings. You have sparse embeddings, depending on the

295
00:18:18,148 --> 00:18:21,676
model you have splayed, you have BG, m three, and then

296
00:18:21,700 --> 00:18:24,508
you have some specific kind of index that will go with it very well.

297
00:18:24,596 --> 00:18:28,156
If you have dense embeddings with OpenAI

298
00:18:28,260 --> 00:18:31,804
cohere, different models

299
00:18:31,844 --> 00:18:35,372
like that, then you have different distances you have IPL two,

300
00:18:35,388 --> 00:18:39,516
or cosine, and then you can use face or H

301
00:18:39,580 --> 00:18:43,294
and SW. And then you have binary as well embeddings.

302
00:18:43,674 --> 00:18:46,854
So those ones are like also cohere as binary embeddings.

303
00:18:47,434 --> 00:18:51,082
And then, yeah, you may use different indexes and

304
00:18:51,098 --> 00:18:54,898
then a different distance. So that's kind of it for the

305
00:18:54,986 --> 00:18:58,954
similarity measures. Now we go into

306
00:18:58,994 --> 00:19:02,254
a bit like a bit deeper. We go into indexes.

307
00:19:02,754 --> 00:19:06,106
So keep in mind, all of them have the pros and cons.

308
00:19:06,250 --> 00:19:09,904
There's not one index that is absolutely the best for everything.

309
00:19:10,064 --> 00:19:12,416
That is going to be the best is going to be the cheapest. That is

310
00:19:12,440 --> 00:19:16,552
going to have the best recall. That doesn't happen. So what it indexes

311
00:19:16,608 --> 00:19:20,304
first is that it allows you to facilitate efficient

312
00:19:20,344 --> 00:19:23,524
searching and retrieval of similar embeddings.

313
00:19:24,144 --> 00:19:27,604
It's a special data structure that is built on top of the embeddings.

314
00:19:28,064 --> 00:19:31,280
So that's basically what index is. You have

315
00:19:31,312 --> 00:19:34,320
different strategies. I have tree based index,

316
00:19:34,392 --> 00:19:37,314
graph based, hash based and cluster based.

317
00:19:37,934 --> 00:19:41,166
Later I will only talk about graph based and cluster base.

318
00:19:41,350 --> 00:19:45,314
You can look for the others, but it's not really relevant for us.

319
00:19:47,534 --> 00:19:50,674
Those are some of the indexes that milve support.

320
00:19:51,094 --> 00:19:55,154
So you can see we have a lot of them. But no worries,

321
00:19:55,854 --> 00:19:59,086
there are many options and as I've said before, they all have their pros

322
00:19:59,110 --> 00:20:02,628
and cons. So I'm going to go a bit deeper now into

323
00:20:02,676 --> 00:20:06,584
the different indexes so you can have like an understanding of how they work.

324
00:20:07,244 --> 00:20:11,252
All right, so the first one is the flat index and

325
00:20:11,308 --> 00:20:14,876
this one is quite straightforward, I would say. And it's

326
00:20:14,900 --> 00:20:19,140
a bit more elementary similarity search, you know. So it utilizes

327
00:20:19,212 --> 00:20:22,184
the knee algorithm,

328
00:20:23,244 --> 00:20:26,788
or also called KNN. And basically

329
00:20:26,876 --> 00:20:31,020
if you consider a database with a collection of like a hundred embeddings and

330
00:20:31,132 --> 00:20:34,644
a query embedding, what does we're going to call Q as

331
00:20:34,684 --> 00:20:37,900
query. Then if you want to find the most similar embeddings

332
00:20:37,932 --> 00:20:41,724
to queue, then KNN will calculate the distance between queue and each

333
00:20:41,804 --> 00:20:45,156
embedding in the database using a specific metrics

334
00:20:45,180 --> 00:20:48,852
that you define as well. Then it's going to identify the embedding

335
00:20:48,868 --> 00:20:51,744
of the smallest distance as the most similar to Q.

336
00:20:52,164 --> 00:20:56,538
And it works amazing for accuracy. It's very accurate.

337
00:20:56,716 --> 00:21:00,150
The problem is that it's an exhaustive search.

338
00:21:00,302 --> 00:21:04,606
So scalability is a concern and

339
00:21:04,790 --> 00:21:07,594
it's very accurate, but then it's very slow.

340
00:21:08,454 --> 00:21:11,766
Also, the time complexity actually increases linearly with the number of

341
00:21:11,790 --> 00:21:15,246
vector embeddings in the database. So that's flat.

342
00:21:15,430 --> 00:21:18,954
It's very straightforward, but it doesn't really scale.

343
00:21:19,774 --> 00:21:23,146
Then you have inverted file flat, which is also called

344
00:21:23,210 --> 00:21:26,570
IVF flat, and it's very similar to flats,

345
00:21:26,722 --> 00:21:29,970
but instead of using k nearest neighbors it's

346
00:21:30,002 --> 00:21:33,454
using Ann algorithm which is approximate nearest neighbors.

347
00:21:34,314 --> 00:21:37,962
So how it works is that it divides

348
00:21:38,058 --> 00:21:41,642
embeddings into several partitions and those partitions

349
00:21:41,738 --> 00:21:45,146
don't intersect as well. That's very important. Each partition

350
00:21:45,210 --> 00:21:49,208
has a centroid and every vector embedding in

351
00:21:49,216 --> 00:21:52,496
the databases associated with a specific partition that

352
00:21:52,520 --> 00:21:55,124
is based on the nearest centroid.

353
00:21:55,744 --> 00:21:59,112
So when you have a query queue or query embedding queue that

354
00:21:59,128 --> 00:22:03,176
is provided, IVF will only calculate

355
00:22:03,200 --> 00:22:06,840
the distance between queue and each centroid rather than the

356
00:22:06,872 --> 00:22:10,072
entire set of embeddings that you have in your database. So you already have a

357
00:22:10,088 --> 00:22:13,472
tremendous gain of time. And then the

358
00:22:13,488 --> 00:22:17,158
centroid with the smallest distance is then selected and the embeddings

359
00:22:17,206 --> 00:22:21,274
associated with that partition are used as candidate for the final search. So then once

360
00:22:22,414 --> 00:22:25,686
you know which entry to use, then you can use and search

361
00:22:25,710 --> 00:22:29,574
for all the embeddings that are in this partition. So that's

362
00:22:29,614 --> 00:22:33,614
nice, like it really speeds up the sales process. You may have a problem

363
00:22:33,694 --> 00:22:37,606
though, which is the candidate that is found might not

364
00:22:37,630 --> 00:22:41,194
be the nearest neighbor. So then you can play with the hyper parameters

365
00:22:41,974 --> 00:22:45,684
and that's basically what you can do. Then it's like you can increase

366
00:22:46,344 --> 00:22:50,444
the number of, like there is a hyper parameter that is called NPRoB

367
00:22:50,864 --> 00:22:54,952
which means the number of partitions that you may search. So yeah,

368
00:22:55,048 --> 00:22:58,320
then you have more partitions that you can search and

369
00:22:58,352 --> 00:23:02,112
then it can really help you ensure that the nearest embedding to the

370
00:23:02,128 --> 00:23:05,484
query is not missed even if it's in a different partition.

371
00:23:06,224 --> 00:23:09,840
With that, the cost of search increases

372
00:23:09,872 --> 00:23:13,888
with time, but also it's going to be better than a

373
00:23:13,896 --> 00:23:17,880
flat anyway. Then you have another one which is

374
00:23:17,952 --> 00:23:21,684
inverted file with confisation. So it's the same,

375
00:23:22,744 --> 00:23:26,752
but when memory is limited then it may not be optimal

376
00:23:26,848 --> 00:23:30,616
because you might struggle with that. So then to

377
00:23:30,640 --> 00:23:34,208
address memory constraints, what we can do is that we

378
00:23:34,216 --> 00:23:37,124
can combine with IVF, we can combine,

379
00:23:37,944 --> 00:23:41,496
you're going to involve the mapping of the values in each vector dimension

380
00:23:41,640 --> 00:23:46,000
to lower precision integers. So it's usually

381
00:23:46,192 --> 00:23:49,768
what we call quantization. And there are two kinds of quantization.

382
00:23:49,856 --> 00:23:53,024
There's the first one which is scalar quantization, and the second

383
00:23:53,064 --> 00:23:54,444
one which is product quantization.

384
00:23:55,664 --> 00:23:59,416
So scalar quantization, how it works, like you're

385
00:23:59,440 --> 00:24:03,424
basically mapping flow to integers, but the first step is to determine

386
00:24:03,464 --> 00:24:07,158
and store the maximum and minimum values of each dimension

387
00:24:07,256 --> 00:24:10,874
of the vector in the database, because that's what you need to calculate the step

388
00:24:10,914 --> 00:24:14,442
size. The step size is crucial to scale the

389
00:24:14,458 --> 00:24:18,154
floating point numbers in each dimension to its integer

390
00:24:18,234 --> 00:24:21,858
integers representation. Sorry. So you can see the formula here. It's max value

391
00:24:21,906 --> 00:24:25,218
minus minimum value. And then you divide it

392
00:24:25,226 --> 00:24:27,774
by the number of bins, which is something that you decide.

393
00:24:28,594 --> 00:24:32,274
How it works is that, you know, like if you take the quantity quantized

394
00:24:32,354 --> 00:24:35,354
version of the any vector dimension,

395
00:24:35,894 --> 00:24:39,486
then you subtract the value of this nth dimension from its

396
00:24:39,510 --> 00:24:43,074
minimum value and then you divide the result by the step size.

397
00:24:43,574 --> 00:24:46,594
Then you have another one, which is product concentration.

398
00:24:47,134 --> 00:24:50,758
So this one, it addresses the limits, the limitation

399
00:24:50,846 --> 00:24:54,814
of the square quantization, you know. So you consider the distribution

400
00:24:54,854 --> 00:24:58,750
of each dimension. You then divide vector embeddings into sub

401
00:24:58,782 --> 00:25:02,670
vectors and you perform clustering within each sub vector

402
00:25:02,702 --> 00:25:06,054
to create centroids. And then you encode

403
00:25:06,134 --> 00:25:08,994
each subjector with the id of the nearest centroid.

404
00:25:10,014 --> 00:25:13,558
So again, you know, you have

405
00:25:13,606 --> 00:25:17,534
here you have the query vector, which is a float. Then you like,

406
00:25:17,654 --> 00:25:20,758
you consider the distribution, then you divide the vector

407
00:25:20,846 --> 00:25:24,262
into sub vectors and

408
00:25:24,278 --> 00:25:26,822
then you perform clustering, which is what you have on the image, you know,

409
00:25:26,838 --> 00:25:30,616
at the third point you really like, you perform clustering to

410
00:25:30,640 --> 00:25:34,416
create centroids and then you encode each subjector with

411
00:25:34,440 --> 00:25:37,888
an id of the nearest centroids. And that's how you

412
00:25:37,896 --> 00:25:41,924
can save a lot of memory. And it's really,

413
00:25:42,744 --> 00:25:46,224
you know, like project consolidation offer more powerful memory

414
00:25:46,264 --> 00:25:49,904
compression. So it can be really good if you have like

415
00:25:49,984 --> 00:25:51,964
massive memory constraints, for example.

416
00:25:52,824 --> 00:25:55,404
Then last but not least,

417
00:25:56,144 --> 00:26:02,240
it's usually called h and SWT

418
00:26:02,272 --> 00:26:05,632
mouthful. And this one is

419
00:26:05,648 --> 00:26:09,096
a graph based indexing algorithm. So it combines two key concepts.

420
00:26:09,160 --> 00:26:12,880
It combines skip list and navigable small world

421
00:26:13,032 --> 00:26:16,764
or otherwise NSW. So it's a graph. So skip list,

422
00:26:17,304 --> 00:26:21,032
it's a probabilistic, probabilistic data structure that

423
00:26:21,048 --> 00:26:24,228
is composed of multiple layers of linked list.

424
00:26:24,416 --> 00:26:27,024
And then NSW graphs.

425
00:26:27,964 --> 00:26:31,900
It's built by randomly shuffling data points and inserting them one

426
00:26:31,932 --> 00:26:35,784
by one. So each point is connected to a predefined number of edges.

427
00:26:36,284 --> 00:26:40,468
And then this creates a graph structure that exhibits the small world phenomenon

428
00:26:40,636 --> 00:26:44,092
where any two points are always

429
00:26:44,148 --> 00:26:47,732
connected through relatively a short path.

430
00:26:47,908 --> 00:26:51,284
And that's how it works, basically. So if we go for the skip list,

431
00:26:52,104 --> 00:26:55,552
so again, you know, probably probabilistic data structure that is

432
00:26:55,568 --> 00:26:58,752
composed of like multiple layers of linked lists.

433
00:26:58,768 --> 00:27:01,936
So the lowest layer, which is layer zero,

434
00:27:02,000 --> 00:27:05,184
here it contains all the original linked list with

435
00:27:05,224 --> 00:27:09,168
all the elements as we move to the higher layers. The linked

436
00:27:09,176 --> 00:27:13,128
lists progressively skip over different elements and

437
00:27:13,136 --> 00:27:17,248
then you have fewer and fewer elements with each layer which each higher

438
00:27:17,296 --> 00:27:20,376
layer, sorry. Then during the search process you

439
00:27:20,400 --> 00:27:23,728
start from the highest, you start from the highest layer and gradually

440
00:27:23,776 --> 00:27:26,896
you're going to descend to lower layer until you find the desired

441
00:27:26,960 --> 00:27:30,296
elements. And because of this skip list

442
00:27:30,320 --> 00:27:33,656
can, you know, like it can really bring some speeds to your

443
00:27:33,680 --> 00:27:37,644
search process. It really speeds up your search process. So as an example,

444
00:27:38,384 --> 00:27:41,192
let's say we have a skip list with three layers, as you can see here.

445
00:27:41,288 --> 00:27:44,204
And we have eight elements in the original linked list.

446
00:27:44,624 --> 00:27:48,000
If you want to find the element seven, then the sales

447
00:27:48,032 --> 00:27:51,360
process will look like we have on the image. So we're going to

448
00:27:51,432 --> 00:27:54,640
look and we're going to be like okay, so four, seven is higher than

449
00:27:54,672 --> 00:27:58,724
four, so we're going to go on the right side of four and then

450
00:27:59,064 --> 00:28:02,416
six is lower than seven. So then

451
00:28:02,480 --> 00:28:04,848
we're going to go on the right side. And that's how then you can find

452
00:28:04,896 --> 00:28:07,684
seven. And that's how it works basically for skip list.

453
00:28:08,184 --> 00:28:11,080
And then you have NSW graphs.

454
00:28:11,112 --> 00:28:14,770
So again, as I said, so they built randomly by shuffling data points

455
00:28:14,922 --> 00:28:18,202
and inserting them one by one. And then

456
00:28:18,218 --> 00:28:21,842
it creates a graph structure that exhibits some small worlds and

457
00:28:21,898 --> 00:28:25,466
any two points, they're always connected through a short path.

458
00:28:25,610 --> 00:28:28,334
So that's how it can be really good and really efficient.

459
00:28:29,314 --> 00:28:32,706
If I were to show you a bit of an example of how it works.

460
00:28:32,890 --> 00:28:36,474
So this is it. This is a multi layer

461
00:28:36,514 --> 00:28:40,110
graphs where the lower layers contains the complete

462
00:28:40,142 --> 00:28:43,726
set of data and the higher layer contains only a small fraction

463
00:28:43,750 --> 00:28:46,474
of the points. Again, as we've seen for the linked list,

464
00:28:47,254 --> 00:28:50,886
then HNSW starts by assigning

465
00:28:50,950 --> 00:28:54,750
a random integers number from zero to I to

466
00:28:54,782 --> 00:28:58,478
each data point. And those can be referred as nodes. We can call it

467
00:28:58,486 --> 00:29:01,862
in a graph algorithm, I is the maximum layer at

468
00:29:01,878 --> 00:29:05,304
which the data point can be present. So if a data point

469
00:29:05,724 --> 00:29:09,140
has like I two and the total number

470
00:29:09,172 --> 00:29:12,676
of layer and the graph is five, then the data point should only

471
00:29:12,700 --> 00:29:16,764
be present up until the second layer and then it shouldn't be available

472
00:29:16,924 --> 00:29:20,580
in the layer three and upwards. So yeah, during the search

473
00:29:20,612 --> 00:29:23,724
process how it works is HNSW

474
00:29:23,844 --> 00:29:27,716
starts by selecting an entry point and then this

475
00:29:27,740 --> 00:29:30,304
should be the data point that is presenting the highest layer.

476
00:29:30,664 --> 00:29:34,032
And then it searches for the nearest neighbor closer

477
00:29:34,048 --> 00:29:37,536
to the query point and then recursively continues the search in lower

478
00:29:37,560 --> 00:29:41,240
layers and again and again and again until the nearest data point

479
00:29:41,272 --> 00:29:45,672
is found. So now we have this image again so

480
00:29:45,808 --> 00:29:49,032
hopefully it makes a bit more sense. And again, you know,

481
00:29:49,048 --> 00:29:52,512
there's no like good or bad index. It really depends on

482
00:29:52,528 --> 00:29:56,404
your application requirements. So are you more interested in like query speeds?

483
00:29:57,524 --> 00:30:01,704
Do you have a lot of inserts or deletes? Are you constructing the machine type?

484
00:30:02,404 --> 00:30:04,024
It really, really depends.

485
00:30:06,244 --> 00:30:10,100
For example, if you need 100% recall then flat would be the best.

486
00:30:10,292 --> 00:30:14,624
Then depending on your size of index you might use a standard IVF.

487
00:30:14,964 --> 00:30:17,988
If your index is between 2gb and 20gb,

488
00:30:18,156 --> 00:30:21,344
maybe consider product consolidation or HNSW.

489
00:30:22,064 --> 00:30:25,752
And then you also have composite index and

490
00:30:25,768 --> 00:30:29,884
then you have disk based indexes as well, which can be really useful for you.

491
00:30:30,664 --> 00:30:34,200
And then I'll finish with that. So you can see the Milvis

492
00:30:34,232 --> 00:30:37,512
architecture is Milvis has been built to be

493
00:30:37,528 --> 00:30:40,872
like fully distributed. So that's what you see here. You can see like on

494
00:30:40,888 --> 00:30:44,312
the bottom part you see the s three storage, the object storage. If you

495
00:30:44,328 --> 00:30:47,920
go above you have the worker nodes and those are fully distributed.

496
00:30:47,952 --> 00:30:51,376
So you have the query nodes, the data nodes and the index node.

497
00:30:51,520 --> 00:30:54,808
And that's how we can then scale up to billions

498
00:30:54,936 --> 00:30:58,496
as thanks to those because they are fully independent, so we can scale them up

499
00:30:58,520 --> 00:31:02,424
and down. And that's kind of how Mailvis

500
00:31:02,464 --> 00:31:05,736
works. That's how vector database work. I hope this

501
00:31:05,760 --> 00:31:09,872
talk was useful to you. If you have any questions you

502
00:31:09,888 --> 00:31:13,584
can chat with me on discord directly. Also check out our GitHub

503
00:31:13,704 --> 00:31:17,288
movers is fully open source so you can check our repository.

504
00:31:17,456 --> 00:31:20,776
And yeah, if you have any questions, chat with me on discord. You can find

505
00:31:20,800 --> 00:31:24,368
me on LinkedIn as well. Thank you very much for watching this talk

506
00:31:24,456 --> 00:31:25,736
and I hope you have a good day.

