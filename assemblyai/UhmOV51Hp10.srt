1
00:00:24,570 --> 00:00:28,294
Hi there, thanks for joining the session. Today I'm going to be sharing how you

2
00:00:28,332 --> 00:00:31,558
can get started on AWS for building and

3
00:00:31,644 --> 00:00:35,714
orchestrating serverless workflows for generative AI generative

4
00:00:35,762 --> 00:00:38,918
AI has taken the world by storm. We are seeing a massive shift in the

5
00:00:38,924 --> 00:00:42,706
way applications are being built. A lot of this is through consumer facing

6
00:00:42,738 --> 00:00:45,602
services that have come out like chat, JPT by Openei,

7
00:00:45,666 --> 00:00:48,726
cloud by anthropic, and we are able to

8
00:00:48,748 --> 00:00:52,574
see and experience how powerful latest machine learning models have

9
00:00:52,612 --> 00:00:56,014
become. Generative AI is a type of AI that can create new

10
00:00:56,052 --> 00:00:59,210
content and ideas, including conversations, stories, images,

11
00:00:59,290 --> 00:01:03,018
videos and music. Like all AI, generative AI is powered

12
00:01:03,034 --> 00:01:06,254
by machine learning models. But generative AI is powered by

13
00:01:06,292 --> 00:01:10,002
very large models that are pretrained on vast amounts of data and

14
00:01:10,056 --> 00:01:13,266
commonly referred to as foundational models. Now, throughout the session and

15
00:01:13,288 --> 00:01:16,882
also in conversation that you'll have out there, you'll see foundational models

16
00:01:17,026 --> 00:01:20,546
being interchangeably used with LLMs. Large language models

17
00:01:20,658 --> 00:01:24,662
just to understand LLMs are a subset of foundational models where LLMs focus

18
00:01:24,796 --> 00:01:28,946
on text. Specifically. There have been some amazing breakthroughs

19
00:01:28,978 --> 00:01:32,106
through using foundational models in different industries. A couple

20
00:01:32,128 --> 00:01:34,998
of these are where we see impacts in life sciences,

21
00:01:35,094 --> 00:01:38,346
with drug discovery being powered by Genei. This has

22
00:01:38,368 --> 00:01:41,610
enabled researchers to understand things like protein synthesis.

23
00:01:42,110 --> 00:01:45,502
In financial services, we see Genei being used to help create

24
00:01:45,556 --> 00:01:48,858
highly tailored investment strategies that are aligned to individuals,

25
00:01:48,954 --> 00:01:52,350
their risk, appetite, and also financial goals that they want to achieve.

26
00:01:52,770 --> 00:01:55,906
In healthcare, we have seen how physicians and

27
00:01:55,928 --> 00:01:59,538
clinicians can use this to enhance medical images and

28
00:01:59,624 --> 00:02:03,150
also to aid in better diagnosis. Think like a medical assistant.

29
00:02:03,310 --> 00:02:06,754
And in the retail space we see teams generating high

30
00:02:06,792 --> 00:02:10,342
quality product descriptions and listings based on product data that they already

31
00:02:10,396 --> 00:02:13,746
have. Now you'll notice a lot of the use cases

32
00:02:13,778 --> 00:02:17,478
for generative AI are for enhancing existing processes or experience

33
00:02:17,564 --> 00:02:21,514
that are already there. A question that usually comes is we

34
00:02:21,552 --> 00:02:24,998
already have services and applications that are out there. How do we take generative AI

35
00:02:25,014 --> 00:02:28,378
and then add that to enhance the experience versus rewriting everything from

36
00:02:28,464 --> 00:02:31,950
scratch? Now, to understand this, what you need to also

37
00:02:32,020 --> 00:02:35,002
understand is how you view generative AI.

38
00:02:35,146 --> 00:02:38,554
So from AWS's perspective, Gen AI

39
00:02:38,602 --> 00:02:42,206
has three macro layers and these three are equally important to us and we are

40
00:02:42,228 --> 00:02:45,986
investing in all of them. The bottom layer is the infrastructure. This is used to

41
00:02:46,008 --> 00:02:50,350
train foundational models and then run these models in production.

42
00:02:50,510 --> 00:02:54,478
Then you have the middle layer that provides access to these large language models

43
00:02:54,574 --> 00:02:57,766
and other FMs that you need and the tools that you need to build and

44
00:02:57,788 --> 00:03:01,126
scale generative AI applications which then use the LLMs under

45
00:03:01,148 --> 00:03:04,578
the hood. Then at the top layer you have applications

46
00:03:04,594 --> 00:03:08,034
that are built leveraging foundational models so they take advantage

47
00:03:08,082 --> 00:03:11,946
of Gen AI quickly and you don't need to have any

48
00:03:11,968 --> 00:03:16,106
specialized knowledge. Now, when you take this and map this against the

49
00:03:16,128 --> 00:03:19,546
services that we provide from AWS, you kind of

50
00:03:19,568 --> 00:03:23,326
see that the three stacks are kind of neatly segregated. At the

51
00:03:23,348 --> 00:03:27,120
lowest layer of the stack is the infrastructure. This is basically where you get

52
00:03:27,730 --> 00:03:31,134
to build cost effective foundational models. You train

53
00:03:31,172 --> 00:03:34,586
them and then you can deploy them at scale. This gives you access to our

54
00:03:34,628 --> 00:03:38,322
hardware, accelerators and GPUs. And also you get access to services

55
00:03:38,376 --> 00:03:42,162
like Amazon Sagemaker that enables ML practitioners and your teams to

56
00:03:42,216 --> 00:03:45,486
build, train and deploy LLMs and foundational models.

57
00:03:45,678 --> 00:03:49,606
Then at the middle layer we have Amazon bedrock. This provides access to

58
00:03:49,628 --> 00:03:53,254
all LLMs and other foundational models that you need to build

59
00:03:53,292 --> 00:03:57,158
and scale generative AI applications without you managing the whole

60
00:03:57,244 --> 00:04:00,646
infrastructure behind it, right? Without you actually managing the scale side of

61
00:04:00,668 --> 00:04:04,550
things. Think serverless, but for machine learning models for FMS,

62
00:04:04,630 --> 00:04:07,898
basically, then at the top layer are applications that help you to

63
00:04:07,904 --> 00:04:11,062
take advantage of Genei quickly as part of your day to day operations.

64
00:04:11,206 --> 00:04:14,586
This includes services like Amazon Q, our new generative AI

65
00:04:14,618 --> 00:04:18,026
powered assistant that is tailored to your business. So think like Personas,

66
00:04:18,138 --> 00:04:22,714
which are business users, data users, or even developers.

67
00:04:22,842 --> 00:04:26,638
You could use Q as part of AWS, a plugin that's

68
00:04:26,654 --> 00:04:30,386
already available for certain services, and then afterwards use that to get

69
00:04:30,408 --> 00:04:33,726
an enhanced operation capability.

70
00:04:33,918 --> 00:04:37,106
Each of these layers builds on the other, and you may need some or all

71
00:04:37,128 --> 00:04:40,590
of these capabilities at different points in your generative AI journey. A lot

72
00:04:40,600 --> 00:04:43,318
of what you see is in an organization, you'll have a mix of Personas that

73
00:04:43,324 --> 00:04:46,946
would use all three layers. You use specific services from those three layers

74
00:04:46,978 --> 00:04:49,670
to enhance productivity.

75
00:04:50,170 --> 00:04:54,326
Now, Amazon Bedrock is the easiest way to build and scale generative AI applications

76
00:04:54,358 --> 00:04:57,962
with foundational models. This is a fully managed service so you can get started

77
00:04:58,016 --> 00:05:01,386
quickly and you can find the right model based on the use case that you

78
00:05:01,408 --> 00:05:04,590
have. You can then also customize your model with your own data,

79
00:05:04,660 --> 00:05:08,218
and you can do this privately. Nothing feeds your data back to the base models,

80
00:05:08,234 --> 00:05:11,614
which then other customers would also have access to.

81
00:05:11,652 --> 00:05:15,026
This doesn't happen and you have the tools that you need to combine the

82
00:05:15,048 --> 00:05:18,494
power of foundational models with your organization data and execute

83
00:05:18,542 --> 00:05:22,354
complex tasks. All of this is with security, privacy and

84
00:05:22,392 --> 00:05:26,526
responsible EI safety, which you need to then put generative AI

85
00:05:26,558 --> 00:05:30,086
into production for your users. Now, there's a lot of

86
00:05:30,108 --> 00:05:33,654
models that are out there and from Amazon bedrock. These are

87
00:05:33,692 --> 00:05:36,774
a couple of models that we provide and one of the reasons that we went

88
00:05:36,812 --> 00:05:39,894
with this model is because everything's moving fast.

89
00:05:40,012 --> 00:05:43,494
Experimenting and learning is the key right now and also generative.

90
00:05:43,542 --> 00:05:47,094
AI as a technology is also evolving quickly with new developments.

91
00:05:47,222 --> 00:05:50,822
Now when things are moving so fast, the ability to adapt is the most valuable

92
00:05:50,886 --> 00:05:54,118
capability that you can have. There is not going to be one

93
00:05:54,144 --> 00:05:57,614
model to rule them all, and certainly not one company providing the models that

94
00:05:57,652 --> 00:06:01,274
everyone uses. So you don't want a cloud provider who is beholden

95
00:06:01,322 --> 00:06:04,766
primarily to one model provider. You need to be able to try out

96
00:06:04,788 --> 00:06:08,494
different models. You should be able to switch between them rapidly

97
00:06:08,542 --> 00:06:12,162
based on the use cases, or even combine multiple models within a

98
00:06:12,216 --> 00:06:15,934
certain use case. You need a real choice of model providers, AWS. You decide

99
00:06:15,982 --> 00:06:19,654
who has the best technology. This is kind of like where we have

100
00:06:19,692 --> 00:06:23,154
seen based on our building services that we want to provide the choice

101
00:06:23,202 --> 00:06:27,442
to customers, which is you. This is why we provide

102
00:06:27,516 --> 00:06:31,274
through Pedrog, access to wide range of foundational models from

103
00:06:31,312 --> 00:06:35,770
leaders like AI 21 labs, anthropic coher stability AI,

104
00:06:36,270 --> 00:06:40,006
also access to our own foundational models like Amazon

105
00:06:40,038 --> 00:06:43,774
Titan. And the idea is that we provide an API as

106
00:06:43,812 --> 00:06:47,706
part of this. So there is a layer, an API layer that provides

107
00:06:47,738 --> 00:06:51,386
you access to the large language models under the hood or the foundational models.

108
00:06:51,498 --> 00:06:54,846
And all you do is as a user or probably as a

109
00:06:54,868 --> 00:06:58,146
developer, you create the prompts in a certain format based on what

110
00:06:58,168 --> 00:07:02,366
the foundational model expects. You take that prompt or text embeddings

111
00:07:02,398 --> 00:07:05,906
if you want to tune that model a bit more, and then afterwards send

112
00:07:05,928 --> 00:07:09,406
that to the API layer and you can then get your responses

113
00:07:09,438 --> 00:07:12,246
back and then use that as part of your applications. Now there are a couple

114
00:07:12,268 --> 00:07:15,798
of ways you can use bedrock. One of the ways customers

115
00:07:15,884 --> 00:07:19,206
usually start is by writing code. And the way you

116
00:07:19,228 --> 00:07:23,178
integrate with Amazon bedrock is that you can use the SDK, right? So you

117
00:07:23,184 --> 00:07:26,342
use the APIs and then afterwards access the foundational models.

118
00:07:26,406 --> 00:07:30,810
So you load the libraries that has a bedrock API and then afterwards you

119
00:07:30,880 --> 00:07:34,518
can also access data in other places like an

120
00:07:34,544 --> 00:07:37,498
S three bucket. If you have data that's bigger than what's normal,

121
00:07:37,594 --> 00:07:41,226
you can then access it in s three bucket for input and even for output.

122
00:07:41,338 --> 00:07:44,866
You can then prepare the input and then handle the JSON to bring convert and

123
00:07:44,888 --> 00:07:48,722
then afterwards decode the responses. If the return data

124
00:07:48,776 --> 00:07:52,562
is image, it's an image of sorts. You can then store

125
00:07:52,616 --> 00:07:55,822
that in an S three bucket. Then if you have retries,

126
00:07:55,886 --> 00:07:59,566
then you'll have to do retry logic inside, and then afterwards,

127
00:07:59,598 --> 00:08:02,438
if you have any errors, you may have to have a certain condition, so on

128
00:08:02,444 --> 00:08:05,078
and so forth. You kind of get an ideas of what happens with code in

129
00:08:05,084 --> 00:08:09,430
general. Now, this is what the code would look like, but how

130
00:08:09,500 --> 00:08:12,714
do we actually look at providing simpler integration without

131
00:08:12,832 --> 00:08:16,442
writing a lot of code? And for this, you need to also understand

132
00:08:16,576 --> 00:08:20,618
the whole idea of sequencing. Right. How do you coordinate between multiple services?

133
00:08:20,704 --> 00:08:23,690
Because a lot of organizations don't just have one specific app,

134
00:08:23,760 --> 00:08:27,614
they would have probably a plethora of apps that power their business.

135
00:08:27,812 --> 00:08:30,974
And you want to understand how these services are going to talk to each other

136
00:08:31,012 --> 00:08:34,634
in a reliable and understandable way, because business processes

137
00:08:34,682 --> 00:08:38,274
usually exhibit different patterns based on the inputs that are coming

138
00:08:38,312 --> 00:08:41,954
in and what needs to be accomplished. Sometimes things need to be done

139
00:08:41,992 --> 00:08:45,326
sequentially. So in this case, let's say you have a number of lambda functions.

140
00:08:45,358 --> 00:08:48,850
So we'll use lambda as a proxy to understand this for different services.

141
00:08:49,000 --> 00:08:51,766
So you have a lambda one, and then you have a lambda two. Now,

142
00:08:51,788 --> 00:08:54,626
this is easy enough because you can have these in sequence.

143
00:08:54,658 --> 00:08:58,326
So lambda one invokes lambda two. But what if you have more than two

144
00:08:58,348 --> 00:09:01,706
lambda functions? What if instead of calling lambda two, you need

145
00:09:01,728 --> 00:09:05,434
lambda one to also call lambda seven before calling another service, or before

146
00:09:05,472 --> 00:09:08,714
calling a foundational model in this case. Now,

147
00:09:08,752 --> 00:09:12,382
if one of these services or functions fail, there's no easy

148
00:09:12,436 --> 00:09:15,466
recovery mechanism, and reprocessing previously executed

149
00:09:15,498 --> 00:09:19,310
steps becomes difficult. So we add some persistence inside.

150
00:09:19,460 --> 00:09:23,066
That's the next step. You have persistence because you have all these executions

151
00:09:23,098 --> 00:09:26,722
happening behind the scenes. And this way we can

152
00:09:26,776 --> 00:09:30,420
deal with state, right? Try to manage some kind of a coordination, try to understand

153
00:09:30,870 --> 00:09:34,178
which service is being executed at this point of time for this

154
00:09:34,264 --> 00:09:37,714
whole execution flow that's happening now. Because of this,

155
00:09:37,752 --> 00:09:41,394
you have to also collaborate all these functions. You need to manage this persistence mechanism.

156
00:09:41,442 --> 00:09:44,966
And there's no elegant way of coordinating flow or error handling between these

157
00:09:44,988 --> 00:09:49,138
services. And not every process is sequential.

158
00:09:49,234 --> 00:09:53,046
So, for example, you could also have certain processes that need to run in parallel,

159
00:09:53,158 --> 00:09:57,018
or perhaps it can follow different paths based on the input or what happens in

160
00:09:57,024 --> 00:10:00,646
an earlier step. That's a lot harder to do, and it gets even harder

161
00:10:00,678 --> 00:10:04,506
the more successful you are, because more people want to use the flow processes you've

162
00:10:04,538 --> 00:10:08,110
built out. You need to be able to handle errors as they occur.

163
00:10:08,610 --> 00:10:12,126
And this could be things like retrying calls, or it

164
00:10:12,148 --> 00:10:15,658
could be something as simple as following a different path in your workflow.

165
00:10:15,834 --> 00:10:19,426
All in said, this is all things that you can still do in code.

166
00:10:19,528 --> 00:10:22,754
This is something that has been done in code for quite some time. But what

167
00:10:22,792 --> 00:10:26,674
if your flow also needs a human as part of the process? For example,

168
00:10:26,712 --> 00:10:29,606
you need a human to review the output of a previous task to see if

169
00:10:29,628 --> 00:10:33,458
it's accurate, like a spot check for example. Or you've

170
00:10:33,474 --> 00:10:36,806
built out an application processing flow where the customer has requested a

171
00:10:36,828 --> 00:10:40,402
credit limit that exceeds the specified auto approved threshold.

172
00:10:40,466 --> 00:10:44,330
And then you need somebody else to come in and then afterwards review

173
00:10:44,400 --> 00:10:47,434
that request, and then after say okay, yes or no, depending on other

174
00:10:47,472 --> 00:10:51,178
data that they have. So that application needs to be routed to a

175
00:10:51,184 --> 00:10:54,214
human for this to work, and this continues.

176
00:10:54,262 --> 00:10:58,238
So as long as you have business processes that need to emulate what happens in

177
00:10:58,244 --> 00:11:01,518
the real world, you're going to have this amount of complexity that you

178
00:11:01,524 --> 00:11:05,482
need to build as part of your applications. So one approach to

179
00:11:05,556 --> 00:11:08,962
manage this complexity is that you don't have to write a lot of code

180
00:11:09,016 --> 00:11:13,026
and communication. Instead, try to visualize your sequences as

181
00:11:13,128 --> 00:11:16,514
part of a workflow. And this is where AWS step functions comes

182
00:11:16,552 --> 00:11:20,086
in. Step functions is service that allows you to create workflows. These are

183
00:11:20,108 --> 00:11:23,158
workflows that allow you to move output of one step to the input of the

184
00:11:23,164 --> 00:11:26,786
next step. You can arrange these in a workflow with conditional logic

185
00:11:26,818 --> 00:11:30,470
branches, parallel states, tools, a map state, or even

186
00:11:30,540 --> 00:11:33,546
specify wait states, like for example if you're running a job and then you need

187
00:11:33,568 --> 00:11:37,146
to wait for a certain period. Over here

188
00:11:37,168 --> 00:11:41,194
you can see a bit of an animation that shows you

189
00:11:41,392 --> 00:11:44,734
how you can choose a service. You then can then

190
00:11:44,932 --> 00:11:48,318
drag it from the left and then after put in the design view. Then the

191
00:11:48,324 --> 00:11:51,930
logic gets added. Then each step or action the workflow is configured.

192
00:11:52,090 --> 00:11:55,886
This also helps you to visualize how you can provide error handling

193
00:11:55,998 --> 00:11:59,502
and also specify retry and backup strategy.

194
00:11:59,646 --> 00:12:03,086
Step functions is serverless, so you only pay for what you use. It scales

195
00:12:03,118 --> 00:12:06,594
automatically, which also means that you can scale to zero.

196
00:12:06,712 --> 00:12:10,546
You're not paying when it's not being invoked. This is fully managed and provides

197
00:12:10,578 --> 00:12:14,578
a visual building experience using a drag and drop interface called workflow Studio.

198
00:12:14,754 --> 00:12:18,214
The visualization experience extends beyond building because when you

199
00:12:18,252 --> 00:12:21,562
run your workflow you can also visualize its progress with each step,

200
00:12:21,616 --> 00:12:24,794
changing colors as it moves forward and under

201
00:12:24,832 --> 00:12:28,086
the hood. What happens is this is using code which is using Amazon

202
00:12:28,118 --> 00:12:32,054
State's language, which is ESL. ESL is a domain specific language

203
00:12:32,102 --> 00:12:35,902
and it's JsoN based. So you can then declaratively create your

204
00:12:36,036 --> 00:12:39,520
workflows. So you provide that and we'll show some examples later.

205
00:12:39,970 --> 00:12:42,906
You can then take that ESL and then add that as part of your deployment

206
00:12:42,938 --> 00:12:46,586
pipelines so you can commit it to your repositories. You can also make pull requests

207
00:12:46,618 --> 00:12:49,406
on this so that other team members can collaborate.

208
00:12:49,598 --> 00:12:52,878
Now one of the things customers have told us with step functions, because step functions

209
00:12:52,894 --> 00:12:57,140
has been there for a few years, is that it integrates natively with 220 services

210
00:12:57,590 --> 00:13:00,486
and you can choose a service that you need to use as part of your

211
00:13:00,508 --> 00:13:04,006
workflow and take advantage of the benefits. Now the

212
00:13:04,028 --> 00:13:07,730
way step functions integrates with these services is through two ways.

213
00:13:07,820 --> 00:13:11,370
First is SDK integrations and the second is optimized integrations.

214
00:13:11,870 --> 00:13:15,766
SDK integrations, as the name applies, are provided

215
00:13:15,798 --> 00:13:19,334
by step functions by directly integrating with the AWS SDK.

216
00:13:19,462 --> 00:13:22,566
So that's over 10,000 API actions that you can use directly

217
00:13:22,598 --> 00:13:25,774
from your workflow without the need to write any customer integration code.

218
00:13:25,812 --> 00:13:29,418
Think blue code, which a lot of folks when they write serverless

219
00:13:29,514 --> 00:13:33,038
applications with lambda you tend to write. You can remove a

220
00:13:33,044 --> 00:13:36,518
lot of that just by using step functions. The other one is optimize

221
00:13:36,554 --> 00:13:40,322
integrations. Now the way they differ from SDK integrations is that each

222
00:13:40,376 --> 00:13:44,318
action has been customized to provide additional functionality for your workflow.

223
00:13:44,334 --> 00:13:47,586
So beyond just the API call, you also get certain things like for

224
00:13:47,608 --> 00:13:51,158
example where an API output is being converted from an

225
00:13:51,164 --> 00:13:54,514
escape JSON to a json object. So depending on the kind of integration

226
00:13:54,562 --> 00:13:58,422
that's bring, provided, those optimized integrations have that added value

227
00:13:58,556 --> 00:14:02,122
needed so that you don't have to then write extra code for

228
00:14:02,176 --> 00:14:05,862
maybe doing those manipulations. Now with any workflow

229
00:14:06,006 --> 00:14:09,574
and orchestration around, you need to have certain patterns that are provided,

230
00:14:09,622 --> 00:14:12,894
and these integration patterns by default are

231
00:14:12,932 --> 00:14:16,666
something that API actions can be provided

232
00:14:16,698 --> 00:14:20,346
with. So when you specify your workflow by default,

233
00:14:20,378 --> 00:14:23,946
it is asynchronous so the workflow doesn't wait or block for the action

234
00:14:23,978 --> 00:14:28,042
to complete. This is what you call as a standard request response call pattern.

235
00:14:28,186 --> 00:14:31,358
So you start the task or the work to be done and the workflow doesn't

236
00:14:31,374 --> 00:14:34,606
wait for complete, it moves on to the next step. This is great because it's

237
00:14:34,638 --> 00:14:37,682
efficient. You can continue moving quickly, but sometimes

238
00:14:37,736 --> 00:14:41,046
there are cases where you may need to wait until the request is complete and

239
00:14:41,068 --> 00:14:45,074
then you progress. And there is an optimized integration pattern

240
00:14:45,122 --> 00:14:48,566
called job run or also called sync. Because of

241
00:14:48,588 --> 00:14:52,098
the word dot sync that's added to the end of the API action. Then you

242
00:14:52,124 --> 00:14:55,514
also have a callback. This is what helps us to introduce a human into our

243
00:14:55,552 --> 00:14:59,340
flow and we're bring to see a bit of that in the architecture later.

244
00:14:59,710 --> 00:15:02,590
Now with these integrations that are available,

245
00:15:02,740 --> 00:15:06,238
you then have an idea of how you can take a business process

246
00:15:06,324 --> 00:15:10,398
and then afterwards integrate that across. But just to understand

247
00:15:10,484 --> 00:15:13,982
why this is important, let's take an example of a standard

248
00:15:14,036 --> 00:15:17,230
serverless application and show you why direct integration

249
00:15:17,310 --> 00:15:20,558
actually makes more sense. So here's a classic example. You're querying

250
00:15:20,574 --> 00:15:23,742
a database. So we have a lambda function that needs to get an item

251
00:15:23,806 --> 00:15:27,166
from a dynamodb table. So from a code perspective,

252
00:15:27,198 --> 00:15:30,786
what do I need to get started? I need the import AWs SDK

253
00:15:30,818 --> 00:15:34,034
to interact with the table. Then I need to set up my parameters

254
00:15:34,082 --> 00:15:37,446
to tell dynamodb what table I need to interact with. So this is

255
00:15:37,468 --> 00:15:41,046
like the table name, the partition key, the sort key, and then

256
00:15:41,068 --> 00:15:43,866
I set up my query so that there is a try catch block and then

257
00:15:43,888 --> 00:15:47,546
I return any errors. Now above that I also need

258
00:15:47,568 --> 00:15:51,014
to add lambda export handlers with my event object, my context

259
00:15:51,062 --> 00:15:54,618
object, and then add another try catch block to catch other errors.

260
00:15:54,794 --> 00:15:57,934
I may also need to convert data

261
00:15:57,972 --> 00:16:01,626
structures like for example an object to a string, for example, for other reasons.

262
00:16:01,738 --> 00:16:04,446
But you can see there's a lot of lines of code just to get one

263
00:16:04,468 --> 00:16:07,794
item from a dynamodb table. Now each of these lines is an

264
00:16:07,832 --> 00:16:10,946
area that something can go wrong. Because one thing

265
00:16:11,048 --> 00:16:14,386
you have to understand is code is also a liability, right? When you write code,

266
00:16:14,408 --> 00:16:17,038
you are responsible for the way it functions. You have to make sure that you're

267
00:16:17,054 --> 00:16:20,946
writing it securely, you're using the right set of dependencies, ensuring that there's

268
00:16:20,978 --> 00:16:24,326
no memory leaks and so on and so forth. Now when you look at it

269
00:16:24,348 --> 00:16:28,086
from a step functions perspective, what you can do is you have a

270
00:16:28,108 --> 00:16:32,178
single step that makes that item call to a dynamodb table and it's

271
00:16:32,194 --> 00:16:35,978
just a scalable, right? I can still configure things like retries, I can still

272
00:16:36,064 --> 00:16:39,418
catch any errors and then send that to a dead letter queue if I

273
00:16:39,424 --> 00:16:42,374
need to so that I can do a retry later. And if you notice,

274
00:16:42,422 --> 00:16:45,994
what happens is that this diagram isn't just a visual representation,

275
00:16:46,042 --> 00:16:49,306
this is actually showing how you can take a certain action

276
00:16:49,338 --> 00:16:52,446
and then after do that, take it from start till finish.

277
00:16:52,548 --> 00:16:55,586
And you can show this to other folks in your engineering team. You can also

278
00:16:55,608 --> 00:16:58,914
show this to business stakeholders so that they can understand what a flow looks like.

279
00:16:59,032 --> 00:17:02,930
So added value with of course the whole idea of errors and

280
00:17:03,000 --> 00:17:06,674
retries and the way it would look at when you

281
00:17:06,712 --> 00:17:10,006
actually add the nodes in the end with certain integrations is like this,

282
00:17:10,028 --> 00:17:13,526
right? So you have dynamodb, you have the getitem side, you have SQs send

283
00:17:13,548 --> 00:17:17,026
message, so on and so forth. One other thing during development,

284
00:17:17,058 --> 00:17:20,246
or even when you deploy a step function to production, is that

285
00:17:20,268 --> 00:17:23,066
you need to understand what's happening in the workflow and when things go wrong.

286
00:17:23,168 --> 00:17:26,746
And the way you do that is you have the execution flow where you can

287
00:17:26,768 --> 00:17:30,614
see different parts of the execution and then you can go within a specific execution,

288
00:17:30,742 --> 00:17:34,166
see the different states, what's happening within each state, what's the input

289
00:17:34,198 --> 00:17:37,534
and what's the output, and also look at things like how much time it takes

290
00:17:37,572 --> 00:17:41,326
to execute a certain state. And this is really critical when there are

291
00:17:41,348 --> 00:17:44,286
issues. So a great way to get all of that together and then see that

292
00:17:44,308 --> 00:17:48,034
in a single pane. Now let's dive into an

293
00:17:48,072 --> 00:17:51,858
actual use case, right? And we have a demo towards the end. I'll show a

294
00:17:51,864 --> 00:17:55,394
couple of demos in the middle, also about bedrock and integration,

295
00:17:55,442 --> 00:17:58,998
and then one where it looks at an application that uses all

296
00:17:59,004 --> 00:18:02,534
of this together. So let's say you

297
00:18:02,732 --> 00:18:06,534
have an application that has videos being

298
00:18:06,572 --> 00:18:10,462
uploaded, and then these videos need to be transcribed,

299
00:18:10,546 --> 00:18:13,686
right? So we already have a service that's available called Amazon

300
00:18:13,718 --> 00:18:17,018
transcribe. And in step functions, all I need to do is I can

301
00:18:17,104 --> 00:18:21,014
drag in a transcription job start node,

302
00:18:21,062 --> 00:18:24,570
so I can drag that in and then afterwards say, okay, fine, for any

303
00:18:24,640 --> 00:18:27,966
image that, and then trigger that step function for any video that comes in,

304
00:18:27,988 --> 00:18:31,406
for example, just kick in and then afterwards do a transcription of

305
00:18:31,428 --> 00:18:34,714
that video. So automatic speech recognition

306
00:18:34,762 --> 00:18:38,462
happens. And this makes it easy for developers to add speech to text capability

307
00:18:38,526 --> 00:18:42,334
to their applications. This integration is super powerful.

308
00:18:42,382 --> 00:18:46,206
This allows you to just have this without any code that's needed. Now let's

309
00:18:46,238 --> 00:18:49,926
say I want to also do something beyond this, right? So I want to take

310
00:18:49,948 --> 00:18:53,062
that transcription and I want to add some additional stuff.

311
00:18:53,116 --> 00:18:56,594
And this is where generative AI can help us. So I want to create multiple

312
00:18:56,642 --> 00:19:00,310
titles and descriptions for a video. I want to ask a human

313
00:19:00,380 --> 00:19:04,106
to provide feedback based on what choice they want to

314
00:19:04,128 --> 00:19:07,434
have from the titles and then also create an avatar for the video.

315
00:19:07,472 --> 00:19:11,018
So you have text also, and you have also image generation happening. And the

316
00:19:11,024 --> 00:19:14,254
way you do this with step functions is you can look at

317
00:19:14,292 --> 00:19:17,374
optimized integrations for Amazon bedrock. Now there are

318
00:19:17,412 --> 00:19:20,586
two new optimized integrations that we have provided, and there's more that's

319
00:19:20,618 --> 00:19:23,994
been added ever since where the first one is invoke

320
00:19:24,042 --> 00:19:27,614
model. And this invoke model API integration allows you to orchestrate

321
00:19:27,742 --> 00:19:31,646
interactions with foundational models. So you call the API directly

322
00:19:31,678 --> 00:19:34,866
through step functions. You give it the parameters that are needed, you provide the

323
00:19:34,888 --> 00:19:37,746
prompt that is needed and then that gets sent to the foundation model. You get

324
00:19:37,768 --> 00:19:41,174
the response back and then you can continue using that. The second one

325
00:19:41,212 --> 00:19:44,326
is the create model customization job. Now what this does

326
00:19:44,348 --> 00:19:47,734
is this supports the run a job, the dot sync call pattern that we saw

327
00:19:47,772 --> 00:19:52,214
earlier. And this means that it is waiting for the asynchronous

328
00:19:52,262 --> 00:19:55,382
job to complete before progressing to the next step in your workflow.

329
00:19:55,446 --> 00:19:59,306
So say for example, you're trying to create a certain customization on top of the

330
00:19:59,328 --> 00:20:02,698
foundational model. It'll wait for that and then it'll go to the next step and

331
00:20:02,704 --> 00:20:05,886
then afterwards continue with that process. This is useful especially in

332
00:20:05,908 --> 00:20:09,758
data processing pipelines because you are trying to do some kind of fine tuning to

333
00:20:09,764 --> 00:20:13,310
the model. I'll quickly jump into demo

334
00:20:13,460 --> 00:20:17,220
so that you can actually see what happens with standard

335
00:20:17,590 --> 00:20:21,218
implementation with bedrock. Just quickly to understand if

336
00:20:21,224 --> 00:20:24,018
you're getting started with bedrock, you need to make sure that you have access to

337
00:20:24,024 --> 00:20:28,034
the models. Right now you have access to foundational models

338
00:20:28,082 --> 00:20:31,830
in two regions, that's North Virginia and also Oregon.

339
00:20:32,170 --> 00:20:35,618
When you go to the bedrock screen you will actually see there's

340
00:20:35,634 --> 00:20:39,206
a section called the model access. And this gives you a list of all the

341
00:20:39,228 --> 00:20:42,742
models that are available right now in those two regions.

342
00:20:42,886 --> 00:20:45,914
And if you're doing it for the first time, you will have to go and

343
00:20:45,952 --> 00:20:49,066
manage your model access and then grant access to it. You'll get

344
00:20:49,088 --> 00:20:52,698
that immediately unless it's a brand new model that takes a bit of time

345
00:20:52,784 --> 00:20:56,126
where you may have to submit certain use cases. In my case right

346
00:20:56,148 --> 00:20:59,774
now I have clot three that's in the pipeline. I'm waiting for the

347
00:20:59,812 --> 00:21:02,926
details to get approved so that I can get access to this clot three just

348
00:21:02,948 --> 00:21:06,106
got announced a few days ago, support in bedrock.

349
00:21:06,298 --> 00:21:10,126
So I have that immediately ready. Now let me jump in directly

350
00:21:10,158 --> 00:21:13,714
into a workflow. When you go to step function and you create a new step

351
00:21:13,752 --> 00:21:17,406
function, you're greeted with a blank canvas. You have a state box that's empty

352
00:21:17,438 --> 00:21:20,678
over here. In my case I already dragged in

353
00:21:20,764 --> 00:21:24,166
bedrock API and if you want to see

354
00:21:24,188 --> 00:21:27,654
the list of bedrock APIs that are currently available, you have much more right now

355
00:21:27,692 --> 00:21:31,178
where you can also manage operations on foundational models if

356
00:21:31,184 --> 00:21:35,270
you need to. Things like the custom models for example and listings,

357
00:21:35,430 --> 00:21:38,854
especially for processing pipelines, MLO Ops, so on and so forth.

358
00:21:38,982 --> 00:21:42,858
In our case I just want to do an invoke model. So I'm going to

359
00:21:43,024 --> 00:21:47,514
just show you what the configuration looks like. I have foundation models already selected,

360
00:21:47,642 --> 00:21:50,766
and these are the list of foundation models that are already available. As you saw

361
00:21:50,788 --> 00:21:54,238
in the previous screen. In this case I have

362
00:21:54,324 --> 00:21:57,426
selected llama. So llama two is already selected in

363
00:21:57,448 --> 00:22:00,946
this case, and now you can configure what are the parameters that

364
00:22:00,968 --> 00:22:04,818
need to be sent. What I'm doing over here is I'm just hard coding the

365
00:22:04,824 --> 00:22:07,970
prompt in another demo. Quickly after this I'm going to show

366
00:22:08,040 --> 00:22:11,606
where you can actually customize the prompts based on input that you

367
00:22:11,628 --> 00:22:14,854
may get from other applications or maybe from the user. In my case.

368
00:22:14,892 --> 00:22:17,682
All I'm saying is, okay, there's a transcript from a video in a paragraph.

369
00:22:17,746 --> 00:22:19,980
This is the same video you're going to see in the last demo.

370
00:22:21,150 --> 00:22:24,806
This is an interview between Amazon's CTO Werner

371
00:22:24,838 --> 00:22:28,202
Vogels and ex Amazon CEO Jeff Bezos. This is from 2012,

372
00:22:28,256 --> 00:22:31,962
so eleven years old, and all it does is it

373
00:22:32,016 --> 00:22:35,262
uses this transcript, and then I'm asking it to provide

374
00:22:35,316 --> 00:22:38,586
a summary of this transcript. So what I'll

375
00:22:38,618 --> 00:22:41,726
do quickly is I'll just do an execution, and we're going to

376
00:22:41,748 --> 00:22:45,274
see how it looks like when you do an execution. I'm not passing any input,

377
00:22:45,322 --> 00:22:48,354
it's optional because I've already hard coded the prompt over there.

378
00:22:48,552 --> 00:22:52,018
Once I run this, and within a certain execution history

379
00:22:52,104 --> 00:22:55,838
or a certain point of execution, you can see the actual path.

380
00:22:55,934 --> 00:22:59,574
You can see what are the different steps that are being executed. And with

381
00:22:59,612 --> 00:23:03,302
bedrock model already done in this case,

382
00:23:03,436 --> 00:23:06,726
you can see that the input just was an optional input that got

383
00:23:06,748 --> 00:23:10,210
sent out. And here is a summary that's come back from llama two. This is

384
00:23:10,220 --> 00:23:13,818
basically a summary of the transcript. It gives an example of what

385
00:23:13,904 --> 00:23:17,562
Jeff Bezos mentioned and what's the whole organization working

386
00:23:17,616 --> 00:23:21,226
on towards. This was eleven years ago. You also get other parameters like how much

387
00:23:21,248 --> 00:23:24,734
prompts were taken and generation token. All in all, without provisioning any

388
00:23:24,852 --> 00:23:28,254
large language models, without you actually managing the scaling side or

389
00:23:28,292 --> 00:23:32,126
even provisioning a large language model. So pretty cool. And the

390
00:23:32,148 --> 00:23:35,822
other thing, what you'll realize is with step functions you also are able to

391
00:23:35,876 --> 00:23:39,694
view the different states and how much time they took to execute.

392
00:23:39,742 --> 00:23:43,534
So really useful, especially if you want to debug certain things. If there's any failures

393
00:23:43,582 --> 00:23:46,840
that you get that. Also over here you can actually see those errors over there.

394
00:23:47,290 --> 00:23:50,786
Now another powerful way of showing what bedrock

395
00:23:50,818 --> 00:23:53,650
is capable of through step functions is chaining.

396
00:23:53,810 --> 00:23:58,114
And this is another demo application. What this does is this emulates

397
00:23:58,162 --> 00:24:01,698
a certain conversation that you can have with

398
00:24:01,724 --> 00:24:05,126
an LLM, with anything that's doing text, right? So, for example, you have a chat

399
00:24:05,158 --> 00:24:08,602
interface, and with any large language model, you have to always

400
00:24:08,656 --> 00:24:11,834
provide the context of, especially the history of the conversation that's happening,

401
00:24:11,872 --> 00:24:14,834
so that the next one can then understand the next conversation,

402
00:24:14,902 --> 00:24:18,334
or the response can be based on that conversation from before.

403
00:24:18,452 --> 00:24:21,918
So in our case, what we are doing is we're creating a chain, and in

404
00:24:21,924 --> 00:24:26,034
this case, I'm leveraging another foundational model called

405
00:24:26,232 --> 00:24:30,500
command text from coher. And what this does is this is

406
00:24:31,510 --> 00:24:34,894
reading a prompt. So it's going to read for the prompt from the input.

407
00:24:35,022 --> 00:24:38,194
So when you invoke the step function, you can actually have a look

408
00:24:38,232 --> 00:24:41,926
at what are the different parameters that are there

409
00:24:42,028 --> 00:24:45,206
in the object, in the json body, and then afterwards you can pick that out.

410
00:24:45,308 --> 00:24:48,822
In our case, what I'm doing is, I'm just saying, okay, dollar prompt one,

411
00:24:48,876 --> 00:24:51,834
send this as a prompt, and these are the maximum tokens. Now, in this case,

412
00:24:51,872 --> 00:24:55,194
you'll see this is a different syntax based

413
00:24:55,232 --> 00:24:58,282
on this model versus what was there for llama two.

414
00:24:58,416 --> 00:25:01,530
And all I'm doing is I'm adding the result

415
00:25:01,600 --> 00:25:05,578
of this conversation back to the initial prompts

416
00:25:05,594 --> 00:25:08,794
that are coming in so that we have context throughout this conversation.

417
00:25:08,922 --> 00:25:11,680
And now if I just go in and execute this,

418
00:25:12,210 --> 00:25:15,598
I'll just copy this from a previous one, because I want to pass a similar

419
00:25:15,684 --> 00:25:18,580
input. I'll just do an execution over here.

420
00:25:18,950 --> 00:25:22,354
In my case, I'm passing three prompts, if you notice, in the state

421
00:25:22,392 --> 00:25:24,914
also, I had three of them. And all I'm doing is, I'm saying, okay,

422
00:25:24,952 --> 00:25:28,534
name a random city from southeast Asia. Just want you to give some information,

423
00:25:28,652 --> 00:25:31,926
provide some description for it, and then provide some more description for it.

424
00:25:31,948 --> 00:25:35,686
So let's start the execution, and as you'll see,

425
00:25:35,868 --> 00:25:39,690
as the execution progresses, you're going to see all these states

426
00:25:39,760 --> 00:25:43,114
changing the colors based on how the

427
00:25:43,312 --> 00:25:47,082
foundational model is responding. So the first result is already

428
00:25:47,136 --> 00:25:50,726
in. So it says, okay, here is a random city from Southeast Asia.

429
00:25:50,758 --> 00:25:53,390
So it picks Ho Chi Minh from Vietnam.

430
00:25:53,810 --> 00:25:57,386
Packages that in as part of this result, one is already added

431
00:25:57,418 --> 00:26:01,422
in, and then afterwards sends it to the second conversation history. You'll see

432
00:26:01,476 --> 00:26:04,786
conversation result two. Here are two interesting aspects of the city,

433
00:26:04,888 --> 00:26:08,594
and it mentions certain parts of this. And then invoke model with

434
00:26:08,712 --> 00:26:12,802
three. And the output over here is that it takes in certain

435
00:26:12,856 --> 00:26:15,906
part. Now, with large language models being

436
00:26:16,008 --> 00:26:19,574
nondeterministic, a lot of times you have to be careful with how you send

437
00:26:19,612 --> 00:26:23,554
your prompts and then ensure that the context is remaining. Now, in a previous execution

438
00:26:23,602 --> 00:26:27,846
of the same workflow, I was able to get the third prompt and

439
00:26:27,948 --> 00:26:31,366
also make sure that it continues with the city, which was previously Ho

440
00:26:31,388 --> 00:26:34,666
Chi Minh. So what I would probably want to do is I would create my

441
00:26:34,688 --> 00:26:37,946
third prompt in such a way that I emphasize it clearly that this is

442
00:26:37,968 --> 00:26:41,418
the city that you're supposed to use. And probably the way I

443
00:26:41,424 --> 00:26:45,174
would do that is I would have certain parts in my inputs, which would probably

444
00:26:45,232 --> 00:26:48,366
take certain things like the city or other things and then enforce that as part

445
00:26:48,388 --> 00:26:52,206
of different prompts. But in a nutshell, you kind of see how you can

446
00:26:52,228 --> 00:26:55,982
do chaining in this case, and you can also bring that within

447
00:26:56,036 --> 00:26:58,914
this and have a bigger application that is using this. And we're going to talk

448
00:26:59,032 --> 00:27:02,450
about the architecture of this for the rest of the session.

449
00:27:02,790 --> 00:27:07,198
So let's continue with that use case of generating titles and descriptions

450
00:27:07,294 --> 00:27:10,118
for the videos in this case. What happens is that,

451
00:27:10,204 --> 00:27:14,854
like you saw earlier in the demo that I showed, you can select the

452
00:27:14,892 --> 00:27:18,146
large language model. In this case, Titan is selected.

453
00:27:18,258 --> 00:27:21,826
And what under the hood happens is that the ESL for Amazon bedrock

454
00:27:21,858 --> 00:27:25,274
looks something like this, right? So there's an invoke model action that's happening, and then

455
00:27:25,312 --> 00:27:29,158
there is a model that is being selected. It could be llama,

456
00:27:29,174 --> 00:27:32,366
it could be anything else. And then there is a dynamic input that's coming in.

457
00:27:32,388 --> 00:27:35,758
So dollar prompt, which basically means something else, is invoking the

458
00:27:35,764 --> 00:27:39,466
step function and then providing this prompt. Now you have also inference

459
00:27:39,498 --> 00:27:43,098
parameters that allow you to tweak the response

460
00:27:43,114 --> 00:27:46,514
that comes back from an LLM for various things like probability and other

461
00:27:46,552 --> 00:27:50,050
things. And when you look at

462
00:27:50,200 --> 00:27:53,566
invoking the model, you can also provide input and output.

463
00:27:53,598 --> 00:27:56,866
So for example, if your input is larger than 256 kb, because a

464
00:27:56,888 --> 00:28:00,014
step function can only take 256 kb of content text,

465
00:28:00,072 --> 00:28:03,286
usually in this case, what you can do is you can point to an S

466
00:28:03,308 --> 00:28:07,106
three bucket for input and for output. It's a good way to ensure

467
00:28:07,138 --> 00:28:10,626
that you're able to scale this application without facing

468
00:28:10,658 --> 00:28:13,850
the restrictions or the constraints by step functions.

469
00:28:14,190 --> 00:28:17,578
So this input and output is then used, and then you can change this and

470
00:28:17,584 --> 00:28:21,562
you can continue using this in different states within step

471
00:28:21,616 --> 00:28:25,258
function. One thing you'll realize is that in the first requirement,

472
00:28:25,354 --> 00:28:28,926
it was actually mentioned about creating multiple titles. Now,

473
00:28:29,028 --> 00:28:33,146
for example, we can continue using just the foundational models within AWS.

474
00:28:33,258 --> 00:28:36,642
But what if we want to access something that's outside, let's say for example,

475
00:28:36,696 --> 00:28:40,034
hugging phase, you want to access this foundational model from

476
00:28:40,152 --> 00:28:43,700
outside AWS. We want to then

477
00:28:44,070 --> 00:28:47,538
get the data, send that across, and then after get the response back and

478
00:28:47,544 --> 00:28:51,138
then continue in our execution. Now when you look at accessing

479
00:28:51,154 --> 00:28:54,454
a public API in general, it might look simple. Then the first question

480
00:28:54,492 --> 00:28:58,098
comes is what is the kind of authentication that you need, right? Is there basic

481
00:28:58,194 --> 00:29:01,602
authentication? Is there API keys? Is there oauth?

482
00:29:01,746 --> 00:29:05,434
Is there anything else token management for example. Then you

483
00:29:05,472 --> 00:29:08,746
also want to ensure that you're saving the secrets because you want to make sure

484
00:29:08,768 --> 00:29:12,150
maybe there's an access key for accessing the API. You want to keep that somewhere.

485
00:29:12,310 --> 00:29:15,774
Then you have input output handling. You also

486
00:29:15,812 --> 00:29:19,134
then have a graceful retry if something goes wrong. Then also

487
00:29:19,172 --> 00:29:22,494
rate control and so many other things. Now the way you would do that

488
00:29:22,532 --> 00:29:26,026
with AWS lambda for example, or maybe a container

489
00:29:26,058 --> 00:29:29,986
or virtual machine on EC two is that you would have your code running and

490
00:29:30,008 --> 00:29:33,234
then you would have these different services which would fetch the credentials, you would manage

491
00:29:33,272 --> 00:29:36,542
the token, you would then retrieve the request data and then afterwards

492
00:29:36,606 --> 00:29:40,374
invoke and get back the data, maybe store it somewhere else also if needed,

493
00:29:40,492 --> 00:29:43,320
this is what a resilient application would look like.

494
00:29:43,850 --> 00:29:47,782
One other way you can do this without writing code is by public

495
00:29:47,836 --> 00:29:51,562
HTTPs API integration on step functions. So step

496
00:29:51,616 --> 00:29:55,414
functions has the ability to call virtually any SaaS application from a workflow

497
00:29:55,462 --> 00:29:59,210
with the integration with HTTPS endpoints. So without

498
00:29:59,280 --> 00:30:03,098
using a lambda function, you can use huggingface for

499
00:30:03,104 --> 00:30:06,826
example, you can invoke an API and hugging face or maybe other APIs

500
00:30:06,858 --> 00:30:10,094
like stripe, Salesforce, GitHub, Adobe for example.

501
00:30:10,292 --> 00:30:13,818
And step functions now with this low code approach

502
00:30:13,914 --> 00:30:17,618
provides you a way to connect AWS services with services that are outside.

503
00:30:17,784 --> 00:30:21,790
And you can then take advantage of workflow studio because now you're dragging drop

504
00:30:21,950 --> 00:30:25,042
all of these things and then after putting that as part of the workflow together

505
00:30:25,096 --> 00:30:28,646
without changing or managing any code as part of

506
00:30:28,668 --> 00:30:32,134
this. So with such

507
00:30:32,172 --> 00:30:35,622
requests you can actually then put in your json object and then

508
00:30:35,676 --> 00:30:38,614
in the request body you can then mention okay, this is the kind of data

509
00:30:38,652 --> 00:30:42,154
that we are sending and this is what we are trying to retrieve back as

510
00:30:42,192 --> 00:30:46,300
part of that transformation. One of the ways that you can actually

511
00:30:47,550 --> 00:30:51,050
use this for integrating with HTTP APIs is that

512
00:30:51,120 --> 00:30:54,462
you can manage the errors also through step functions like we saw kind of you

513
00:30:54,516 --> 00:30:58,286
have that ability to do error handling. You can also

514
00:30:58,308 --> 00:31:02,046
manage authorization as part of that integration. You can

515
00:31:02,068 --> 00:31:05,246
also mention transformation of data because step functions already provides that for

516
00:31:05,268 --> 00:31:09,106
optimized integrations. So you can also leverage that if needed for things like

517
00:31:09,128 --> 00:31:12,434
URL encoding for request body and there's also a test

518
00:31:12,472 --> 00:31:16,500
state that allows you to execute that specific state

519
00:31:17,510 --> 00:31:20,694
without deploying that step function directly outside. So you can just

520
00:31:20,732 --> 00:31:24,118
execute that specific state as a test and then afterwards make sure

521
00:31:24,124 --> 00:31:28,038
that you're getting the kind of response that is needed. So with

522
00:31:28,204 --> 00:31:31,658
the task state that's available, you have that single unit of work. You can

523
00:31:31,664 --> 00:31:34,966
do an HTTP invoke and you can see that an existing

524
00:31:34,998 --> 00:31:38,794
resource field is available now and you also have the new

525
00:31:38,832 --> 00:31:43,098
option. And you can also then provide things like what methods

526
00:31:43,114 --> 00:31:46,638
are being invoked. For example, what's the authentication field that is there?

527
00:31:46,724 --> 00:31:50,702
The parameters block. This under the hood is actually using another

528
00:31:50,756 --> 00:31:54,786
service called Eventbridge. So Amazon Eventbridge is being used for API destinations because it

529
00:31:54,808 --> 00:31:58,766
has that ability to invoke or send requests

530
00:31:58,798 --> 00:32:02,514
to an API destination. So the same connection is actually being

531
00:32:02,552 --> 00:32:05,934
used as part of that. A lot of these parameters are actually optional.

532
00:32:05,982 --> 00:32:09,910
So when you're invoking a certain API, probably you're just getting a response back.

533
00:32:09,980 --> 00:32:13,286
You don't need to pass any query parameters. In this case, what we're doing

534
00:32:13,308 --> 00:32:17,014
is that we can add a request for headers and then anything

535
00:32:17,052 --> 00:32:20,266
else that's needed as part of the request. Now let's go

536
00:32:20,288 --> 00:32:23,846
back to a requirement directly. So in our case, since we want to generate

537
00:32:23,878 --> 00:32:28,074
multiple titles, we want to make sure that we're able to access

538
00:32:28,272 --> 00:32:31,926
one title from our model ourselves,

539
00:32:32,038 --> 00:32:35,534
and then after one from hugging phase. So we have a parallel state.

540
00:32:35,572 --> 00:32:39,274
Now through step functions, this allows us to use both the foundational

541
00:32:39,322 --> 00:32:42,702
models and you simply configure the endpoints on the right hand side.

542
00:32:42,836 --> 00:32:46,786
This way the parallel state will then execute and

543
00:32:46,808 --> 00:32:50,354
it will invoke each task in parallel. It then requires that

544
00:32:50,392 --> 00:32:54,846
each branch completes successfully for the parallel state to be considered successful.

545
00:32:55,038 --> 00:32:58,574
Now what happens if one of these branches doesn't complete successfully?

546
00:32:58,622 --> 00:33:01,894
Right? So what if something goes wrong? Maybe there's an issue in the call

547
00:33:01,932 --> 00:33:04,966
for one of our two FMs, and errors happen for

548
00:33:04,988 --> 00:33:09,026
various reasons. And if it's a transient issue such as a network interruption,

549
00:33:09,058 --> 00:33:12,746
you want to make sure that you're able to do a retry, and maybe you

550
00:33:12,768 --> 00:33:16,086
want to do that retry for a couple of times. And then you also configure

551
00:33:16,118 --> 00:33:19,706
something called as a backup rate to ensure that you don't overload the

552
00:33:19,728 --> 00:33:23,142
third party system. And for these momentary blips,

553
00:33:23,286 --> 00:33:27,034
it's just important. You just need to make sure that you have a retry mechanism

554
00:33:27,082 --> 00:33:30,830
of sorts. But what if that underlying error is actually something

555
00:33:30,980 --> 00:33:34,526
which requires a longer investigation, right, or a longer resolution? Time, because maybe

556
00:33:34,548 --> 00:33:37,706
it's not under your control, maybe it's independent of your team,

557
00:33:37,748 --> 00:33:40,574
and maybe it's somebody else who's managing it, or maybe even a third party.

558
00:33:40,702 --> 00:33:44,850
And what may happen is you may exhaust your retry strategy and then eventually

559
00:33:45,510 --> 00:33:49,074
that workflow step will actually fail. So you

560
00:33:49,112 --> 00:33:52,518
want to make sure that you're able to run this entire workflow, but then at

561
00:33:52,524 --> 00:33:54,946
the same time, if you can't, then you want to move it to an error

562
00:33:54,978 --> 00:33:58,760
state, or then move it somewhere else so that you can retry it later.

563
00:33:59,450 --> 00:34:02,554
So if you want to visualize this, this is basically what it looks

564
00:34:02,592 --> 00:34:06,486
like. So you have a success tip that then kicks off a parallel workflow.

565
00:34:06,598 --> 00:34:09,898
This parallel workflow has two branches, so you

566
00:34:09,904 --> 00:34:12,986
have bedrock on the left, hugging face on the left. And let's say we invoke

567
00:34:13,018 --> 00:34:16,574
the foundation model, we have some transformations we want to do using another

568
00:34:16,612 --> 00:34:20,698
AWS service, and that is an extra step. But let's

569
00:34:20,714 --> 00:34:24,346
say there is some failure in transformation because we have invoked something hugging

570
00:34:24,378 --> 00:34:26,980
face, and then when we get it back, something's not working.

571
00:34:27,670 --> 00:34:31,346
And this transcription job needs to continue. Right. There's some form that

572
00:34:31,368 --> 00:34:34,610
needs to happen, and we have stopped it over here before actually moving

573
00:34:34,680 --> 00:34:37,970
it to the next step, which is a human review.

574
00:34:38,120 --> 00:34:41,826
This is where you have the option of Redrive. Now, Redrive allows

575
00:34:41,858 --> 00:34:45,206
you to easily restart workflows, maybe because you have figured out,

576
00:34:45,228 --> 00:34:47,798
okay, there's a problem, and then maybe it's got resolved, and then you want to

577
00:34:47,804 --> 00:34:51,118
retry that workflow all over again so you can recover from failure

578
00:34:51,154 --> 00:34:54,378
faster, and you only pay for what you need. So you don't have

579
00:34:54,384 --> 00:34:57,994
to keep retrying it unless it's really necessary. So the way this

580
00:34:58,032 --> 00:35:01,786
works is you will have these two branches on

581
00:35:01,808 --> 00:35:04,926
the left and hugging face on the left. And let's say that when

582
00:35:04,948 --> 00:35:09,280
we invoke, we do the transformation, but it fails in the transformation step,

583
00:35:09,650 --> 00:35:13,166
so it gets fixed, and then after you come back again, and then you

584
00:35:13,188 --> 00:35:17,378
do a retry again once more, and this time the transcription actually kicks in

585
00:35:17,544 --> 00:35:21,154
because your transformations are already completed. And now it goes into the human review

586
00:35:21,192 --> 00:35:24,994
space if needed. So one of the other things

587
00:35:25,112 --> 00:35:27,586
you want to do also as a part of workflows is you want to have

588
00:35:27,608 --> 00:35:30,774
observability. Execution event history is very important as

589
00:35:30,812 --> 00:35:33,666
part of this because you have different states that are coming in, you have events

590
00:35:33,698 --> 00:35:37,142
being fired. You want to make sure that you're able to filter and

591
00:35:37,276 --> 00:35:41,178
drill down to what's actually happening within your workflow. This is kind

592
00:35:41,184 --> 00:35:44,490
of like where you can see execution Redriven and it also shows

593
00:35:44,560 --> 00:35:48,266
a count, a redrive count of how many times you are actually retrying that

594
00:35:48,448 --> 00:35:52,046
execution through Redrive. So cool. I think it's a great way

595
00:35:52,068 --> 00:35:55,034
to understand how you can actually manage events,

596
00:35:55,162 --> 00:35:58,346
especially errors in this case, and then ensure that your workflows

597
00:35:58,458 --> 00:36:01,470
are able to then continue properly.

598
00:36:02,050 --> 00:36:05,378
Now, with multiple titles out of the way,

599
00:36:05,544 --> 00:36:09,214
let's talk about asking a human to provide feedback.

600
00:36:09,342 --> 00:36:13,154
Now, having a human approval is an automated business process and it is super

601
00:36:13,192 --> 00:36:17,362
common. You have this as part of any approvals that are happening,

602
00:36:17,416 --> 00:36:21,350
probably in the banking space, in the financial space. You have

603
00:36:21,420 --> 00:36:25,238
probably also a human in the loop as part of maybe a foundational model that

604
00:36:25,244 --> 00:36:28,802
you have created or you have custom built, or maybe you're fine tuned

605
00:36:28,866 --> 00:36:31,126
and then you want to make sure that you're able to check the response that

606
00:36:31,148 --> 00:36:34,262
are coming in. Maybe you have an EB flow that's happening, right? For a few

607
00:36:34,316 --> 00:36:37,414
requests that need to come in, you want to have a human response that needs

608
00:36:37,452 --> 00:36:41,262
to happen, human review that needs to happen. So the requirement is super

609
00:36:41,316 --> 00:36:44,638
simple, but possibilities are endless when you need to do this.

610
00:36:44,804 --> 00:36:48,320
So step functions integrates with services in multiple ways,

611
00:36:48,770 --> 00:36:51,806
and one of the ways you can do this is through long running jobs of

612
00:36:51,828 --> 00:36:55,950
a service, and you want to wait for the job to complete and

613
00:36:56,100 --> 00:36:59,170
we'll use this integration pattern to achieve this requirement.

614
00:36:59,510 --> 00:37:01,618
So what you want to do is you want to make a call to a

615
00:37:01,624 --> 00:37:05,154
service integration. This passes a unique token and this token then gets

616
00:37:05,192 --> 00:37:08,566
embedded in an email. It goes to maybe a server or

617
00:37:08,668 --> 00:37:12,418
on premise and legacy server, or to a long running task

618
00:37:12,434 --> 00:37:16,466
in a container, for example. And then once that response

619
00:37:16,498 --> 00:37:20,106
is maybe it's reviewed, and then after they click on going ahead or

620
00:37:20,128 --> 00:37:23,738
not, it returns using the step functions API, send task success.

621
00:37:23,904 --> 00:37:28,106
And then the workflow continues from there. So as

622
00:37:28,128 --> 00:37:31,338
part of the send response and wait workflow, there will be

623
00:37:31,344 --> 00:37:34,746
a token that's sent out like I mentioned earlier, and this email notification is

624
00:37:34,768 --> 00:37:37,774
already there. Maybe as part of this use case at least,

625
00:37:37,812 --> 00:37:40,158
what will happen is there will be options that are being set. So choose the

626
00:37:40,164 --> 00:37:43,546
title that's being generated by Amazon Bedrock, or choose the title that's

627
00:37:43,578 --> 00:37:46,994
being generated by hugging face and then regenerate that.

628
00:37:47,192 --> 00:37:50,450
Now the last part of this requirement is creating an avatar for the video,

629
00:37:50,520 --> 00:37:55,170
which basically is an image in this case. And machine learning models,

630
00:37:55,590 --> 00:37:59,822
especially in the foundational model space, you have built in algorithms.

631
00:37:59,886 --> 00:38:03,458
We also have pre built ML solutions that are already available. You can probably invoke

632
00:38:03,474 --> 00:38:06,866
a third party API again for this case, and there are multiple

633
00:38:06,898 --> 00:38:09,846
ways you can do this as a part of bedrock. You also have access to

634
00:38:09,868 --> 00:38:13,018
stability diffusion models, so you

635
00:38:13,024 --> 00:38:16,186
can use that also as part of the step. What this does is in the

636
00:38:16,208 --> 00:38:19,322
end, once you have tried the feedback, you can then generate that video,

637
00:38:19,456 --> 00:38:22,762
sorry, the avatar for the video, and then you can store

638
00:38:22,816 --> 00:38:26,400
that in an S three bucket and then share that link later.

639
00:38:26,930 --> 00:38:29,834
Now, one of the things you'll realize when you want to create such a complex

640
00:38:29,882 --> 00:38:33,502
workflow is that especially with foundation models, you want to have this whole

641
00:38:33,556 --> 00:38:36,882
idea of creating chains of prompts, which we kind of saw in the demo.

642
00:38:37,016 --> 00:38:41,054
Now this is a technique of writing multiple prompts and responses. A sequence of steps.

643
00:38:41,182 --> 00:38:44,450
Step functions is a workflow is a great way to actually

644
00:38:44,520 --> 00:38:47,698
leverage chaining, so you can actually use this.

645
00:38:47,784 --> 00:38:51,010
And step function simplifies the way you invoke your foundational models,

646
00:38:51,090 --> 00:38:54,214
and you have state independency management already in place. You can then

647
00:38:54,252 --> 00:38:57,314
create chaining easily. You can also pass the state, as we saw earlier,

648
00:38:57,362 --> 00:39:00,886
pass that to the next state that's needed, the response of a

649
00:39:00,908 --> 00:39:04,214
state, and then pass that to the next one, maybe specific parts of the prompt

650
00:39:04,262 --> 00:39:07,626
also if you need to. And all of this is again serverless. So think of

651
00:39:07,648 --> 00:39:11,686
use cases like writing blogs and articles, response validation, conversational LLMs

652
00:39:11,718 --> 00:39:15,566
that we see a lot these days. Now, if you want to now take all

653
00:39:15,588 --> 00:39:18,158
of what we have seen and then put that in an architecture, this is what

654
00:39:18,164 --> 00:39:21,386
it looks like. So for example, you have an API gateway

655
00:39:21,498 --> 00:39:25,410
that a user would invoke through an application,

656
00:39:25,560 --> 00:39:29,394
and then that would then put in an event into

657
00:39:29,432 --> 00:39:32,674
a queue, and then this

658
00:39:32,712 --> 00:39:35,426
event in the queue then gets picked up by a lambda function, which then would

659
00:39:35,448 --> 00:39:38,754
trigger this step functions workflow. And in this case,

660
00:39:38,792 --> 00:39:43,014
what happens is that you have a lot of these steps already in place

661
00:39:43,212 --> 00:39:46,934
as part of the workflow. It sends the title and description to the user back,

662
00:39:46,972 --> 00:39:50,486
and then afterwards you can then send the chosen title and description

663
00:39:50,598 --> 00:39:54,326
as part of the human workflow, if needed, for the response,

664
00:39:54,518 --> 00:39:57,930
for the review part. Then as part of the final

665
00:39:58,000 --> 00:40:01,478
part where you have the generating the avatar,

666
00:40:01,654 --> 00:40:05,306
you actually get an S three presigned URL, because that avatar

667
00:40:05,338 --> 00:40:09,178
image gets created, generated, and then afterwards put in an S three bucket.

668
00:40:09,354 --> 00:40:12,378
So here's a demonstration of this final architecture.

669
00:40:12,474 --> 00:40:16,322
So there's a short video of an interview between Jeff Bezos and

670
00:40:16,376 --> 00:40:19,666
Werner Vogels. What's going to happen is that

671
00:40:19,768 --> 00:40:23,666
we want to generate a title and a description and an avatar for this video.

672
00:40:23,848 --> 00:40:27,346
So there's a simple UI that you saw earlier.

673
00:40:27,538 --> 00:40:30,866
This uses a websocket communication to talk to AWS

674
00:40:30,898 --> 00:40:34,754
iot core service. And once you select the button, it then sends

675
00:40:34,802 --> 00:40:39,058
that video's details. And then the workflow then gets executed

676
00:40:39,074 --> 00:40:43,114
from the lambda function. And then you see that that step starts kicking in.

677
00:40:43,312 --> 00:40:46,506
This gives a nice view of the execution. You have the color coding of the

678
00:40:46,528 --> 00:40:50,314
state. And with transcribe being used initially, you get the text

679
00:40:50,432 --> 00:40:54,258
back for the speech that is there in the video. And this transcribe

680
00:40:54,294 --> 00:40:57,646
job is asynchronous. So there is a wait loop that is there to make sure

681
00:40:57,668 --> 00:41:01,230
that we can wait for it to complete.

682
00:41:01,300 --> 00:41:04,654
Right, so that's the wait loop that's there in the beginning. And once that

683
00:41:04,692 --> 00:41:08,274
wait loop is done, wait loop is done. And using

684
00:41:08,312 --> 00:41:11,870
the get transcription job API, we get the final response

685
00:41:11,950 --> 00:41:15,486
from that transcription job. And then we read that transcript.

686
00:41:15,678 --> 00:41:19,026
And that transcript is available in an S three bucket

687
00:41:19,058 --> 00:41:22,662
already because transcription job will put it in over there. And then

688
00:41:22,716 --> 00:41:26,866
once that is read, it is then passed down to this parallel execution.

689
00:41:26,978 --> 00:41:30,680
As part of the parallel execution, we have this two calls that are being done.

690
00:41:31,050 --> 00:41:34,806
One to bedrock, one of the foundational models

691
00:41:34,838 --> 00:41:38,186
at bedrock, and then one to hugging face. In this case, we're just keeping it

692
00:41:38,208 --> 00:41:41,962
simple. So we want to just make sure that we're able to execute this back.

693
00:41:42,096 --> 00:41:45,342
And we want to then get user feedback now

694
00:41:45,396 --> 00:41:48,206
quickly, just to show you what the outputs look like.

695
00:41:48,388 --> 00:41:51,754
These are the inputs that are coming in. This is the transcript that's

696
00:41:51,802 --> 00:41:55,586
there, the prompts. You kind of notice that the models that are being invoked is

697
00:41:55,608 --> 00:41:58,530
also there as part of that. Here are the parameters,

698
00:41:58,950 --> 00:42:02,786
here's the conversation that's happening with the video and the

699
00:42:02,808 --> 00:42:06,606
s three bucket URL for the video and other things. And there's

700
00:42:06,638 --> 00:42:10,566
a task token that's already there. This is part of the review flow that

701
00:42:10,588 --> 00:42:14,278
is being invoked. So we have this task token that's being sent

702
00:42:14,444 --> 00:42:18,102
to the page. And this page is basically where someone

703
00:42:18,156 --> 00:42:21,506
can actually go in and then say, okay, do they want to select this title,

704
00:42:21,538 --> 00:42:25,494
the first one or the second one? So we select one of the titles,

705
00:42:25,542 --> 00:42:28,746
and then afterwards it goes down, and then it creates an avatar as part of

706
00:42:28,768 --> 00:42:33,200
that title that's created. It sends that as part of a prompt to

707
00:42:34,690 --> 00:42:37,902
one of the foundation models. So stability in this case,

708
00:42:38,036 --> 00:42:41,438
and once you have that, that gets displayed over here, and that's an

709
00:42:41,444 --> 00:42:44,766
avatar that is used now, for example, the team that uses it can then

710
00:42:44,868 --> 00:42:48,226
copy this image and then put it in because it's already there in s three

711
00:42:48,248 --> 00:42:51,874
bucket. Or probably it gets picked by another flow that then is used as part

712
00:42:51,912 --> 00:42:55,666
of their content publishing pipeline. Now, to know more about how

713
00:42:55,688 --> 00:42:59,446
you can build applications like this, there is a sample that's already available that has

714
00:42:59,468 --> 00:43:03,282
different use cases. Also covers things like the error retries.

715
00:43:03,346 --> 00:43:06,614
It covers prompt chaining and all

716
00:43:06,652 --> 00:43:10,194
the other parts of creating a complex workflow with step functions for generative

717
00:43:10,242 --> 00:43:13,206
AI. So have a look at this resource. A great way to do this,

718
00:43:13,228 --> 00:43:16,870
and also with our blog posts that are linked as part of this resource.

719
00:43:17,210 --> 00:43:20,814
So with that, I would like to thank you for attending the session and have

720
00:43:20,852 --> 00:43:23,980
a good day and rest of conf 42. Thank you so much.

