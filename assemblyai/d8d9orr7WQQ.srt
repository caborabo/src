1
00:00:21,050 --> 00:00:24,850
Hey folks, thank you very much for joining me today at Comp 42 Python.

2
00:00:24,970 --> 00:00:28,566
Today you get to learn about sketching algorithms and how you can make sense of

3
00:00:28,588 --> 00:00:32,434
big data in a single pass. My name is Tun Shui

4
00:00:32,482 --> 00:00:36,038
and I'm the vp of data at Quix. Wix is a remote company and we

5
00:00:36,044 --> 00:00:39,314
have teams across Europe and the US and our headquarters in London

6
00:00:39,362 --> 00:00:42,666
in the United Kingdom. A bit about myself I

7
00:00:42,688 --> 00:00:46,026
have a background working in high growth startups. As a data engineer and head

8
00:00:46,048 --> 00:00:49,990
of data. I help companies determine their data and AI strategy

9
00:00:50,070 --> 00:00:53,126
and where possible, center it around streaming technologies.

10
00:00:53,318 --> 00:00:56,826
I'm a big fan of the mantra less is more and simplifying

11
00:00:56,858 --> 00:01:01,018
things because I think that just makes everything easier to build on and maintain.

12
00:01:01,194 --> 00:01:04,714
And I'm really interested in the real time data and AI ecosystem,

13
00:01:04,842 --> 00:01:08,606
which should explain why I'm so excited about sketching algorithms.

14
00:01:08,798 --> 00:01:12,420
I've popped my LinkedIn there in the corner, so feel free to connect.

15
00:01:12,950 --> 00:01:17,054
First of all, a bit of information about Quix, so our mission is Python.

16
00:01:17,102 --> 00:01:20,294
First stream processing. The streaming data landscape was

17
00:01:20,332 --> 00:01:23,606
started and built in Java, and to this day it's mainly accessible to

18
00:01:23,628 --> 00:01:27,254
those who use Java and SQL. Quicks focuses on

19
00:01:27,292 --> 00:01:31,226
the Python developer experience, and we have two products. We have

20
00:01:31,248 --> 00:01:34,906
Quickstreams, which is an open source library that uses data frames to work

21
00:01:34,928 --> 00:01:38,246
with data and build streaming and event driven applications.

22
00:01:38,358 --> 00:01:41,626
And since sketching is based on streaming data, it goes hand in

23
00:01:41,648 --> 00:01:44,986
hand naturally, with quicks. Quix Cloud is a fully

24
00:01:45,018 --> 00:01:48,814
managed platform where you can run your streaming applications and build out real time

25
00:01:48,852 --> 00:01:52,110
data pipelines quickly using open source connectors.

26
00:01:52,690 --> 00:01:55,634
We're pushing the idea of streaming data frames, and as you can see,

27
00:01:55,672 --> 00:01:59,138
it's reminiscent of the other data frames you may have encountered in pandas or

28
00:01:59,144 --> 00:02:02,958
Pyspark. You can build your data transformations using Quickstream's

29
00:02:02,974 --> 00:02:06,930
applications, and they're designed to work with Docker and be containerized.

30
00:02:07,670 --> 00:02:11,206
You can deploy those containers on your own infrastructure, or if you prefer not

31
00:02:11,228 --> 00:02:15,030
to manage infrastructure, you can deploy it on Quicksloud. We provide

32
00:02:15,100 --> 00:02:18,666
a nice interface so you can build out streaming data pipelines. You can get

33
00:02:18,688 --> 00:02:21,994
started quickly integrating data with connectors to

34
00:02:22,032 --> 00:02:25,910
data sources such as Kafka, MQTT, and databases

35
00:02:25,990 --> 00:02:29,062
such as postgres, as well as popular destinations

36
00:02:29,126 --> 00:02:32,518
for data such as Snowflake, redis, and influxdB.

37
00:02:32,694 --> 00:02:36,330
The connectors and the transformation code samples are all open source,

38
00:02:36,410 --> 00:02:39,898
and we have contributions from our friends and partners such as influxdB,

39
00:02:39,994 --> 00:02:43,920
and we'd love to have more contributions, so please get in touch if you're interested.

40
00:02:45,010 --> 00:02:48,690
So what is a sketch? Note that this is a really large

41
00:02:48,760 --> 00:02:51,906
topic, because there are families of sketching algorithms, all for

42
00:02:51,928 --> 00:02:55,746
different use cases. And my hope here is to introduce you to a few of

43
00:02:55,768 --> 00:02:59,286
my favorites and inspire you, and hopefully get you introduced to

44
00:02:59,308 --> 00:03:02,070
the possibilities. So you go out and explore by yourselves.

45
00:03:02,970 --> 00:03:06,834
After internalizing all the documentation. My experience using sketches,

46
00:03:06,882 --> 00:03:10,934
I've come up with this that sketches are small, stateful streaming

47
00:03:10,982 --> 00:03:14,202
data programs that deal with a huge volume of data,

48
00:03:14,336 --> 00:03:18,374
and they provide approximate answers orders of magnitude faster

49
00:03:18,422 --> 00:03:21,990
than exact techniques. It's grounded in the truth

50
00:03:22,070 --> 00:03:25,982
that when it comes to big data, approximate solutions are favorable over exact

51
00:03:26,036 --> 00:03:29,466
solutions, because they're a good tradeoff for memory resources

52
00:03:29,498 --> 00:03:32,698
and time. And you might not realize it, but you're

53
00:03:32,714 --> 00:03:36,258
already probably accepting of approximations. You may have

54
00:03:36,264 --> 00:03:39,906
heard about this little tool called Chat GPT, which works by responding with

55
00:03:39,928 --> 00:03:43,586
its best guess, an approximation. And here are some

56
00:03:43,608 --> 00:03:47,262
characteristics of sketches. The first is that they're small,

57
00:03:47,336 --> 00:03:50,726
and they have to be small, and they're usually only a few kilobytes in

58
00:03:50,748 --> 00:03:54,102
size, meaning that they have reduced memory requirements and will grow

59
00:03:54,156 --> 00:03:57,926
sublinearly in space. They're stateful, and they

60
00:03:57,948 --> 00:04:01,994
maintain a data structure that keeps the state of the observations. So every

61
00:04:02,032 --> 00:04:06,410
data point that comes in isn't retained in its exact representation,

62
00:04:06,990 --> 00:04:10,602
it's streaming, meaning that it deals with data in a single pass.

63
00:04:10,736 --> 00:04:14,480
So the sketch needs to look only at each item in the stream once.

64
00:04:15,250 --> 00:04:18,878
And they're fast. They're orders of magnitude faster than exact

65
00:04:18,964 --> 00:04:22,186
techniques. The results are also mergeable without a loss

66
00:04:22,218 --> 00:04:26,130
in accuracy, and that keeps everything fast. And these

67
00:04:26,200 --> 00:04:29,986
are the components. So the first typical stage of a sketching process is

68
00:04:30,008 --> 00:04:33,602
a transformation that makes the input data stream resemble white.

69
00:04:33,656 --> 00:04:37,266
So you have a uniform distribution of values, and this

70
00:04:37,288 --> 00:04:40,854
is usually achieved by hashing the input keys and then normalizing the result into

71
00:04:40,892 --> 00:04:44,006
a fraction, that is, a random number between zero and

72
00:04:44,028 --> 00:04:47,410
one. The second stage of the sketch is the creation

73
00:04:47,490 --> 00:04:50,626
and updating of a data structure that follows a set of rules

74
00:04:50,658 --> 00:04:53,974
for compressing down the values it receives from the transform stage.

75
00:04:54,022 --> 00:04:57,046
So in the middle there, when you initialize the sketch, you can configure

76
00:04:57,078 --> 00:05:00,282
the parameters for the data structure, which enable you to manage

77
00:05:00,336 --> 00:05:04,314
its memory. The last stage, there are estimator algorithms

78
00:05:04,362 --> 00:05:07,594
that take a query, look it up in the sketch's data structure,

79
00:05:07,642 --> 00:05:11,306
and returns a result. Now this result would be an approximate,

80
00:05:11,418 --> 00:05:15,406
but the important thing to note is that it will have mathematically proven error

81
00:05:15,438 --> 00:05:18,494
distribution bounds. So this helps in dealing with the confidence

82
00:05:18,542 --> 00:05:22,238
of the results. So whilst tools like chat JBT

83
00:05:22,334 --> 00:05:26,594
give you an approximation. It's not always transparent about its error thresholds,

84
00:05:26,642 --> 00:05:29,350
which is why you get those things called hallucinations.

85
00:05:29,690 --> 00:05:33,586
But with sketches you have mathematically provable error bounds, which helps

86
00:05:33,618 --> 00:05:36,630
with confidence that the approximation is sufficient.

87
00:05:37,450 --> 00:05:40,550
And let's talk about why being exact is slow.

88
00:05:41,130 --> 00:05:44,202
We live in a world where the majority of the time we use exact

89
00:05:44,256 --> 00:05:47,898
techniques. And what are exact techniques? It's when you run a

90
00:05:47,904 --> 00:05:51,322
query to get the exact answer. For example, a common one I'll go through

91
00:05:51,376 --> 00:05:54,698
today is a count of the number of unique identifiers.

92
00:05:54,874 --> 00:05:58,494
The most common example is a database query, and hopefully everyone here

93
00:05:58,532 --> 00:06:01,470
has experienced a query that runs for a really long time.

94
00:06:01,620 --> 00:06:05,246
If you have billions of rows with high cardinality, that is, you have lots

95
00:06:05,278 --> 00:06:08,590
of unique identifiers. You could experience a really slow query

96
00:06:08,670 --> 00:06:12,334
when you're running a group by could. So distributed systems

97
00:06:12,382 --> 00:06:15,646
exist and they use parallelization to attempt

98
00:06:15,678 --> 00:06:19,030
to speed up these queries. And here's an example

99
00:06:19,100 --> 00:06:22,694
of how parallelization works to solve a problem such as word

100
00:06:22,732 --> 00:06:25,990
count. So on the left there you have some input data coming in.

101
00:06:26,060 --> 00:06:29,706
And to get the unique count of words, count of unique words,

102
00:06:29,808 --> 00:06:32,730
it has to go through these stages of splitting the data,

103
00:06:32,880 --> 00:06:36,874
mapping, shuffling and reducing. And these operations happen on

104
00:06:36,912 --> 00:06:40,586
different nodes or machines, so each box there represents a node or

105
00:06:40,608 --> 00:06:44,614
a separate machine. So splitting and mapping are self explanatory.

106
00:06:44,742 --> 00:06:47,498
But why does it need to go through shuffling and reducing?

107
00:06:47,674 --> 00:06:50,426
The reason is because if you look at the mapping step, each of the nodes

108
00:06:50,458 --> 00:06:53,566
have local answers and they are non additive, meaning that you

109
00:06:53,588 --> 00:06:57,566
can't simply sum them up since the same word occurs in multiple nodes.

110
00:06:57,678 --> 00:07:01,246
Here you can see that at the mapping step you can see the word deer

111
00:07:01,358 --> 00:07:04,398
and the word bear appear in two different nodes.

112
00:07:04,494 --> 00:07:08,238
So shuffling ensures that the same unique words are colocated

113
00:07:08,334 --> 00:07:11,906
so that they can be accurately counted. So when the reducer performs

114
00:07:11,938 --> 00:07:15,366
the sum operation, you get accurate results. There at the end you

115
00:07:15,388 --> 00:07:18,498
get bear is two, can is three, deer is two, river is

116
00:07:18,524 --> 00:07:21,990
two, and this is taken from Hadoop's documentation.

117
00:07:22,150 --> 00:07:25,606
And this is common to all massively parallel processing systems

118
00:07:25,638 --> 00:07:28,838
such as spark and snowflake. And really all distributed

119
00:07:28,934 --> 00:07:31,180
databases work in a similar way.

120
00:07:32,110 --> 00:07:36,234
And the take home message here is that shuffling is slow. And when processing queries

121
00:07:36,282 --> 00:07:39,438
you have to do what you can to avoid ad hoc network I o.

122
00:07:39,524 --> 00:07:43,054
And there are techniques in distributed systems like Spark that help you to get around

123
00:07:43,092 --> 00:07:46,862
this by using broadcasting, where you would use broadcast joins

124
00:07:46,926 --> 00:07:50,066
where you want to join two tables. So that rather than a

125
00:07:50,088 --> 00:07:53,774
huge amount of shuffling taking place. A copy of the smaller table

126
00:07:53,822 --> 00:07:57,530
can be stored upfront alongside the larger table on the same machine,

127
00:07:57,630 --> 00:08:00,710
so it's available when the join needs to be performed.

128
00:08:01,370 --> 00:08:04,710
And I refer back to the classic notes here of latency numbers

129
00:08:04,780 --> 00:08:08,086
that every programmer should know. When you compare the time it

130
00:08:08,108 --> 00:08:11,734
takes to read from memory versus reading from disk versus

131
00:08:11,782 --> 00:08:15,754
round trips over a network, we're talking orders of magnitude difference.

132
00:08:15,872 --> 00:08:19,034
Even though at certain scale we're counting things

133
00:08:19,072 --> 00:08:22,942
in nanoseconds and milliseconds. When the data gets large

134
00:08:23,076 --> 00:08:26,542
and you're dealing with many different unique items, we're talking

135
00:08:26,596 --> 00:08:30,234
orders of magnitude difference. So why are sketches

136
00:08:30,282 --> 00:08:33,566
fast? There are four main parts of sketching I should

137
00:08:33,588 --> 00:08:37,346
go through first. So the first is the stream processor. It processes the

138
00:08:37,368 --> 00:08:41,570
data as it streams in with a random algorithms selection.

139
00:08:42,150 --> 00:08:45,794
There's also a data structure which starts off as an empty summary and

140
00:08:45,832 --> 00:08:49,126
is updated with each data point over time. As it comes in,

141
00:08:49,228 --> 00:08:53,074
the size grows sublinearly. The query processor,

142
00:08:53,122 --> 00:08:57,506
it computes the desired result and models the error properties using probability

143
00:08:57,618 --> 00:09:01,034
statistics. And lastly, we have merge and set

144
00:09:01,072 --> 00:09:04,406
operations. So these are the operations that operate

145
00:09:04,438 --> 00:09:07,914
on multiple sketches, and there is no loss in accuracy doing

146
00:09:07,952 --> 00:09:11,478
so. Here's an illustration of those four parts.

147
00:09:11,574 --> 00:09:15,438
So you have the front end stream processor that uses a stochastic process

148
00:09:15,524 --> 00:09:19,850
to randomly select data, which it uses to populate a data structure.

149
00:09:20,010 --> 00:09:23,374
This data structure can be queried at any time using a back

150
00:09:23,412 --> 00:09:27,138
end query processor that has probabilistic analysis to return

151
00:09:27,224 --> 00:09:30,658
an approximation with known error thresholds. And at

152
00:09:30,664 --> 00:09:34,466
the bottom there you have a way of dealing with multiple sketches where you can

153
00:09:34,488 --> 00:09:38,294
stream them in and perform merge or set operations to combine them

154
00:09:38,332 --> 00:09:41,974
into their resulting sketches. And you should note that the sketches are

155
00:09:42,012 --> 00:09:44,680
designed to be cross language compatible as well.

156
00:09:45,050 --> 00:09:48,530
The data structures should be easily queryable

157
00:09:48,610 --> 00:09:51,450
by the implementation in any language.

158
00:09:52,430 --> 00:09:55,978
I used the word sublinear earlier, and what does that mean?

159
00:09:56,064 --> 00:10:00,326
So here on the x axis we're plotting the number of items or the cardinality,

160
00:10:00,518 --> 00:10:04,094
and on the y axis it's the memory or the space. So in most

161
00:10:04,132 --> 00:10:07,310
data structures that take exact representations of input data.

162
00:10:07,380 --> 00:10:10,958
So for example, if you're using words in a key value store,

163
00:10:11,124 --> 00:10:14,930
the growth in memory as cardinality goes up is somewhere between

164
00:10:15,000 --> 00:10:18,306
linear and superlinear. But with sketches it's always

165
00:10:18,408 --> 00:10:21,410
smaller. So with sketches it's always sublinear.

166
00:10:21,830 --> 00:10:24,722
The mergeability I mentioned also keeps things fast.

167
00:10:24,856 --> 00:10:28,386
This enables the merging of multiple sketches without loss in accuracy, and this

168
00:10:28,408 --> 00:10:31,526
solves nonadditive challenges. So I've thrown another

169
00:10:31,548 --> 00:10:34,402
term at you. So I'm going to define that too. So what are non additive

170
00:10:34,466 --> 00:10:37,618
challenges? So, aside from being absolutely everywhere,

171
00:10:37,714 --> 00:10:41,330
let's take this example where Quix has gone into the retail industry.

172
00:10:41,410 --> 00:10:45,290
So we've opened up four stores in some of the busiest cities in the world.

173
00:10:45,440 --> 00:10:49,162
They happen to be the fashion centers of the world. We have the annual sales

174
00:10:49,216 --> 00:10:52,550
figures from each store, as well as the count of unique products in

175
00:10:52,560 --> 00:10:56,126
each store. And with the actual sums of

176
00:10:56,148 --> 00:10:59,534
the annual sell figures, since money is additive, you can

177
00:10:59,572 --> 00:11:03,006
sum it up to get a global total sum. So you can see we're doing

178
00:11:03,028 --> 00:11:06,814
pretty good here. But when it comes to the unique item counts,

179
00:11:06,862 --> 00:11:10,206
you can't do this, because the last time I checked, the quicks

180
00:11:10,238 --> 00:11:13,906
London store definitely has some of the same items as the quicks Paris store.

181
00:11:14,008 --> 00:11:17,294
And the quicks Tokyo store has some of the items as the Paris store.

182
00:11:17,352 --> 00:11:21,874
So there's usually an overlap. And since that information about unique keys

183
00:11:22,002 --> 00:11:25,522
is lost at aggregation time between the stores, you cannot

184
00:11:25,586 --> 00:11:29,350
simply sum them up. You would just get incorrect unique counts.

185
00:11:30,010 --> 00:11:33,482
But if you use sketches, you could solve this. You could save

186
00:11:33,536 --> 00:11:37,626
the sketch for daily records in the database in their own column here on

187
00:11:37,648 --> 00:11:40,826
the left, we have two tables. So one is for all

188
00:11:40,848 --> 00:11:44,190
the days in February for the London store, and the one below

189
00:11:44,260 --> 00:11:47,786
is all the days in February for the quicks Paris

190
00:11:47,818 --> 00:11:50,958
store. We could union the number of days in the

191
00:11:50,964 --> 00:11:54,526
month to get the sketches for the unique products per store. Then to get

192
00:11:54,548 --> 00:11:57,698
the monthly unique count, it would be a case of doing a merge on the

193
00:11:57,704 --> 00:12:01,234
store's sketches. So here we can combine the sketches from each

194
00:12:01,272 --> 00:12:04,782
of the european stores to get a european approximation

195
00:12:04,846 --> 00:12:08,606
of uniques. And of course, you could employ data modeling

196
00:12:08,638 --> 00:12:12,422
to get the exact unique counts, but that would involve combining data

197
00:12:12,476 --> 00:12:16,278
across doors. That's probably a lot of network I o. And you

198
00:12:16,284 --> 00:12:19,078
would have to do this in a batch job. You'd wait for all the data

199
00:12:19,164 --> 00:12:22,986
to come in and could perform historical batch. You'd perform it

200
00:12:23,008 --> 00:12:26,922
as a historical batch job. So this is a really good example of using

201
00:12:26,976 --> 00:12:30,314
sketches where you're able to do approximate analysis with

202
00:12:30,352 --> 00:12:33,898
data that is being streamed in, and to perform that approximation

203
00:12:33,994 --> 00:12:37,854
in real time or near real time. And this is really why

204
00:12:37,892 --> 00:12:41,070
merge ability with sketches is such a superpower.

205
00:12:41,650 --> 00:12:44,290
Let's have a look at some of my favorite sketches.

206
00:12:45,110 --> 00:12:48,386
So the first is counting unique. So the ones I really

207
00:12:48,408 --> 00:12:51,874
like are the ones that estimate count distinct. So there's one

208
00:12:51,912 --> 00:12:56,066
called the count min sketch. Also, for estimating

209
00:12:56,258 --> 00:13:00,066
the cardinality amongst many duplicates. So for that you have hyperlog

210
00:13:00,098 --> 00:13:03,670
log and you have compressed probabilistic counting sketches.

211
00:13:04,490 --> 00:13:07,766
You also can estimate frequent items. So this is good for

212
00:13:07,788 --> 00:13:11,178
recommended systems as well. So when you estimate what are termed the

213
00:13:11,184 --> 00:13:14,218
heavy hitters or top k, for example,

214
00:13:14,304 --> 00:13:17,962
during comp 42, if we were streaming in the data, we could

215
00:13:18,016 --> 00:13:22,510
perform an approximation for what are the top most viewed talks in this conference.

216
00:13:23,490 --> 00:13:27,562
We have quantiles and data distributions. So this estimates the distribution.

217
00:13:27,626 --> 00:13:31,306
So you've got the different percentiles, like 25th percentile,

218
00:13:31,418 --> 00:13:35,374
95th percentile, 99 percentile. You can do all of those with well

219
00:13:35,412 --> 00:13:39,134
defined error bounds. So if you're streaming in data again from this conference,

220
00:13:39,182 --> 00:13:42,834
you could determine and approximate how much time people spend on

221
00:13:42,872 --> 00:13:46,158
each of the web pages. And lastly, we have sampling.

222
00:13:46,254 --> 00:13:49,714
So sampling keeps some semblance or the transformation

223
00:13:49,762 --> 00:13:53,106
of the item from the stream. For example, with reservoir

224
00:13:53,138 --> 00:13:57,026
sampling, you choose k samples from a list of n items and you retain

225
00:13:57,058 --> 00:14:00,254
that. And to give you an idea of how the algorithms

226
00:14:00,322 --> 00:14:03,946
work, I'll cover the count min sketch. Let's start with a

227
00:14:03,968 --> 00:14:08,102
naive example, and I'm going to go with food items. So here we're counting,

228
00:14:08,166 --> 00:14:11,578
or rather approximating food items. So on the left there we

229
00:14:11,584 --> 00:14:15,114
have a continuous stream of food items coming in. So in practice,

230
00:14:15,162 --> 00:14:18,814
this could be food items from orders in an online supermarket or

231
00:14:18,852 --> 00:14:22,142
a food delivery service. And on the right we have

232
00:14:22,196 --> 00:14:26,030
a hash or key value data structure where we're using

233
00:14:26,100 --> 00:14:29,794
the key as a representation of the food item, and the value is the

234
00:14:29,832 --> 00:14:33,038
incrementing count of the number of times it's observed in the stream.

235
00:14:33,054 --> 00:14:36,514
So as we go through the stream one at a time, we're counting this.

236
00:14:36,552 --> 00:14:39,954
So by the end, when we get to the last item there, we have counted

237
00:14:40,002 --> 00:14:43,666
one burger, three pizzas, one salad, one burrito, et cetera,

238
00:14:43,698 --> 00:14:47,074
et cetera. So this example does only have ten items,

239
00:14:47,122 --> 00:14:50,326
but this approach won't scale at high cardinality. So when there's

240
00:14:50,358 --> 00:14:53,130
a huge increase in the number of unique items,

241
00:14:53,950 --> 00:14:57,242
it just simply wouldn't scale. So if this stream introduced all the food

242
00:14:57,296 --> 00:15:01,286
items in the known universe, the data structure would have to grow linearly

243
00:15:01,318 --> 00:15:04,494
or sublinearly this way to hold

244
00:15:04,532 --> 00:15:08,382
all the keys. It would mean reserving huge amounts of memory and

245
00:15:08,436 --> 00:15:12,490
long wait times when you're querying to retrieve the cans for a food item.

246
00:15:12,650 --> 00:15:16,238
And if you wanted this in real time or near real time, forget about it.

247
00:15:16,404 --> 00:15:19,586
And from here, there's not much you could do to improve things, like in a

248
00:15:19,608 --> 00:15:23,586
naive way, you could reduce the representation of the food item down to

249
00:15:23,608 --> 00:15:27,842
perhaps the first three characters. So, for example, instead of storing

250
00:15:27,986 --> 00:15:31,142
the graphical bitmap, you could save pizza as

251
00:15:31,196 --> 00:15:34,962
P-I-Z burger becomes bur. But then eventually,

252
00:15:35,026 --> 00:15:38,322
as the list got longer and the cardinality increased,

253
00:15:38,386 --> 00:15:41,962
could start having key collisions, like when a burrito comes along

254
00:15:42,016 --> 00:15:44,620
and tries to increment bur as well.

255
00:15:45,390 --> 00:15:49,286
So let's see an example with hashing, which is a better representation

256
00:15:49,318 --> 00:15:52,650
of the count min sketch. So, in this example, rather than taking

257
00:15:52,720 --> 00:15:56,174
the exact representation of the food item, we'll run it through two

258
00:15:56,212 --> 00:16:00,046
hashing functions. And you can see that there on the right, we've got two

259
00:16:00,068 --> 00:16:03,326
of the columns, each of which represent the hashing function and

260
00:16:03,348 --> 00:16:06,766
the number that they return. And we're going to use this to know how

261
00:16:06,788 --> 00:16:10,002
to store it in the Data. So it's kind of like encoding a Burger as

262
00:16:10,056 --> 00:16:13,538
one and six and storing it in the Data structure here in this Array in

263
00:16:13,544 --> 00:16:18,406
the Middle, that Way. So we'll use the

264
00:16:18,428 --> 00:16:22,274
hashing functions there on the right to increment the counts in this 2d array

265
00:16:22,322 --> 00:16:25,346
in the middle. So this array starts off all blank,

266
00:16:25,378 --> 00:16:28,706
or rather all zeros. And the first item

267
00:16:28,738 --> 00:16:31,702
in the Stream is a Burger. So we pass it through to the two hashing

268
00:16:31,766 --> 00:16:35,222
functions there on the right. And we see, for Burger, we get the representation

269
00:16:35,286 --> 00:16:39,082
of one and six. We then go to the one

270
00:16:39,136 --> 00:16:42,234
in the first column and increment that by one. So it goes from zero to

271
00:16:42,272 --> 00:16:45,642
one. And then we go to six in the second column, and we increment

272
00:16:45,706 --> 00:16:49,178
that also by one. Now we have one and one in those positions, and we're

273
00:16:49,194 --> 00:16:52,762
done for this item. So we read the next item in the stream.

274
00:16:52,906 --> 00:16:56,266
It's the pizza. We look it up on the right there, and it's

275
00:16:56,298 --> 00:16:59,954
encoded as four and one. So we go to positions four and

276
00:16:59,992 --> 00:17:03,246
one in our array in the Middle, and we increment at those positions,

277
00:17:03,278 --> 00:17:07,118
four and one. So we have a count of one and one. And we'll

278
00:17:07,134 --> 00:17:09,686
keep doing that. So we can do that for Salad as well. But let's see

279
00:17:09,708 --> 00:17:13,026
what happens when a duplicate appears. So when another pizza appears,

280
00:17:13,138 --> 00:17:16,550
we look up its hash representation again, which is for one.

281
00:17:16,700 --> 00:17:20,422
And since we already have a count of one in each, we increment them again.

282
00:17:20,476 --> 00:17:24,186
So now we have the number two in both those positions. So you

283
00:17:24,208 --> 00:17:27,478
just keep incrementing them by time you encounter them in the Stream.

284
00:17:27,574 --> 00:17:31,066
So let's fast forward to the last item. So this is what it looks

285
00:17:31,088 --> 00:17:34,474
like when all ten food items are stored in the Array. And now

286
00:17:34,512 --> 00:17:38,394
let's try querying the Data structure now. Because it's a stream, you can query

287
00:17:38,442 --> 00:17:41,582
it at any point in time. That's the beauty of this. So you can perform

288
00:17:41,636 --> 00:17:45,182
an approximation at any time that you like.

289
00:17:45,316 --> 00:17:48,706
So let's say at the 10th item mark, we want

290
00:17:48,728 --> 00:17:52,466
to query this Data structure. We want to get an approximation for the count of

291
00:17:52,488 --> 00:17:56,414
burgers. So we look there, on the right there, we see the hash representation

292
00:17:56,462 --> 00:17:59,638
is one and six. So we look into our data structure in

293
00:17:59,644 --> 00:18:03,126
the middle and we get the counts of three and one.

294
00:18:03,308 --> 00:18:07,014
Now you've probably figured out why this sketch is called count min, because we take

295
00:18:07,052 --> 00:18:10,554
the minimum value of the obtained counts. So out of three and one,

296
00:18:10,672 --> 00:18:13,994
the smaller number is one. So we have our approximation of

297
00:18:14,032 --> 00:18:17,306
account of one for burgers, which in this case matches the

298
00:18:17,328 --> 00:18:21,110
exact count. Let's do another example. Do the same with pizza,

299
00:18:21,190 --> 00:18:24,606
which has a hash representation of four one. So we

300
00:18:24,628 --> 00:18:28,494
look up four one in the middle data structure, and that gives us account of

301
00:18:28,532 --> 00:18:32,014
three four, and we take the min. So that's an approximate could

302
00:18:32,052 --> 00:18:36,110
of three for pizza, which again matches the exact count. So you get the idea.

303
00:18:36,260 --> 00:18:38,898
You can see that for some of the food items, for example, if you look

304
00:18:38,904 --> 00:18:41,618
at the last one there for tacos, that's six and one,

305
00:18:41,704 --> 00:18:44,658
which gives us a could in the middle of one and four. So you can

306
00:18:44,664 --> 00:18:48,470
see that though we take the min as one and our approximation becomes one,

307
00:18:48,620 --> 00:18:52,386
one of its counts got as high as four. So some errors can start creeping

308
00:18:52,418 --> 00:18:56,134
in, like with this particular size. And so

309
00:18:56,252 --> 00:18:59,554
this would suggest that you either need to increase the space, making the arrays

310
00:18:59,602 --> 00:19:02,906
bigger, or adding more hashing functions. And these are sort of parameters you

311
00:19:02,928 --> 00:19:06,394
can control in sketching. So this would all work together to

312
00:19:06,432 --> 00:19:10,300
reduce the collisions and keep the counts as accurate as possible.

313
00:19:11,630 --> 00:19:14,826
So now that you know what sketches are, how they work and how

314
00:19:14,848 --> 00:19:18,462
useful they are, how do you get started with them today? So you're probably thinking,

315
00:19:18,516 --> 00:19:22,266
will I need to implement them like it sounds like some complex statistics and math

316
00:19:22,298 --> 00:19:25,934
going on. Is there anything available in open source? Well, the good news

317
00:19:25,972 --> 00:19:29,266
is you'll be glad to hear such a project does exist. So please go and

318
00:19:29,288 --> 00:19:32,642
check out the Apache data sketches project. It's available

319
00:19:32,696 --> 00:19:36,466
in Java, C Plus plus, and you'll be happy to know it's also available in

320
00:19:36,488 --> 00:19:40,186
Python. This project was started at Yahoo in 2019,

321
00:19:40,188 --> 00:19:44,438
and it achieved Apache's top level project status in 2021.

322
00:19:44,524 --> 00:19:48,546
So it has a really active community and it's got a good roadmap for implementing

323
00:19:48,578 --> 00:19:52,214
sketches. There's a very clear picture of the sketches

324
00:19:52,262 --> 00:19:55,958
that have been implemented and the ones that are coming up. And the beauty

325
00:19:55,974 --> 00:19:58,982
of this project is that it's cross language compatible,

326
00:19:59,046 --> 00:20:02,054
meaning that you can mix and match languages for different components.

327
00:20:02,102 --> 00:20:05,486
So like I mentioned before, with the stream processor you

328
00:20:05,508 --> 00:20:09,374
could implement that in Java and then the back end query processor could be

329
00:20:09,412 --> 00:20:13,354
implemented in Python and it would all work. But we're at Conf 42 Python,

330
00:20:13,402 --> 00:20:16,938
so of course we're going to do it all in Python and I'm

331
00:20:16,954 --> 00:20:20,306
going to do that here. So I'm going to use these final few slides to

332
00:20:20,328 --> 00:20:24,014
show you some code in Python. So this is an example in a Jupyter notebook

333
00:20:24,142 --> 00:20:27,298
where I load in some Spotify data and filter it

334
00:20:27,304 --> 00:20:31,094
for the artist information. So you see I've imported in the library there,

335
00:20:31,212 --> 00:20:35,266
and I've also imported in pandas to load in the data so they're interoperable.

336
00:20:35,378 --> 00:20:39,058
You can see there at the bottom there that I've specified a confidence score

337
00:20:39,074 --> 00:20:42,578
that I want, which is percentage. So I want 90% confidence.

338
00:20:42,674 --> 00:20:46,774
And you see that the library gives you some convenience functions

339
00:20:46,822 --> 00:20:50,634
that helps you get a suggestion for the number of hashes you should be

340
00:20:50,672 --> 00:20:54,318
using to get that level of confidence. And the same again for

341
00:20:54,324 --> 00:20:57,374
the relative error, which I'm going for 1%, it will suggest the number

342
00:20:57,412 --> 00:21:00,666
of buckets that you want to instantiate this count min sketch

343
00:21:00,698 --> 00:21:04,046
with. And to make it easier to

344
00:21:04,068 --> 00:21:07,058
follow, you'll see that I load in the records one at a time using a

345
00:21:07,064 --> 00:21:10,338
loop. So when you run it with such a

346
00:21:10,344 --> 00:21:13,538
large data store, such a large data source as the

347
00:21:13,544 --> 00:21:16,978
Spotify data that I'm using, it actually takes a while.

348
00:21:17,064 --> 00:21:20,566
And in reality what you'd be doing is you'd be reading the records one

349
00:21:20,588 --> 00:21:23,862
at a time from a stream and then updating the sketch that way.

350
00:21:23,996 --> 00:21:27,734
So for that you could use Kafka with quicks for this because

351
00:21:27,772 --> 00:21:31,066
it's sort of a general purpose use case. But if your use case was

352
00:21:31,088 --> 00:21:35,174
something else like getting approximation statistics like quantiles

353
00:21:35,222 --> 00:21:38,758
and counts from changes that are happening in one of your databases,

354
00:21:38,934 --> 00:21:42,266
you could use change data capture with something like debesium to

355
00:21:42,288 --> 00:21:45,566
obtain a stream of changes, and then you perform the updates in that

356
00:21:45,588 --> 00:21:49,066
way. So getting into a stream is really the first part of the puzzle.

357
00:21:49,098 --> 00:21:52,958
There you can see the sketches provide functions there

358
00:21:53,044 --> 00:21:57,086
which allow you to specify different things. There you've

359
00:21:57,118 --> 00:22:00,386
got a lovely print statement there that shows you

360
00:22:00,408 --> 00:22:03,826
what their sketch summary is with their number of hashes, the number of

361
00:22:03,848 --> 00:22:07,398
buckets, the number of filled bins, et cetera. So this is a good way for

362
00:22:07,404 --> 00:22:11,190
you to keep can eye on how you should size out your sketches.

363
00:22:12,090 --> 00:22:15,986
So yeah, you can see they maintain

364
00:22:16,018 --> 00:22:19,334
the desired confidence score. You've got nice summaries, and at the end you can see

365
00:22:19,372 --> 00:22:23,178
I've put in the estimations. I've requested the estimations for some

366
00:22:23,184 --> 00:22:25,930
of the popular artists in that data source,

367
00:22:26,910 --> 00:22:30,602
and this one's my last slide. So you can pip install the Python library today

368
00:22:30,656 --> 00:22:34,362
and get started. You may already even have sketching algorithms

369
00:22:34,426 --> 00:22:38,154
available to you if you use certain database technologies or processing engines.

370
00:22:38,202 --> 00:22:41,274
So we have Druid, we have postgres and Hive,

371
00:22:41,322 --> 00:22:44,826
which have been supported for some years, and it's been implemented

372
00:22:44,858 --> 00:22:48,194
recently in Apache Pinot and Apache Spark last

373
00:22:48,232 --> 00:22:51,666
year as well. So if you're using any of these, you should be able to

374
00:22:51,848 --> 00:22:55,858
just search for in the documentation the sketching algorithms available,

375
00:22:55,944 --> 00:22:59,974
and a lot of them will have things like hyperlog log and some

376
00:23:00,012 --> 00:23:03,718
sort of counting or quantile operation they should have.

377
00:23:03,884 --> 00:23:07,074
So as I mentioned, there is an active community of mathematicians,

378
00:23:07,122 --> 00:23:10,466
data scientists and engineers working on sketches, so you'd

379
00:23:10,498 --> 00:23:13,466
be amongst really good company if you do check out the project and want to

380
00:23:13,488 --> 00:23:16,874
contribute. And so yes, I'm certainly

381
00:23:16,912 --> 00:23:19,798
going to be exploring more sketches in future, but I'm curious to know how you'll

382
00:23:19,814 --> 00:23:22,778
get on as well. So please do let me know what you think. Here are

383
00:23:22,784 --> 00:23:26,010
the links to the Quickstream's open source library in GitHub,

384
00:23:26,090 --> 00:23:29,598
where you can start the project to follow its process and

385
00:23:29,684 --> 00:23:33,502
follow its progress, and most importantly, show us your support. And also

386
00:23:33,556 --> 00:23:36,206
please do come hang out with me and the rest of the Python team in

387
00:23:36,228 --> 00:23:39,374
the quicks community on slack, and we'd love nothing more than

388
00:23:39,412 --> 00:23:42,606
to help you get started with streaming data using Python today.

389
00:23:42,708 --> 00:23:45,878
So if you're going to implement the algorithms and you

390
00:23:45,884 --> 00:23:49,286
don't know where to start, please join us there. So thank you

391
00:23:49,308 --> 00:23:52,566
very much for taking the time to join me today, and I hope to see

392
00:23:52,588 --> 00:23:54,020
you soon. Peace out.

