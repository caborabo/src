1
00:00:20,810 --> 00:00:24,174
Hi. Welcome everyone, and thank you so much for joining this

2
00:00:24,212 --> 00:00:28,598
talk on how to test your Apache Airflow data pipelines using

3
00:00:28,684 --> 00:00:32,482
common Python frameworks. So I'm very excited

4
00:00:32,546 --> 00:00:36,134
to talk to you about this topic. First, I'll be giving a

5
00:00:36,172 --> 00:00:39,186
quick introduction to Apache Airflow and astronomer,

6
00:00:39,298 --> 00:00:42,810
and I will talk a little bit about why you want to test your data

7
00:00:42,880 --> 00:00:46,342
pipelines. After that, I will go over the technical

8
00:00:46,406 --> 00:00:50,042
details on how to implement testing inside of local

9
00:00:50,096 --> 00:00:53,738
development environments and also how you can add

10
00:00:53,824 --> 00:00:57,338
your airflow tests to your CI CD pipeline.

11
00:00:57,514 --> 00:01:01,326
I will show everything that I show in the slides in a demo as well,

12
00:01:01,428 --> 00:01:04,818
and the demo repository code will be available to you

13
00:01:04,904 --> 00:01:07,330
in an open source GitHub repository.

14
00:01:08,950 --> 00:01:12,610
But first, what is Airflow? Who is astronomer now?

15
00:01:12,680 --> 00:01:16,382
Airflow is the open standard for workflow management.

16
00:01:16,526 --> 00:01:20,246
It's a very popular tool. It's being downloaded millions of

17
00:01:20,268 --> 00:01:24,194
times per month, which is just a lot. We have a big slack

18
00:01:24,242 --> 00:01:27,862
community. If you ever have a question about Apache Airflow, this is the best

19
00:01:27,916 --> 00:01:30,986
place to ask. Just join our slack and you

20
00:01:31,008 --> 00:01:35,290
will be able to talk to all sorts of airflow users and also developers.

21
00:01:35,630 --> 00:01:39,162
It's a very actively developed project. There are over

22
00:01:39,216 --> 00:01:42,526
2800 contributors now, and there are a

23
00:01:42,548 --> 00:01:47,022
lot of building blocks in the ecosystem now.

24
00:01:47,076 --> 00:01:51,054
That's the high level, a workflow management system. If you are

25
00:01:51,092 --> 00:01:54,498
working on data, you will recognize some or a

26
00:01:54,504 --> 00:01:58,050
lot of these logos. Usually if you are working on data,

27
00:01:58,120 --> 00:02:02,062
you have your data in different places and you perform actions

28
00:02:02,126 --> 00:02:05,554
on the data using different tools, like one or more

29
00:02:05,592 --> 00:02:08,966
of these most likely. And some of these

30
00:02:08,988 --> 00:02:13,270
tools talk together, but not all of them. And you really are missing something

31
00:02:13,340 --> 00:02:17,174
in the middle there, right? You're missing a centerpiece, and that is really

32
00:02:17,212 --> 00:02:20,954
what Airflow is. Airflow is the orchestrator to your

33
00:02:20,992 --> 00:02:24,394
data symphony. So Airflow sits in the middle and is

34
00:02:24,432 --> 00:02:28,214
able to talk to all of these pieces to orchestrate

35
00:02:28,262 --> 00:02:32,010
actions in all of these data tools with the right dependencies,

36
00:02:32,090 --> 00:02:35,200
on the right cadence at the exact right time.

37
00:02:36,530 --> 00:02:40,202
We very recently did a survey of airflow users,

38
00:02:40,266 --> 00:02:43,706
and among the about 800 people who responded,

39
00:02:43,818 --> 00:02:47,666
81% actually said that Airflow is important

40
00:02:47,768 --> 00:02:51,106
or even very important to their business. So if

41
00:02:51,128 --> 00:02:54,546
there is something going on in their airflow pipelines, it is

42
00:02:54,568 --> 00:02:57,810
a big deal in their companies and in their businesses.

43
00:02:59,290 --> 00:03:02,726
That's the high level. But what does it actually look like day to

44
00:03:02,748 --> 00:03:06,454
day? If you are working with Airflow in general for this talk,

45
00:03:06,492 --> 00:03:10,482
I will assume that you've used airflow before, that you've written a couple of dags,

46
00:03:10,546 --> 00:03:13,594
but in case you have not, or in case you need a refresher on more

47
00:03:13,632 --> 00:03:17,290
modern syntax. This is the Airflow 101 slide.

48
00:03:17,710 --> 00:03:22,086
On the left hand side you can see a screenshot from the airflow UI.

49
00:03:22,278 --> 00:03:25,786
So this shows one pipelines within airflow which

50
00:03:25,808 --> 00:03:29,706
we call dag that stands for directed asyclic graph.

51
00:03:29,898 --> 00:03:33,866
And on the right hand side you can see all of the code that went

52
00:03:33,908 --> 00:03:38,050
into creating this DAC. So we have a DAC.

53
00:03:38,470 --> 00:03:41,534
In this case it is called intro to airflow DAC.

54
00:03:41,662 --> 00:03:45,714
And this is just defined by using a decorator in this case.

55
00:03:45,912 --> 00:03:49,414
So I use this Act DAC decorator on line seven, I put

56
00:03:49,452 --> 00:03:52,866
it on a python function and if I put this file

57
00:03:52,898 --> 00:03:56,898
into the right folder, which I'm going to show in the demo, then airflow knows

58
00:03:56,994 --> 00:04:00,394
this is a DAG and automatically will pick

59
00:04:00,432 --> 00:04:04,230
it up. A dag itself contains

60
00:04:04,310 --> 00:04:08,106
tasks. So in the graph on the left hand side you

61
00:04:08,128 --> 00:04:11,386
can see we have a graph with two nodes. Those are the

62
00:04:11,408 --> 00:04:14,270
tasks, pick a number and add 23,

63
00:04:14,420 --> 00:04:18,062
and the edge in between is the dependency. So the first

64
00:04:18,116 --> 00:04:21,262
task, pick a number needs to happen before the second

65
00:04:21,316 --> 00:04:24,020
task add 23 is happening.

66
00:04:24,550 --> 00:04:29,154
Now these tasks can be defined in two main different ways.

67
00:04:29,352 --> 00:04:32,958
You can use decorators or operator classes

68
00:04:33,054 --> 00:04:36,882
because it is all just python code under the hood. So the first

69
00:04:36,936 --> 00:04:40,802
task, pick a number I've defined using the add task decorator.

70
00:04:40,866 --> 00:04:44,326
I just used a regular Python function, put add task on

71
00:04:44,348 --> 00:04:48,134
top of it and then I can put any python code in between and

72
00:04:48,172 --> 00:04:51,546
airflow will automatically create a task out

73
00:04:51,568 --> 00:04:54,778
of this. This is already how you can turn all of

74
00:04:54,784 --> 00:04:57,978
your Python scripts into airflow tasks if you want to

75
00:04:57,984 --> 00:05:00,330
do that without any additional knowledge.

76
00:05:01,250 --> 00:05:05,054
Now this is one way that you can create a task. The other one

77
00:05:05,092 --> 00:05:09,246
is by using an operator class. There are

78
00:05:09,428 --> 00:05:13,040
thousands of these operator classes around

79
00:05:13,490 --> 00:05:16,670
and a lot of them are bundled into provider packages.

80
00:05:16,830 --> 00:05:20,034
So they abstract away a lot of the

81
00:05:20,072 --> 00:05:23,454
logic that you need to interact with certain data tools.

82
00:05:23,502 --> 00:05:26,974
For example, there's an Azure provider, there's a Google provider,

83
00:05:27,022 --> 00:05:30,550
there's an Amazon provider, and they contain a lot of pre made

84
00:05:30,620 --> 00:05:34,854
operators that make it easier for you to talk to those services.

85
00:05:35,052 --> 00:05:38,534
In this case, I used an operator called my basic

86
00:05:38,582 --> 00:05:41,766
math operator, and this is an operator that I actually wrote

87
00:05:41,798 --> 00:05:45,446
myself for demonstration purposes.

88
00:05:45,478 --> 00:05:49,770
So I can also show you the best later that test this specific operator.

89
00:05:50,190 --> 00:05:54,080
In this case, this operator just adds the number 23,

90
00:05:54,530 --> 00:05:58,286
in this case to the number that is being returned from

91
00:05:58,308 --> 00:06:01,806
the first task. So you can see on line 25 I can actually

92
00:06:01,908 --> 00:06:05,454
use the output of an upstream task in my downstream

93
00:06:05,502 --> 00:06:09,570
task, so you can also exchange information in between tasks.

94
00:06:10,150 --> 00:06:13,874
Lastly, on line 30, I say, well, let's chain those

95
00:06:13,912 --> 00:06:18,040
tasks. I need to pick a number before I add 23.

96
00:06:18,490 --> 00:06:21,826
That is already all you need to know to define

97
00:06:21,858 --> 00:06:25,298
your first few airflow. Dax two notes on best practices.

98
00:06:25,474 --> 00:06:27,698
Usually, DAX run periodically,

99
00:06:27,794 --> 00:06:31,510
incrementally, and automatically. So you usually

100
00:06:31,580 --> 00:06:35,034
have some sort of cadence that can be based on time. Like in this

101
00:06:35,072 --> 00:06:38,806
case, I have a daily schedule, but it can also be based on something that's

102
00:06:38,838 --> 00:06:41,994
data driven. So you can, for example, wait on an action

103
00:06:42,042 --> 00:06:45,726
in a different data tool. Or you can also

104
00:06:45,908 --> 00:06:49,470
say that one dag needs to happen once another dag has

105
00:06:49,540 --> 00:06:52,922
finished, and usually it works incrementally

106
00:06:52,986 --> 00:06:57,102
on data if you're using it for more traditional ETL ELT pipelines.

107
00:06:57,246 --> 00:07:01,074
And ideally it runs automatically with only little input from

108
00:07:01,112 --> 00:07:04,434
you as soon as you're done developing them. Now,

109
00:07:04,472 --> 00:07:08,382
for the tasks themselves, we have three best practice

110
00:07:08,446 --> 00:07:11,974
words that you can keep in mind when you're writing a task. The first

111
00:07:12,012 --> 00:07:15,960
one is they should be item potent, which I already broke in this little example.

112
00:07:16,330 --> 00:07:19,926
You might have noticed I'm using random, and this will not be

113
00:07:19,948 --> 00:07:23,866
item potent because I'm not seeding it in this case. But ideally you

114
00:07:23,888 --> 00:07:27,386
want your task to always have the same output for the

115
00:07:27,408 --> 00:07:31,226
same input. This is important when you're rerunning tasks after the

116
00:07:31,248 --> 00:07:34,862
fact, which airflow gives you a lot of options to do that

117
00:07:34,996 --> 00:07:38,510
maybe a month later or even a year later to rerun a specific

118
00:07:38,580 --> 00:07:42,014
run of a dag. The next one is to keep

119
00:07:42,052 --> 00:07:45,626
your tasks atomic, so you want your tasks to do

120
00:07:45,668 --> 00:07:49,202
as little as possible. You do not want to have a very long

121
00:07:49,256 --> 00:07:53,746
script that does a lot of things, that even talks to different outside

122
00:07:53,848 --> 00:07:57,710
tools all in the same task, because then you lose observability

123
00:07:57,870 --> 00:08:01,286
and you lose a lot of the additional features that airflow can

124
00:08:01,308 --> 00:08:04,278
give you, or they are just not that useful for you if you pack everything

125
00:08:04,364 --> 00:08:08,086
into the same task. And lastly, because it is all just python

126
00:08:08,118 --> 00:08:11,766
code, you can create this in a modular structure.

127
00:08:11,958 --> 00:08:15,210
You can put functions in

128
00:08:15,280 --> 00:08:18,422
supporting folders, import them, use them in different dags.

129
00:08:18,486 --> 00:08:22,026
You can even build your own custom framework on top of

130
00:08:22,048 --> 00:08:25,854
airflow on how you define and write dags. It is all just

131
00:08:25,892 --> 00:08:28,350
python, and it is all fully open source.

132
00:08:30,450 --> 00:08:33,574
All right, I already said airflow is open source.

133
00:08:33,722 --> 00:08:37,218
What is astronomer? So astronomer is

134
00:08:37,304 --> 00:08:41,134
the best place to run Apache Airflow in production. I'm obviously biased,

135
00:08:41,182 --> 00:08:44,674
but I truly believe that it is a managed service that

136
00:08:44,712 --> 00:08:48,178
gives you the infrastructure to run your Apache airflow pipelines,

137
00:08:48,274 --> 00:08:51,506
especially once you grow and you have hundreds of pipelines,

138
00:08:51,698 --> 00:08:55,218
this helps you run airflow reliably in production.

139
00:08:55,394 --> 00:08:59,686
Of course, there are other ways you can run airflow with

140
00:08:59,708 --> 00:09:02,726
open source tools. Like there is an official helm chart.

141
00:09:02,758 --> 00:09:06,362
A lot of people have custom helm charts. There are a lot of different

142
00:09:06,416 --> 00:09:10,214
options that you have. Astronomer also provides

143
00:09:10,342 --> 00:09:14,278
additional tooling on top of airflow. If you want to

144
00:09:14,304 --> 00:09:17,514
try this, there's a free trial. If you go to astronomer IO

145
00:09:17,562 --> 00:09:21,278
slash Triastro, you can take it for a spin. But everything

146
00:09:21,364 --> 00:09:25,202
that I'm going to show you in this talk is possible with fully open

147
00:09:25,256 --> 00:09:28,946
source tools. You do not need to be an astronomer customer to

148
00:09:29,128 --> 00:09:33,140
do all of the testing ways that I'm going to show you in this talk.

149
00:09:34,970 --> 00:09:38,310
All right. Why should you test your data pipelines?

150
00:09:39,610 --> 00:09:42,966
Well, I already alluded to something. Airflow is really

151
00:09:42,988 --> 00:09:46,342
important in a lot of businesses. So if something goes

152
00:09:46,396 --> 00:09:49,974
wrong, it can look like this. Of course,

153
00:09:50,012 --> 00:09:53,114
there are a ton of reasons why there could be something up with

154
00:09:53,152 --> 00:09:56,746
your data pipelines. The worst way to find out is

155
00:09:56,768 --> 00:10:00,694
usually by a slack message from someone high up in the organization.

156
00:10:00,822 --> 00:10:04,874
That sounds something like, where is my dashboard? Or why is my dashboard

157
00:10:04,922 --> 00:10:08,366
showing something nonsensical? You cannot prevent all

158
00:10:08,388 --> 00:10:11,786
of it, but you can prevent some of these events by testing

159
00:10:11,818 --> 00:10:15,226
your data pipelines. And even if there

160
00:10:15,268 --> 00:10:18,734
is something going on, if your pipelines are tested,

161
00:10:18,782 --> 00:10:22,434
it will make it easier for you to debug because you know which part

162
00:10:22,472 --> 00:10:24,340
of your pipelines you can trust.

163
00:10:26,230 --> 00:10:29,846
This is kind of the takeaway message of this talk. Airflow is written in

164
00:10:29,868 --> 00:10:33,270
Python, and airflow pipelines are just Python code,

165
00:10:33,340 --> 00:10:37,394
so it's Python all the way down. And this means that all software

166
00:10:37,442 --> 00:10:41,494
engineering and DevOps best practices apply, including testing

167
00:10:41,542 --> 00:10:45,002
and CI CD. You would never ship application code

168
00:10:45,056 --> 00:10:48,490
that is not unit tested, and you should also not do that

169
00:10:48,560 --> 00:10:51,874
with Apache Airflow. You should also test all of your DAx,

170
00:10:51,942 --> 00:10:53,230
all of your pipelines.

171
00:10:55,250 --> 00:10:58,794
Okay, told you why you should test it. Now let's

172
00:10:58,842 --> 00:11:02,414
get into how you actually can do the testing. And first we will

173
00:11:02,452 --> 00:11:06,290
talk about local development. So this is the overview.

174
00:11:06,630 --> 00:11:09,890
This is one of the potential structures that you can have

175
00:11:09,960 --> 00:11:13,650
when you're working with data pipelines. One important

176
00:11:13,720 --> 00:11:17,446
thing that is really just a DevOps best practice is all of the

177
00:11:17,468 --> 00:11:20,934
code is in source control. In version control. In this case,

178
00:11:20,972 --> 00:11:24,198
I use GitHub. But of course, you can use any version control

179
00:11:24,364 --> 00:11:26,120
system that you want.

180
00:11:26,970 --> 00:11:30,718
And if you're using Airflow for machine

181
00:11:30,754 --> 00:11:34,042
learning purposes, you will also have your ML artifacts inside

182
00:11:34,096 --> 00:11:37,734
of this version control. And in a lot of cases you have configuration files

183
00:11:37,782 --> 00:11:41,466
for infrastructure as code as well. But we will focus on

184
00:11:41,488 --> 00:11:43,470
the DAC code for this presentation.

185
00:11:44,290 --> 00:11:47,966
Now we have our version control. We have three branches, in this case,

186
00:11:48,068 --> 00:11:51,646
development, staging and production. And then we have all of

187
00:11:51,668 --> 00:11:55,354
the steps that our code takes. And on each of these steps

188
00:11:55,402 --> 00:11:59,058
you can see I put a little gear icon and CI CD because

189
00:11:59,144 --> 00:12:03,074
we are doing testing all the way along. And first

190
00:12:03,112 --> 00:12:06,194
of all, we are going to talk about the local part. So we

191
00:12:06,232 --> 00:12:09,718
have our code, we have our development branch. We are working locally on

192
00:12:09,724 --> 00:12:13,430
a machine, but we can already do some testing.

193
00:12:14,970 --> 00:12:18,166
Now, if you want to develop with airflow locally, you have

194
00:12:18,188 --> 00:12:21,626
a lot of different options. You can run a standalone instance. You can

195
00:12:21,648 --> 00:12:25,482
run airflow in Docker. There's one that I highly, highly recommend.

196
00:12:25,616 --> 00:12:29,338
It's called the Astro Cli. This is also a fully open source tool.

197
00:12:29,424 --> 00:12:33,438
It's developed by Astronomer, but it's not specific to astronomer customers.

198
00:12:33,604 --> 00:12:37,610
Everyone can use it. And this allows you to very easily

199
00:12:37,690 --> 00:12:41,354
spin up a local airflow environment within Docker.

200
00:12:41,482 --> 00:12:44,750
And it also includes built in testing features.

201
00:12:45,410 --> 00:12:49,566
So if you go to this link that is on the screen on the astronomer

202
00:12:49,598 --> 00:12:53,614
docs, the Astro Cli, you can get some install instructions.

203
00:12:53,742 --> 00:12:57,590
But what you really need to know is you just need three common first

204
00:12:57,660 --> 00:13:00,498
you install the package brew install Astro.

205
00:13:00,674 --> 00:13:04,130
Then you go to any empty directory on your machine

206
00:13:04,290 --> 00:13:08,382
and you run astrodefinit. And once you run astrodefinite,

207
00:13:08,466 --> 00:13:11,542
a local astro project will be initialized.

208
00:13:11,686 --> 00:13:15,334
And this is already fully functional with two example DAX.

209
00:13:15,462 --> 00:13:18,634
So you can directly say Astrodef start.

210
00:13:18,752 --> 00:13:22,214
And after one or two minutes you have airflow running locally

211
00:13:22,262 --> 00:13:25,914
within Docker, and you can access the efflow UI at localhost

212
00:13:25,962 --> 00:13:28,782
80 80. Of course,

213
00:13:28,836 --> 00:13:32,320
then you can start adding your own DAX and start to develop.

214
00:13:34,850 --> 00:13:37,986
All right, let's say you've done that. You used the

215
00:13:38,008 --> 00:13:41,534
Astro Cli, you have your project, you added your DAX.

216
00:13:41,662 --> 00:13:45,362
How do you go about testing those DAX? And the first option

217
00:13:45,416 --> 00:13:48,626
that you have is by using CLI features. If you've

218
00:13:48,658 --> 00:13:52,386
used airflow before, you are probably familiar with the blue

219
00:13:52,418 --> 00:13:56,482
ones on the screen. Those are part of the general Airflow CLI.

220
00:13:56,626 --> 00:14:00,106
You can also use those with the Astro Cli. You just have

221
00:14:00,128 --> 00:14:03,814
to append Astrodev run, as I've shown

222
00:14:03,862 --> 00:14:07,782
in the parentheses to the command. And the first testing

223
00:14:07,846 --> 00:14:12,038
command that you can use is airflow Dags test. And this just executes

224
00:14:12,134 --> 00:14:15,406
one dag run of your dag. So it runs through

225
00:14:15,428 --> 00:14:19,280
your dag, and if you have any issues along the way, it will tell you.

226
00:14:19,650 --> 00:14:22,734
The second option is airflow tasks test,

227
00:14:22,852 --> 00:14:26,270
so you can also best specific tasks inside

228
00:14:26,340 --> 00:14:29,806
of your dag. This can be very useful if you're working on a long DAC

229
00:14:29,838 --> 00:14:33,086
with a lot of dependencies, but you have a task in the middle that doesn't

230
00:14:33,118 --> 00:14:36,946
really need anything from upstream and you just want to develop and

231
00:14:36,968 --> 00:14:40,194
iterate on that one. You can just test a specific task

232
00:14:40,242 --> 00:14:43,926
directly from the CLI and you get the full logs of the task as

233
00:14:43,948 --> 00:14:47,442
well. Now, if you're using the Astro Cli,

234
00:14:47,506 --> 00:14:50,614
you have more options. So the green ones here,

235
00:14:50,652 --> 00:14:53,974
the first one is called Astrodev parse, and this

236
00:14:54,012 --> 00:14:57,702
parses your dax and makes sure there are no import errors.

237
00:14:57,846 --> 00:15:01,206
Of course it parses for python syntax, so if you made a basic

238
00:15:01,238 --> 00:15:04,894
syntax error, it will notice. It also checks for things like

239
00:15:05,012 --> 00:15:08,350
there cannot be two tasks that have the same id

240
00:15:08,420 --> 00:15:11,802
in the same dag, or you cannot, because it's called directed

241
00:15:11,866 --> 00:15:15,146
asyclic graph, you cannot accidentally define a circle.

242
00:15:15,258 --> 00:15:18,446
So these are things that Astrodev parse would catch,

243
00:15:18,558 --> 00:15:22,290
and you can also do that very easily from the CLI.

244
00:15:22,870 --> 00:15:26,382
The next one, which I will talk about more in the upcoming slides,

245
00:15:26,446 --> 00:15:30,594
is Astrodefpy test. This command runs

246
00:15:30,642 --> 00:15:34,610
all of the tests that you've defined in the best directory.

247
00:15:34,770 --> 00:15:38,306
I will show you that in a demo. And even though it's called Pytest,

248
00:15:38,418 --> 00:15:41,674
you can use any Python framework that you like. You do not need to use

249
00:15:41,712 --> 00:15:45,334
pytest can be unit best, another Python framework. This command

250
00:15:45,382 --> 00:15:48,902
just runs all of the best in that folder. And lastly,

251
00:15:48,966 --> 00:15:52,494
a little bit of a different kind of test. There is a command called

252
00:15:52,532 --> 00:15:55,774
Astrodef upgrade best that will test your

253
00:15:55,812 --> 00:15:59,674
current environment and all of your dags against a newer airflow

254
00:15:59,722 --> 00:16:03,322
version. So if you are planning to upgrade and you're unsure

255
00:16:03,386 --> 00:16:07,494
whether there might be a breaking change, you can run this command

256
00:16:07,562 --> 00:16:11,138
and it will test your DaX. It will also tell you about all of

257
00:16:11,144 --> 00:16:14,466
the version changes of packages. It's another thing that I will show you

258
00:16:14,488 --> 00:16:18,594
in the demo in more detail. Right, this was the CLI,

259
00:16:18,722 --> 00:16:22,034
but I promised there would be something about IDe debugging.

260
00:16:22,082 --> 00:16:26,040
It's one of my favorite little features that not that many people know about.

261
00:16:26,810 --> 00:16:30,434
You can actually test airflow DAx interactively

262
00:16:30,562 --> 00:16:34,074
in your ide with your favorite debugging tool. So no matter

263
00:16:34,112 --> 00:16:38,298
which ide you use, if you use vs code Pycharm, you can actually

264
00:16:38,384 --> 00:16:42,378
run through your Dax the same way you run through other Python scripts

265
00:16:42,554 --> 00:16:45,854
with your debugger. And the way that you do that is

266
00:16:45,972 --> 00:16:49,594
at the end of your dag file,

267
00:16:49,722 --> 00:16:53,630
you add if name equals main. So if you run the file directly,

268
00:16:54,290 --> 00:16:58,274
then you take the dag object that is in that file and you

269
00:16:58,312 --> 00:17:02,302
say best. And that's already it. That makes the DAC

270
00:17:02,366 --> 00:17:05,614
interactive. And then you can run that file with a debugging

271
00:17:05,662 --> 00:17:09,174
tool. But you have additional options. So if you

272
00:17:09,212 --> 00:17:12,710
want to, you can say, well, I want to run this DAC for a specific

273
00:17:12,860 --> 00:17:16,546
date. If you're worried about your DAC breaking

274
00:17:16,578 --> 00:17:19,862
on the 1 January, if you're doing a lot of logic around

275
00:17:19,916 --> 00:17:23,386
date times, it's definitely one of the worries on my mind with some of the

276
00:17:23,408 --> 00:17:27,546
dags that I'm writing sometimes, then you can just test it for that date.

277
00:17:27,728 --> 00:17:31,286
You can also use different connections. You can store them in a yaml file

278
00:17:31,318 --> 00:17:35,434
and just inject them. So you could test your DAC against a new database,

279
00:17:35,482 --> 00:17:39,338
for example, or a new schema. You can use different variables

280
00:17:39,434 --> 00:17:42,846
and you can also inject the DAC configuration. I will show

281
00:17:42,868 --> 00:17:45,120
you that in the demo as well.

282
00:17:46,290 --> 00:17:49,646
All right, so let's move on to CI CD, because so far we've

283
00:17:49,678 --> 00:17:52,994
stayed local. We were just working locally on a machine with our

284
00:17:53,032 --> 00:17:55,870
local development created by the astral CLI.

285
00:17:56,030 --> 00:17:59,490
But eventually we want our code to live in production.

286
00:17:59,570 --> 00:18:04,082
Right? That's the dream of all of the code. So let's

287
00:18:04,146 --> 00:18:08,022
move on to the staging branch. And there is a CI CD step

288
00:18:08,076 --> 00:18:11,926
here. Now, we recommend and different teams

289
00:18:11,958 --> 00:18:15,578
will have different processes around these. The important thing is that you have a process

290
00:18:15,664 --> 00:18:18,954
that you follow to have a PR from

291
00:18:18,992 --> 00:18:22,426
a dev branch to a staging branch. Usually there are a lot of developer

292
00:18:22,458 --> 00:18:26,186
branches, especially in larger teams. But one dedicated staging

293
00:18:26,218 --> 00:18:29,614
branch and the PR will be in your version control and

294
00:18:29,652 --> 00:18:32,080
ideally reviewed by your team members.

295
00:18:32,450 --> 00:18:35,714
And as part of this pr, you run some

296
00:18:35,752 --> 00:18:39,566
tests automatically. And this is where you can reuse Astrodef

297
00:18:39,598 --> 00:18:43,506
pytests, because this is not just a command that

298
00:18:43,528 --> 00:18:47,030
you can run locally, but you can incorporate this command into

299
00:18:47,100 --> 00:18:51,174
your CI CD and it will run all of the best in

300
00:18:51,212 --> 00:18:55,298
your tests folder. Usually this means you have DAC validation tests,

301
00:18:55,394 --> 00:18:58,838
unit tests, and integration tests. I will talk about that in

302
00:18:58,844 --> 00:19:02,454
the next slide. And I put this on the slide

303
00:19:02,502 --> 00:19:05,846
because I've done this before. I didn't

304
00:19:05,878 --> 00:19:09,178
want to wait for all of the checks to complete. I was 100% sure

305
00:19:09,264 --> 00:19:13,034
that my code was fine. Just a small change, right? You should

306
00:19:13,072 --> 00:19:16,270
wait at only merge if the tests have passed.

307
00:19:17,570 --> 00:19:21,294
Now, what are those three kinds of tests. The first one is

308
00:19:21,332 --> 00:19:25,294
airflow specific, so this is something that you do not have with your other python

309
00:19:25,342 --> 00:19:29,362
files. You can have DAC validation tests. The first test

310
00:19:29,416 --> 00:19:32,766
that all DAC validation should run is if the dags

311
00:19:32,798 --> 00:19:37,290
are valid. So what I mentioned earlier, are there any circles defined?

312
00:19:37,390 --> 00:19:41,334
Are the task ids valid? Is it a valid type

313
00:19:41,372 --> 00:19:44,998
of schedule? Did you miss a mandatory parameter? Those sort of things

314
00:19:45,084 --> 00:19:48,150
that are also run when you run Astrodev parse.

315
00:19:48,810 --> 00:19:52,294
The second part is that you can add custom dag

316
00:19:52,342 --> 00:19:56,150
rules. So for example, you could have a constraint on schedules.

317
00:19:56,230 --> 00:19:59,654
If you have a team member who might accidentally push

318
00:19:59,702 --> 00:20:03,514
a dag with the wrong cron string and suddenly your dag runs

319
00:20:03,562 --> 00:20:06,640
every five minutes and it's supposed to run once per day,

320
00:20:07,090 --> 00:20:11,226
you can add a test that makes sure that only approved schedules

321
00:20:11,258 --> 00:20:15,198
come through. You could theoretically even define for like a specific

322
00:20:15,284 --> 00:20:18,980
dag what schedules are allowed. If that fits your use case.

323
00:20:19,430 --> 00:20:22,622
You can do the same with start dates or with tags.

324
00:20:22,766 --> 00:20:25,906
The screenshot in this example makes sure that catch up is set

325
00:20:25,928 --> 00:20:29,346
to false, which all airflow developers who are listening

326
00:20:29,458 --> 00:20:33,206
know or have done that before on accident, set catch up to

327
00:20:33,228 --> 00:20:36,566
true, or forgot about catch up to false, and then all

328
00:20:36,588 --> 00:20:40,070
of the backfills have scheduled automatically. So this is a very common

329
00:20:40,140 --> 00:20:43,626
one that people use to prevent that. Or you

330
00:20:43,648 --> 00:20:47,066
can also say, yeah, I want to have specific tags. If you have ways of

331
00:20:47,088 --> 00:20:50,794
organizing your dags, you can really enforce your architectural

332
00:20:50,842 --> 00:20:55,038
decisions onto your dag offers by using these

333
00:20:55,124 --> 00:20:59,294
custom dag rules. Another one that

334
00:20:59,332 --> 00:21:03,134
is very common is because you can also have constraints on your

335
00:21:03,172 --> 00:21:06,706
tasks. You could, for example, only allow specific

336
00:21:06,808 --> 00:21:10,020
operators. This is something that I will show you in the demo,

337
00:21:10,630 --> 00:21:14,606
and this example uses Pytest, but you can use any Python

338
00:21:14,638 --> 00:21:18,374
test framework. The reason I've used Pytest here is because if you use

339
00:21:18,412 --> 00:21:22,486
the astro CLI, there will be example tests already

340
00:21:22,668 --> 00:21:26,150
automatically created, and those will use pytest.

341
00:21:27,370 --> 00:21:30,726
All right, the next one is unit testing. Now, all of the Python

342
00:21:30,758 --> 00:21:34,634
developers, of course, are familiar with unit testing. Where does that

343
00:21:34,672 --> 00:21:37,754
come in? If you're using airflow, it really is important

344
00:21:37,872 --> 00:21:41,562
if you have custom hooks and operators. So like the example

345
00:21:41,616 --> 00:21:44,794
that I've showed in the intro slide, my basic

346
00:21:44,842 --> 00:21:48,554
math operator is an operator that I wrote. So it's really an operator

347
00:21:48,602 --> 00:21:52,606
that I should also write best for. The second use case is if

348
00:21:52,628 --> 00:21:56,286
you have functions used and add task decorated tasks,

349
00:21:56,318 --> 00:22:00,034
you can also write unit tests for those as well. So really,

350
00:22:00,152 --> 00:22:03,602
if you write your custom python code, you should also add

351
00:22:03,656 --> 00:22:07,146
your testing. Again, you can use any Python test framework

352
00:22:07,198 --> 00:22:10,806
that you like. This code snippet here uses unit best, but you

353
00:22:10,828 --> 00:22:14,630
could use pytest. You could use any package that you prefer.

354
00:22:15,130 --> 00:22:18,630
A little side note here, I mentioned that there are a lot of building

355
00:22:18,700 --> 00:22:23,110
blocks in Apache airflow. If you're using one of those open source

356
00:22:23,270 --> 00:22:26,746
building blocks, then the tests are already done for you. So you

357
00:22:26,768 --> 00:22:29,978
only have to write unit tests for your own custom

358
00:22:30,064 --> 00:22:33,946
operators. If you want to figure out if there is already an operator for

359
00:22:33,968 --> 00:22:37,482
your use case, the best place to do so is the astronomer registry.

360
00:22:37,546 --> 00:22:41,018
If you go to registry astronomer IO, you can find all sorts

361
00:22:41,034 --> 00:22:43,810
of information on all sorts of operators.

362
00:22:45,590 --> 00:22:49,330
All right. And the third type of best is integration testing.

363
00:22:49,670 --> 00:22:53,806
This is if you are talking to an API. So with unit tests

364
00:22:53,918 --> 00:22:57,318
you usually try to only test the code that you yourself

365
00:22:57,404 --> 00:23:01,046
wrote. So if a custom operator calls out

366
00:23:01,068 --> 00:23:04,566
to an API, for example, you're writing a custom operator to

367
00:23:04,588 --> 00:23:07,922
interact with Snowflake, and then you would start

368
00:23:07,996 --> 00:23:12,042
to mock that API call to make sure that

369
00:23:12,176 --> 00:23:16,202
it is your code that is working or failing and not

370
00:23:16,256 --> 00:23:20,042
something with the API. And with integration tests, you actually

371
00:23:20,096 --> 00:23:23,886
want to test that API call, which again you do for custom code that

372
00:23:23,908 --> 00:23:27,454
you write and you can do with any Python test framework. In this

373
00:23:27,492 --> 00:23:31,694
code snippet, I simply use the built in assert. One little note here.

374
00:23:31,812 --> 00:23:35,474
Be careful, of course, if you call out to an API, that can

375
00:23:35,512 --> 00:23:39,230
incur cost and sometimes take a lot of time, especially nowadays

376
00:23:39,310 --> 00:23:42,642
with llms. So be careful of what

377
00:23:42,776 --> 00:23:46,086
kind of payload you are sending and what kind of code call you

378
00:23:46,108 --> 00:23:47,910
are making inside of your tests.

379
00:23:49,530 --> 00:23:52,760
All right, so we merged our PR.

380
00:23:53,130 --> 00:23:56,486
Our code is now on the staging branch. It is very happy on that

381
00:23:56,508 --> 00:24:00,518
branch, but it wants to move to the deployment. And CI

382
00:24:00,534 --> 00:24:03,878
CD stands for continuous integration, continuous deployment.

383
00:24:03,974 --> 00:24:07,610
So we have a step that does this automatically.

384
00:24:08,190 --> 00:24:11,770
Ideally the code, once it is moved onto the staging branch,

385
00:24:11,850 --> 00:24:15,774
it will run the tests again, and if those tests pass, it will

386
00:24:15,812 --> 00:24:19,902
be automatically deployed to the staging deployment. Some people also

387
00:24:19,956 --> 00:24:23,806
add more integrated best to the stage that

388
00:24:23,828 --> 00:24:27,006
maybe test your code in context,

389
00:24:27,118 --> 00:24:30,866
in the context of the whole environment. You have a lot

390
00:24:30,888 --> 00:24:34,094
of options on what you can put into your CI CD script.

391
00:24:34,222 --> 00:24:37,826
Since airflow is all just Python code, you can use any CI CD

392
00:24:37,858 --> 00:24:41,522
tool that you like. In the demo. I'm going to show GitHub actions,

393
00:24:41,586 --> 00:24:45,746
but there are example scripts for different tools available in the astronomer

394
00:24:45,778 --> 00:24:50,390
documentation. You can also consider having automated

395
00:24:50,470 --> 00:24:54,138
creation of your deployment. So for example,

396
00:24:54,304 --> 00:24:57,606
maybe you want the deployment to automatically update

397
00:24:57,638 --> 00:25:01,322
a configuration and have that configuration file in

398
00:25:01,376 --> 00:25:05,360
your source code as well, or in your source control system as well.

399
00:25:05,730 --> 00:25:09,342
Then you can use infrastructure management as

400
00:25:09,396 --> 00:25:12,478
part of your CI CD. This is kind of the next step in DevOps in

401
00:25:12,484 --> 00:25:17,090
a lot of cases, and you can also do that for your airflow infrastructure.

402
00:25:17,430 --> 00:25:20,930
This will depend on how you are running airflow in production.

403
00:25:21,350 --> 00:25:24,734
But if you're an astronomer customer, you can use the Astro

404
00:25:24,782 --> 00:25:28,614
API to manage your infrastructure in a code based

405
00:25:28,652 --> 00:25:32,550
way. Okay, so our code

406
00:25:32,620 --> 00:25:35,974
made it to the staging deployment. It is running. There it is.

407
00:25:36,012 --> 00:25:38,550
Fine, let's go to production.

408
00:25:39,850 --> 00:25:43,434
This is just a recommendation. It's not always possible, but we

409
00:25:43,472 --> 00:25:47,206
recommend that you run your code for a few days in your staging

410
00:25:47,238 --> 00:25:51,082
deployment as an end to end test, because you can never

411
00:25:51,136 --> 00:25:54,830
think of anything. There will always be edge cases that you did not think of

412
00:25:54,900 --> 00:25:59,198
that maybe come up only with data on certain days or.

413
00:25:59,284 --> 00:26:02,974
Yeah, something specific that you only see once your code is running

414
00:26:03,012 --> 00:26:06,466
in the full context. So that is one of the main purposes of the

415
00:26:06,488 --> 00:26:09,890
staging deployment, making sure your new code is working,

416
00:26:09,960 --> 00:26:12,820
your new pipelines are working end to end,

417
00:26:13,430 --> 00:26:17,186
and then usually a couple of prs get bundled, and there is a staging

418
00:26:17,218 --> 00:26:19,862
to production pr that gets reviewed again.

419
00:26:19,996 --> 00:26:23,414
And once that is merged, all of our tests run

420
00:26:23,452 --> 00:26:26,360
again and the deployment is automatic again.

421
00:26:27,690 --> 00:26:31,018
All right, that was the overview over the different ways

422
00:26:31,104 --> 00:26:34,874
and contexts of testing. Now I will move to the

423
00:26:34,912 --> 00:26:38,202
demo. So, as I said, all of the code

424
00:26:38,256 --> 00:26:41,806
in this demo is available to you if you go to

425
00:26:41,828 --> 00:26:45,710
GitHub and then astronomer externaltalkdemos.

426
00:26:46,130 --> 00:26:50,750
It is on the branch for this conference. So 2024

427
00:26:50,820 --> 00:26:54,466
conf 42 Python airflow testing prod. Don't worry, you don't have

428
00:26:54,488 --> 00:26:58,194
to write that down. The readme on the main branch will

429
00:26:58,232 --> 00:27:02,334
have a link to this branch. But let's

430
00:27:02,382 --> 00:27:06,550
look at this. All right, so this is

431
00:27:06,620 --> 00:27:10,754
the airflow environment you can see here. I'm running on localhost

432
00:27:10,802 --> 00:27:13,974
80 80. So this is running on my local machine with

433
00:27:14,012 --> 00:27:18,214
the Astro CLi. And the code repository that

434
00:27:18,252 --> 00:27:21,574
is available to you just contains two dacs. The first one

435
00:27:21,612 --> 00:27:25,066
is the intro to airflow DAC that I've shown in the slide. So you have

436
00:27:25,088 --> 00:27:28,554
the code, and the second one is the one where I'm going to show

437
00:27:28,592 --> 00:27:32,490
you the testing options. So let's click on this math DAC.

438
00:27:33,230 --> 00:27:36,846
Scroll down. Okay, this is our DAC. Can already see

439
00:27:36,868 --> 00:27:40,654
that I've tested a specific task here. But if you're looking

440
00:27:40,692 --> 00:27:44,266
at a full run, you can see the structure

441
00:27:44,298 --> 00:27:47,278
of this dag. So it is a very, very simple dag.

442
00:27:47,374 --> 00:27:50,686
We have two tasks. At first one picks

443
00:27:50,718 --> 00:27:54,466
a random number. It actually uses an API to do that.

444
00:27:54,648 --> 00:27:58,306
The second one retrieves an operation from an airflow

445
00:27:58,338 --> 00:28:02,690
variable, and then we will perform that operation

446
00:28:02,850 --> 00:28:06,674
with that number and the number 23 using this basic

447
00:28:06,722 --> 00:28:10,630
math operator that is a custom operator. And lastly,

448
00:28:10,710 --> 00:28:14,470
we make sure that a table exists in a local postgres database

449
00:28:14,550 --> 00:28:17,334
and then we write that result to that table.

450
00:28:17,462 --> 00:28:21,386
So a very simple DAX structure, but it has all of the pieces

451
00:28:21,418 --> 00:28:25,294
that we need to look at testing. So with

452
00:28:25,332 --> 00:28:28,880
that said, let's hop over into the code.

453
00:28:29,410 --> 00:28:33,370
I'm using vs code here. You can use any ide that you prefer,

454
00:28:33,530 --> 00:28:37,454
but what I've done here is in an empty folder. I ran

455
00:28:37,502 --> 00:28:41,234
the command astrodef in it, and this

456
00:28:41,272 --> 00:28:44,882
will create the Astro project and it will create

457
00:28:44,936 --> 00:28:47,922
most of the files that you see here and most of the folders.

458
00:28:48,066 --> 00:28:51,734
The important one here is the Docker file. So what

459
00:28:51,772 --> 00:28:55,362
is happening here is that it pulls the astro

460
00:28:55,426 --> 00:28:59,002
runtime and this is functionally equivalent to open

461
00:28:59,056 --> 00:29:02,794
source airflow. It just adds a little few features that

462
00:29:02,832 --> 00:29:06,234
help with running airflow with the Astro CLI and

463
00:29:06,272 --> 00:29:09,802
with Astronomer. But this is also ready

464
00:29:09,856 --> 00:29:13,114
for you to use and you can expect feature parity

465
00:29:13,242 --> 00:29:17,278
from Apache Airflow. In this case, version ten two

466
00:29:17,444 --> 00:29:20,702
corresponds to a 2.8 version of

467
00:29:20,756 --> 00:29:24,174
open source airflow. So here is where we pull

468
00:29:24,212 --> 00:29:27,506
in airflow into the Docker environment. You do not need to

469
00:29:27,528 --> 00:29:31,234
know anything about Docker in depth or in particular to use this

470
00:29:31,272 --> 00:29:35,074
tool, but you can add your customization here if there's something

471
00:29:35,112 --> 00:29:38,574
you want to customize. Since Docker is running

472
00:29:38,632 --> 00:29:42,658
in an isolated environment, we need to install our packages.

473
00:29:42,834 --> 00:29:46,530
And this is where you can do that. You can add all of the packages

474
00:29:46,610 --> 00:29:50,102
that you would pip install to the requirements file. And then once you

475
00:29:50,156 --> 00:29:53,546
run Astrodefstart, these automatically get installed in

476
00:29:53,568 --> 00:29:57,018
your airflow environment. One little note here.

477
00:29:57,104 --> 00:30:01,482
It is a best practice to always pin your versions here. This is also something

478
00:30:01,616 --> 00:30:05,834
that has caused me some headache before when I forgot that for a simple package,

479
00:30:05,962 --> 00:30:09,726
always pin your versions. All right, so the

480
00:30:09,748 --> 00:30:13,342
next thing is we have the DAX folder. And I said

481
00:30:13,396 --> 00:30:16,878
earlier, airflow will know that a file is an

482
00:30:16,884 --> 00:30:20,466
airflow DAC. If you use this add DAC decorator and

483
00:30:20,488 --> 00:30:23,630
you put the file into the right folder and this is the right folder.

484
00:30:23,710 --> 00:30:27,598
So if you put a file in here, like our intro to airflow file,

485
00:30:27,774 --> 00:30:30,886
and it has the word dag in it, it will automatically be

486
00:30:30,908 --> 00:30:34,326
picked up by the scheduler. We have an

487
00:30:34,348 --> 00:30:38,134
include file. This is where we can put all of our supporting code. So,

488
00:30:38,172 --> 00:30:41,530
for example, the custom operator lives here.

489
00:30:41,680 --> 00:30:45,434
And most importantly for this talk, we have the tests folder where

490
00:30:45,472 --> 00:30:48,666
all of our tests live. And I will go through those in more detail in

491
00:30:48,688 --> 00:30:51,500
a second. Okay,

492
00:30:51,950 --> 00:30:56,186
so this is our astro project that is created by Astrodefinit.

493
00:30:56,218 --> 00:30:59,966
We can start it by Astrodef start. It is already running here.

494
00:31:00,148 --> 00:31:03,054
Show you the docker containers that are running.

495
00:31:03,252 --> 00:31:06,754
And now we want to do some development. So we

496
00:31:06,792 --> 00:31:10,530
wrote this DaG. This is the DAG that I've shown you with

497
00:31:10,680 --> 00:31:14,286
this very simple structure, picking a random number and performing

498
00:31:14,318 --> 00:31:17,814
an operation on it. Let's say we've written this

499
00:31:17,852 --> 00:31:21,526
DaG and we want to test it. Now. If you remember,

500
00:31:21,628 --> 00:31:25,026
the first option that we have is by using CLI commands.

501
00:31:25,138 --> 00:31:29,046
So one of the commands would be we can just test the

502
00:31:29,068 --> 00:31:32,250
whole DaG. So airflow DAX test,

503
00:31:32,400 --> 00:31:35,814
if you're using the Astro Cli translates to Astrodef

504
00:31:35,862 --> 00:31:39,130
run DAX test, and then we use the DAC id.

505
00:31:39,200 --> 00:31:42,526
So I run Astrodef, run DAx test,

506
00:31:42,628 --> 00:31:46,254
math DAC. Let's run this. This will just run for the whole

507
00:31:46,292 --> 00:31:49,934
DAC. And it's pretty fast. So you can

508
00:31:49,972 --> 00:31:53,774
see here it executed all of the

509
00:31:53,972 --> 00:31:57,518
alt tasks. We ended up with eleven

510
00:31:57,614 --> 00:32:01,010
times 23. So the random number was eleven.

511
00:32:01,350 --> 00:32:05,166
And this result eventually gets inserted into our postgres

512
00:32:05,198 --> 00:32:08,726
database. All right, so let's say I do some

513
00:32:08,748 --> 00:32:12,550
more development on a specific task. Maybe this pick random number

514
00:32:12,620 --> 00:32:16,342
task. And I don't want to run everything all of the time,

515
00:32:16,476 --> 00:32:20,246
so let's just run this specific task. This is

516
00:32:20,268 --> 00:32:23,926
also something you can easily do with the CLI. We can do Astrodef

517
00:32:23,958 --> 00:32:27,530
run tasks, test, map dag, pick a random number.

518
00:32:27,600 --> 00:32:30,958
So we use the DAC id and then the task id.

519
00:32:31,124 --> 00:32:34,974
And this will just run this task. I get

520
00:32:35,012 --> 00:32:38,650
14. Let's do it again. It should be random, not item potent.

521
00:32:38,730 --> 00:32:41,760
Get 71. So this is working.

522
00:32:42,690 --> 00:32:45,246
All right. So this is already very helpful.

523
00:32:45,358 --> 00:32:49,150
But sometimes you need to debug something in depth.

524
00:32:49,230 --> 00:32:52,446
So you need to debug your tasks in more depth

525
00:32:52,478 --> 00:32:56,130
and you want to use the python debugger. And what you can do then

526
00:32:56,200 --> 00:32:59,842
is let's add a breakpoint here. Let's add the breakpoint

527
00:32:59,906 --> 00:33:03,382
here. This just means that I tell my

528
00:33:03,436 --> 00:33:06,886
project, I think there's something going on here. I want to stop here

529
00:33:06,988 --> 00:33:10,234
if I'm using my debugger tool. And let's use

530
00:33:10,272 --> 00:33:13,914
the vs code debugger. So what this

531
00:33:13,952 --> 00:33:17,626
does is it runs and debugs the stack and

532
00:33:17,648 --> 00:33:21,142
it stops here. And then I have all of my debug

533
00:33:21,206 --> 00:33:25,054
options. So can actually see here, I can see the

534
00:33:25,092 --> 00:33:29,866
maximum and minimum that I am running with, can see the global variables,

535
00:33:30,058 --> 00:33:33,906
something that is very useful if you're an airflow developer. I can see all

536
00:33:33,928 --> 00:33:37,422
of the options that I have within the context

537
00:33:37,486 --> 00:33:40,882
that I injected into this task and I can perform

538
00:33:40,936 --> 00:33:44,594
actions here. Like I can say, well, is there

539
00:33:44,632 --> 00:33:47,894
already a num at this point? There is not. Num has not

540
00:33:47,932 --> 00:33:51,190
been defined, but I have minimum.

541
00:33:51,530 --> 00:33:54,898
Minimum is defined. And from the context,

542
00:33:54,994 --> 00:33:58,730
I think I have context ts.

543
00:34:00,110 --> 00:34:02,890
I have all of the context that is within ts.

544
00:34:03,550 --> 00:34:06,966
And I can of course, use all of the debugging.

545
00:34:07,078 --> 00:34:10,742
I can do a step down, step further,

546
00:34:10,806 --> 00:34:14,126
and now I'm within the function that is being called. So I

547
00:34:14,148 --> 00:34:17,374
can see what is happening inside of the function. I can

548
00:34:17,412 --> 00:34:21,278
step through the request, and at this point I will probably

549
00:34:21,364 --> 00:34:25,406
have the request ready do r not defined,

550
00:34:25,518 --> 00:34:29,266
but I can step through and then I return the

551
00:34:29,288 --> 00:34:32,914
jSOn here and say, well, what is r here?

552
00:34:32,952 --> 00:34:36,898
R is the response. So it's 200. What is R

553
00:34:36,984 --> 00:34:40,806
Json? That should be the payload. And I can get

554
00:34:40,828 --> 00:34:44,454
the payload. So it's 43 at that point in time. So this is

555
00:34:44,492 --> 00:34:48,134
really how you can prevent all of those print statements that

556
00:34:48,252 --> 00:34:51,586
as a junior developer, I had a lot of print statements in my code because

557
00:34:51,628 --> 00:34:55,402
I needed to know what is going on in debugging. But if

558
00:34:55,456 --> 00:34:59,066
you're using a debugging tool like this, you can just step through your code

559
00:34:59,168 --> 00:35:02,394
and do this in a much cleaner way and in a very interactive

560
00:35:02,442 --> 00:35:06,734
way. Okay, let's exit this debugger and go

561
00:35:06,772 --> 00:35:09,790
back to our code. Go back to our terminal.

562
00:35:10,530 --> 00:35:13,946
All right, so how did I do that? Because this is not

563
00:35:13,988 --> 00:35:17,346
working just by default. Right. You may

564
00:35:17,368 --> 00:35:20,654
have tried this before with your flow, Dax. I added

565
00:35:20,702 --> 00:35:24,482
the DAC test function. So what I've done here on line

566
00:35:24,536 --> 00:35:28,386
100 is the same thing that you've seen in the slides. I added

567
00:35:28,418 --> 00:35:32,102
if name equals main. So if this file is run

568
00:35:32,156 --> 00:35:35,318
directly, I want you to run this instead.

569
00:35:35,484 --> 00:35:38,966
So I say the dag object. I assigned

570
00:35:38,998 --> 00:35:42,140
the call of my dag function to an object.

571
00:35:42,910 --> 00:35:46,742
I take this dag object and I use the dot test method,

572
00:35:46,886 --> 00:35:50,634
and all of these parameters are optional. But this

573
00:35:50,672 --> 00:35:53,946
just means now you can run this dag in your debugging

574
00:35:53,978 --> 00:35:57,710
tool, and I can run it for a specific date. For example,

575
00:35:57,780 --> 00:36:01,214
I can say, well, I'm worried in a year is

576
00:36:01,252 --> 00:36:05,006
this still going to work? Like if I'm using some date time, and then

577
00:36:05,028 --> 00:36:08,290
I can run it for a specific time of the year.

578
00:36:08,440 --> 00:36:12,098
I'm also injecting connections and variables, and I

579
00:36:12,104 --> 00:36:16,200
can show you how I do that. I have this folder daC test up here,

580
00:36:17,210 --> 00:36:21,046
and then I have my connections in

581
00:36:21,068 --> 00:36:24,466
a yaml format. This is just my local postgres,

582
00:36:24,578 --> 00:36:27,986
and this is the database

583
00:36:28,018 --> 00:36:31,242
that I want to connect to when I'm testing the DaG. But I could point

584
00:36:31,296 --> 00:36:35,066
this at any database and I also inject a

585
00:36:35,088 --> 00:36:38,570
variable. In this case, I inject the operation. So if I change that,

586
00:36:38,640 --> 00:36:42,762
for example, to change it to minus,

587
00:36:42,906 --> 00:36:46,746
then it would subtract instead of multiply.

588
00:36:46,858 --> 00:36:51,230
So I can test for different situations.

589
00:36:52,210 --> 00:36:55,326
Of course, you can also have configurations. In this case I

590
00:36:55,348 --> 00:36:58,402
have an upper limit and a lower limit for the random number.

591
00:36:58,536 --> 00:37:01,778
And I can just inject a different limit so I

592
00:37:01,784 --> 00:37:04,994
can say, well, I want the upper limit to be 20.

593
00:37:05,192 --> 00:37:09,510
Okay, let's run this again with the different configurations.

594
00:37:10,410 --> 00:37:14,134
And now it runs through. I removed the breakpoint so it

595
00:37:14,172 --> 00:37:17,282
didn't break, but what you can see now if we scroll

596
00:37:17,346 --> 00:37:20,758
up is here we got the equation 14

597
00:37:20,854 --> 00:37:24,694
-23 so we didn't get the multiplication

598
00:37:24,742 --> 00:37:27,766
that it was earlier, but we ran it with the new variable,

599
00:37:27,878 --> 00:37:29,930
which was the minus operation.

600
00:37:31,390 --> 00:37:34,766
All right, so this is super useful if you're working locally and

601
00:37:34,788 --> 00:37:38,094
if you're debugging while developing. But let's look

602
00:37:38,132 --> 00:37:41,194
at the tests that are in the tests folder.

603
00:37:41,322 --> 00:37:44,666
So if we look through that, we have DAC validation tests

604
00:37:44,698 --> 00:37:48,222
first. And these are actually some that are generated

605
00:37:48,286 --> 00:37:52,338
automatically if you run Astrodef in it. So if you scroll up here,

606
00:37:52,504 --> 00:37:56,214
this code up here will be generated for you. And this

607
00:37:56,252 --> 00:37:59,622
just gives you the function that

608
00:37:59,676 --> 00:38:03,414
gets the input errors. So the function that makes sure that your dags are

609
00:38:03,452 --> 00:38:06,582
passing correctly and the function that

610
00:38:06,636 --> 00:38:10,470
gets your dags. And what I added here is

611
00:38:10,540 --> 00:38:14,026
the first one is the one that I've shown in the slides. So this is

612
00:38:14,048 --> 00:38:18,090
a test that makes sure that the catch up parameter is set to false,

613
00:38:18,430 --> 00:38:22,266
which is very very useful. And then I also added

614
00:38:22,298 --> 00:38:26,090
a test that makes sure that I only use allowed operators.

615
00:38:26,170 --> 00:38:29,786
So I say I only allow this add task decorator,

616
00:38:29,898 --> 00:38:33,410
which the official name is the python decorated operator

617
00:38:34,310 --> 00:38:38,254
and my own custom one, plus the SQL execute

618
00:38:38,302 --> 00:38:41,986
query operator. And this is the test that actually makes sure

619
00:38:42,088 --> 00:38:46,290
that for all of these tasks in my DAC tasks. So for

620
00:38:46,440 --> 00:38:50,534
all DACs that I have in my flow environment, please make sure

621
00:38:50,652 --> 00:38:54,354
that the type of the task is in the allowed

622
00:38:54,402 --> 00:38:57,654
operators. All right,

623
00:38:57,772 --> 00:39:01,098
integration tests just have a very simple one.

624
00:39:01,184 --> 00:39:04,662
I take this function that makes the API

625
00:39:04,726 --> 00:39:08,106
call that you've seen earlier can show you

626
00:39:08,128 --> 00:39:11,994
that it's in the utils very simple function that just calls this random

627
00:39:12,042 --> 00:39:15,902
number API. And it makes sure that this

628
00:39:16,036 --> 00:39:20,974
returns a result that makes sense for our constraints and

629
00:39:21,012 --> 00:39:24,182
the unit tests. Of course, if I write a custom operator,

630
00:39:24,266 --> 00:39:28,322
I should really unit test this. This tests the operator for

631
00:39:28,376 --> 00:39:32,338
all operations. It makes sure that if I divide by zero

632
00:39:32,424 --> 00:39:35,686
I get an error, and it makes sure that if

633
00:39:35,708 --> 00:39:38,950
I have an invalid operation that it also

634
00:39:39,020 --> 00:39:42,582
throws an error. And you can see those were three different

635
00:39:42,636 --> 00:39:46,326
python frameworks. Unit best built in

636
00:39:46,348 --> 00:39:49,882
assert and pytest. And we can run

637
00:39:49,936 --> 00:39:54,490
all of these with one command. So if I run Astrodef pytest,

638
00:39:55,390 --> 00:39:58,490
this will actually create a little container and

639
00:39:58,560 --> 00:40:02,730
then it will run all of tests and they all passed.

640
00:40:02,890 --> 00:40:05,520
Very happy. But let's see,

641
00:40:06,690 --> 00:40:10,746
let's make one fail a test. So we say we want catch

642
00:40:10,778 --> 00:40:16,194
up to always be set to false. So what happens if

643
00:40:16,232 --> 00:40:19,700
catch up is suddenly true? Let's rerun the test.

644
00:40:20,070 --> 00:40:24,420
This DaG should not get passed the test, and it did not.

645
00:40:24,790 --> 00:40:28,578
One test failed. Catch up equals false.

646
00:40:28,674 --> 00:40:32,054
This is not allowed. So this is how you can get real control

647
00:40:32,172 --> 00:40:35,654
over your DAX, and not just in

648
00:40:35,692 --> 00:40:39,366
your local situation to run it, but you can take

649
00:40:39,468 --> 00:40:42,934
this test and add it to your CI CD.

650
00:40:43,062 --> 00:40:46,534
So in this case I'm using GitHub

651
00:40:46,582 --> 00:40:49,820
actions, but you can do this with any CI CD tool.

652
00:40:50,430 --> 00:40:53,822
I'm saying I want to always run these tests when

653
00:40:53,876 --> 00:40:57,694
I push to these branches or when a pull request is

654
00:40:57,732 --> 00:41:01,294
closed to these branches, then I have some customization which

655
00:41:01,332 --> 00:41:04,914
tests I want to run. And I say,

656
00:41:05,032 --> 00:41:08,894
well, check out the repository, install the Astro Cli

657
00:41:08,942 --> 00:41:12,994
so we can run Astrodef Pytest and then run

658
00:41:13,032 --> 00:41:17,318
the test harness. So this is running the exact same command inside

659
00:41:17,404 --> 00:41:20,150
of the CI CD that I can run locally.

660
00:41:21,210 --> 00:41:24,882
We have the same thing on merge to production.

661
00:41:25,026 --> 00:41:28,994
And the last step here is just because I'm using astronomer,

662
00:41:29,042 --> 00:41:32,614
that it also automatically sends my code to my astronomer

663
00:41:32,662 --> 00:41:35,706
deployment using my deployment id.

664
00:41:35,808 --> 00:41:39,334
But if you're using a different way of running airflow in production

665
00:41:39,382 --> 00:41:43,360
or in the cloud, you just can replace this last step.

666
00:41:44,850 --> 00:41:48,634
Now to show you that this is actually working, let's hop

667
00:41:48,682 --> 00:41:52,302
over. And this is the repository that I've talked about.

668
00:41:52,356 --> 00:41:55,838
If we scroll up, it's the astronomer external talk demos

669
00:41:55,854 --> 00:41:59,826
repository. And you can see here this was

670
00:42:00,008 --> 00:42:03,486
happening on a push to a branch and it runs

671
00:42:03,518 --> 00:42:06,950
the exact same test. So if we scroll down,

672
00:42:07,100 --> 00:42:10,738
you can see we also have those twelve best that passed.

673
00:42:10,834 --> 00:42:14,630
We had the DAC validation tests, the integration tests and

674
00:42:14,700 --> 00:42:18,710
the unit tests. All right,

675
00:42:18,780 --> 00:42:23,578
so the last thing that I wanted to show you is that

676
00:42:23,664 --> 00:42:27,146
you can also do upgrade testing. So one thing that you

677
00:42:27,168 --> 00:42:30,714
might have noticed in the demo earlier, we had a banner up

678
00:42:30,752 --> 00:42:34,874
here, new version is available. This happens a lot because new versions

679
00:42:34,922 --> 00:42:39,262
come out all the time. So we are on ten two and

680
00:42:39,316 --> 00:42:42,414
there's actually a runtime version ten three available.

681
00:42:42,612 --> 00:42:45,774
Now I want to upgrade, but I want to be sure that my DAX

682
00:42:45,822 --> 00:42:49,314
are fine and I want to know which packages have

683
00:42:49,352 --> 00:42:52,898
had upgraded versions in this change. So what I

684
00:42:52,904 --> 00:42:56,450
can do is I can run Astrodef

685
00:42:56,530 --> 00:43:00,582
upgrade test. So let's do this. I already

686
00:43:00,636 --> 00:43:04,614
ran it before, but it's actually fairly quick, takes a little longer

687
00:43:04,812 --> 00:43:08,534
the more DAx you have, but it

688
00:43:08,572 --> 00:43:11,862
ran and it tells us no errors detected in your dax.

689
00:43:11,926 --> 00:43:15,306
This is great. This also tells you it is not running the

690
00:43:15,328 --> 00:43:19,194
Astrodef pytest, it's just testing your DaX against the new

691
00:43:19,232 --> 00:43:22,554
version because the Astrodef pytests would

692
00:43:22,592 --> 00:43:26,480
still fail our intro DAC for having catch up to true.

693
00:43:27,810 --> 00:43:31,566
But this best against the new version. And what you get is you

694
00:43:31,588 --> 00:43:35,646
get this little folder here. And this contains several

695
00:43:35,678 --> 00:43:41,954
files. The first one is can HTML file that you can open in

696
00:43:41,992 --> 00:43:45,842
Firefox or in chrome. And you can see that this

697
00:43:45,896 --> 00:43:49,442
tells you all of the details of your environment, which can be super useful,

698
00:43:49,506 --> 00:43:53,286
especially if you're debugging a platform issue. But we can see what

699
00:43:53,308 --> 00:43:56,646
I'm running on here, which Python version, the packages that are

700
00:43:56,668 --> 00:44:00,294
relevant, and we can see both of our dags and

701
00:44:00,332 --> 00:44:03,494
we can see that both of them passed all of the tests.

702
00:44:03,542 --> 00:44:07,050
They will work in ten three as they are in ten two

703
00:44:07,120 --> 00:44:11,022
right now. The other thing that I personally find super useful is

704
00:44:11,076 --> 00:44:14,826
dependency comparison. So there will be pinned versions

705
00:44:14,938 --> 00:44:18,526
of different packages for each astro runtime and each

706
00:44:18,548 --> 00:44:21,850
airflow version. We actually get the full pip freeze

707
00:44:22,010 --> 00:44:25,726
for both versions here. And this one tells

708
00:44:25,758 --> 00:44:29,522
you what actually changed. So you can see that

709
00:44:29,576 --> 00:44:32,830
we are using the same base airflow version, so we're

710
00:44:33,330 --> 00:44:36,854
using the same airflow, but the Astro runtime extra has changed. We did

711
00:44:36,892 --> 00:44:40,742
some provider upgrades of the built in providers and

712
00:44:40,796 --> 00:44:44,262
for example, Titest had a major update in this

713
00:44:44,316 --> 00:44:48,022
version. And this can be very important if you're using

714
00:44:48,076 --> 00:44:51,698
one of these packages inside of your airflow tasks and you

715
00:44:51,724 --> 00:44:55,130
may have a specific dependency or you need to update something

716
00:44:55,200 --> 00:44:58,506
in your airflow tasks. This will tell you, and this will make sure that

717
00:44:58,528 --> 00:45:00,060
your Dax do not break.

718
00:45:02,050 --> 00:45:08,606
Okay, so that

719
00:45:08,628 --> 00:45:12,174
was the whole journey of our code from

720
00:45:12,212 --> 00:45:15,550
local development to production and

721
00:45:15,700 --> 00:45:19,502
how you can test each step of the way. As a little recap,

722
00:45:19,566 --> 00:45:22,946
if you're doing local testing, you have two main options. You can

723
00:45:22,968 --> 00:45:26,754
use the Astro CLI airflow CLI commands, and you can use

724
00:45:26,792 --> 00:45:30,658
DAX test to debug interactively and for all of

725
00:45:30,664 --> 00:45:34,322
the CI CD steps. We really recommend to have a robust

726
00:45:34,386 --> 00:45:38,466
testing suite inside of your tests folder to run with Astrodefpy

727
00:45:38,498 --> 00:45:42,370
tests on every step on the way, with DAC validation tests,

728
00:45:42,450 --> 00:45:46,402
unit tests for all of your custom code, and selected integrated tests

729
00:45:46,466 --> 00:45:49,320
for APIs that you specifically want to test.

730
00:45:50,450 --> 00:45:54,762
The take home message airflow is written in Python. Airflow pipelines

731
00:45:54,826 --> 00:45:58,014
are just python code. This means you should really test

732
00:45:58,052 --> 00:46:01,614
it. Software engineering best practices and DevOps best

733
00:46:01,652 --> 00:46:05,406
practices do apply. I hope this was helpful for

734
00:46:05,428 --> 00:46:08,734
you. Thank you so much again for joining, and I hope

735
00:46:08,772 --> 00:46:10,890
you have a great rest of the conference.

