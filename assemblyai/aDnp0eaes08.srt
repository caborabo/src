1
00:00:27,320 --> 00:00:30,816
Hi, thank you for joining. I'm super excited to talk

2
00:00:30,840 --> 00:00:34,656
to you today about how you can use Apache Airflow to create

3
00:00:34,760 --> 00:00:38,184
generative AI pipelines. Specifically, I will talk about

4
00:00:38,224 --> 00:00:41,728
a demo that shows a full generative AI pipeline for

5
00:00:41,776 --> 00:00:44,084
content generation with Airflow.

6
00:00:45,504 --> 00:00:49,328
This is the agenda for today. I will give a quick introduction into

7
00:00:49,376 --> 00:00:52,736
the topics of Airflow and astronomer. Then I'll talk about some of

8
00:00:52,760 --> 00:00:55,952
the challenges that you might run into when you want to take generative

9
00:00:56,008 --> 00:01:00,294
AI projects into production, and how airflow addresses those challenges.

10
00:01:00,794 --> 00:01:04,730
Next, I will cover some key airflow features that I think

11
00:01:04,802 --> 00:01:08,454
you should know when you are creating generative AI challenges,

12
00:01:08,834 --> 00:01:12,650
especially features that are used in the demo code. So wanting to

13
00:01:12,682 --> 00:01:16,434
help you to understand the demo better, I will specifically

14
00:01:16,474 --> 00:01:20,250
focus on two features that got a major update in

15
00:01:20,282 --> 00:01:23,626
the latest airflow release. Airflow 29 came out a little lower

16
00:01:23,650 --> 00:01:27,864
a month ago, and there were significant updates to advanced dataset

17
00:01:27,904 --> 00:01:31,224
scheduling as well as a dynamic task mapping that I will cover

18
00:01:31,304 --> 00:01:34,912
in the slides. Then we will spend some time in

19
00:01:34,928 --> 00:01:39,232
the demo. The demo is a pipeline that creates content

20
00:01:39,408 --> 00:01:42,544
using a retrieval augmented generation pattern,

21
00:01:42,664 --> 00:01:45,960
together with also fine tuning GPT for your

22
00:01:45,992 --> 00:01:50,168
use case all of the code in the demo is open source

23
00:01:50,216 --> 00:01:53,848
and available for you to use, and this is the link to the GitHub repository

24
00:01:53,896 --> 00:01:57,442
where you can find that code. I will show this link several

25
00:01:57,498 --> 00:02:01,034
times throughout the talk. One note before we get

26
00:02:01,074 --> 00:02:04,794
started, I will assume that you have some familiarity with

27
00:02:04,834 --> 00:02:08,506
Apache Airflow, so I will not cover the very basics

28
00:02:08,570 --> 00:02:12,162
of how to write an airflow dag or how to define an

29
00:02:12,178 --> 00:02:15,642
airflow task. Really, if you are completely new to

30
00:02:15,658 --> 00:02:19,762
the topic, if you want to do generative AI pipelines and are new to airflow,

31
00:02:19,898 --> 00:02:23,032
I highly recommend that you take a look at some of our get started started

32
00:02:23,088 --> 00:02:27,560
resources. Those are available at Astronomer IO docs.

33
00:02:27,712 --> 00:02:31,200
Learn or you go to the Airflow Academy

34
00:02:31,312 --> 00:02:35,200
that astronomer has, which is available at Academy Astronomer

35
00:02:35,272 --> 00:02:39,056
IO. With that said,

36
00:02:39,200 --> 00:02:43,208
what is airflow? It's of course the open source standard for workflow

37
00:02:43,256 --> 00:02:47,104
management. It's extremely popular. It's being downloaded 22

38
00:02:47,144 --> 00:02:50,494
million times per month, which is just wild to think about.

39
00:02:50,954 --> 00:02:54,538
And the one thing that I want to call out on this slide is

40
00:02:54,586 --> 00:02:58,242
that airflow now has 46,000 members

41
00:02:58,418 --> 00:03:02,294
in the airflow slack. So if you ever have a question

42
00:03:03,314 --> 00:03:06,682
about airflow, about any of your use cases, it's one

43
00:03:06,698 --> 00:03:08,694
of the best places to ask.

44
00:03:11,234 --> 00:03:14,706
If your question is, well, are people using airflow

45
00:03:14,770 --> 00:03:18,282
for ML and AI already, or is this completely new?

46
00:03:18,378 --> 00:03:22,152
The answer is it's all already a very established pattern.

47
00:03:22,288 --> 00:03:26,112
So at the end of December last year,

48
00:03:26,288 --> 00:03:29,896
we did a survey among airflow users, the Apache Airflow survey.

49
00:03:30,000 --> 00:03:32,816
And among the almost 800 people who responded,

50
00:03:32,920 --> 00:03:36,904
28% said that they are already using airflow

51
00:03:36,944 --> 00:03:40,528
to train or serve generally machine learning

52
00:03:40,576 --> 00:03:44,064
models, or just manage their mlops use cases.

53
00:03:44,224 --> 00:03:47,496
So this is not something new. This is a pattern that already a lot of

54
00:03:47,520 --> 00:03:52,190
people are using now. Why is airflow so popular

55
00:03:52,262 --> 00:03:56,030
to use for AI and also for general data orchestration?

56
00:03:56,222 --> 00:04:00,270
It's because it can sit in the middle of your whole data

57
00:04:00,342 --> 00:04:03,742
and ML stack. So if you are in the data and ML space,

58
00:04:03,798 --> 00:04:07,214
you will recognize some of these logos, maybe you recognize all of them,

59
00:04:07,334 --> 00:04:11,374
but these are just a few of the tools that airflow can connect to

60
00:04:11,414 --> 00:04:15,390
and orchestrate actions. In and with airflow being the orchestrator,

61
00:04:15,462 --> 00:04:18,686
what you really want is to have a way to make sure that

62
00:04:18,750 --> 00:04:22,299
all of the tasks, all of the actions, and all of these tools are

63
00:04:22,331 --> 00:04:26,291
happening at the exact right moment and with the right dependencies.

64
00:04:26,427 --> 00:04:29,619
The prime example is you cannot train your model before

65
00:04:29,691 --> 00:04:32,763
your training data is ready. So all of this is connected,

66
00:04:32,883 --> 00:04:36,395
and airflow can sit in the middle and be the orchestrator to

67
00:04:36,419 --> 00:04:39,891
your data symphony. The reason why airflow is

68
00:04:39,907 --> 00:04:43,547
so good at this is because in airflow, you can define

69
00:04:43,635 --> 00:04:47,355
everything you need in python code. So if you have any action

70
00:04:47,419 --> 00:04:51,192
that you can define in python code, any call to another API

71
00:04:51,248 --> 00:04:54,496
of a different data tool, you can turn that into a

72
00:04:54,520 --> 00:04:57,284
task inside of airflow very easily.

73
00:04:59,224 --> 00:05:03,168
That's airflow. Who is astronomer? Why am I talking about airflow to you?

74
00:05:03,336 --> 00:05:06,752
Astronomer is the driving force behind Apache Airflow. So astronomer

75
00:05:06,768 --> 00:05:09,920
is a company that drives. Currently, all of the new airflow

76
00:05:09,952 --> 00:05:13,720
releases, has a lot of the major committers and PMC members

77
00:05:13,792 --> 00:05:17,416
on board, and also contributed over half of the current

78
00:05:17,480 --> 00:05:20,960
airflow code. So Airflow is open source, but a lot of the code is

79
00:05:20,992 --> 00:05:25,192
contributed by employees of Astronomer. And there's also a growing

80
00:05:25,248 --> 00:05:29,344
academy ecosystem. I mentioned the academy earlier. It has beginner courses

81
00:05:29,384 --> 00:05:33,176
and more advanced modules, and now there are over 30,000 people who have taken

82
00:05:33,240 --> 00:05:36,952
courses in the academy. So airflow education is also something

83
00:05:37,048 --> 00:05:40,624
that's very important to astronomer. Of course,

84
00:05:40,664 --> 00:05:44,484
Astronomer is a company, so there's a commercial offering which is called Astro.

85
00:05:44,784 --> 00:05:48,530
Astro is a place where you can run airflow deployments

86
00:05:48,602 --> 00:05:53,018
at scale, and many of them without needing to worry about infrastructure.

87
00:05:53,106 --> 00:05:56,586
So Airflow itself is open source, and you can run it however you want

88
00:05:56,610 --> 00:05:59,858
to. But if you want to have an easy way to spin up a lot

89
00:05:59,866 --> 00:06:03,570
of airflow deployments, especially if you have different teams

90
00:06:03,602 --> 00:06:07,706
that use airflow and want to have additional features on top of airflow like

91
00:06:07,730 --> 00:06:11,626
RBAC management, then Astral is a great option for

92
00:06:11,650 --> 00:06:15,432
you. If this is something that interests you, this QR

93
00:06:15,488 --> 00:06:18,880
code and the link on the slide here give you

94
00:06:18,912 --> 00:06:22,240
a free trial, a 14 day trial with $300 in

95
00:06:22,272 --> 00:06:25,976
credits so you can try out what it feels like to run airflow

96
00:06:26,000 --> 00:06:29,544
with astronomer. And if you sign up with this link or with this QR

97
00:06:29,584 --> 00:06:33,488
code, you also get a complimentary airflow fundamentals certification

98
00:06:33,576 --> 00:06:37,496
exam try. So if you go to the academy that I mentioned and

99
00:06:37,520 --> 00:06:41,216
want to take the certification exam, you get one try for free with

100
00:06:41,240 --> 00:06:44,744
the code that sent to you. If you sign up via this link,

101
00:06:45,284 --> 00:06:49,436
I will show this again at the very end of the talk. But important

102
00:06:49,500 --> 00:06:52,876
to note, everything else that I'm showing, all of the code that I'm

103
00:06:52,900 --> 00:06:56,304
showing is all possible with open source airflow.

104
00:06:58,924 --> 00:07:02,428
All right, so you want to do Genai with airflow,

105
00:07:02,556 --> 00:07:05,988
and usually you have some prototype, maybe in a Jupyter

106
00:07:06,036 --> 00:07:09,610
notebook, and the prototype works great, but putting

107
00:07:09,642 --> 00:07:12,978
this into production is a different beast. So you have all

108
00:07:12,986 --> 00:07:16,586
of these challenges that arise. You run into API outages and

109
00:07:16,610 --> 00:07:19,882
rate limits. You need to make sure that your training data is always

110
00:07:19,938 --> 00:07:23,514
up to date or you're augmenting data. Because if you think

111
00:07:23,554 --> 00:07:26,738
about it, everyone who's watching this talk can

112
00:07:26,906 --> 00:07:30,210
make a call to an API of a large language model.

113
00:07:30,322 --> 00:07:34,410
So it's very easy now to just create a wrapper around these

114
00:07:34,522 --> 00:07:38,482
APIs of large language models. But you need to set yourself apart

115
00:07:38,538 --> 00:07:41,856
from your competitors and the way that you can do that, or one way that

116
00:07:41,880 --> 00:07:44,664
you can do that is by having better data,

117
00:07:44,784 --> 00:07:48,480
is by using your valuable, your organization's data or your

118
00:07:48,512 --> 00:07:52,016
own data to augment the responses that you can get

119
00:07:52,120 --> 00:07:55,648
from a large language model. You have changing

120
00:07:55,696 --> 00:07:59,240
tools and APIs every day. I see a new model that has

121
00:07:59,272 --> 00:08:02,696
come out almost every day, literally, and new benchmarks.

122
00:08:02,800 --> 00:08:06,376
So it's possible that a month from now there will be a new model for

123
00:08:06,400 --> 00:08:09,336
your use case that is just a lot better than the one that you are

124
00:08:09,360 --> 00:08:12,870
using now, and that might be hosted it on a different service.

125
00:08:13,022 --> 00:08:16,954
So you really need to be able to adapt to changing tools and APIs.

126
00:08:17,574 --> 00:08:20,634
You can end up with quite complex pipeline structures,

127
00:08:21,254 --> 00:08:24,822
and you need to be able to determine which data went to do training.

128
00:08:24,878 --> 00:08:27,914
In a lot of cases, especially for compliance reasons,

129
00:08:28,534 --> 00:08:31,926
everyone who, like me, is in Europe. You will

130
00:08:31,950 --> 00:08:35,046
be aware of GDPR. That's one of the

131
00:08:35,070 --> 00:08:38,754
reasons why you always you need to be able to audit your pipelines.

132
00:08:39,324 --> 00:08:42,604
And then of course, every time you put something into production, you need to worry

133
00:08:42,644 --> 00:08:44,824
about scalability and reliability.

134
00:08:46,164 --> 00:08:49,224
Airflow has an answer for all of these challenges.

135
00:08:49,964 --> 00:08:53,732
With API outages and rate limits, airflow can have automatic

136
00:08:53,788 --> 00:08:57,004
weak tries configured Airflow is already the standard

137
00:08:57,044 --> 00:09:00,260
to keep your training data up to date. And as I mentioned before,

138
00:09:00,412 --> 00:09:03,564
Airflow can connect to any tool that you can connect

139
00:09:03,604 --> 00:09:07,142
to using Python code. So Airflow is tool agnostic at

140
00:09:07,158 --> 00:09:10,686
its core. And especially if you use the task flow API,

141
00:09:10,750 --> 00:09:14,550
which is something I'm going to show in this talk, then it's very easy to

142
00:09:14,582 --> 00:09:17,514
just change which tool you are connecting to.

143
00:09:18,174 --> 00:09:21,910
You can create complex pipeline structures, among other things,

144
00:09:22,022 --> 00:09:25,478
with data sets and dynamic task mapping and branching, which I will all

145
00:09:25,526 --> 00:09:28,894
cover in this talk. And there are ways to

146
00:09:28,934 --> 00:09:32,544
determine which data went into training by using observability features

147
00:09:32,584 --> 00:09:36,120
and an open lineage integration. For scalability, you have

148
00:09:36,152 --> 00:09:39,536
pluggable compute and reliability wise. Airflow has been

149
00:09:39,560 --> 00:09:43,400
around for a while now and is really battle tested. And because Airflow

150
00:09:43,432 --> 00:09:46,800
itself is all defined in code, it's all defined in Python.

151
00:09:46,912 --> 00:09:50,704
You can have CI CD and DevOps best practices on top

152
00:09:50,744 --> 00:09:54,496
of your pipeline code to really make sure that your pipelines are

153
00:09:54,520 --> 00:09:57,644
robust and your genai application is reliable.

154
00:09:58,244 --> 00:10:01,996
Little side note, in the Conf 42 talk

155
00:10:02,060 --> 00:10:05,260
about Python this year, in 2024,

156
00:10:05,372 --> 00:10:09,164
I gave a talk about how to do testing and CI CD with Airflow,

157
00:10:09,204 --> 00:10:12,708
if that's something that interests you. So after

158
00:10:12,756 --> 00:10:16,036
seeing all of these answers to the challenges, we can kind

159
00:10:16,060 --> 00:10:19,548
of append our statement from earlier and say it's not just your

160
00:10:19,596 --> 00:10:23,060
data that sets you apart from your competitors, it's also the

161
00:10:23,092 --> 00:10:27,074
quality of your orchestration. If your orchestration is reliable,

162
00:10:27,154 --> 00:10:30,826
never goes down, and is scalable, you will have the better product than

163
00:10:30,850 --> 00:10:31,814
your competitor.

164
00:10:33,914 --> 00:10:37,346
All right, picking out some of these features. These are the

165
00:10:37,370 --> 00:10:40,874
features that I think are essential to learn about when you

166
00:10:40,914 --> 00:10:44,778
create best practice generative AI pipelines. And I

167
00:10:44,786 --> 00:10:48,014
will cover the first six ones in this talk,

168
00:10:48,514 --> 00:10:51,910
but for the other ones there are guides available as well

169
00:10:51,982 --> 00:10:56,118
on our learn page on astronomer IO learn docs

170
00:10:56,206 --> 00:10:59,854
learn. For the first four, the light blue

171
00:10:59,894 --> 00:11:03,694
ones here, I will just cover them briefly, and then I will spend some more

172
00:11:03,734 --> 00:11:07,286
time talking about dataset scheduling and dynamic task mapping,

173
00:11:07,390 --> 00:11:10,726
specifically about what is new about these features in

174
00:11:10,750 --> 00:11:14,878
Airflow 29. So let's get started with quick

175
00:11:14,926 --> 00:11:17,394
airflow features in rapid succession.

176
00:11:18,224 --> 00:11:22,352
First of all, the task flow API. Now this is something that

177
00:11:22,488 --> 00:11:26,472
a lot of you might be familiar with if you are writing modern airflow DAX,

178
00:11:26,608 --> 00:11:30,056
but if you've used airflow a while ago, or if you are on an older

179
00:11:30,080 --> 00:11:33,976
version, you might not know about this yet. And the task

180
00:11:34,000 --> 00:11:37,488
flow API really is a pythonic way to write airflow dags,

181
00:11:37,536 --> 00:11:41,104
and it's a much more natural way that you can create and

182
00:11:41,144 --> 00:11:44,412
chain your tasks. On the left

183
00:11:44,468 --> 00:11:48,212
hand side here you can see a traditionally defined airflow task.

184
00:11:48,308 --> 00:11:51,916
So we're using a Python operator to run a function called

185
00:11:51,980 --> 00:11:55,628
say hi func. And this is

186
00:11:55,676 --> 00:11:59,428
possible. It's still possible with airflow, but it takes a lot of boilerplate

187
00:11:59,476 --> 00:12:03,092
code. And what you can do with the task flow API is you can

188
00:12:03,188 --> 00:12:06,372
create the exact same output, so you can run the exact

189
00:12:06,428 --> 00:12:09,960
same task by simply having a python function and

190
00:12:09,992 --> 00:12:13,656
putting task on top of it. So what happens now is

191
00:12:13,680 --> 00:12:17,448
I can have any Python function, and by putting at task on

192
00:12:17,456 --> 00:12:21,520
top of it, Airflow realizes this is an airflow task now and does

193
00:12:21,592 --> 00:12:25,684
everything that you previously had to define this Python operator for.

194
00:12:26,264 --> 00:12:30,392
It's also much easier to create dependencies in between tasks when

195
00:12:30,408 --> 00:12:33,720
you're using the task flow API, and you can see what this looks

196
00:12:33,752 --> 00:12:37,000
like in the example code. You can also mix and

197
00:12:37,032 --> 00:12:40,248
match traditional operators and taskflow API. That's also something

198
00:12:40,296 --> 00:12:43,312
I've done in demo. And there are not just this.

199
00:12:43,368 --> 00:12:47,120
There's not just this one decorator at task, there are many others.

200
00:12:47,192 --> 00:12:50,720
There's kubernetes, there's at task bash,

201
00:12:50,832 --> 00:12:55,004
and you can learn more about them in our guide about airflow decorators.

202
00:12:55,904 --> 00:12:59,304
Personally, I've switched over to using decorators whenever

203
00:12:59,344 --> 00:13:02,570
possible over traditional operators, though traditional

204
00:13:02,602 --> 00:13:06,506
operators still have their use, especially when you

205
00:13:06,530 --> 00:13:09,730
are interacting with complex tools and the operator does a lot of

206
00:13:09,762 --> 00:13:13,534
work for you and abstracts over a lot of python logic for you.

207
00:13:15,474 --> 00:13:18,978
All right, now we're writing pythonic airflow Dax let's protect

208
00:13:19,026 --> 00:13:22,346
against API failures and rate limits. When I was creating

209
00:13:22,370 --> 00:13:25,810
the demo for this talk, I actually rate limited myself. I ran

210
00:13:25,842 --> 00:13:29,342
into this exact problem. So what you can do is you

211
00:13:29,358 --> 00:13:33,086
can configure automatic retries, and this is really a best practice in

212
00:13:33,110 --> 00:13:36,326
production. You always want retries configured,

213
00:13:36,430 --> 00:13:40,054
unless for a specific task, there's a reason why you do

214
00:13:40,094 --> 00:13:43,670
not want this task to automatically retry. You can

215
00:13:43,702 --> 00:13:47,974
of course configure the number of retries. You can configure a specific delay

216
00:13:48,054 --> 00:13:51,758
in between the retries, or you can also say you want to have an exponential

217
00:13:51,806 --> 00:13:54,992
back off, so it takes longer and longer after each

218
00:13:55,048 --> 00:13:58,656
try to try again. You can set a maximum delay as

219
00:13:58,680 --> 00:14:01,804
well. The ways to configure retries,

220
00:14:02,144 --> 00:14:05,568
you can configure them in the airflow configuration so you

221
00:14:05,576 --> 00:14:08,768
can set them for your whole airflow environment. You can set them

222
00:14:08,816 --> 00:14:12,504
for all tasks in a specific DAC in the default arguments,

223
00:14:12,624 --> 00:14:16,240
and you can override that at the specific task level. So a

224
00:14:16,272 --> 00:14:20,124
common pattern is to have an airflow configuration that says all

225
00:14:20,164 --> 00:14:24,092
tasks have free retries by default. But for specific tasks where

226
00:14:24,108 --> 00:14:27,956
you already know that API is finicky, you might set retries

227
00:14:28,020 --> 00:14:30,344
to ten with an exponential back off.

228
00:14:33,444 --> 00:14:36,972
Okay, we have task flow API retries. Now.

229
00:14:37,028 --> 00:14:40,844
Our pipelines are getting more complex because a lot of times you run into

230
00:14:40,884 --> 00:14:44,500
the issue that depending on different aspects of your

231
00:14:44,532 --> 00:14:48,022
data or different situations, you want different tasks to

232
00:14:48,038 --> 00:14:52,314
be running. And the way you can achieve that is by using branching.

233
00:14:53,014 --> 00:14:56,510
What you can see here is an abbreviated version of a task that

234
00:14:56,542 --> 00:15:00,174
exists in the demo. And the the graph view of this

235
00:15:00,214 --> 00:15:04,702
dag is the challenger versus champion dag

236
00:15:04,878 --> 00:15:08,702
that is also in the demo as well. And what you can see

237
00:15:08,758 --> 00:15:11,862
is I have a python function and I'm again using a

238
00:15:11,878 --> 00:15:15,906
decorator on line 35. I'm saying at task

239
00:15:16,010 --> 00:15:19,330
branch, and this turns my task into a branching

240
00:15:19,402 --> 00:15:22,690
function. And this function is very simple.

241
00:15:22,802 --> 00:15:26,530
I have some logic that I abbreviated here that figures out if

242
00:15:26,562 --> 00:15:29,802
a current champion model exists, and if

243
00:15:29,818 --> 00:15:33,418
it does, I return the string champion exists. If it does not,

244
00:15:33,466 --> 00:15:37,018
I return, there's no champion. So this task

245
00:15:37,146 --> 00:15:40,174
is what creates this 1st 1st branching task here.

246
00:15:40,834 --> 00:15:44,458
And the string that is returned is the

247
00:15:44,586 --> 00:15:48,322
task id of the downstream task that I want to run.

248
00:15:48,418 --> 00:15:52,306
So if there is a champion present, I want to run the task

249
00:15:52,370 --> 00:15:55,938
called champion exists and go that way in my dag

250
00:15:55,986 --> 00:15:59,610
structure. If there's no champion present, I want to run the

251
00:15:59,642 --> 00:16:04,162
task no champion and skip all of the intermediate

252
00:16:04,218 --> 00:16:07,936
tasks that are here. Because if there's no champion, I don't need to do a

253
00:16:07,960 --> 00:16:11,844
champion challenger comparison, I will just use the challenger.

254
00:16:12,624 --> 00:16:15,696
In this specific DAC, we have two more branching tasks.

255
00:16:15,760 --> 00:16:19,264
We have ischallenger, which does the same thing to

256
00:16:19,304 --> 00:16:23,368
evaluate whether a current challenger model exists when this DAG is running.

257
00:16:23,536 --> 00:16:27,044
Because if there's no challenger, I would just keep the current champion.

258
00:16:27,504 --> 00:16:31,418
And if both exist, if a current champion model exists

259
00:16:31,496 --> 00:16:35,542
and a challenger model has been presented, then I want to actually

260
00:16:35,638 --> 00:16:39,638
pick the better one. And then I have my comparing accuracies

261
00:16:39,726 --> 00:16:43,014
task. And depending on which model was more

262
00:16:43,054 --> 00:16:46,870
accurate, I either switch to champion, so I'm using the challenger,

263
00:16:46,982 --> 00:16:50,782
or I keep my champion. Now you can

264
00:16:50,918 --> 00:16:54,502
create very fun dags with this. You can create little games

265
00:16:54,558 --> 00:16:58,122
with this as well. One thing that you have to keep in mind is

266
00:16:58,178 --> 00:17:01,834
by default, airflow only runs tasks

267
00:17:01,914 --> 00:17:05,194
if all the upstream tasks were successful.

268
00:17:05,354 --> 00:17:08,586
But if you are using branching, it's very easy to get a

269
00:17:08,610 --> 00:17:11,850
situation where one task gets skipped and

270
00:17:11,882 --> 00:17:15,498
one is successful, and the downstream task that is scheduled,

271
00:17:15,586 --> 00:17:18,762
depending on both of these tasks, should always run

272
00:17:18,818 --> 00:17:22,250
anyway. And that's when you need to adjust the trigger

273
00:17:22,282 --> 00:17:26,342
rules. So the trigger rules downstream of branching tasks

274
00:17:26,438 --> 00:17:30,342
always need a little thought. A common trigger rule that people use in

275
00:17:30,358 --> 00:17:33,870
this case would be, for example, one success. In that case,

276
00:17:33,942 --> 00:17:37,678
the task runs as soon as one upstream task

277
00:17:37,726 --> 00:17:41,686
has been successful, and it doesn't matter if the other ones get skipped

278
00:17:41,710 --> 00:17:42,754
or even fail.

279
00:17:45,494 --> 00:17:48,838
The other thing I wanted to mention really quickly in just

280
00:17:48,886 --> 00:17:51,710
one slide are deferrable operators,

281
00:17:51,862 --> 00:17:55,606
because if you are creating generative AI pipelines, it's likely

282
00:17:55,670 --> 00:17:58,662
that you have tasks that run for a long time.

283
00:17:58,798 --> 00:18:02,790
So the prime example is model training and fine tuning.

284
00:18:02,902 --> 00:18:06,310
That might take a while, or you are waiting for an event

285
00:18:06,382 --> 00:18:09,470
to occur in an external system with a sensor.

286
00:18:09,502 --> 00:18:13,862
You're probably familiar with sensors. A lot of deferrable operators are deferrable

287
00:18:13,918 --> 00:18:17,362
versions of sensors. If you

288
00:18:17,458 --> 00:18:21,210
are waiting for a long time, you don't want that worker slot

289
00:18:21,282 --> 00:18:25,066
to be taken up while waiting in your production environment, you want

290
00:18:25,090 --> 00:18:28,722
to release the worker slot and for that worker to be able to run

291
00:18:28,738 --> 00:18:32,730
a different task in parallel. So what you're doing is

292
00:18:32,802 --> 00:18:36,122
you define it. You use a deferrable operator which will

293
00:18:36,178 --> 00:18:40,690
go to a whole different airflow component called the trigger component,

294
00:18:40,802 --> 00:18:43,934
and held that component to start an asynchronous

295
00:18:43,974 --> 00:18:46,714
process waiting for something to happen.

296
00:18:47,614 --> 00:18:51,366
Meanwhile, the worker slot gets released and you can have

297
00:18:51,390 --> 00:18:55,102
the advantage of resource optimization so the worker can

298
00:18:55,118 --> 00:18:59,150
do a different task in the meantime. And as soon as the condition is

299
00:18:59,182 --> 00:19:02,814
happening that the async process is waiting for, the task will

300
00:19:02,854 --> 00:19:06,582
get picked up by a worker. Again, the best practice here

301
00:19:06,638 --> 00:19:10,010
is to use the variable operators whenever possible for

302
00:19:10,042 --> 00:19:14,194
longer running tasks. In the demo, there is one deferrable operator,

303
00:19:14,314 --> 00:19:17,174
and I will show you how that works in practice.

304
00:19:19,394 --> 00:19:23,014
Okay, those were the four features I wanted to cover a little bit.

305
00:19:23,354 --> 00:19:27,178
Now let's dive deeper into data set scheduling. This is my personal

306
00:19:27,266 --> 00:19:30,858
favorite way of scheduling DAX, and I hope it will become

307
00:19:30,906 --> 00:19:34,250
yours as well, because what you can do with datasets is

308
00:19:34,282 --> 00:19:37,590
you can chain your dags based on the data that

309
00:19:37,622 --> 00:19:41,634
they are updating. So this is the data sets view in the airflow environment.

310
00:19:42,094 --> 00:19:45,294
This is available as of Airflow 24. So if you do not

311
00:19:45,334 --> 00:19:48,674
have that tab, you might need to upgrade your airflow.

312
00:19:49,494 --> 00:19:52,622
And what we have here is we have one upstream dac

313
00:19:52,678 --> 00:19:56,614
which is called my ETL Dac. And there is a task

314
00:19:56,654 --> 00:20:00,478
in this DAC where I as the DAC offer know this

315
00:20:00,526 --> 00:20:04,510
task updates some data. So this task updates

316
00:20:04,622 --> 00:20:07,994
the data CSV file in my s three bucket.

317
00:20:08,294 --> 00:20:11,806
It doesn't actually matter if that update happens.

318
00:20:11,910 --> 00:20:15,766
Task could do something completely different or it could be an empty task.

319
00:20:15,950 --> 00:20:18,742
I, as the DAG offer tell the task, hey,

320
00:20:18,838 --> 00:20:22,878
when you finish successfully, you produce

321
00:20:22,926 --> 00:20:26,742
an update to that data set. I always imagine it as the task waving a

322
00:20:26,758 --> 00:20:30,240
little flak at as soon as it's done and then

323
00:20:30,392 --> 00:20:33,672
it updates this data set and you can have

324
00:20:33,688 --> 00:20:37,848
as many data sets as you want and then you can have downstream

325
00:20:37,896 --> 00:20:41,432
dags. So my ML DAC here is a different Dag

326
00:20:41,608 --> 00:20:45,432
and you can have these dags scheduled based on updates

327
00:20:45,568 --> 00:20:49,512
to datasets. So I can say, well, whenever that data

328
00:20:49,608 --> 00:20:53,112
CSV file is updated, I want to run the

329
00:20:53,168 --> 00:20:56,500
my ML DAc in this

330
00:20:56,532 --> 00:20:59,184
case to show you how this can be chained.

331
00:20:59,564 --> 00:21:02,916
There is another task inside of this mYml dag

332
00:21:03,020 --> 00:21:06,140
which produces an update to a new data set

333
00:21:06,212 --> 00:21:09,780
called Snowflake Mytable. A few

334
00:21:09,812 --> 00:21:13,156
notes here. The data sets are defined in a

335
00:21:13,180 --> 00:21:17,084
URI syntax. That is just a best practice.

336
00:21:17,244 --> 00:21:21,306
You could theoretically call the data set, my model, or something else.

337
00:21:21,490 --> 00:21:25,402
There are a few strings that are not allowed, but generally

338
00:21:25,578 --> 00:21:29,658
it's very possible to use different strings as soon as they would

339
00:21:29,706 --> 00:21:32,394
evaluate to a valid Uri. But it is,

340
00:21:32,514 --> 00:21:36,674
as I said, a best practice to use uris, especially waiting for

341
00:21:36,714 --> 00:21:39,334
potential future airflow features.

342
00:21:39,954 --> 00:21:43,610
And the other question that always comes up is airflow is currently

343
00:21:43,682 --> 00:21:47,464
not aware of the underlying data. So it doesn't

344
00:21:47,504 --> 00:21:51,576
matter whether that snowflake table exists, whether that s three bucket exists.

345
00:21:51,720 --> 00:21:55,040
You as the DAG offer name these datasets and

346
00:21:55,112 --> 00:21:58,552
you determine which tasks actually produce updates to

347
00:21:58,568 --> 00:22:01,952
these datasets. This is what it

348
00:22:01,968 --> 00:22:05,472
looks like in the UI if you are running Airflow 29. So this is

349
00:22:05,488 --> 00:22:09,344
a brand new 2.9 feature. You can see this is the consumer

350
00:22:09,384 --> 00:22:13,308
DAC that we had in the previous slide, and we have

351
00:22:13,356 --> 00:22:17,420
one task in this DAG called my task. But you can also see

352
00:22:17,452 --> 00:22:21,180
the data set that this DAg is scheduled on, that's to the very

353
00:22:21,252 --> 00:22:25,020
left of the DaG graph. And then you can

354
00:22:25,052 --> 00:22:29,124
see we have the new producer task and the next dataset that

355
00:22:29,164 --> 00:22:32,892
this task is producing too. So now you can see all of

356
00:22:32,908 --> 00:22:36,244
your data sets in your DaG graph as well, which I personally find really

357
00:22:36,284 --> 00:22:39,896
nice. But that was not the only

358
00:22:39,960 --> 00:22:43,624
update to dataset scheduling in Airflow 29. We also have

359
00:22:43,664 --> 00:22:47,664
these three new additions which open up a ton of new use cases for

360
00:22:47,704 --> 00:22:51,024
dataset scheduling. The first one is you can

361
00:22:51,064 --> 00:22:54,224
now schedule on logical dataset expressions.

362
00:22:54,344 --> 00:22:58,368
So previously you could schedule either on one dataset or

363
00:22:58,496 --> 00:23:02,440
having an update to a whole list of datasets with and

364
00:23:02,512 --> 00:23:06,296
logic. But now you can use any combination of and

365
00:23:06,360 --> 00:23:10,040
and or logic. So you can say, I want the stack to

366
00:23:10,072 --> 00:23:13,384
run as soon as this data set or this

367
00:23:13,424 --> 00:23:17,128
other data set has been updated, and you can do that with as many data

368
00:23:17,176 --> 00:23:20,688
sets as you want, and you can build complex logical structures

369
00:23:20,736 --> 00:23:24,216
like that. The other new update

370
00:23:24,320 --> 00:23:28,000
is that you can also have a DaG now that is running on both time

371
00:23:28,112 --> 00:23:31,196
and data set. The prime use case is that

372
00:23:31,220 --> 00:23:34,476
you have a Dag that updates a table and you want this

373
00:23:34,500 --> 00:23:38,452
dag to run once per day, but also whenever

374
00:23:38,508 --> 00:23:41,892
a certain data set is updated. And you can now do that with

375
00:23:41,908 --> 00:23:45,708
the data set or time schedule. And the last one

376
00:23:45,756 --> 00:23:49,340
which I'm really excited about because it opens up airflow to the

377
00:23:49,372 --> 00:23:53,460
world in a way. We now have a rest API

378
00:23:53,532 --> 00:23:56,024
endpoint to update datasets.

379
00:23:56,184 --> 00:23:59,640
So previously, before Airflow 29, you could only

380
00:23:59,712 --> 00:24:03,448
have an update to a data set from a task within the same

381
00:24:03,496 --> 00:24:07,872
airflow deployment, but now you can send that update from anywhere.

382
00:24:08,008 --> 00:24:11,248
This could be used for cross deployment dependencies. So you have

383
00:24:11,296 --> 00:24:15,024
two Airflow deployments and they can now talk to each other very easily using

384
00:24:15,064 --> 00:24:18,344
this feature, or it could also be

385
00:24:18,424 --> 00:24:21,812
from anywhere. So you could have a button on a website and

386
00:24:21,828 --> 00:24:25,428
then you run some JavaScript code and that creates the

387
00:24:25,476 --> 00:24:29,580
update to the dataset. And that kicks off a

388
00:24:29,612 --> 00:24:33,444
whole set of downstream dags. So you can really make these

389
00:24:33,484 --> 00:24:36,956
updates now from anywhere. I will show how

390
00:24:36,980 --> 00:24:40,436
to do that and also how to do the two other features that are

391
00:24:40,460 --> 00:24:44,324
new in the demo. So the demo shows all three of these new advanced

392
00:24:44,364 --> 00:24:46,104
dataset scheduling features.

393
00:24:48,184 --> 00:24:51,560
The other airflow feature that I want to

394
00:24:51,592 --> 00:24:54,864
dive in a little bit more deeply is dynamic task mapping,

395
00:24:54,904 --> 00:24:58,152
because dynamic task mapping is really a core

396
00:24:58,248 --> 00:25:01,864
airflow feature now that you should be aware of, especially if you're using

397
00:25:01,904 --> 00:25:05,680
airflow for generative AI. What dynamic task mapping

398
00:25:05,712 --> 00:25:09,448
can do is you can have kind of like a template task,

399
00:25:09,576 --> 00:25:13,370
and you can say, well, at runtime I want airflow to

400
00:25:13,402 --> 00:25:17,162
figure out how many copies we need of that task

401
00:25:17,258 --> 00:25:21,370
to run in parallel. And this can be based on inputs

402
00:25:21,402 --> 00:25:24,530
that occurs at runtime. At the bottom of the

403
00:25:24,562 --> 00:25:28,482
slide you can see a very simple dag that has two tasks.

404
00:25:28,618 --> 00:25:31,906
We have one task that's called getfilepaths.

405
00:25:32,010 --> 00:25:36,074
So we are getting a number of different file paths and then the downstream

406
00:25:36,114 --> 00:25:38,934
task is processing files.

407
00:25:39,054 --> 00:25:42,606
And I want to have one task for each of the file

408
00:25:42,630 --> 00:25:46,302
paths that I'm getting, but I don't know how many file paths

409
00:25:46,358 --> 00:25:49,990
I will get every day. Maybe these are files that are dropped into an

410
00:25:50,022 --> 00:25:53,998
object storage by a domain expert on my topic and

411
00:25:54,046 --> 00:25:57,126
I never know to how many files they will get that day. Maybe one day

412
00:25:57,150 --> 00:26:01,334
it's two, like in this example, and the other day they are dropping ten files.

413
00:26:01,494 --> 00:26:05,014
And this way my pipeline is the dynamic, so it will

414
00:26:05,054 --> 00:26:09,094
automatically create one parallel task instance

415
00:26:09,214 --> 00:26:11,994
for each of the file piles that it is given.

416
00:26:12,814 --> 00:26:16,638
The important thing to remember is that you have two ways of defining

417
00:26:16,686 --> 00:26:20,158
parameters. In a dynamically mapped task, you can have parameters

418
00:26:20,206 --> 00:26:23,734
that stay the same in between these copies. These go into

419
00:26:23,814 --> 00:26:27,142
partial and parameters that change in between the

420
00:26:27,158 --> 00:26:30,850
dynamically mapped task instances. These go into existence. Expand or

421
00:26:30,922 --> 00:26:34,994
expand quarks the best practice here, use dynamic tasks

422
00:26:35,034 --> 00:26:39,134
whenever possible, especially over dynamic Dax.

423
00:26:39,434 --> 00:26:42,994
Dynamic Dax, that's still a valid use case.

424
00:26:43,154 --> 00:26:46,410
Use case for them in many cases. But there are some cases where

425
00:26:46,442 --> 00:26:50,130
people use dynamic DAX that would be really possible to address

426
00:26:50,202 --> 00:26:54,010
with dynamic tasks which are much more observable and

427
00:26:54,042 --> 00:26:57,758
easier to use. The other best practice is to

428
00:26:57,806 --> 00:27:01,274
customize the map index. This is an airflow 29 feature.

429
00:27:02,094 --> 00:27:05,406
So this is fairly new, but it makes it much easier

430
00:27:05,510 --> 00:27:09,154
to find the logs of a specific dynamically mapped task.

431
00:27:09,894 --> 00:27:12,918
Now, how do you do that in practice?

432
00:27:13,086 --> 00:27:17,318
So we have the basics, which is we have the partial method,

433
00:27:17,366 --> 00:27:21,718
and as I said before, all the parameters that you put into partial

434
00:27:21,846 --> 00:27:25,430
will stay the same for each dynamically mapped task instance.

435
00:27:25,542 --> 00:27:29,222
So in this case I'm saying the parameter a is always

436
00:27:29,318 --> 00:27:32,766
equal to two. The second method

437
00:27:32,830 --> 00:27:36,142
that you put on your call of a task

438
00:27:36,278 --> 00:27:40,542
is dot expand. So here I have another parameter

439
00:27:40,598 --> 00:27:44,102
called b, and I'm saying, well, b is

440
00:27:44,118 --> 00:27:47,890
the parameter that I want to change in between the dynamically mapped

441
00:27:47,922 --> 00:27:51,378
task instances. So to b I'm giving a list of

442
00:27:51,426 --> 00:27:55,490
inputs, in this case zero and one. This will create two

443
00:27:55,562 --> 00:27:59,674
dynamically mapped task instances, the first one using zero

444
00:27:59,794 --> 00:28:03,330
for the parameter b, and the second one using one

445
00:28:03,402 --> 00:28:04,814
for the parameter b.

446
00:28:06,314 --> 00:28:09,874
Of course, here I hard coded a list. This could be the output

447
00:28:09,914 --> 00:28:13,358
of an upstream task, and that's how you can create these dynamics

448
00:28:13,426 --> 00:28:17,486
pipelines. One thing to note, it's important that

449
00:28:17,510 --> 00:28:21,206
you name the keyword argument here you have to say b equals.

450
00:28:21,350 --> 00:28:24,886
You cannot forget that, otherwise you will get a dag input error,

451
00:28:24,950 --> 00:28:28,074
but the error tells you specifically what you did wrong in this case.

452
00:28:28,974 --> 00:28:32,454
And lastly, the other basic if you are running Airflow 29,

453
00:28:32,574 --> 00:28:36,350
you want to create a map index template. This is a parameter

454
00:28:36,462 --> 00:28:40,098
that goes directly into either the partial method in

455
00:28:40,106 --> 00:28:45,722
a traditional operator or into the task decorator if

456
00:28:45,738 --> 00:28:50,014
you're using the task flow API, and then you can customize the map index.

457
00:28:50,954 --> 00:28:54,426
If you're already familiar with dynamic task mapping or a more advanced

458
00:28:54,450 --> 00:28:58,050
airflow user, you will probably be interested to learn about the more

459
00:28:58,082 --> 00:29:01,594
advanced methods. There's expand quarks.

460
00:29:01,754 --> 00:29:05,570
This allows you to map over sets of keyword arguments. I highly recommend

461
00:29:05,602 --> 00:29:10,366
looking into that because it also opens up a lot of use cases and

462
00:29:10,430 --> 00:29:13,774
one of my favorite little airflow things that not that many people know about,

463
00:29:13,894 --> 00:29:17,878
there is a function called dot map that allows you to transform

464
00:29:17,966 --> 00:29:21,414
the output of an upstream task before you map over

465
00:29:21,454 --> 00:29:25,294
it. So without having an extra task, you can change

466
00:29:25,374 --> 00:29:28,550
the output of any upstream task if it's

467
00:29:28,582 --> 00:29:32,132
not in the right format for your mapping. I use this a lot,

468
00:29:32,198 --> 00:29:36,072
especially if I have an upstream traditional operator and the output isn't

469
00:29:36,128 --> 00:29:40,124
exactly what I need. And it's a very neat feature

470
00:29:40,464 --> 00:29:44,008
to use in dynamic task mapping. If you want

471
00:29:44,016 --> 00:29:47,688
to learn more about dynamic tasks, especially about the more advanced use cases,

472
00:29:47,856 --> 00:29:51,124
I highly recommend our guide on dynamic task mapping.

473
00:29:53,384 --> 00:29:57,280
Now, there are several examples of dynamic task mapping in

474
00:29:57,312 --> 00:30:01,146
the demo, but since there's always a lot of code in the demo, I wanted

475
00:30:01,170 --> 00:30:04,954
to show you just the very basics of what is necessary so

476
00:30:04,994 --> 00:30:08,134
you can use this today in your airflow pipelines.

477
00:30:08,714 --> 00:30:12,562
All right, so here we have two tasks to find. The first one,

478
00:30:12,618 --> 00:30:16,254
line 44, we have the task decorator, and then we have

479
00:30:16,554 --> 00:30:20,490
the function on line 45. So the first task is get file

480
00:30:20,522 --> 00:30:24,310
paths, and the second task is on line 51,

481
00:30:24,442 --> 00:30:28,198
called process file, also defined with a taskflow API

482
00:30:28,246 --> 00:30:31,694
decorator. Little side note, I'm using the task flow

483
00:30:31,734 --> 00:30:35,606
API a lot here, but you can also dynamically map traditional

484
00:30:35,670 --> 00:30:39,542
operators. And in the demo there are traditional operators that are

485
00:30:39,558 --> 00:30:43,302
dynamically mapped. So I can show you what this looks like with a traditional operator

486
00:30:43,358 --> 00:30:47,006
in a second in the demo. So we

487
00:30:47,030 --> 00:30:50,166
had three components for basic dynamic task mapping. The first one

488
00:30:50,190 --> 00:30:54,060
was the elements that stay the same in between

489
00:30:54,132 --> 00:30:57,396
the dynamically mapped tasks. So this is

490
00:30:57,420 --> 00:31:00,588
what goes into that partial. So we have our function process

491
00:31:00,676 --> 00:31:05,164
file and we call that function with the method partial.

492
00:31:05,324 --> 00:31:08,740
And here is where we enter the elements that we want to

493
00:31:08,772 --> 00:31:12,012
stay the same in between the copies of the task.

494
00:31:12,188 --> 00:31:16,260
I very creatively named this parameter constantly.

495
00:31:16,282 --> 00:31:19,480
Course this could be named anything. And I'm saying the constant is

496
00:31:19,512 --> 00:31:22,624
always 42. All right,

497
00:31:22,664 --> 00:31:26,296
so no matter how many map task instances we get, we could get

498
00:31:26,360 --> 00:31:29,984
two. Like in the example here, we could get 100. The constant

499
00:31:30,024 --> 00:31:34,280
will always be 42. Now, what is the element that changes

500
00:31:34,472 --> 00:31:37,808
we from the upstream task, get a list of

501
00:31:37,896 --> 00:31:41,200
file paths. In this case it's hard coded. We will get two

502
00:31:41,232 --> 00:31:44,460
file paths, but this is the task that would,

503
00:31:44,612 --> 00:31:48,604
in our example return maybe two file paths one

504
00:31:48,644 --> 00:31:52,140
day and 100 the next day. So this is what we

505
00:31:52,172 --> 00:31:54,584
put into the expand method.

506
00:31:55,044 --> 00:31:58,556
We have our keyword argument called file. We set that

507
00:31:58,620 --> 00:32:02,396
equal to that list of inputs. And in this case

508
00:32:02,500 --> 00:32:05,908
I don't hard code the list in the method call, but I

509
00:32:05,956 --> 00:32:09,408
get the list as the output from my upstream task.

510
00:32:09,456 --> 00:32:12,888
On line 60 you can see file paths equals get

511
00:32:12,936 --> 00:32:16,808
file paths. So this is the output. Whatever my upstream task returns,

512
00:32:16,936 --> 00:32:20,720
whatever that list is, gets parsed to file.

513
00:32:20,832 --> 00:32:23,984
And then I get one dynamically mapped task instance.

514
00:32:24,024 --> 00:32:27,536
So one copy per element in that list.

515
00:32:27,720 --> 00:32:31,088
So in this case I will get two copies of this task,

516
00:32:31,216 --> 00:32:34,736
one for the first file and one for the second file.

517
00:32:34,920 --> 00:32:38,376
Lastly, let's say we are on Airflow 29. I want

518
00:32:38,400 --> 00:32:41,656
to customize the map index template. So on

519
00:32:41,680 --> 00:32:45,000
line 50 you can see I add a parameter

520
00:32:45,072 --> 00:32:48,840
into my task decorator. I say the map index template is

521
00:32:48,872 --> 00:32:52,208
equal to this jinja template, and this is a ginger template that

522
00:32:52,256 --> 00:32:55,432
doesn't exist yet. You could use an existing one, but I want

523
00:32:55,448 --> 00:32:58,364
to create my new one specifically for this task.

524
00:32:59,064 --> 00:33:02,730
And the way to create that inside of your task is, is at

525
00:33:02,762 --> 00:33:06,934
line 57 you can see I get the current airflow context

526
00:33:07,234 --> 00:33:10,266
and then I add this variable to the context on

527
00:33:10,290 --> 00:33:13,650
line 58. In this case it's equal to a string that

528
00:33:13,682 --> 00:33:17,734
uses both the name of the file and the constant.

529
00:33:18,234 --> 00:33:22,050
And the cool thing now here is because I can define

530
00:33:22,162 --> 00:33:25,674
this parameter value inside of the task,

531
00:33:25,834 --> 00:33:29,448
I could set this to outputs that I'm generating in the task.

532
00:33:29,576 --> 00:33:33,088
So the prime example here would be if I'm testing a machine learning

533
00:33:33,136 --> 00:33:37,048
model and I get a test accuracy back. I could use

534
00:33:37,096 --> 00:33:40,976
the test accuracy in my custom map index template

535
00:33:41,080 --> 00:33:44,568
by defining the custom map index at the very bottom of this

536
00:33:44,616 --> 00:33:47,856
task. So when I hop over into the airflow

537
00:33:47,880 --> 00:33:51,560
UI, you can see here the map index. I clicked on the

538
00:33:51,592 --> 00:33:54,964
mapped task tab and then I can see all my dynamically map task

539
00:33:55,004 --> 00:33:58,468
indices. The custom map index

540
00:33:58,516 --> 00:34:02,516
is not zero and one anymore as in previous airflow versions. But now I

541
00:34:02,540 --> 00:34:05,612
have this string that I defined. So what you could do,

542
00:34:05,708 --> 00:34:09,428
for example, with model testing, you could have your model

543
00:34:09,476 --> 00:34:13,100
testing parameter like the accuracy or your r

544
00:34:13,132 --> 00:34:16,308
squared value displayed on your map index.

545
00:34:16,356 --> 00:34:19,532
And then you can very easily find the one mapped

546
00:34:19,588 --> 00:34:23,163
task index for the model that had the best value.

547
00:34:26,423 --> 00:34:29,959
Okay, that was all of the airflow features. I hope this gives you

548
00:34:29,991 --> 00:34:33,247
a good basis to understand the demo code better.

549
00:34:33,415 --> 00:34:37,263
And let's hop into the demo. So we are talking about content

550
00:34:37,343 --> 00:34:40,839
creation today, and I wanted to

551
00:34:40,871 --> 00:34:44,695
have a demo repository that creates LinkedIn posts

552
00:34:44,719 --> 00:34:48,456
for me about Apache Airflow. So that's useful for

553
00:34:48,480 --> 00:34:52,384
me personally. But you can very easily adapt this

554
00:34:52,424 --> 00:34:56,392
repository to create any type of content for

555
00:34:56,448 --> 00:35:00,424
any type of topic. So let's say you are really into metal music

556
00:35:00,504 --> 00:35:04,884
and you are running a blog post about metal. You could provide

557
00:35:05,424 --> 00:35:09,560
this application with specific information about

558
00:35:09,752 --> 00:35:12,804
the music you want to write about, like the newest albums.

559
00:35:13,434 --> 00:35:16,538
And you could also provide it with examples of your previous blog

560
00:35:16,586 --> 00:35:20,162
posts. And then tell the app instead of posts

561
00:35:20,218 --> 00:35:24,130
about Airflow, I want you to create whole blog posts about

562
00:35:24,162 --> 00:35:27,814
the newest metal bands so you can really adapt this to anything.

563
00:35:28,794 --> 00:35:32,498
Again, the code is in the repository and at a high

564
00:35:32,546 --> 00:35:36,258
level. This airflow project has two distinct

565
00:35:36,306 --> 00:35:39,906
paths. The first one on top here in blue is a

566
00:35:39,930 --> 00:35:43,402
traditional RG pipeline. So we take some knowledge,

567
00:35:43,498 --> 00:35:47,146
some proprietary knowledge that in my case is

568
00:35:47,170 --> 00:35:50,866
about airflow. I'm using some of our own airflow guides and

569
00:35:50,930 --> 00:35:54,306
we ingest that and embed it into a vector database.

570
00:35:54,370 --> 00:35:58,042
In this case, I'm using viviate. Little side note, because Airflow

571
00:35:58,058 --> 00:36:01,706
is too liagnostic, you could very easily swap out any of the elements

572
00:36:01,770 --> 00:36:05,518
here. Like you could, for example, use a different vector database or a different large

573
00:36:05,566 --> 00:36:09,182
language model if you wanted to. The second part

574
00:36:09,238 --> 00:36:12,790
of the demo pipeline is the fine tuning. So not only

575
00:36:12,822 --> 00:36:15,862
do I want more information to be available to

576
00:36:15,878 --> 00:36:19,758
the large language model, I want to fine tune the language model to

577
00:36:19,806 --> 00:36:23,494
use a similar tone that I want and similar length of posts.

578
00:36:23,614 --> 00:36:27,750
So I have some very short examples of LinkedIn posts about airflow,

579
00:36:27,862 --> 00:36:31,290
both training and validation examples. Then I make

580
00:36:31,322 --> 00:36:34,786
sure that the format is correct for fine tuning. If you've fine

581
00:36:34,810 --> 00:36:38,858
tuned GPT before you know that you have a very specific format that

582
00:36:38,906 --> 00:36:41,994
the examples need to be in. And I also added

583
00:36:42,034 --> 00:36:45,282
a little step that makes sure that the fine tuning will be in budget.

584
00:36:45,338 --> 00:36:49,106
So I'm actually counting the tokens and estimating how much the fine tuning

585
00:36:49,130 --> 00:36:52,610
will cost. Then I run the actual fine tuning,

586
00:36:52,722 --> 00:36:55,606
this is run in a deferrable way because it takes a while.

587
00:36:55,770 --> 00:36:59,222
And lastly, I have the champion versus challenger Dag that

588
00:36:59,238 --> 00:37:03,110
I showed in the slides earlier when I talked about branching. That makes sure

589
00:37:03,142 --> 00:37:07,078
that we pick the actual better model. So either the current champion

590
00:37:07,126 --> 00:37:10,726
model or the new challenger model. So if you keep training this with different

591
00:37:10,790 --> 00:37:14,878
examples, it will always pick the best accurate

592
00:37:15,006 --> 00:37:18,686
model. Then on the front end, I just use

593
00:37:18,710 --> 00:37:22,222
the streamlit app. So this is a very simple frame, but you

594
00:37:22,238 --> 00:37:25,314
could connect this to any front end that you wanted to connect to.

595
00:37:25,694 --> 00:37:29,502
And this is where the user can put in a prompt. It gets augmented

596
00:37:29,558 --> 00:37:32,942
with the newest knowledge provided by the retrieval augmented generation

597
00:37:32,998 --> 00:37:36,814
pipeline. And then it uses the current champion GPT

598
00:37:36,894 --> 00:37:40,278
model to create a response. I also have a second

599
00:37:40,326 --> 00:37:44,006
part of the app where the response gets sent to Dali to create a

600
00:37:44,030 --> 00:37:47,922
picture that is mostly for fun because I find the pictures quite amusing that

601
00:37:47,938 --> 00:37:51,394
it creates about airflow. All right,

602
00:37:51,514 --> 00:37:54,974
that's the general pipeline. Let's look.

603
00:37:55,354 --> 00:37:58,234
Let's hop into the code and see what actually happens.

604
00:37:58,394 --> 00:38:01,698
So, as I said, this is the repository. It's all open

605
00:38:01,746 --> 00:38:04,654
source. You're very free to fork it and to use it.

606
00:38:05,034 --> 00:38:08,530
And this is what you get if you fork this repository and

607
00:38:08,562 --> 00:38:11,722
clone it locally. This is the whole

608
00:38:11,858 --> 00:38:15,024
repository. And you can see. See, there are a lot of dags here.

609
00:38:15,404 --> 00:38:19,292
Not only are there dags tagged with use case, those are the dags that

610
00:38:19,308 --> 00:38:22,916
are actually pertaining to the use case that I'm going to show.

611
00:38:23,060 --> 00:38:26,108
There are also a lot of dags tagged with simple dags.

612
00:38:26,236 --> 00:38:29,964
And you can see by the title here that these are about the features

613
00:38:30,004 --> 00:38:33,428
that I talked about in the slides. So I

614
00:38:33,436 --> 00:38:37,344
was thinking you might want to have more code examples for these features.

615
00:38:38,004 --> 00:38:42,228
So with these dags, you can check out more simpler

616
00:38:42,276 --> 00:38:44,544
code examples about a specific feature.

617
00:38:45,084 --> 00:38:48,740
But in our case, we want to look at the use case. So let's

618
00:38:48,772 --> 00:38:52,044
only filter for use case and zoom out a little.

619
00:38:52,204 --> 00:38:55,788
So these are all the dags that are about the use case

620
00:38:55,836 --> 00:39:00,104
that we're going to talk about. You can see it already ran a few times,

621
00:39:00,404 --> 00:39:04,300
and here we have two dags that are currently paused.

622
00:39:04,492 --> 00:39:08,218
They are tagged with helper. Those are helper tags that help you

623
00:39:08,266 --> 00:39:11,306
when you're developing. So we have one dag that can

624
00:39:11,370 --> 00:39:14,914
start the whole fine tuning pipeline. And we have one dag that

625
00:39:14,954 --> 00:39:18,842
deletes everything that's currently in vv eight. Because when

626
00:39:18,858 --> 00:39:22,306
you're developing, you might want to iterate over the content that

627
00:39:22,330 --> 00:39:25,906
you put into VB eight. So we can ignore these two

628
00:39:25,930 --> 00:39:29,074
dags for now and let's just focus on the other ones.

629
00:39:29,234 --> 00:39:32,650
And the first stack that I want to talk about is the

630
00:39:32,682 --> 00:39:36,276
one for the full first part of the pipeline. So this blue

631
00:39:36,340 --> 00:39:40,084
part of the pipeline, the rig pipeline, is all happening

632
00:39:40,124 --> 00:39:43,340
in one dag. So let's click on this dag.

633
00:39:43,492 --> 00:39:46,304
It's called the ingest knowledge base stack.

634
00:39:47,404 --> 00:39:50,924
And when we zoom into the graph view, you can see the stack is

635
00:39:50,964 --> 00:39:54,772
scheduled based on both time. So we have

636
00:39:54,788 --> 00:39:58,688
a data set or time schedule, one of the new airflow to nine features,

637
00:39:58,876 --> 00:40:02,164
or on either of these data sets receiving an update.

638
00:40:02,504 --> 00:40:05,640
Then what happens next? And we can actually see some of the

639
00:40:05,672 --> 00:40:09,044
scheduled runs. This is one of the runs that happened recently.

640
00:40:09,864 --> 00:40:13,264
And you can see I make sure that my

641
00:40:13,304 --> 00:40:16,792
class exists in the VV eight database, so I interact with

642
00:40:16,808 --> 00:40:19,844
viviate. I have a branching task here.

643
00:40:20,264 --> 00:40:23,392
If the task already exists, then I do nothing,

644
00:40:23,448 --> 00:40:26,920
just an empty operator if it doesn't exist. If it's the

645
00:40:26,952 --> 00:40:30,284
first run of the DaG, I actually will create the class.

646
00:40:30,984 --> 00:40:34,480
Then we start with ingesting the document sources.

647
00:40:34,672 --> 00:40:39,000
And here you can see that I have two arms that are being created

648
00:40:39,112 --> 00:40:42,904
to ingest from different sources. This is just

649
00:40:42,984 --> 00:40:46,416
one way that you can structure an IG Dag. And this is actually

650
00:40:46,480 --> 00:40:49,872
structured in a very modular, best practice way that you can write

651
00:40:49,928 --> 00:40:53,468
airflow data so that you can very easily

652
00:40:53,556 --> 00:40:57,676
add another one of these branches here if

653
00:40:57,700 --> 00:41:01,460
you had a new ingesting source. So if you had a different

654
00:41:01,532 --> 00:41:04,904
type of file that you want to ingest, maybe CSV files.

655
00:41:06,244 --> 00:41:09,476
And what happens with each of the sources is

656
00:41:09,580 --> 00:41:13,588
that we extract the information from the source. I mentioned that we use

657
00:41:13,636 --> 00:41:16,858
airflow guides, so extract from guides. We extract

658
00:41:16,906 --> 00:41:20,626
from text files. We use Lang chain to split up the

659
00:41:20,650 --> 00:41:23,858
guides and text files and then they are being

660
00:41:23,906 --> 00:41:27,818
prepared for the ingestion. So they are being put into the exact

661
00:41:27,906 --> 00:41:31,570
right format that we need to ingest into viviate.

662
00:41:31,722 --> 00:41:35,970
And this all happens with the task flow API. You can see add task here.

663
00:41:36,162 --> 00:41:39,562
But for the last task I'm using an operator because

664
00:41:39,618 --> 00:41:43,526
the viviate ingest operator makes it a lot easier to interact with

665
00:41:43,550 --> 00:41:47,078
VB eight. And I don't need to think about the call to the VB eight

666
00:41:47,126 --> 00:41:50,886
API at all. In this case I just provide the data and

667
00:41:50,910 --> 00:41:54,354
then it will be ingested into the local VV eight instance.

668
00:41:56,134 --> 00:42:00,230
This is the general dag. Let's look at the code that was used

669
00:42:00,382 --> 00:42:03,462
to create this dag. So let's hop over. This is the

670
00:42:03,478 --> 00:42:07,206
versus code environment where I'm running this airflow

671
00:42:07,270 --> 00:42:10,804
project. Of course you can use any editor that you like.

672
00:42:11,664 --> 00:42:15,416
And if we look at the dags here, ingest knowledge base,

673
00:42:15,560 --> 00:42:19,648
then we can see all of the code that went into creating

674
00:42:19,696 --> 00:42:23,504
the dag that I've just shown. And you can see here

675
00:42:23,544 --> 00:42:27,512
we have some description. We have all of our imports. This is

676
00:42:27,568 --> 00:42:31,008
where we import functions that are being modularized.

677
00:42:31,136 --> 00:42:34,724
So one way that you can make your airflow easier to use and to

678
00:42:34,764 --> 00:42:38,124
reuse certain parts of your code is by

679
00:42:38,164 --> 00:42:42,348
modularizing functions. So extract, split and prep for ingest

680
00:42:42,476 --> 00:42:46,484
are functions that I've modularized here. I'm importing

681
00:42:46,564 --> 00:42:49,812
some environment variables and then

682
00:42:49,868 --> 00:42:53,212
I define my different document sources. Again, this is just the

683
00:42:53,228 --> 00:42:56,772
way that the Dag has been optimized. You could also

684
00:42:56,908 --> 00:43:00,304
just have the functions directly in your dag code and

685
00:43:00,344 --> 00:43:03,792
only use one source here. In this case

686
00:43:03,848 --> 00:43:07,192
I'm saying, well, I have a source that's called guides.

687
00:43:07,368 --> 00:43:10,896
And when we are using the guides, I want to use this

688
00:43:10,960 --> 00:43:14,680
specific extract function. So this is the extract function that

689
00:43:14,712 --> 00:43:17,936
can extract from markdown files and I

690
00:43:17,960 --> 00:43:21,712
want to use this specific folder path. The other

691
00:43:21,768 --> 00:43:24,944
source is I have text files and then I want to use

692
00:43:24,984 --> 00:43:28,396
a different extraction function because, because I have to handle text a

693
00:43:28,420 --> 00:43:32,324
little bit differently than markdown and I will use

694
00:43:32,444 --> 00:43:34,384
this other folder path.

695
00:43:37,484 --> 00:43:40,972
Now next we define the actual dag. And here you

696
00:43:40,988 --> 00:43:44,572
can see several features that I mentioned in the slides earlier.

697
00:43:44,748 --> 00:43:48,220
So I said that we are using a data set

698
00:43:48,252 --> 00:43:51,756
or time schedule. So I want this dag to run every day at midnight

699
00:43:51,820 --> 00:43:55,736
to update, but also if I send a specific update

700
00:43:55,840 --> 00:43:59,568
to one of the datasets. So what I did here is I'm using

701
00:43:59,616 --> 00:44:02,656
the data set or time schedule. You can scroll up,

702
00:44:02,680 --> 00:44:06,600
you can see this is being imported from airflow timetables,

703
00:44:06,672 --> 00:44:10,448
datasets and to the schedule

704
00:44:10,536 --> 00:44:14,240
I have to provide two arguments. I provide a timetable.

705
00:44:14,392 --> 00:44:17,496
In this case I'm using the cron trigger timetable and this

706
00:44:17,520 --> 00:44:20,872
just says every day as midnight. And I

707
00:44:20,888 --> 00:44:24,760
have to provide some data sets. I here modularize

708
00:44:24,832 --> 00:44:27,984
this so it will actually use a list of

709
00:44:28,024 --> 00:44:31,472
data sets and then it will create a list

710
00:44:31,528 --> 00:44:35,304
that says, well use update on any of these

711
00:44:35,344 --> 00:44:38,744
data sets. But you could also just put any data

712
00:44:38,784 --> 00:44:41,992
set expression here. So any logical data set expression

713
00:44:42,048 --> 00:44:45,664
directly if you don't want to make it dynamic like this.

714
00:44:47,604 --> 00:44:51,796
Alright, so now this is running both every day at midnight

715
00:44:51,900 --> 00:44:55,812
and whenever any of the datasets that have a URI

716
00:44:55,948 --> 00:44:58,824
in this environment variable get updated.

717
00:45:00,044 --> 00:45:03,644
The next feature that I covered in the slides is

718
00:45:03,684 --> 00:45:07,628
retries. In this case, I'm setting my retries at the DAG level.

719
00:45:07,676 --> 00:45:10,784
So I'm saying all of the tasks in this dagger should

720
00:45:10,824 --> 00:45:15,144
have at least three retries, should have free retries, and I

721
00:45:15,224 --> 00:45:18,764
want to wait five minutes in between retrying my tasks.

722
00:45:19,384 --> 00:45:23,176
This is the default for all the tasks in the dag, but you can override

723
00:45:23,200 --> 00:45:26,464
this at the task level. Two more things

724
00:45:26,504 --> 00:45:29,764
I wanted to mention, because maybe some of you are thinking,

725
00:45:31,704 --> 00:45:35,160
are thinking, wait a second, how do you have emojis in your dag

726
00:45:35,192 --> 00:45:38,900
names? That's possible. Also, as of Airflow 29,

727
00:45:39,012 --> 00:45:42,996
you can have a dag display name. So the Dag id is still

728
00:45:43,060 --> 00:45:46,164
ingest knowledge base, but you can have now a dag

729
00:45:46,204 --> 00:45:49,584
display name that includes emojis or special characters.

730
00:45:50,724 --> 00:45:54,180
And the other thing which is still considered experimental.

731
00:45:54,252 --> 00:45:58,700
But I think a great best practice tool going forward is,

732
00:45:58,852 --> 00:46:02,192
I say if the stack fails five times in

733
00:46:02,208 --> 00:46:05,736
a row, I want it to stop. If I go on

734
00:46:05,760 --> 00:46:09,304
vacation and this dag runs every day at midnight and it has failed for

735
00:46:09,344 --> 00:46:12,736
five days, then there's probably something wrong that I need to

736
00:46:12,760 --> 00:46:16,560
fix. So I don't want it to incur more costs by running

737
00:46:16,592 --> 00:46:20,656
more times. And you can very easily set that now by using Max

738
00:46:20,720 --> 00:46:23,864
consecutive failed Dag runs. I set that to five

739
00:46:23,944 --> 00:46:26,968
here and in the UI, you can also,

740
00:46:27,016 --> 00:46:30,402
if you hover here, you can see this dagger auto pauses itself

741
00:46:30,578 --> 00:46:34,562
after it failed for five times. Very neat, very small

742
00:46:34,618 --> 00:46:38,026
feature that is new. And that I think gives

743
00:46:38,050 --> 00:46:41,602
you a great way to make your dags stop if they

744
00:46:41,618 --> 00:46:45,134
fail too many times. All right,

745
00:46:45,434 --> 00:46:48,962
we have our branching. So this is in essence

746
00:46:49,018 --> 00:46:53,482
the same as I've showed in the slides. We have the task branch decorator,

747
00:46:53,618 --> 00:46:56,614
and then we return a string based.

748
00:46:57,074 --> 00:47:01,122
This is modularized. I put the string into the task call, but we return

749
00:47:01,178 --> 00:47:04,186
the string of the downstream tasks that we want to run.

750
00:47:04,370 --> 00:47:08,002
If there is current, if our current, the class that we want to have in

751
00:47:08,018 --> 00:47:11,266
our VV eight database doesn't exist yet, then we use the VV

752
00:47:11,290 --> 00:47:14,714
eight hook from the provider in order to interact

753
00:47:14,754 --> 00:47:16,974
with viviate and create that class.

754
00:47:18,714 --> 00:47:22,486
And then lastly, we want to ingest our sources. So we

755
00:47:22,510 --> 00:47:25,886
have a loop to create one of the branches for each of

756
00:47:25,910 --> 00:47:29,334
the sources. And I can say the first task,

757
00:47:29,374 --> 00:47:32,918
which is the extraction task, this one should use

758
00:47:33,046 --> 00:47:36,790
my extraction function. So what this creates

759
00:47:36,862 --> 00:47:40,238
is it creates both of these tasks here,

760
00:47:40,406 --> 00:47:43,198
one in the first run of the loop and the other one in the next

761
00:47:43,246 --> 00:47:46,486
run. Again, very modularized. You could also

762
00:47:46,550 --> 00:47:50,644
have your regular at task decorated tasks defined in here.

763
00:47:51,624 --> 00:47:54,880
Then I'm splitting up my text. I use the

764
00:47:54,912 --> 00:47:58,544
splittext function, which is also modularized. Maybe in this case

765
00:47:58,584 --> 00:48:02,400
I can show you what the function looks like. If we go here,

766
00:48:02,512 --> 00:48:05,976
split, you can see this is all of the code that

767
00:48:06,040 --> 00:48:10,344
is used to chunk the text into

768
00:48:10,424 --> 00:48:14,020
sizable elements that can be used and ingested into

769
00:48:14,072 --> 00:48:17,364
viviate. And because I didn't want all of this code

770
00:48:17,404 --> 00:48:20,660
to be in my dag, I modularized it out and just

771
00:48:20,732 --> 00:48:24,340
imported it because at the end airflow is all just python code.

772
00:48:24,452 --> 00:48:27,824
So you can do things like that. And the texts

773
00:48:28,244 --> 00:48:31,716
are the ones that were extracted by the first task and I can

774
00:48:31,740 --> 00:48:34,224
just input it into my second task.

775
00:48:35,524 --> 00:48:38,284
Next I'm preparing the embedding for import.

776
00:48:38,444 --> 00:48:42,038
And this is a task that, that is dynamically mapped. So you can see

777
00:48:42,086 --> 00:48:45,598
here, last time I had 14 chunks

778
00:48:45,726 --> 00:48:49,350
of airflow guides. And if I click here you can

779
00:48:49,382 --> 00:48:52,486
actually see the map index. So I say which

780
00:48:52,510 --> 00:48:56,434
is the guide that I'm ingesting? I'm ingesting the one about dynamic tasks

781
00:48:56,854 --> 00:49:00,794
very fittingly. And which chunk am I ingesting?

782
00:49:01,854 --> 00:49:05,350
And this is what happens here. I have this task

783
00:49:05,382 --> 00:49:09,382
flow decorated task that I call with the function that

784
00:49:09,398 --> 00:49:13,054
is modularized. And then I say, well, the VV eight class is always

785
00:49:13,094 --> 00:49:16,598
the same, so that's my default, that's goes into partial,

786
00:49:16,726 --> 00:49:20,702
but I want to create one dynamically mapped task instance per

787
00:49:20,758 --> 00:49:24,254
chunk, so per element that is being returned from

788
00:49:24,294 --> 00:49:27,822
this upstream task. And lastly

789
00:49:27,878 --> 00:49:31,438
we want to ingest this information into viviate. And in this case

790
00:49:31,486 --> 00:49:34,634
I'm using, using a traditional operator, the VV eight

791
00:49:34,674 --> 00:49:38,698
ingest operator. And I'm also dynamically mapping this operator.

792
00:49:38,826 --> 00:49:42,934
So I'm saying I want to have one ingestion

793
00:49:44,834 --> 00:49:48,394
click from here. So we also have 14 dynamically mapped

794
00:49:48,434 --> 00:49:51,770
task instances here. I say I want to have one

795
00:49:51,842 --> 00:49:55,298
instance of this VB eight ingest operator for

796
00:49:55,386 --> 00:49:59,526
each of the chunks that I'm giving it. And here

797
00:49:59,550 --> 00:50:01,754
you can see they are named in the same way.

798
00:50:02,374 --> 00:50:06,406
And this is done by using the partial on most of

799
00:50:06,430 --> 00:50:10,006
the arguments. Like most of the arguments stay the same. I always use

800
00:50:10,030 --> 00:50:13,534
the same connection to bv eight. I use the same trigger

801
00:50:13,574 --> 00:50:16,982
rule, but I want to have one dynamically mapped task

802
00:50:17,038 --> 00:50:20,854
instance per element in the list that was outputted

803
00:50:20,894 --> 00:50:24,384
by the upstream task. So per embed option object.

804
00:50:24,544 --> 00:50:28,448
And this has a length of 14, had that in the last run.

805
00:50:28,536 --> 00:50:32,964
So that's why I get 14 dynamically mapped task instances here,

806
00:50:34,024 --> 00:50:37,448
the map index template, if you're using a traditional

807
00:50:37,496 --> 00:50:41,120
operator can be set with direct jinja templates.

808
00:50:41,312 --> 00:50:44,776
You can see here, I'm saying I'm using of this task,

809
00:50:44,880 --> 00:50:49,208
the input data parameter and then I'm indexing

810
00:50:49,256 --> 00:50:52,780
into that and I'm using the URI that is being returned

811
00:50:52,852 --> 00:50:56,196
from this upstream task. And that's what gives me

812
00:50:56,220 --> 00:50:59,500
the Uri of this or the path in this

813
00:50:59,532 --> 00:51:01,424
case of this guide.

814
00:51:03,564 --> 00:51:07,504
All right, lastly, I set some dependencies. I chained this all together

815
00:51:08,164 --> 00:51:11,284
down here as well. And that gives me this highly

816
00:51:11,324 --> 00:51:15,236
adaptable, highly dynamic rag dag. So this is already

817
00:51:15,340 --> 00:51:19,438
all that is necessary to do this first part of the demo.

818
00:51:19,526 --> 00:51:23,030
So to get the up to date knowledge and get it into my

819
00:51:23,062 --> 00:51:26,942
VBA database now, next I

820
00:51:26,958 --> 00:51:30,222
want to actually fine tune a model. And all of

821
00:51:30,238 --> 00:51:34,646
the other dags are about the fine tuning. And since I modularized

822
00:51:34,750 --> 00:51:38,150
this pipeline into different dags, but I want them to always

823
00:51:38,222 --> 00:51:41,398
happen in the same order, I always need the examples first.

824
00:51:41,566 --> 00:51:44,600
Then I check the examples. Alright, then I fine tune.

825
00:51:44,712 --> 00:51:47,764
Then I have champion versus challenger competition.

826
00:51:48,824 --> 00:51:52,824
Extremely great case for using datasets. So let's

827
00:51:52,864 --> 00:51:58,480
look at the datasets view of this project. So these

828
00:51:58,512 --> 00:52:01,856
up here are just data sets from the simpler DAX,

829
00:52:02,000 --> 00:52:05,000
but these are the data sets from the actual use case.

830
00:52:05,192 --> 00:52:08,816
And what you can see here is that we have all

831
00:52:08,840 --> 00:52:12,462
of these local file uris that I've scheduled

832
00:52:12,558 --> 00:52:16,222
these dags on. Of course, in a production case you're usually not using

833
00:52:16,278 --> 00:52:19,750
local files and this is likely to refer to

834
00:52:19,782 --> 00:52:21,554
files in an object storage.

835
00:52:22,734 --> 00:52:26,254
And what you can see here, for example, on this site

836
00:52:26,414 --> 00:52:31,022
is we have some training files and as soon as any

837
00:52:31,038 --> 00:52:34,574
of these training files are updated, our ingest train

838
00:52:34,654 --> 00:52:38,168
examples starts. Because if the training examples

839
00:52:38,216 --> 00:52:41,640
are new, I say okay, this means I want to kick off a new

840
00:52:41,672 --> 00:52:45,176
fine tuning run. This ends

841
00:52:45,240 --> 00:52:48,656
with the formatted examples. So I'm getting the examples,

842
00:52:48,760 --> 00:52:52,124
putting them into the right format, I have a new data set,

843
00:52:52,504 --> 00:52:55,976
then I evaluate the cost and also the

844
00:52:56,000 --> 00:52:58,644
format of the examples with the next tag.

845
00:52:59,344 --> 00:53:02,968
This produces a new file with the evaluated

846
00:53:03,016 --> 00:53:06,138
format and then I can fine tune my DaX.

847
00:53:06,186 --> 00:53:09,754
So everything is happening in succession based on dataset

848
00:53:09,794 --> 00:53:13,554
scheduling. Once the fine tuning is done, I have a new

849
00:53:13,594 --> 00:53:16,682
results file that gives me an accuracy.

850
00:53:16,858 --> 00:53:19,894
And this kicks off the champion challenger dag.

851
00:53:21,434 --> 00:53:24,890
Okay, so there are different ways to kick this off when you are

852
00:53:24,922 --> 00:53:26,774
using this repository locally,

853
00:53:27,674 --> 00:53:31,660
and one is to just run this dag. That's just for convenience.

854
00:53:31,812 --> 00:53:34,984
But there was one more new data set feature that I wanted to show.

855
00:53:35,404 --> 00:53:38,544
So let's kick this off using the API.

856
00:53:39,124 --> 00:53:42,464
So if I select this here,

857
00:53:42,844 --> 00:53:46,068
this is the script. It's a very, very basic API script.

858
00:53:46,116 --> 00:53:49,708
You could run this from anywhere. This could be run

859
00:53:49,876 --> 00:53:53,820
from a web application, this could be run from a curl

860
00:53:53,852 --> 00:53:57,538
command. But in my case just want to run Python

861
00:53:57,586 --> 00:54:01,626
three API scripts and

862
00:54:01,690 --> 00:54:04,810
then start fine tuning pipeline. So let's run

863
00:54:04,882 --> 00:54:08,314
this example. And you can see I print

864
00:54:08,354 --> 00:54:11,738
out the response here and it says it updated this

865
00:54:11,786 --> 00:54:16,014
one data set, and this is one of the data sets where

866
00:54:16,474 --> 00:54:19,978
my ingest train example stack is being scheduled on.

867
00:54:20,066 --> 00:54:23,278
So this stack just started because I made this call to

868
00:54:23,286 --> 00:54:27,514
the airflow API, and this will kick off this whole pipeline.

869
00:54:28,174 --> 00:54:31,918
The stack itself is very simple. It just

870
00:54:31,966 --> 00:54:36,094
gets the example folders, it creates the JSON L that

871
00:54:36,214 --> 00:54:39,974
GPT needs for fine tuning, and then it makes

872
00:54:40,014 --> 00:54:43,422
sure that we update the next dataset. We say that

873
00:54:43,478 --> 00:54:47,246
now this JSON L file exists that has just finished,

874
00:54:47,430 --> 00:54:51,140
and once this has finished, the evaluate tag is very.

875
00:54:51,262 --> 00:54:55,104
So we didn't see this running, but the evaluation dag,

876
00:54:55,144 --> 00:54:58,704
this is the dag that just ran, you can

877
00:54:58,744 --> 00:55:02,000
see is very structured Dag that uses

878
00:55:02,032 --> 00:55:05,392
a lot of branching. So this is an example that you can look at

879
00:55:05,408 --> 00:55:08,968
in your own time, where the branch will have

880
00:55:09,056 --> 00:55:13,072
four possible outcomes and decides which parts are running.

881
00:55:13,168 --> 00:55:16,756
Because if we have no validation examples or no training examples,

882
00:55:16,840 --> 00:55:20,116
for whatever reason, we do not need to kick off the fine tuning.

883
00:55:20,180 --> 00:55:24,264
It won't work. So we stop the pipeline here in its tracks.

884
00:55:24,764 --> 00:55:28,420
But if we have both, we want to evaluate the formatting

885
00:55:28,452 --> 00:55:31,892
of both. So both the validation examples and the training

886
00:55:31,948 --> 00:55:36,004
examples get evaluated. And for the training examples,

887
00:55:36,164 --> 00:55:39,484
I actually want to know. I want to count the tokens

888
00:55:39,524 --> 00:55:43,520
and calculate the expected cost here. Decide if

889
00:55:43,552 --> 00:55:47,344
that's within budget and only if it's within budget.

890
00:55:47,504 --> 00:55:51,080
I will actually continue and this will later kick off the fine

891
00:55:51,112 --> 00:55:55,112
tuning task. If this is over budget, I currently put the budget, I think at

892
00:55:55,168 --> 00:55:58,776
$20. Then the DAC will stop as

893
00:55:58,800 --> 00:56:02,672
well. So this is one way that you can use branching

894
00:56:02,808 --> 00:56:06,724
to protect yourself from accidentally incurring a lot of cost.

895
00:56:09,164 --> 00:56:13,156
All right, so the

896
00:56:13,180 --> 00:56:16,300
fine tuning DAC is currently running. So let's look at

897
00:56:16,332 --> 00:56:20,340
this. And you can see here the

898
00:56:20,372 --> 00:56:23,964
fine tuning is also scheduled on some datasets.

899
00:56:24,124 --> 00:56:27,580
And we make sure we have the right validations file and

900
00:56:27,612 --> 00:56:30,988
examples file. We upload those files to OpenAI

901
00:56:31,076 --> 00:56:34,036
because I'm using OpenAI to fine tune.

902
00:56:34,220 --> 00:56:38,028
And then we have our fine tuning task. And this

903
00:56:38,076 --> 00:56:41,964
uses a deferrable operator. You can see currently this task is deferred

904
00:56:42,084 --> 00:56:44,904
because it's waiting for the fine tuning to finish.

905
00:56:46,004 --> 00:56:49,668
If we go into the details of this stack and the details of this

906
00:56:49,716 --> 00:56:53,284
task here, this uses the deferrable finetune

907
00:56:53,364 --> 00:56:56,940
openair operator. We actually have an extra link here.

908
00:56:57,092 --> 00:57:01,594
So if I click on this link, it takes me directly to my finetune.

909
00:57:01,764 --> 00:57:05,182
So this just started. It's not very smart yet,

910
00:57:05,318 --> 00:57:08,798
but this is starting my fine tuning and I

911
00:57:08,806 --> 00:57:12,342
can directly look at the job and I could copy the ID or

912
00:57:12,358 --> 00:57:16,246
I could also cancel it from here. But this

913
00:57:16,270 --> 00:57:19,154
will take a while to fine tune a new model.

914
00:57:19,494 --> 00:57:23,790
But yeah, let's look at the operator in the meantime,

915
00:57:23,862 --> 00:57:27,868
because this is a deferrable operator, which actually currently doesn't exist

916
00:57:27,916 --> 00:57:31,684
yet in the provider. So let's hop over into

917
00:57:31,724 --> 00:57:35,804
the code again, and this

918
00:57:35,844 --> 00:57:38,972
is our fine tuning dag. So let's

919
00:57:39,028 --> 00:57:43,104
clear this and

920
00:57:43,724 --> 00:57:47,356
look at the fine tuning dag. So again we have imports,

921
00:57:47,420 --> 00:57:50,884
we get some environment variables, we get this operator,

922
00:57:51,004 --> 00:57:54,354
and because it's a custom operator, we can get it from a local file

923
00:57:54,394 --> 00:57:57,714
in the include folder. And this fine tune

924
00:57:57,754 --> 00:58:01,490
operator is then used in this dag down here.

925
00:58:01,682 --> 00:58:04,850
So here you can see all of the code that I need

926
00:58:04,882 --> 00:58:08,186
to write in order to fine tune an OpenAI model. If I

927
00:58:08,210 --> 00:58:09,974
use this fine tune operator,

928
00:58:11,274 --> 00:58:14,434
I give it a task id, I give it my fine tuning

929
00:58:14,474 --> 00:58:17,778
file, give it a validation file, I tell it which

930
00:58:17,826 --> 00:58:21,248
model to use, I create a suffix for it.

931
00:58:21,336 --> 00:58:24,408
I'm using a parameter here, but you could also hard code this.

932
00:58:24,576 --> 00:58:28,320
And then I have two options to

933
00:58:28,392 --> 00:58:31,368
make this operator more efficient.

934
00:58:31,536 --> 00:58:34,604
So the first option is wait for completion.

935
00:58:34,944 --> 00:58:38,480
By default, this is false and the operator will just kick

936
00:58:38,512 --> 00:58:41,800
off the fine tuning job and then be successful

937
00:58:41,912 --> 00:58:45,120
and the pipeline continues. But in a lot of

938
00:58:45,152 --> 00:58:48,914
cases you only want to continue your pipeline if you know

939
00:58:48,954 --> 00:58:52,322
the fine tuning has been successful and has happened. So that's when

940
00:58:52,338 --> 00:58:56,298
you turn wait for completion to true, and then the operator

941
00:58:56,386 --> 00:59:00,122
will wait. In that case it still uses up a

942
00:59:00,138 --> 00:59:03,674
worker slot. So if you're saying, well, I have a trigger or component,

943
00:59:03,794 --> 00:59:07,082
I want you to actually release the worker and be deferable.

944
00:59:07,218 --> 00:59:11,378
Then you also set the parameter deferable to true. This is a pattern

945
00:59:11,426 --> 00:59:14,542
that is also, for example, used in the trigger diagram operator

946
00:59:14,598 --> 00:59:17,074
with the exact same two parameters.

947
00:59:18,654 --> 00:59:22,134
We can have a quick look at the code of this

948
00:59:22,174 --> 00:59:23,394
custom operator.

949
00:59:26,854 --> 00:59:30,902
Let's meet here. Custom operators GPT fine tune you

950
00:59:30,918 --> 00:59:34,954
can use this operator as a template for your own custom operators,

951
00:59:35,254 --> 00:59:38,118
especially if you want to fine tune a model, or if you want to tune

952
00:59:38,126 --> 00:59:41,918
a model for service where there is no provider yet.

953
00:59:42,046 --> 00:59:45,470
You're very welcome to use this. You can

954
00:59:45,502 --> 00:59:48,822
see that's the link that I defined. So that's how to

955
00:59:48,838 --> 00:59:52,630
define an operator extra link. And this is the trigger.

956
00:59:52,782 --> 00:59:56,150
So this is the process that's actually being handed to

957
00:59:56,182 --> 00:59:59,310
the trigger, a component to do the async call.

958
00:59:59,502 --> 01:00:03,030
And this is where the asynchronous waiting is happening.

959
01:00:03,182 --> 01:00:07,354
And this is using a function to wait for

960
01:00:07,434 --> 01:00:11,654
the OpenAI API to return that the fine tuning

961
01:00:12,034 --> 01:00:15,894
status is succeeded or failed or canceled.

962
01:00:16,274 --> 01:00:20,418
So this is what's happening currently in the trigger component running

963
01:00:20,466 --> 01:00:24,214
asynchronously. Once this has completed,

964
01:00:24,554 --> 01:00:28,234
the operator will actually complete and then it will give me

965
01:00:28,274 --> 01:00:31,432
all of the information that I want to have about the fine tune.

966
01:00:31,578 --> 01:00:35,276
So if we scroll down in the operator code, you can see down

967
01:00:35,340 --> 01:00:39,260
here is all of the information that it will log to the logs

968
01:00:39,412 --> 01:00:42,604
about which model. I fine tuned the model id, how many

969
01:00:42,644 --> 01:00:46,144
training tokens, all those things. That's the fine tuning duration.

970
01:00:46,564 --> 01:00:49,876
And then it will also push the model name

971
01:00:49,980 --> 01:00:53,424
to XCom because I can use that in a downstream dag.

972
01:00:54,884 --> 01:00:57,864
I know this was quick, just scrolling through the code.

973
01:00:58,724 --> 01:01:02,316
As I said, feel free to use it. Feel free to look at it in

974
01:01:02,340 --> 01:01:06,244
more detail. And it's just one of the ways that you can

975
01:01:06,364 --> 01:01:09,464
standardize how you interact with generative AI.

976
01:01:11,284 --> 01:01:15,476
Okay, it has finished, so we can look at the logs.

977
01:01:15,620 --> 01:01:19,140
And here you can see the output of all of these print statements.

978
01:01:19,252 --> 01:01:22,348
And you can see this is my fine tuned model.

979
01:01:22,476 --> 01:01:26,530
And it took about five minutes in this case, and I have a result file.

980
01:01:26,722 --> 01:01:31,106
So the next stack that then automatically runs and already finished

981
01:01:31,170 --> 01:01:34,434
because it's also a very fast stack, is the dag

982
01:01:34,474 --> 01:01:38,034
that actually pulls this results file and then

983
01:01:38,074 --> 01:01:41,682
makes sure it compares the champion and the challenger.

984
01:01:41,818 --> 01:01:45,322
And in this case, you can see I have a champion and I

985
01:01:45,338 --> 01:01:49,450
have a challenger. I've ran this before. So it ran the middle part and

986
01:01:49,482 --> 01:01:53,064
did all of the comparison. And when it compared the

987
01:01:53,104 --> 01:01:56,720
accuracies, the new model was better. So it decided

988
01:01:56,792 --> 01:01:58,164
to switch to champion.

989
01:01:59,904 --> 01:02:03,376
And that's all that is happening on the airflow side. And if you're

990
01:02:03,400 --> 01:02:06,912
running this in production, this is just running without you needing to

991
01:02:06,928 --> 01:02:10,368
do anything. You can run that on any schedule or on

992
01:02:10,496 --> 01:02:13,696
specific triggers like with the data set, and you don't have to

993
01:02:13,720 --> 01:02:17,484
worry about any individual tasks that are happening.

994
01:02:18,944 --> 01:02:23,024
Let's see the results. So this is the

995
01:02:23,184 --> 01:02:26,464
frontend app, this is the streamlit app. And let's say,

996
01:02:26,504 --> 01:02:27,084
actually,

997
01:02:31,504 --> 01:02:35,104
let's say I want a LinkedIn post about time and dataset

998
01:02:35,144 --> 01:02:38,160
schedules. All of my examples are very short,

999
01:02:38,232 --> 01:02:42,422
so I'm expecting a very short post also to

1000
01:02:42,438 --> 01:02:45,758
be able to show this better. So it's thinking for a little bit.

1001
01:02:45,846 --> 01:02:48,926
This makes a call to my fine tuned model. So this call

1002
01:02:48,990 --> 01:02:52,766
did not go to regular chat GPT or to the

1003
01:02:52,790 --> 01:02:56,006
regular GPT model, but it went to this fine tuned

1004
01:02:56,070 --> 01:03:00,510
iteration, the last one. And yeah,

1005
01:03:00,662 --> 01:03:04,310
you can see. Did you know you combine data set based schedule with time based

1006
01:03:04,342 --> 01:03:07,734
scheduling in Airflow 29? I did, but the

1007
01:03:07,774 --> 01:03:11,732
current GPT 3.5 model does not because this is very

1008
01:03:11,788 --> 01:03:15,740
new information. And that's exactly how you can be better than your

1009
01:03:15,772 --> 01:03:20,620
competition, by providing your own data to your

1010
01:03:20,732 --> 01:03:23,964
genai applications. It even gives me the exact

1011
01:03:24,004 --> 01:03:27,228
name of the class, which is cool. And it gives me a fun fact

1012
01:03:27,276 --> 01:03:30,892
about space. Also for fun we

1013
01:03:30,908 --> 01:03:34,890
can generate a picture. This takes a second. This takes the

1014
01:03:34,922 --> 01:03:37,694
post that was created and sends it to Dali.

1015
01:03:38,834 --> 01:03:42,514
Okay, while this is running, let's talk about

1016
01:03:42,554 --> 01:03:45,946
the steps that you need to take to run this repository

1017
01:03:46,090 --> 01:03:49,134
by yourself today. Because it's not many steps.

1018
01:03:50,154 --> 01:03:53,774
First you go to this GitHub repository.

1019
01:03:54,394 --> 01:03:56,946
If you want to be nice, you can star it, but what you need to

1020
01:03:56,970 --> 01:04:00,400
do is you fork it. Then you clone it locally

1021
01:04:00,482 --> 01:04:04,292
so you get the local address. Then you

1022
01:04:04,308 --> 01:04:07,676
need to make sure that you have the Astro CLI installed.

1023
01:04:07,820 --> 01:04:11,052
This is an open source project that astronomer provides.

1024
01:04:11,148 --> 01:04:14,620
That makes it very easy to run airflow in Docker. So even

1025
01:04:14,652 --> 01:04:17,860
if you've never run airflow on your computer, you can do this.

1026
01:04:17,892 --> 01:04:20,948
Today. There are install instructions

1027
01:04:21,036 --> 01:04:24,704
in our documentation. But if you are on a Mac,

1028
01:04:26,624 --> 01:04:29,904
close all of this. If you are on a Mac,

1029
01:04:30,064 --> 01:04:33,284
you can just run brew, install Astro,

1030
01:04:33,664 --> 01:04:35,924
and then you get the Astro Cli,

1031
01:04:36,664 --> 01:04:40,744
you clone the repository, you go to the root of it, and then

1032
01:04:40,784 --> 01:04:44,400
you need to make a few things. So one thing that I

1033
01:04:44,432 --> 01:04:47,808
have here that you will not have is the env file.

1034
01:04:47,936 --> 01:04:51,896
You just have the env example file. And what you need

1035
01:04:51,920 --> 01:04:55,214
to do is you need need to copy paste the whole content

1036
01:04:55,294 --> 01:04:58,654
of this file and create

1037
01:04:58,774 --> 01:05:02,726
your own env file and copy the content in

1038
01:05:02,750 --> 01:05:06,614
it. Because this is the file that airflow will use for all of the

1039
01:05:06,654 --> 01:05:10,034
environment variables to configure in the DAG.

1040
01:05:11,214 --> 01:05:15,238
What you need to do here, because we're using OpenAI, you need to put in

1041
01:05:15,286 --> 01:05:18,790
your OpenAI API key once here in the environment

1042
01:05:18,862 --> 01:05:23,092
variable and ones here in

1043
01:05:23,148 --> 01:05:26,396
the vv eight connection. So with this you'll

1044
01:05:26,420 --> 01:05:29,628
be able to able to use OpenAI both for

1045
01:05:29,756 --> 01:05:33,436
your call to create the inference. So to create the

1046
01:05:33,460 --> 01:05:37,196
post, that's the first one and also in

1047
01:05:37,260 --> 01:05:40,716
your vv eight connection in order to create the

1048
01:05:40,740 --> 01:05:43,504
embeddings. These are also created by using OpenAI.

1049
01:05:44,604 --> 01:05:48,448
That's the only tool you need. You do not need a vv eight account

1050
01:05:48,636 --> 01:05:52,644
because this project runs vv eight locally.

1051
01:05:53,024 --> 01:05:56,472
Here you can see the docker compose override file and this

1052
01:05:56,528 --> 01:05:59,800
creates a vv eight container as well as the streamlit

1053
01:05:59,832 --> 01:06:03,216
container. So you do not need to have streamlit vv eight installed

1054
01:06:03,240 --> 01:06:07,224
or anything. This will all happen automatically. Once you

1055
01:06:07,264 --> 01:06:10,528
copy pasted the contents here and created the env file,

1056
01:06:10,656 --> 01:06:14,512
you just run Astro def start and the

1057
01:06:14,528 --> 01:06:18,558
first time you run this it takes a couple of minutes. And then at localhost

1058
01:06:18,606 --> 01:06:22,326
8080 you will have your

1059
01:06:22,350 --> 01:06:25,766
airflow environment and at localhost

1060
01:06:25,870 --> 01:06:29,794
8501 you will have the finished app.

1061
01:06:30,134 --> 01:06:33,654
Also, that's our dataset and time schedule image. That's actually

1062
01:06:33,734 --> 01:06:36,674
very pretty, especially the second one.

1063
01:06:38,894 --> 01:06:42,438
Then you just run this first stack or run the API

1064
01:06:42,486 --> 01:06:45,900
script that I showed and all of the dags

1065
01:06:45,932 --> 01:06:49,740
will be running. You also need to unpause them of course, but as soon

1066
01:06:49,772 --> 01:06:53,308
as all of the dags have run, you can start creating your own

1067
01:06:53,436 --> 01:06:56,828
posts about airflow. If you want to adapt

1068
01:06:56,876 --> 01:06:59,948
this to your own data, to your own use case, for example

1069
01:06:59,996 --> 01:07:02,024
to write blog posts,

1070
01:07:03,044 --> 01:07:05,824
let's say blog posts about your favorite dog,

1071
01:07:06,284 --> 01:07:09,632
then you need to make a few more changes in

1072
01:07:09,648 --> 01:07:12,616
the include folder here, the knowledge base.

1073
01:07:12,760 --> 01:07:16,000
This is where all of the knowledge is. So what you want to do is

1074
01:07:16,072 --> 01:07:19,768
you switch out these markdown files and these text files with

1075
01:07:19,816 --> 01:07:23,336
your own information. Let's say if you really like golden

1076
01:07:23,360 --> 01:07:26,960
Retrievers and you run a dog about golden retrievers, you could put

1077
01:07:27,032 --> 01:07:31,344
all of your golden retriever extra information into these files here,

1078
01:07:31,464 --> 01:07:34,920
and that way the app will have access to this extra

1079
01:07:34,952 --> 01:07:38,706
information. You will also want to have different fine

1080
01:07:38,730 --> 01:07:42,586
tuning examples. So in the examples folder you will need

1081
01:07:42,610 --> 01:07:46,050
to have different training examples here. These folders

1082
01:07:46,082 --> 01:07:48,054
will not exist the first time you run this,

1083
01:07:49,234 --> 01:07:53,354
and then you add your training examples here and

1084
01:07:53,394 --> 01:07:55,774
you can also add validation examples.

1085
01:07:56,994 --> 01:08:00,530
So with the examples you change the tone

1086
01:08:00,602 --> 01:08:04,634
and the format of the response, and with the knowledge base

1087
01:08:04,714 --> 01:08:08,934
you change the information that your application has access to.

1088
01:08:09,994 --> 01:08:13,210
The last thing that you need to change if you adapt this to your own

1089
01:08:13,242 --> 01:08:17,690
use case is if you go into the streamlit folder, include streamlit.

1090
01:08:17,882 --> 01:08:21,474
This is the app code. So this is the code that creates the streamlit

1091
01:08:21,514 --> 01:08:25,634
app down here in the getresponse function

1092
01:08:25,754 --> 01:08:30,178
you can see the actual prompt so if I look

1093
01:08:30,226 --> 01:08:33,466
bigger, you can see here is

1094
01:08:33,490 --> 01:08:36,962
what I tell the large language model every

1095
01:08:37,018 --> 01:08:40,474
time additionally to the user provided query.

1096
01:08:40,634 --> 01:08:44,458
So I'm saying you are the social post generator Astra,

1097
01:08:44,506 --> 01:08:48,058
you might want to change this to. You are the blog

1098
01:08:48,106 --> 01:08:52,218
post generator and you don't want to have interesting fact

1099
01:08:52,306 --> 01:08:56,357
post about airflow, but interesting fact about golden retrievers.

1100
01:08:56,525 --> 01:09:00,437
Then it retrieves all of the articles, it gets the query.

1101
01:09:00,565 --> 01:09:04,005
And lastly, this was my addition to add a

1102
01:09:04,029 --> 01:09:07,433
space fact. You may want to change that to something else,

1103
01:09:08,813 --> 01:09:12,909
but that's all that's needed. Important side note,

1104
01:09:13,021 --> 01:09:16,749
you need to have at least something in

1105
01:09:16,781 --> 01:09:20,155
your VBA database in order to be able to run

1106
01:09:20,309 --> 01:09:24,284
the pipeline and this file

1107
01:09:25,464 --> 01:09:28,472
in order to need to run the pipeline. But you do not need to run

1108
01:09:28,488 --> 01:09:33,256
the fine tuning pipeline if you don't want to. If there's no

1109
01:09:33,320 --> 01:09:37,688
fine tuned model, it will just use GPT 3.5 turbo

1110
01:09:37,736 --> 01:09:41,084
by default. All right,

1111
01:09:41,704 --> 01:09:45,404
so that was everything that I had to show you today.

1112
01:09:47,524 --> 01:09:51,044
Again, in case you missed it. If you want to try out Astro,

1113
01:09:51,164 --> 01:09:55,380
if you want to take Dax like these and run them in the cloud automatically

1114
01:09:55,492 --> 01:09:58,860
and have that very easy, then you can use

1115
01:09:58,892 --> 01:10:02,596
this QR code or the link on this slide and you will

1116
01:10:02,620 --> 01:10:06,624
also get a free complimentary airflow fundamentals certification exam

1117
01:10:08,684 --> 01:10:12,142
with that. The take home message of this whole talk.

1118
01:10:12,268 --> 01:10:15,014
Other than use this code and play around with it,

1119
01:10:15,594 --> 01:10:19,066
it's really both your data and your orchestration

1120
01:10:19,130 --> 01:10:22,930
with airflow that sets you apart from your competitors

1121
01:10:22,962 --> 01:10:26,810
when creating any genai applications. Because anyone can

1122
01:10:26,842 --> 01:10:29,818
wrap around the OpenAI API,

1123
01:10:29,946 --> 01:10:33,602
but only you have your data. And now you are one of

1124
01:10:33,618 --> 01:10:37,402
the people who know how to best orchestrate the data pipelines and

1125
01:10:37,418 --> 01:10:41,502
the ML pipelines with Apache Airflow with that.

1126
01:10:41,598 --> 01:10:44,990
Thanks so much for watching. I hope this is helpful for

1127
01:10:45,022 --> 01:10:48,246
you. I hope you fork the repository and have fun with it,

1128
01:10:48,350 --> 01:10:51,942
create amazing things with it, and that you have a great rest of the

1129
01:10:51,958 --> 01:10:52,750
conference.

