1
00:00:24,650 --> 00:00:27,810
Hi, I'm Akshay, data engineer,

2
00:00:27,890 --> 00:00:32,834
currently working as an engineering manager to deliver the data migration solution AI

3
00:00:32,882 --> 00:00:36,946
related solutions, primarily around the topic modeling and the NLP

4
00:00:37,058 --> 00:00:40,546
related text based use cases. The topic for today's

5
00:00:40,578 --> 00:00:43,846
talk is best practices for a streaming data pipeline using

6
00:00:43,868 --> 00:00:47,094
an AWS. And in this particular talk, I'm just going to share

7
00:00:47,132 --> 00:00:50,650
some of my experiences while working on one of the large scale streaming

8
00:00:50,730 --> 00:00:54,126
data projects that we have worked on. So let's go into

9
00:00:54,148 --> 00:00:57,998
the use case and understand more in a detail that what

10
00:00:58,004 --> 00:01:01,406
we are trying to solve and we can just go further on that. So in

11
00:01:01,428 --> 00:01:05,406
our case we used to have an upstream side dynamodB

12
00:01:05,438 --> 00:01:09,250
based system where the event

13
00:01:09,320 --> 00:01:13,118
used to be updated via the APIs and the batch processing jobs

14
00:01:13,294 --> 00:01:16,966
those event used to get passed down to us via the

15
00:01:16,988 --> 00:01:20,614
DynamodB stream. Take those events, we used to

16
00:01:20,652 --> 00:01:24,370
enrich it and then based on the agreed schema,

17
00:01:24,450 --> 00:01:28,550
we used to pass those events to our downstream consumers.

18
00:01:28,890 --> 00:01:32,506
Our downstream consumer used to store that data into the

19
00:01:32,528 --> 00:01:35,914
elasticsearch based system and then they used to display that

20
00:01:35,952 --> 00:01:39,450
data on the web app to display the metric where

21
00:01:39,520 --> 00:01:43,070
the metric was reviewed by the respective user. The entire

22
00:01:43,140 --> 00:01:46,606
goal of the system was to ensure that once the

23
00:01:46,628 --> 00:01:50,026
data become available onto DynamoDB,

24
00:01:50,218 --> 00:01:54,234
we need to pass that data and make it available into the web

25
00:01:54,292 --> 00:01:56,820
app as soon as possible.

26
00:01:57,270 --> 00:02:00,974
Now, in order to implement this particular solution on the data enrichment

27
00:02:01,022 --> 00:02:05,122
layer, what we have did is that in order to listen to this

28
00:02:05,256 --> 00:02:08,898
DynamoDB events, we have set up a ECS cluster

29
00:02:09,074 --> 00:02:13,254
which used to read a data using a KCL application and

30
00:02:13,292 --> 00:02:17,510
then used to write the data onto the queuing system SQs.

31
00:02:17,850 --> 00:02:21,318
Once the data was available over there, we used to trigger

32
00:02:21,334 --> 00:02:24,890
a lambda that used to process those SQS messages,

33
00:02:25,630 --> 00:02:29,226
enrich that data as per the data contract agreed with

34
00:02:29,248 --> 00:02:33,210
the consumers. Once the data is processing

35
00:02:33,290 --> 00:02:36,926
that data, we used to write on the Kinesis stream again and

36
00:02:36,948 --> 00:02:40,506
then it used to go further for the downstream consumption purposes.

37
00:02:40,698 --> 00:02:44,430
In this particular diagram, I have not added a component

38
00:02:44,510 --> 00:02:48,158
which are related to the self service bi or analytics requirement

39
00:02:48,254 --> 00:02:51,762
or a monitoring or alert requirement just to keep the things

40
00:02:51,816 --> 00:02:55,620
simple and explain the problem in appropriate way.

41
00:02:56,070 --> 00:03:00,386
The next set of things, what we will be talking about would be primarily focused

42
00:03:00,418 --> 00:03:04,674
on this particular lambda and the kinesis layer to solve

43
00:03:04,722 --> 00:03:08,074
a particular problems and the scale that we have

44
00:03:08,112 --> 00:03:11,594
seen in this particular use case. Now let me

45
00:03:11,632 --> 00:03:15,418
just go down further and explain you the

46
00:03:15,504 --> 00:03:18,842
requirements or the challenges that we have while

47
00:03:18,896 --> 00:03:22,814
implementing this particular solution on the side of the requirements or

48
00:03:22,852 --> 00:03:26,414
the challenges on the dynamoDB site system we

49
00:03:26,452 --> 00:03:30,074
used to have 950,000,000 events either added

50
00:03:30,122 --> 00:03:33,406
or updated on a daily basis. So that

51
00:03:33,428 --> 00:03:36,274
is the amount of load that we used to get as a part of hours

52
00:03:36,312 --> 00:03:40,046
stream. And our responsibility was to accept

53
00:03:40,078 --> 00:03:43,666
that data, enrich that data as

54
00:03:43,688 --> 00:03:47,218
per the contract, and then pass it to the consumer in a

55
00:03:47,224 --> 00:03:50,886
reliable manner. Over here, the challenge was

56
00:03:50,988 --> 00:03:54,086
most in terms of a pattern in which we used to get a data,

57
00:03:54,268 --> 00:03:58,294
because on the dynamodB, the data used to get updated via the

58
00:03:58,332 --> 00:04:02,058
APIs or via the batch processing jobs. So whenever a

59
00:04:02,064 --> 00:04:05,706
batch process used to update a data on the dynamodb, we used

60
00:04:05,728 --> 00:04:09,222
to get a very high amount of event for a particular period.

61
00:04:09,366 --> 00:04:12,734
And we need to design our system to ensure that

62
00:04:12,772 --> 00:04:16,686
we support those kind of load and pass that data to

63
00:04:16,708 --> 00:04:19,760
the downstream system as soon as possible.

64
00:04:20,690 --> 00:04:24,354
Now let's go and talk more about it. That how we

65
00:04:24,392 --> 00:04:27,954
have managed our infrastructure to meet

66
00:04:27,992 --> 00:04:31,874
those kind of requirements and what is the kind of thought process we have took

67
00:04:31,912 --> 00:04:35,730
over there to ensure that we can manage

68
00:04:35,800 --> 00:04:39,814
those kind of loads. As we go further into this particular

69
00:04:40,012 --> 00:04:43,366
talk. If I talk about this kinesis as a

70
00:04:43,388 --> 00:04:46,934
stream, then in the kinesis generally what happened is that we need to

71
00:04:47,052 --> 00:04:50,698
define a number of shards that we want to keep in

72
00:04:50,704 --> 00:04:53,946
hours stream. This number of shard is something which we

73
00:04:53,968 --> 00:04:57,782
can either use on demand capacity or we can provision

74
00:04:57,846 --> 00:05:00,934
it in advance as well. The on demand

75
00:05:01,062 --> 00:05:04,526
capacity with this kind of scale is going to be a

76
00:05:04,548 --> 00:05:08,046
costly operation. So you need to be a bit careful about what

77
00:05:08,068 --> 00:05:11,434
is the number of amount of shards that you want to use in auto scale

78
00:05:11,482 --> 00:05:14,890
mode or if you're going for a provisional capacity,

79
00:05:14,970 --> 00:05:17,906
then in that case also you need to be very careful around what are the

80
00:05:17,928 --> 00:05:22,126
number of shards that you want to keep in a particular stream

81
00:05:22,318 --> 00:05:25,506
to process this data and move further on that site.

82
00:05:25,688 --> 00:05:29,346
Also from the producer side, when the bulk

83
00:05:29,378 --> 00:05:32,726
of data come in a way and you are processing it with a

84
00:05:32,748 --> 00:05:36,406
lambda and writing it to the shrouds, you also

85
00:05:36,428 --> 00:05:40,186
need to ensure that you write it at a speed to

86
00:05:40,208 --> 00:05:44,198
the shrouds where the throttle

87
00:05:44,294 --> 00:05:48,294
don't occur and you don't go into the throttling

88
00:05:48,342 --> 00:05:51,478
capacity of writing a data onto a particular shroud.

89
00:05:51,574 --> 00:05:55,646
So you need to basically derive a right combination over here that what

90
00:05:55,668 --> 00:05:59,086
is the number of shards that you want to keep in

91
00:05:59,108 --> 00:06:02,566
your stream and what is the kind of a parallel or a concurrent

92
00:06:02,618 --> 00:06:06,270
processing that you want to support in the lambda

93
00:06:06,430 --> 00:06:10,722
with some compromise against the latency to

94
00:06:10,776 --> 00:06:14,702
ensure that your system work in a smooth and a reliable manner.

95
00:06:14,846 --> 00:06:18,354
So that was like one of the core goal for us as a first exercise

96
00:06:18,482 --> 00:06:21,894
to identify that right amount of number of shrouds and the

97
00:06:21,932 --> 00:06:25,942
kind of a concurrency that we can support over here which

98
00:06:25,996 --> 00:06:29,494
can ensure that at a runtime the throttle

99
00:06:29,542 --> 00:06:33,130
don't happen while writing this data on the shroud.

100
00:06:33,550 --> 00:06:37,658
Now also, when we are talking about the shards, we need to ensure that

101
00:06:37,824 --> 00:06:41,894
whatever shroud, while we are writing a data to the shards,

102
00:06:42,022 --> 00:06:45,646
we have a uniform distribution across the shard as soon

103
00:06:45,668 --> 00:06:48,846
AWS possible. So the first thing what we have did is we

104
00:06:48,868 --> 00:06:52,298
have just tried to identify those things that whatever number of shards

105
00:06:52,314 --> 00:06:55,486
we are using over there, we have initially started with the

106
00:06:55,508 --> 00:06:59,010
200 odd shards to go with a particular

107
00:06:59,080 --> 00:07:02,914
stream and we just want to ensure that that whatever number

108
00:07:02,952 --> 00:07:07,234
of shards we are using, our data become uniformly

109
00:07:07,282 --> 00:07:11,586
distributed across the shards that we are having in the stream.

110
00:07:11,778 --> 00:07:16,402
Otherwise, what can happen is that if your data is not getting uniformly distributed

111
00:07:16,466 --> 00:07:20,374
across the shard, then you may go into a scenario where some

112
00:07:20,412 --> 00:07:23,226
of the shards are the hot shards where you have a lot of number of

113
00:07:23,248 --> 00:07:26,614
records being written by your producer

114
00:07:26,662 --> 00:07:29,962
systems, or you may have some of the shards where you have a very less

115
00:07:30,016 --> 00:07:33,050
number of records and they turn into the cold shards.

116
00:07:33,210 --> 00:07:37,086
Now, in order to identify those things that whether the data on

117
00:07:37,108 --> 00:07:40,720
the shards are getting distributed in appropriate manner or not,

118
00:07:41,410 --> 00:07:45,122
you can just enable the logging related activities on those

119
00:07:45,176 --> 00:07:49,262
particular shards and you can just monitor those details accordingly.

120
00:07:49,406 --> 00:07:53,438
So how we can check those detail is that by default,

121
00:07:53,534 --> 00:07:57,062
this particular metric related to the shards are not

122
00:07:57,116 --> 00:08:00,578
enabled for the monitoring purposes.

123
00:08:00,754 --> 00:08:04,482
In order to enable those kind of metrics, you need to use enable

124
00:08:04,546 --> 00:08:08,102
enhanced monitoring API to make

125
00:08:08,156 --> 00:08:12,406
this shard related matrix enable on the AWS ecosystem.

126
00:08:12,598 --> 00:08:15,530
Here the batch is this particular metric.

127
00:08:16,830 --> 00:08:20,506
Monitoring don't come for free, so you

128
00:08:20,528 --> 00:08:23,834
basically need to pay for this metric

129
00:08:23,882 --> 00:08:27,162
availability. But once you get an access to this metric,

130
00:08:27,226 --> 00:08:30,686
you would be able to very reliably see that across the

131
00:08:30,708 --> 00:08:33,986
shards, whether the data is getting written in the uniform format or

132
00:08:34,008 --> 00:08:37,602
not. And that will basically give you an confidence in the sense

133
00:08:37,656 --> 00:08:41,278
of whatever number of shards you are provisioning. Those shards

134
00:08:41,294 --> 00:08:44,446
are getting utilized in appropriate manner

135
00:08:44,638 --> 00:08:48,466
for a further data processing. If you are not willing

136
00:08:48,498 --> 00:08:52,150
to pay and go with this in house solution

137
00:08:53,450 --> 00:08:56,934
Sri the uniform distribution, you can build your

138
00:08:56,972 --> 00:09:00,602
own solution in way as well where you can either

139
00:09:00,656 --> 00:09:04,442
on the consumer side you can just use the custom logging where for

140
00:09:04,496 --> 00:09:08,186
each record you can just basically

141
00:09:08,368 --> 00:09:11,734
identify those details or you can customize

142
00:09:11,782 --> 00:09:15,326
the producer and log the put record response and it will

143
00:09:15,348 --> 00:09:18,474
return you the data that is placed under a particular shroud.

144
00:09:18,602 --> 00:09:22,622
So there are alternate ways also that you can use to check

145
00:09:22,676 --> 00:09:26,046
whether the data is uniformly distributed or not. But I

146
00:09:26,068 --> 00:09:29,266
would prefer in the initial stage, it would be great to just go with the

147
00:09:29,288 --> 00:09:33,122
inbuilt matrix, and that will just give you a good amount of confidence that

148
00:09:33,176 --> 00:09:37,018
whether your data is distributed uniformly across a shard

149
00:09:37,054 --> 00:09:41,030
or not. Now once you identify that, let's say

150
00:09:41,100 --> 00:09:44,898
your data is not uniformly distributed across a shard, if it's

151
00:09:44,914 --> 00:09:49,014
distributed uniformly across a shard, there is a no issue and you can just go

152
00:09:49,052 --> 00:09:52,666
further with a similar kind of implementation. But if, let's say your

153
00:09:52,688 --> 00:09:56,534
data is not uniformly distributed across a shard, and you want to enforce

154
00:09:56,582 --> 00:10:00,446
those kind of mechanism where your data get

155
00:10:00,548 --> 00:10:06,122
distributed uniformly across the shards from the upstream

156
00:10:06,186 --> 00:10:09,354
system, whatever we are using over here for a data processing layer,

157
00:10:09,402 --> 00:10:13,230
in this case we were using a lambda. So in order to

158
00:10:13,380 --> 00:10:16,674
ensure the kind of uniform distribution, there are

159
00:10:16,712 --> 00:10:20,798
two ways in which you can implement it in the AWS

160
00:10:20,894 --> 00:10:24,642
lambda way, where one of the way what you can do is that

161
00:10:24,776 --> 00:10:28,274
whatever put record command,

162
00:10:28,322 --> 00:10:31,766
you are firing. Put record or put records command you are firing to write

163
00:10:31,788 --> 00:10:35,622
a data on the stream. On that one you can explicitly pass

164
00:10:35,676 --> 00:10:39,306
the partition key as an hash value of

165
00:10:39,328 --> 00:10:43,206
this particular record, or by having some kind of hash

166
00:10:43,238 --> 00:10:47,178
of a random number generator, which will basically ensure that

167
00:10:47,344 --> 00:10:51,626
your data will go in a uniformly

168
00:10:51,658 --> 00:10:55,280
manner, because generally the random distribution is

169
00:10:56,610 --> 00:11:00,270
gaussian distribution and that ensure the uniformity

170
00:11:00,930 --> 00:11:04,810
and the uniform distribution. So you can either go with this particular approach

171
00:11:04,890 --> 00:11:08,558
where you can just take a hash of the records

172
00:11:08,654 --> 00:11:11,170
and then you can just pass it further,

173
00:11:11,510 --> 00:11:14,866
or in the second approach, what you can do is you can just

174
00:11:15,048 --> 00:11:17,878
share the data with the explicit hash key.

175
00:11:18,044 --> 00:11:21,682
So in an Aws, you can basically use a command

176
00:11:21,826 --> 00:11:25,618
where you can just describe the stream

177
00:11:25,714 --> 00:11:29,494
and you can get access to what all shard it has and

178
00:11:29,532 --> 00:11:33,066
to that shroud, what are the hash key ranges it

179
00:11:33,088 --> 00:11:36,394
is using to distribute the data. Now once you have those

180
00:11:36,432 --> 00:11:39,702
information, what you can do is in a round robin manner,

181
00:11:39,846 --> 00:11:43,258
whatever data you are having in your stream,

182
00:11:43,354 --> 00:11:46,686
like whatever data you are trying to write on the

183
00:11:46,708 --> 00:11:50,382
stream using a put record or put records command, you can

184
00:11:50,436 --> 00:11:54,382
assign it a partition key, which is a mandatory parameter. And along

185
00:11:54,436 --> 00:11:57,410
with that you can also assign the explicit hash key.

186
00:11:57,560 --> 00:12:00,894
Now if you assign both the parameter partition key and the explicit

187
00:12:00,942 --> 00:12:04,738
hash key, the explicit hash key will get a priority. It is going

188
00:12:04,744 --> 00:12:08,354
to override hours partition key and it is going to deliver

189
00:12:08,402 --> 00:12:11,814
a data to a particular partition where you

190
00:12:11,852 --> 00:12:15,718
are passing with a random distribution. So in this particular

191
00:12:15,804 --> 00:12:19,814
manner, with any of these two approaches, you would be able to

192
00:12:19,932 --> 00:12:23,590
ensure the uniform distribution across hours shard.

193
00:12:23,750 --> 00:12:27,050
And if you are going to do that, it is going to ensure two things.

194
00:12:27,120 --> 00:12:30,438
The first thing is that, that you don't go into the scenario where

195
00:12:30,464 --> 00:12:34,206
you have a hot or cold shard related issues. And second, it will

196
00:12:34,228 --> 00:12:37,738
ensure that you will have a proper utilization

197
00:12:37,834 --> 00:12:41,600
of the shards that you are using in your

198
00:12:42,870 --> 00:12:46,514
ecosystem. So that will helped optimize a

199
00:12:46,552 --> 00:12:49,278
cost and a proper utilization.

200
00:12:49,454 --> 00:12:52,754
And this kind of metric will also help you

201
00:12:52,872 --> 00:12:56,498
manage and see the load. In case of an issue,

202
00:12:56,584 --> 00:13:00,530
if you want to restrict the number of records

203
00:13:00,610 --> 00:13:03,554
that you are writing to the shrouds, not a number of records,

204
00:13:03,602 --> 00:13:07,254
but you want to restrict or slow the speed of writing to your

205
00:13:07,292 --> 00:13:11,046
shrouds, one of the thing, what you can do is that whatever layer you are

206
00:13:11,068 --> 00:13:14,614
using over here, you can just slow down that particular layer

207
00:13:14,662 --> 00:13:18,266
speed as well. To write a data like in our case, you were having a

208
00:13:18,288 --> 00:13:22,158
lambda, and in the lambda like, you can use the

209
00:13:22,324 --> 00:13:25,854
concurrent execution property to say

210
00:13:25,892 --> 00:13:29,870
that how much concurrent executions is going to happen for this particular

211
00:13:29,940 --> 00:13:34,622
layer. And that layer will ensure that only

212
00:13:34,676 --> 00:13:38,286
that much amount of concurrent execution happen. So that will slow

213
00:13:38,318 --> 00:13:41,966
down the speed of writing onto the shards and the processing

214
00:13:41,998 --> 00:13:45,606
speed on the shroud. So you don't go into the throttle issue as well.

215
00:13:45,708 --> 00:13:49,526
So till the time you don't have

216
00:13:49,548 --> 00:13:53,314
a clear idea that to which extent you want to increase

217
00:13:53,362 --> 00:13:57,394
your shard and go in a reliable manner,

218
00:13:57,522 --> 00:14:00,954
you can just use this kind of combination mechanism and

219
00:14:00,992 --> 00:14:04,234
it can help you out derive the right value of your number

220
00:14:04,272 --> 00:14:07,606
of shards and the execution concurrency layer

221
00:14:07,638 --> 00:14:10,750
over here, that can ensure

222
00:14:11,330 --> 00:14:15,102
uniform distribution and a transfer of data into

223
00:14:15,156 --> 00:14:17,760
your stream. Now,

224
00:14:18,290 --> 00:14:22,560
after identifying this kind of right numbers

225
00:14:23,010 --> 00:14:27,218
and ensuring that, that you have a uniform distribution of

226
00:14:27,304 --> 00:14:31,154
records across your shard. The other thing, problem, what we have

227
00:14:31,192 --> 00:14:34,974
seen in our implementation is that the way the

228
00:14:35,032 --> 00:14:38,834
schema was evolving, that schema,

229
00:14:38,962 --> 00:14:42,694
we were getting lots of columns which were having details like

230
00:14:42,732 --> 00:14:46,834
a comment and the descriptions and those kind of descriptive

231
00:14:46,882 --> 00:14:50,554
columns were more and more getting added to

232
00:14:50,592 --> 00:14:53,914
the particular data contract. Now, when we are

233
00:14:53,952 --> 00:14:57,306
getting and adding those kind of more descriptive columns and

234
00:14:57,328 --> 00:15:01,258
the details into our data,

235
00:15:01,424 --> 00:15:05,120
what we are essentially doing is we are basically increasing the

236
00:15:05,490 --> 00:15:08,686
size of the data or a record that

237
00:15:08,708 --> 00:15:12,026
we are writing to the shroud. In the shrouds,

238
00:15:12,058 --> 00:15:15,506
there is a limitation that it can handle up to

239
00:15:15,608 --> 00:15:18,226
1 mb/second data write.

240
00:15:18,408 --> 00:15:22,114
So again, you will go into the same scenario where

241
00:15:22,152 --> 00:15:26,626
if you are writing a more number of data in

242
00:15:26,648 --> 00:15:29,994
a way to your shroud then either you need to increase your shrouds

243
00:15:30,062 --> 00:15:33,414
or you will go into a problem where you will get a

244
00:15:33,452 --> 00:15:36,946
throttling of the records on a particular shard.

245
00:15:37,058 --> 00:15:40,218
So in order to overcome this solution, one of the thing, you know, what you

246
00:15:40,224 --> 00:15:43,834
can do is that whatever data you are writing on

247
00:15:43,872 --> 00:15:46,838
the Kinesis stream,

248
00:15:47,014 --> 00:15:50,618
before writing that data, you can just apply

249
00:15:50,704 --> 00:15:54,994
some kind of compression algorithms like Gzip or LZ

250
00:15:55,062 --> 00:15:59,054
hours or something like that. Or you can just choose a format like

251
00:15:59,092 --> 00:16:02,270
Avro or something, which is a compressed data format.

252
00:16:02,770 --> 00:16:06,254
This will add an additional computation in your lambda layer

253
00:16:06,382 --> 00:16:10,146
and on the consumer side as well to either unzip the

254
00:16:10,168 --> 00:16:13,540
data or to read the data

255
00:16:13,910 --> 00:16:17,470
in a particular format. But it will ensure that

256
00:16:17,560 --> 00:16:21,410
your data transfer happen in a seamless manner.

257
00:16:21,570 --> 00:16:24,742
So if you are going across a similar kind of problem,

258
00:16:24,876 --> 00:16:28,086
this is the kind of thing which you can use for the

259
00:16:28,108 --> 00:16:31,702
other thing. What I recommend is, which was unfortunately we were not able to implement

260
00:16:31,766 --> 00:16:35,494
in our project that for this kind of descriptive

261
00:16:35,542 --> 00:16:39,190
columns and descriptive attributes,

262
00:16:39,270 --> 00:16:44,106
if it's possible, then rather than processing

263
00:16:44,138 --> 00:16:48,238
that data to the stream, it is good to pass that data

264
00:16:48,404 --> 00:16:52,574
via some kind of references where your data can be

265
00:16:52,772 --> 00:16:56,846
stored on let's say some s three locations or databases,

266
00:16:57,038 --> 00:17:00,526
and the end consumers can access those kind of descriptive

267
00:17:00,558 --> 00:17:04,034
data from there if the cardinality of that data

268
00:17:04,072 --> 00:17:07,906
is not high. So in those cases you will a save

269
00:17:08,088 --> 00:17:11,670
on the processing side and b you save on the space

270
00:17:11,740 --> 00:17:15,570
as well. So it can become a minimum situation on both of the sides.

271
00:17:15,650 --> 00:17:18,954
So you can choose either your format rightfully or

272
00:17:18,992 --> 00:17:22,182
you can just define your data context appropriately

273
00:17:22,326 --> 00:17:26,378
to ensure that you are keeping some control

274
00:17:26,544 --> 00:17:30,254
on the amount of data that you are passing in

275
00:17:30,292 --> 00:17:33,594
each record to your the next practices.

276
00:17:33,642 --> 00:17:37,402
What I recommend generally while working with any kind of streaming

277
00:17:37,466 --> 00:17:41,374
platform, is that whatever data we are

278
00:17:41,412 --> 00:17:45,042
passing to the stream, that data has to be bind with

279
00:17:45,096 --> 00:17:49,042
some kind of a data contract. So that while

280
00:17:49,096 --> 00:17:52,814
writing the data, the producer

281
00:17:52,862 --> 00:17:56,418
can basically write a data as per that contract.

282
00:17:56,514 --> 00:18:00,582
And the consumer can also understand using that contract what

283
00:18:00,636 --> 00:18:04,230
data it is going to using

284
00:18:04,300 --> 00:18:07,654
and how to basically use that data or read that data now

285
00:18:07,692 --> 00:18:10,666
in order to maintain those things. If you are going to work with any of

286
00:18:10,688 --> 00:18:14,234
the streaming solution, they generally provide some kind of a

287
00:18:14,272 --> 00:18:17,290
schema registry. Like on the AWS world,

288
00:18:17,360 --> 00:18:20,606
there is a glue catalog available where in the glue you can

289
00:18:20,628 --> 00:18:24,334
define the AWS glue schema registry service

290
00:18:24,532 --> 00:18:27,962
where you can basically define the schema.

291
00:18:28,026 --> 00:18:32,842
And that schema is something that can be consumed by both producer and consumer

292
00:18:32,986 --> 00:18:37,170
to understand the data along with that schema registry,

293
00:18:37,590 --> 00:18:41,838
what feature it provide is it also support a versioning of the schema.

294
00:18:41,934 --> 00:18:45,054
So if you have a scenarios where hours schema is getting

295
00:18:45,112 --> 00:18:48,354
evolved over a period of time, then the admin

296
00:18:48,402 --> 00:18:52,520
can control that particular schema and based on the

297
00:18:52,970 --> 00:18:56,614
version and the contract the schema get

298
00:18:56,652 --> 00:19:00,806
evolved. So your producer always know in

299
00:19:00,828 --> 00:19:04,454
which way it has to write a data and your consumer

300
00:19:04,502 --> 00:19:08,294
will also get an idea that how to read a data. So what I generally

301
00:19:08,342 --> 00:19:11,662
recommend is to go for the schema registry first before even

302
00:19:11,716 --> 00:19:14,842
implementing streaming solutions.

303
00:19:14,986 --> 00:19:18,830
And that will give you and save you lots of time

304
00:19:18,980 --> 00:19:22,978
while working further on the project. In case you don't want to use any kind

305
00:19:22,984 --> 00:19:26,642
of schema registry based solutions. In that way

306
00:19:26,696 --> 00:19:30,306
you can just deliver some kind of code bases that basically

307
00:19:30,408 --> 00:19:34,002
contain the definitions and that shared code

308
00:19:34,056 --> 00:19:37,586
base can be or those schema

309
00:19:37,618 --> 00:19:41,266
definitions can be used in both the sides on a producer

310
00:19:41,298 --> 00:19:44,950
and consumer to write a data in a particular

311
00:19:45,020 --> 00:19:48,566
schema and deliver it in an appropriate way. After doing

312
00:19:48,588 --> 00:19:52,422
this schema registry, the last thing, what I would like to mention is the validation

313
00:19:52,486 --> 00:19:55,946
technique which we have used and which

314
00:19:55,968 --> 00:19:59,398
was more kind of inspired from the canary implementation

315
00:19:59,574 --> 00:20:03,374
to say that whatever data we are getting from our

316
00:20:03,412 --> 00:20:06,606
producer system and whatever data we are passing to our

317
00:20:06,708 --> 00:20:11,070
consumer, that data is getting passed in a reliable manner.

318
00:20:11,490 --> 00:20:15,054
So over here, one of the thing what we have did is that whatever data

319
00:20:15,092 --> 00:20:18,914
we are getting from a producer and our processing used to write that data

320
00:20:18,952 --> 00:20:22,674
to the stream and used to consume from the along

321
00:20:22,712 --> 00:20:26,926
with that, what we have did is we have also start publishing

322
00:20:26,958 --> 00:20:30,466
a dummy events into our data stream.

323
00:20:30,578 --> 00:20:33,830
So what we used to do is we used to have a certain

324
00:20:33,900 --> 00:20:37,738
lifecycle of the product that we are having, which used to have a

325
00:20:37,744 --> 00:20:40,966
lifecycle state like open, in review, progress,

326
00:20:41,078 --> 00:20:44,486
close, et cetera. So whatever those dummy

327
00:20:44,518 --> 00:20:48,518
events were there, that dummy events we used to publish

328
00:20:48,694 --> 00:20:52,234
on the stream for a further consumption

329
00:20:52,282 --> 00:20:56,046
purposes. And those dummy events related metric we

330
00:20:56,068 --> 00:20:58,480
used to publish on the cloud batch as well.

331
00:20:59,010 --> 00:21:02,478
Once those event get passed over here,

332
00:21:02,564 --> 00:21:05,150
those event also used to go to the stream.

333
00:21:05,310 --> 00:21:08,834
And on the consumer side also we used to read those

334
00:21:08,872 --> 00:21:12,210
particular dummy event and we used to pass

335
00:21:12,360 --> 00:21:16,360
the custom metrics around those event status and the other

336
00:21:17,050 --> 00:21:20,710
dimensions into this particular cloudwatch matrix.

337
00:21:21,210 --> 00:21:24,738
After publishing these events on a periodic manner

338
00:21:24,914 --> 00:21:28,594
and consuming again and publishing

339
00:21:28,642 --> 00:21:32,038
its data onto the Cloudwatch kind of layer,

340
00:21:32,214 --> 00:21:35,526
what we were able to achieve is that on a periodic manner,

341
00:21:35,638 --> 00:21:39,370
we used to compare both the metrics that whether whatever

342
00:21:39,440 --> 00:21:42,990
metrics we have published from this dummy event

343
00:21:43,140 --> 00:21:47,002
versus whatever metric we are catching on the consumer

344
00:21:47,066 --> 00:21:50,640
side, whether those metrics are matching and

345
00:21:51,170 --> 00:21:54,306
with an acceptable deviation range or not.

346
00:21:54,488 --> 00:21:58,114
And if we identify that, that the number of open or

347
00:21:58,152 --> 00:22:01,826
closed events, if that deviation range is

348
00:22:01,848 --> 00:22:05,774
too high across the one which we are sending

349
00:22:05,822 --> 00:22:09,910
from the producer side and one we are getting from the consumer side,

350
00:22:10,060 --> 00:22:13,766
then in that case there is a delay is happening in

351
00:22:13,788 --> 00:22:17,798
the streaming data that we are passing through our

352
00:22:17,884 --> 00:22:21,434
processes, or there is a high volume load or some

353
00:22:21,472 --> 00:22:24,854
kind of issues are there. So on those kind of metrics,

354
00:22:24,902 --> 00:22:28,218
we have basically created an alerts in which

355
00:22:28,304 --> 00:22:31,546
that we used to say that yep, whether the

356
00:22:31,568 --> 00:22:35,006
data is getting processed in a smooth manner or not. So if

357
00:22:35,028 --> 00:22:38,298
you are also working on a streaming related solutions,

358
00:22:38,474 --> 00:22:42,522
then this is one of the canary based technique that can be implemented

359
00:22:42,666 --> 00:22:45,950
where you can put some kind of dummy events

360
00:22:46,110 --> 00:22:49,426
on the producer side, on the consumer side you read the

361
00:22:49,448 --> 00:22:52,878
same event and on those events lifecycle

362
00:22:53,054 --> 00:22:57,010
you produce a custom matrix on the Cloudwatch

363
00:22:57,090 --> 00:23:00,630
or the related tool. And from there

364
00:23:00,700 --> 00:23:04,818
you can generate alerts in case you see a higher deviation

365
00:23:04,994 --> 00:23:08,890
while processing the data and the respective turnaround time.

366
00:23:09,040 --> 00:23:13,094
So that can help you identify that whether hours streaming

367
00:23:13,142 --> 00:23:16,730
pipeline is working, AWS expected or not,

368
00:23:16,880 --> 00:23:20,022
or there is some fix or concurrency,

369
00:23:20,086 --> 00:23:23,854
or a capacity increase is required to support those

370
00:23:23,892 --> 00:23:27,374
kind of use cases. So that's all about it.

371
00:23:27,412 --> 00:23:30,958
And these are like some of the things which I just want to cover in

372
00:23:30,964 --> 00:23:34,590
this particular sessions around the best practices.

373
00:23:35,010 --> 00:23:39,002
If you want to talk more about the streaming

374
00:23:39,066 --> 00:23:42,238
solutions or the things around it, feel free to reach out

375
00:23:42,244 --> 00:23:45,926
to me on the LinkedIn or some social media platform

376
00:23:46,108 --> 00:23:48,774
and would be happy to talk more over there.

377
00:23:48,972 --> 00:23:52,180
Thank you and thanks for your attention while listening to this talk.

