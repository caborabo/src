1
00:00:27,320 --> 00:00:31,450
Hello. This talk is about actionable alerts

2
00:00:31,482 --> 00:00:34,674
and how we can use them to automate responses

3
00:00:34,714 --> 00:00:38,706
to incidents. Alerting is usually perceived as

4
00:00:38,730 --> 00:00:41,574
a system to human notification solution.

5
00:00:42,274 --> 00:00:45,970
However, alerts are not about messaging

6
00:00:46,042 --> 00:00:49,202
only. This talk looks into maximizing

7
00:00:49,258 --> 00:00:53,474
efficiency of the alerts by better targeting and reducing response

8
00:00:53,514 --> 00:00:57,668
times. My name is Leonid. I'm a senior

9
00:00:57,756 --> 00:01:00,384
developer relation engineer at Google Cloud.

10
00:01:00,804 --> 00:01:04,744
In my work I help to improve workload observability

11
00:01:05,084 --> 00:01:08,812
by using cloud infrastructure, telemetry, alerting and other

12
00:01:08,868 --> 00:01:12,612
services and tools. You can

13
00:01:12,748 --> 00:01:17,972
reach me out via LinkedIn on

14
00:01:17,988 --> 00:01:21,136
my posts on medium or using a link to the

15
00:01:21,160 --> 00:01:24,284
feedback form on the last slide of this presentation.

16
00:01:26,264 --> 00:01:30,764
Being actionable is one of the properties of the efficient alerts.

17
00:01:31,464 --> 00:01:35,568
So first we will talk what efficient alerting

18
00:01:35,656 --> 00:01:39,444
is and how we can make our alerts efficient.

19
00:01:40,184 --> 00:01:44,280
Then we will look into automating and alerts when

20
00:01:44,312 --> 00:01:47,336
it is possible and how to use alerting solution

21
00:01:47,480 --> 00:01:51,006
for automating responses. At the end,

22
00:01:51,190 --> 00:01:54,406
I will demonstrate how to implement automating

23
00:01:54,510 --> 00:01:57,670
responses using alerting solution in

24
00:01:57,702 --> 00:02:00,794
Google Cloud. Let's begin.

25
00:02:02,734 --> 00:02:06,398
Efficient means achieving maximum productivity

26
00:02:06,446 --> 00:02:09,990
with minimum wasted effort or expense. We can

27
00:02:10,022 --> 00:02:12,750
ask aren't any alert like that?

28
00:02:12,942 --> 00:02:16,334
The truth is that many companies reported their

29
00:02:16,374 --> 00:02:18,914
alerts overload support systems.

30
00:02:19,264 --> 00:02:23,448
Processing large number of alerts leads to extended

31
00:02:23,536 --> 00:02:27,564
response times and churning. In DevOps and Sre teams.

32
00:02:28,104 --> 00:02:32,304
Companies end up changing priorities of their alerts

33
00:02:32,424 --> 00:02:35,816
which in turn result in opening tickets or bugs as an

34
00:02:35,840 --> 00:02:38,724
automatic response to the new alerts.

35
00:02:39,424 --> 00:02:43,088
Many such bugs end up abundant. Increasing the technical

36
00:02:43,136 --> 00:02:47,010
depth of engineering teams alerting should help us

37
00:02:47,042 --> 00:02:51,454
address urgent problems and fix found problems quickly.

38
00:02:51,794 --> 00:02:55,294
So what means to have an efficient alert?

39
00:02:56,914 --> 00:03:00,530
Let's look into three components of the

40
00:03:00,562 --> 00:03:03,818
alert. When focusing on alerting

41
00:03:03,866 --> 00:03:07,938
conditions, it is responsible for controlling frequency

42
00:03:07,986 --> 00:03:10,534
and relevancy of created alerts.

43
00:03:11,414 --> 00:03:14,550
What is responsible for information related

44
00:03:14,622 --> 00:03:18,710
to the alert? Right. Information can increase efficiency

45
00:03:18,742 --> 00:03:22,574
of the alerts outcome and improve time to resolve.

46
00:03:22,694 --> 00:03:26,394
Metric of alerts the last

47
00:03:27,134 --> 00:03:30,590
how is one that covers the messaging part of

48
00:03:30,622 --> 00:03:34,830
the alert. Notifying about the alert can take different forms

49
00:03:34,862 --> 00:03:38,746
and formats. Lets look into

50
00:03:38,810 --> 00:03:42,894
each of these components of the efficient alert cloth.

51
00:03:44,954 --> 00:03:48,426
One of the methods to make alert efficient is not to

52
00:03:48,450 --> 00:03:52,074
rise them frequently.

53
00:03:52,154 --> 00:03:56,362
Raised alerts are like the story about

54
00:03:56,418 --> 00:04:00,066
a shepherd boy who cried wolf wolf

55
00:04:00,170 --> 00:04:03,762
all the time. When rising alert depends on

56
00:04:03,818 --> 00:04:07,582
several factors, good practice is to start

57
00:04:07,638 --> 00:04:11,190
from the end. Think about relevance of

58
00:04:11,222 --> 00:04:15,114
this alert. What is the problem this alert discovers?

59
00:04:15,774 --> 00:04:18,994
Does this problem affect the users of our system?

60
00:04:19,854 --> 00:04:23,406
If this alert catches the problem or responses

61
00:04:23,470 --> 00:04:25,894
to some transient change in the system,

62
00:04:25,974 --> 00:04:29,758
state review the signals used

63
00:04:29,806 --> 00:04:33,392
to identify the problem. It identifies

64
00:04:33,448 --> 00:04:37,200
the best suited metrics to discover the problem. A good

65
00:04:37,232 --> 00:04:40,944
practice is to use metrics that reflect user experience about the

66
00:04:40,984 --> 00:04:43,684
system and not the systems resources.

67
00:04:44,664 --> 00:04:47,672
When unsure, consider using one of

68
00:04:47,688 --> 00:04:50,432
the monitoring golden signals, latency,

69
00:04:50,488 --> 00:04:53,124
traffic errors and saturation.

70
00:04:54,664 --> 00:04:58,096
Another factor influencing efficiency when to rise

71
00:04:58,120 --> 00:05:01,444
an alert is using threshold versus tendency.

72
00:05:02,644 --> 00:05:06,380
Sometime the speed of deterioration can be

73
00:05:06,412 --> 00:05:10,104
much more useful signal for alert than a simple threshold.

74
00:05:11,484 --> 00:05:14,780
We always want to condition rising alert by

75
00:05:14,812 --> 00:05:17,876
observing the same condition along a predefined period

76
00:05:17,940 --> 00:05:21,324
of time. Consider to trigger an alert when

77
00:05:21,364 --> 00:05:24,876
a request latency reach 10 seconds

78
00:05:25,060 --> 00:05:28,884
versus triggering an alert when the average latency is

79
00:05:28,924 --> 00:05:30,984
10 seconds for the last two minutes.

80
00:05:31,394 --> 00:05:34,014
Okay, 10 seconds may be too much.

81
00:05:35,234 --> 00:05:38,882
The first condition will result in many accident alerts

82
00:05:38,978 --> 00:05:42,714
when some one time delays happens which

83
00:05:42,794 --> 00:05:45,574
aren't actually affect any users,

84
00:05:46,434 --> 00:05:50,010
while the second condition will provide a clear

85
00:05:50,082 --> 00:05:53,974
signal that the system responses are delayed.

86
00:05:54,474 --> 00:05:57,850
When implementing an alert, it is important to think about

87
00:05:57,922 --> 00:06:01,432
outcome. We want to respond to the alert to

88
00:06:01,448 --> 00:06:05,272
be efficient too. In other words, the alert

89
00:06:05,328 --> 00:06:09,184
should contain enough information to ensure successful resolution

90
00:06:09,304 --> 00:06:13,644
in a reasonably short time. For example,

91
00:06:14,024 --> 00:06:18,144
an alert saying that cpu usage is above

92
00:06:18,264 --> 00:06:21,704
90% for last five minutes is

93
00:06:21,744 --> 00:06:24,864
really hard to handle. However,

94
00:06:25,284 --> 00:06:28,996
if the alert comes with a link to a dashboard capturing

95
00:06:29,100 --> 00:06:32,380
all processes consuming that cpu for the

96
00:06:32,412 --> 00:06:36,028
last five minutes, the mitigation of the problem will be much

97
00:06:36,076 --> 00:06:39,944
faster and easier. We don't always

98
00:06:40,484 --> 00:06:44,500
can know what type of information will be useful when responding

99
00:06:44,532 --> 00:06:47,424
to the alert. As a rule of thumb,

100
00:06:48,164 --> 00:06:51,524
all relevant information about the system that we can collect

101
00:06:51,604 --> 00:06:55,374
is useful. We often reference this information

102
00:06:55,494 --> 00:06:57,954
as context or metadata.

103
00:06:59,294 --> 00:07:02,286
Additional materials like playbooks,

104
00:07:02,390 --> 00:07:07,126
team chats, and other supporting information when

105
00:07:07,150 --> 00:07:11,434
it is part of the alert data can increase alert efficiency.

106
00:07:12,574 --> 00:07:16,582
When alert notification provides enough information for efficient response,

107
00:07:16,638 --> 00:07:19,414
we can call such alerts actionable.

108
00:07:20,594 --> 00:07:24,586
Remember that information that is easy to obtain during

109
00:07:24,650 --> 00:07:28,882
the alert is often very hard or even impossible

110
00:07:28,938 --> 00:07:32,574
to discover when it is time to respond to the notification.

111
00:07:33,194 --> 00:07:36,466
How to alert also plays an important role

112
00:07:36,530 --> 00:07:40,402
in enabling actionable alerts. How can enable

113
00:07:40,498 --> 00:07:43,994
response to be automated and control possible outcome

114
00:07:44,034 --> 00:07:47,832
of the alert? The selected method should be resilient

115
00:07:47,928 --> 00:07:51,976
in delivering notification about alert with

116
00:07:52,000 --> 00:07:55,760
an unexpected delivery times. The method also

117
00:07:55,832 --> 00:07:59,584
has to support escalation path in event of the notification

118
00:07:59,664 --> 00:08:03,672
failure or if a response doesn't happen within expected

119
00:08:03,768 --> 00:08:06,984
time interval. Needless to say that

120
00:08:07,024 --> 00:08:10,864
efficient notification should be able to deliver all

121
00:08:10,904 --> 00:08:15,398
alert information as well. Since many alert responses

122
00:08:15,486 --> 00:08:18,334
are subject to the post mortem analysis,

123
00:08:18,494 --> 00:08:21,934
how should include auditing feature as well.

124
00:08:22,094 --> 00:08:25,790
Actionable alerts can leverage multiple notification

125
00:08:25,862 --> 00:08:29,046
methods to increase efficiency, alerts can be

126
00:08:29,070 --> 00:08:33,398
sent via several channels while supporting distinct formatting

127
00:08:33,446 --> 00:08:36,354
of alert information for each of them.

128
00:08:38,074 --> 00:08:41,882
As I previously mentioned, when talking about alerts,

129
00:08:41,938 --> 00:08:46,374
we mainly mean system to human communication, where engineers

130
00:08:46,674 --> 00:08:50,370
manually identify problems behind the alert conditions

131
00:08:50,522 --> 00:08:54,330
and remediate them. What about automating the

132
00:08:54,362 --> 00:08:58,146
response to the alert notification? Think about

133
00:08:58,210 --> 00:09:01,934
it. Automation is already a big part of alerts.

134
00:09:02,674 --> 00:09:06,054
The process of raising an alert is automated.

135
00:09:06,484 --> 00:09:10,108
Instead of having a DevOps engineer looking into

136
00:09:10,196 --> 00:09:13,644
histograms and bar charts on the dashboard and

137
00:09:13,684 --> 00:09:17,612
tracking gorges, curves and numbers to start responding

138
00:09:17,668 --> 00:09:20,940
if something goes wrong, we have software that does

139
00:09:20,972 --> 00:09:24,164
it for us all. What is

140
00:09:24,204 --> 00:09:26,904
left to engineers is to respond,

141
00:09:27,844 --> 00:09:31,584
and even that part doesn't always happen.

142
00:09:33,184 --> 00:09:36,832
What about notifications that

143
00:09:36,888 --> 00:09:40,136
open a bug or a ticket? What it

144
00:09:40,160 --> 00:09:44,928
is, if not automated, response to the alert let's

145
00:09:44,976 --> 00:09:48,808
look at the classical example of the alert. When an on call

146
00:09:48,936 --> 00:09:53,288
engineer receives a pager or another type of notification that

147
00:09:53,336 --> 00:09:56,004
requires them to address the problem immediately.

148
00:09:56,744 --> 00:10:01,034
A simplified algorithm that they follow looks

149
00:10:01,074 --> 00:10:04,802
like first, identify a

150
00:10:04,818 --> 00:10:08,614
type of the problem or a cause.

151
00:10:09,074 --> 00:10:12,898
Second, find the playbook with instructions

152
00:10:12,946 --> 00:10:16,494
to this problem type and third,

153
00:10:16,874 --> 00:10:18,534
execute instructions.

154
00:10:19,834 --> 00:10:23,810
Note that with efficient alerts,

155
00:10:24,002 --> 00:10:28,098
the first two steps are optimized into single step

156
00:10:28,226 --> 00:10:32,174
because the playbook is already part of the information about the alert.

157
00:10:32,944 --> 00:10:36,544
So the difference between a human engineer and an automated

158
00:10:36,624 --> 00:10:40,800
response is what is written in the playbook or

159
00:10:40,912 --> 00:10:43,084
is there a playbook in the first place?

160
00:10:44,384 --> 00:10:47,760
One of the oldest playbooks the system engineers know

161
00:10:47,872 --> 00:10:51,924
has a simple restart. The problematic instance

162
00:10:52,384 --> 00:10:55,244
do we really need a human to do this today?

163
00:10:56,744 --> 00:11:00,284
Obviously not. Any alert response can be automated.

164
00:11:01,184 --> 00:11:05,204
There are distinctive signs that an alert response can be automated.

165
00:11:05,624 --> 00:11:09,056
Alert is actionable, meaning three components of the

166
00:11:09,080 --> 00:11:12,656
alert when, what, and how follow the efficient

167
00:11:12,720 --> 00:11:16,328
alert principles. The cause of the alert

168
00:11:16,456 --> 00:11:20,440
is deterministic and can be derived from the information captured with

169
00:11:20,472 --> 00:11:24,184
the alert. And the response does not require human

170
00:11:24,264 --> 00:11:27,510
decisions or interactions, and vice versa.

171
00:11:27,662 --> 00:11:31,910
Signs that an alert cannot be automated include the ambiguous

172
00:11:31,982 --> 00:11:35,038
cause, despite alert being actionable,

173
00:11:35,086 --> 00:11:39,390
maybe, or a response expects

174
00:11:39,582 --> 00:11:43,582
some human interaction or

175
00:11:43,678 --> 00:11:47,702
decision making process. It is all arguments

176
00:11:47,758 --> 00:11:50,902
such as alerts are not intended

177
00:11:50,998 --> 00:11:55,320
for automation or high priority alerts

178
00:11:55,512 --> 00:12:00,920
require some human attention or alerts

179
00:12:00,952 --> 00:12:04,744
with potentially high business impact should be

180
00:12:04,784 --> 00:12:08,488
handled by humans should not influence

181
00:12:08,616 --> 00:12:11,684
the decision about alert response automation.

182
00:12:12,384 --> 00:12:16,040
Of course, automated responses don't guarantee

183
00:12:16,112 --> 00:12:19,384
to be free of errors. However, majority of

184
00:12:19,424 --> 00:12:21,484
these errors are human errors.

185
00:12:23,784 --> 00:12:27,448
Let's quickly review the three alert components for

186
00:12:27,496 --> 00:12:30,724
actionable alerts. With automated response,

187
00:12:31,464 --> 00:12:34,984
alerts should be triggered on individual resources to narrow

188
00:12:35,024 --> 00:12:37,844
the problem area and simplify automation.

189
00:12:38,624 --> 00:12:42,752
Other metrics and time windows should serve to determine the exact

190
00:12:42,808 --> 00:12:44,404
cause of the alert. Further,

191
00:12:46,184 --> 00:12:49,812
all resource metadata and all alert conditions

192
00:12:49,908 --> 00:12:53,300
should be captured to provide detailed context, helping to

193
00:12:53,332 --> 00:12:57,540
discover the cause. Notification channel

194
00:12:57,692 --> 00:13:01,076
should be resilient, asynchronous and support

195
00:13:01,180 --> 00:13:04,708
formatting of all alert information in

196
00:13:04,756 --> 00:13:07,304
parsable format such as JSON.

197
00:13:09,684 --> 00:13:13,100
Now it is time to demonstrate how

198
00:13:13,172 --> 00:13:17,344
actionable alerts help with automation. Of the responses.

199
00:13:18,184 --> 00:13:21,432
To those of you who are not familiar with Google Cloud,

200
00:13:21,528 --> 00:13:25,184
don't worry. Most of cloud providers have alerting

201
00:13:25,224 --> 00:13:28,284
solutions that let you automate responses.

202
00:13:30,704 --> 00:13:34,048
Alerting solution in Google Cloud is part of cloud monitoring

203
00:13:34,096 --> 00:13:38,392
service. Cloud monitoring service implements

204
00:13:38,448 --> 00:13:42,056
alerting policy using the three component

205
00:13:42,120 --> 00:13:46,176
model when, what and how that we reviewed in

206
00:13:46,200 --> 00:13:49,808
the previous slides. It also supports multiple

207
00:13:49,856 --> 00:13:53,752
notification mechanisms called notification channels,

208
00:13:53,928 --> 00:13:57,352
including sending emails, texting to a phone number,

209
00:13:57,488 --> 00:14:00,680
asynchronous messaging, using pub sub service,

210
00:14:00,872 --> 00:14:04,792
posting to slack channel, Google Chat, and integration

211
00:14:04,848 --> 00:14:08,536
with pagerduty. An application used for

212
00:14:08,560 --> 00:14:12,256
this demo has a single HTTP endpoint and is deployed

213
00:14:12,280 --> 00:14:16,558
to cloud functions. Cloud functions is a serverless solution

214
00:14:16,646 --> 00:14:20,430
to run your code similar to lambda in AWS and

215
00:14:20,462 --> 00:14:24,166
serverless functions in Azure. You can see the code

216
00:14:24,190 --> 00:14:26,914
of the application in Go on this slide.

217
00:14:27,414 --> 00:14:31,398
Besides initialization code, it has a single function that

218
00:14:31,446 --> 00:14:36,154
returns ping string along with 200 status

219
00:14:36,614 --> 00:14:40,634
for requests coming to path ping, and it

220
00:14:40,674 --> 00:14:44,226
responds with a 204 status to any other

221
00:14:44,290 --> 00:14:48,314
path. The alerting policy condition can

222
00:14:48,354 --> 00:14:51,946
be implemented using proprietary cloud monitoring query

223
00:14:52,010 --> 00:14:55,570
language. As you can see, it requires some

224
00:14:55,642 --> 00:14:59,014
deciphering to make it easier.

225
00:15:00,234 --> 00:15:04,002
Let's look at the same condition written using Prometheus query

226
00:15:04,058 --> 00:15:07,394
language. Cloud monitoring supports querying

227
00:15:07,474 --> 00:15:11,614
metrics and defining alert conditions using ProMQL.

228
00:15:12,424 --> 00:15:16,328
If you are not familiar with both these languages, you will

229
00:15:16,376 --> 00:15:19,404
have to take my word for what this query does.

230
00:15:20,824 --> 00:15:24,536
The condition uses the predefined metric of the cloud

231
00:15:24,640 --> 00:15:27,524
functions that counts function executions.

232
00:15:28,784 --> 00:15:32,512
This metric has a label status which

233
00:15:32,568 --> 00:15:35,936
gets a value ok each time a

234
00:15:35,960 --> 00:15:39,548
function is executed with httpstatus

235
00:15:39,696 --> 00:15:43,184
in the range 200 to 299.

236
00:15:44,724 --> 00:15:49,104
Following the best practices that were previously explained.

237
00:15:49,404 --> 00:15:51,624
This condition tracks the rate,

238
00:15:52,524 --> 00:15:56,428
the ratio sorry between error responses and

239
00:15:56,476 --> 00:15:59,504
all valid responses of the function execution.

240
00:15:59,924 --> 00:16:03,104
When this ratio is above 20%,

241
00:16:03,804 --> 00:16:07,294
an alert is raised. The alert policy

242
00:16:07,754 --> 00:16:11,330
uses an email notification

243
00:16:11,402 --> 00:16:16,134
channel to send information about the alert in the human readable format.

244
00:16:16,914 --> 00:16:20,274
The email is sent once the condition is fulfilled.

245
00:16:20,434 --> 00:16:24,066
So what would be the best course of action to mitigate such

246
00:16:24,130 --> 00:16:28,774
alert when an email is received? There are several scenarios.

247
00:16:30,274 --> 00:16:34,172
In our case, we want to review how

248
00:16:34,228 --> 00:16:38,524
to automate the response, so let's simplify the demo

249
00:16:38,644 --> 00:16:42,148
and assume that we are enabling this

250
00:16:42,196 --> 00:16:45,820
alert each time we deploy a new version

251
00:16:45,852 --> 00:16:49,364
of cloud function. Such thing can

252
00:16:49,404 --> 00:16:52,784
be easily incorporated into deployment pipeline.

253
00:16:53,804 --> 00:16:57,044
In this case, the fastest mitigation would be rolling

254
00:16:57,124 --> 00:17:00,576
to the previous version of the function. Let's now check if

255
00:17:00,600 --> 00:17:04,568
we can automate the response to this alert following our

256
00:17:04,616 --> 00:17:07,924
decision tree. The alert is actionable.

257
00:17:08,624 --> 00:17:12,472
It monitors the resource because luckily for

258
00:17:12,528 --> 00:17:16,040
us, our application and the resource are essentially the

259
00:17:16,072 --> 00:17:20,104
same. It captures the resource metadata and alert context

260
00:17:20,184 --> 00:17:24,004
as out of the box feature of the cloud alerting solution.

261
00:17:24,764 --> 00:17:28,492
It has a notification channel supporting

262
00:17:28,548 --> 00:17:32,132
sending asynchronous messages reliably, which is

263
00:17:32,148 --> 00:17:36,268
a pub sub service that also can send all

264
00:17:36,356 --> 00:17:40,356
the data about alert in the JSON form. The conditions

265
00:17:40,420 --> 00:17:43,916
of the alert let us deterministically pin the cause of the

266
00:17:43,940 --> 00:17:47,852
alert to the fact that the current version of cloud function generates

267
00:17:47,908 --> 00:17:51,620
too many errors. It makes the cause of the alert

268
00:17:51,692 --> 00:17:53,664
100% predictable.

269
00:17:54,454 --> 00:17:57,646
Mind that we dont need to find out the root cause of

270
00:17:57,670 --> 00:18:01,582
the problem. The cause of alert we are interested

271
00:18:01,638 --> 00:18:05,150
in should be sufficient to effectively mitigate the problem

272
00:18:05,222 --> 00:18:09,046
as fast as possible. And at last we can rollback

273
00:18:09,070 --> 00:18:12,958
to the previous version of cloud functions without human intervention

274
00:18:13,126 --> 00:18:16,718
using cloud API in this demo,

275
00:18:16,806 --> 00:18:20,650
the rollback is implementing using cloud build job

276
00:18:20,762 --> 00:18:24,674
cloud build job can be triggered by receiving a pub sub message

277
00:18:24,834 --> 00:18:28,490
sent via alert notification channel. The script

278
00:18:28,562 --> 00:18:31,494
of the job is not important for this demo.

279
00:18:31,874 --> 00:18:35,370
If you are interested in the implementation, you can find the

280
00:18:35,402 --> 00:18:39,650
script in my post on medium using the link shown

281
00:18:39,722 --> 00:18:41,454
in the wrap up slide.

282
00:18:42,994 --> 00:18:46,668
What you see right now is a Google Cloud console

283
00:18:46,716 --> 00:18:50,300
window. First thing we need to do

284
00:18:50,452 --> 00:18:54,348
is to modify alert policy and to

285
00:18:54,396 --> 00:18:58,012
add notification channels that sends notifications

286
00:18:58,108 --> 00:19:02,092
to the cloud. Pubsub I've

287
00:19:02,148 --> 00:19:05,588
already created and defined the pubsub

288
00:19:05,716 --> 00:19:09,788
topic that will be receiving the notifications.

289
00:19:09,956 --> 00:19:14,254
For our convenience, it is straightforward.

290
00:19:14,634 --> 00:19:17,454
We save the changes.

291
00:19:17,874 --> 00:19:21,154
Next we go to cloud build and create a new

292
00:19:21,194 --> 00:19:24,858
trigger that

293
00:19:24,946 --> 00:19:28,594
automates that implements

294
00:19:28,634 --> 00:19:32,586
automation. The trigger is working

295
00:19:32,730 --> 00:19:37,894
by receiving notifications from the pub sub topic

296
00:19:38,254 --> 00:19:43,070
and notifications that

297
00:19:43,102 --> 00:19:47,714
are handled use the same topic that was used for

298
00:19:48,934 --> 00:19:52,794
notification channel. Now we

299
00:19:53,574 --> 00:19:57,294
simulate the change in the source

300
00:19:57,334 --> 00:20:01,726
code of the cloud function. The current function

301
00:20:01,870 --> 00:20:05,530
responds with 200 statuses.

302
00:20:05,602 --> 00:20:09,378
We change it to respond on any request with

303
00:20:09,426 --> 00:20:13,178
the 400

304
00:20:13,266 --> 00:20:17,074
status. It will take some time to

305
00:20:17,114 --> 00:20:20,634
deploy. Once it deploys, we will

306
00:20:20,674 --> 00:20:25,210
be able to see changes in the alert

307
00:20:25,282 --> 00:20:26,334
monitoring.

308
00:20:28,314 --> 00:20:32,054
When we look into the alert

309
00:20:32,554 --> 00:20:36,282
policy, we can see a dashboard

310
00:20:36,458 --> 00:20:40,174
that actually shows tracking the changes.

311
00:20:43,954 --> 00:20:47,934
Let's check that cloud function. Successfully deployed?

312
00:20:48,634 --> 00:20:49,694
Not yet.

313
00:20:51,954 --> 00:20:55,170
Okay, cloud function is deployed and

314
00:20:55,202 --> 00:20:57,974
it starts responding with errors.

315
00:20:59,134 --> 00:21:03,474
Now we can return to alert dashboard.

316
00:21:05,014 --> 00:21:08,886
Let's fast forward a

317
00:21:08,910 --> 00:21:12,314
little so we'll be able to see what is going on.

318
00:21:14,134 --> 00:21:18,142
The responses are not immediate because they

319
00:21:18,158 --> 00:21:21,790
are aggregated by the infrastructure and

320
00:21:21,822 --> 00:21:25,534
now you can see the incident was reported.

321
00:21:27,234 --> 00:21:31,214
If we go to the cloud job, we can see

322
00:21:32,754 --> 00:21:36,374
a triggered job script executing right now.

323
00:21:36,874 --> 00:21:40,354
What the job does, it just roll backs to the previous

324
00:21:40,434 --> 00:21:43,534
version of the cloud function.

325
00:21:45,994 --> 00:21:49,706
As you can see, the version is already in the

326
00:21:49,730 --> 00:21:53,386
process of deploying and in

327
00:21:53,410 --> 00:21:56,954
a moment we will fast forward to

328
00:21:56,994 --> 00:22:00,250
see that the original

329
00:22:00,322 --> 00:22:03,714
version that responds with 200 statuses is

330
00:22:03,754 --> 00:22:04,334
there.

331
00:22:09,234 --> 00:22:13,454
Returning back to the alerting window,

332
00:22:14,634 --> 00:22:22,632
we can see that after

333
00:22:22,688 --> 00:22:26,744
metric returned to normal below

334
00:22:26,824 --> 00:22:30,392
the threshold alerting solution

335
00:22:30,448 --> 00:22:33,952
will automatically mark the incident

336
00:22:34,048 --> 00:22:37,120
as closed. Let me summarize.

337
00:22:37,272 --> 00:22:40,888
Efficient alerts are the key to quick detection

338
00:22:40,976 --> 00:22:45,112
and response to discovered threats. Not every alert

339
00:22:45,168 --> 00:22:49,440
is efficient. Validate your alert configurations

340
00:22:49,632 --> 00:22:52,984
to make sure they respond to actual problems that

341
00:22:53,024 --> 00:22:56,808
affect your users and not your systems. Narrow your

342
00:22:56,856 --> 00:23:00,884
conditions to maximize chances for detecting alert calls.

343
00:23:01,624 --> 00:23:06,224
Capture service and environment context with the alert and

344
00:23:06,264 --> 00:23:09,840
reach alert information with additional materials that

345
00:23:09,872 --> 00:23:13,244
will be helpful in responding to the alert notifications.

346
00:23:14,394 --> 00:23:17,690
Use auditable notification methods

347
00:23:17,762 --> 00:23:22,650
that are able reliably deliver your alert information for

348
00:23:22,682 --> 00:23:26,210
timely response. You have to make your alert

349
00:23:26,282 --> 00:23:30,362
actionable in order to be able to implement automatic responses,

350
00:23:30,538 --> 00:23:33,774
but not every actionable alert can be automated.

351
00:23:34,434 --> 00:23:37,962
Look to automate when the cause of alert is deterministic and

352
00:23:37,978 --> 00:23:41,214
the response does not require human intervention.

353
00:23:41,944 --> 00:23:46,176
Be aware of your service provider capabilities to optimize alert

354
00:23:46,240 --> 00:23:50,256
implementation. If you have any

355
00:23:50,320 --> 00:23:53,792
questions or comments about the stock, other feedback,

356
00:23:53,848 --> 00:23:57,464
or you just want to get in touch, scan this QR code

357
00:23:57,504 --> 00:24:00,564
and write your feedback in the anonymous Google form.

358
00:24:01,064 --> 00:24:01,584
Thank you.

