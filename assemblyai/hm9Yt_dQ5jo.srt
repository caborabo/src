1
00:00:27,644 --> 00:00:30,836
My name is Alex. Today I'm going to talk about practical strategies for

2
00:00:30,860 --> 00:00:34,772
navigating complexity in distributed systems. Here's a quick agenda

3
00:00:34,788 --> 00:00:37,508
of what I'm planning to talk about today. So first of all, we'll take a

4
00:00:37,516 --> 00:00:41,076
look at what this distributed system is. What are the main complexities of

5
00:00:41,100 --> 00:00:45,260
building such systems? Core principles of system engineering and cybernetics,

6
00:00:45,372 --> 00:00:48,732
and the three practices for reducing complexity overhead.

7
00:00:48,868 --> 00:00:53,228
So first of all, let me define what distributed system is. The attributed

8
00:00:53,276 --> 00:00:56,440
system is simply a network of computers which communicate

9
00:00:56,472 --> 00:00:59,528
with each other to achieve a common goal. Each computer,

10
00:00:59,616 --> 00:01:02,904
or you may name it as not, had its own local memory

11
00:01:02,944 --> 00:01:06,120
and cpu and run its own processes. All of

12
00:01:06,152 --> 00:01:10,324
them communicate with each other via network to coordinate their actions.

13
00:01:10,944 --> 00:01:14,208
The main differences between distributed and centralized systems

14
00:01:14,256 --> 00:01:17,592
is that in centralized computing systems, all computing

15
00:01:17,648 --> 00:01:21,524
is performed by a single computer in one location.

16
00:01:21,954 --> 00:01:25,490
Also, nodes of a centralized system all access

17
00:01:25,562 --> 00:01:29,282
the central node. I think that's obvious, which can lead to network congestion

18
00:01:29,338 --> 00:01:32,858
and slowness. A centralized system has a single

19
00:01:32,906 --> 00:01:37,266
point of failure or it's represented as a single point of failure, while destroy

20
00:01:37,290 --> 00:01:41,494
the system by design has no single point of failure.

21
00:01:42,674 --> 00:01:46,234
But before we start looking at real complexities of building destroy the systems,

22
00:01:46,274 --> 00:01:49,214
I want to talk a bit about what complexity is.

23
00:01:49,754 --> 00:01:53,426
And complexity can be defined from different point of views and aspects.

24
00:01:53,450 --> 00:01:57,386
To be honest, in terms of software engineering, there are two main definitions which are

25
00:01:57,410 --> 00:01:59,978
important for us. So in systems theory,

26
00:02:00,106 --> 00:02:03,770
complexity describes how various independent parts of the system

27
00:02:03,882 --> 00:02:06,534
interact and communicate with each other.

28
00:02:07,074 --> 00:02:09,986
Where from software and technology perspective,

29
00:02:10,090 --> 00:02:13,690
complexity refers to the details of software architecture, such as the

30
00:02:13,722 --> 00:02:18,084
number of components, how they define interactions between each other,

31
00:02:19,024 --> 00:02:22,288
how many dependencies they have, and also how they interact within

32
00:02:22,336 --> 00:02:26,800
the whole system. I think monolithic architecture

33
00:02:26,832 --> 00:02:30,760
is a great example of centralized system. It's represented as a single

34
00:02:30,832 --> 00:02:34,160
deployable and single executable component.

35
00:02:34,272 --> 00:02:37,616
For instance, such component may contain user interface,

36
00:02:37,720 --> 00:02:39,884
different modulus in one place,

37
00:02:41,384 --> 00:02:45,104
and sometimes even more and

38
00:02:45,144 --> 00:02:48,728
more modular. Although this architecture is a traditional one

39
00:02:48,856 --> 00:02:52,328
for building a software, I think over time and even today.

40
00:02:52,496 --> 00:02:56,244
But it has several important drawbacks we need to talk about.

41
00:02:56,664 --> 00:03:00,520
So first of all, the main complexity or

42
00:03:00,552 --> 00:03:03,816
main disadvantage of monolithic architecture is inability to

43
00:03:03,840 --> 00:03:08,224
scale modulus independently. Another problem is

44
00:03:08,344 --> 00:03:11,960
that over time it's much harder to control growing

45
00:03:11,992 --> 00:03:15,688
complexity of this big system. The lack of modulus independent

46
00:03:15,736 --> 00:03:19,208
deployment also brings a lot of complexities because you will be

47
00:03:19,216 --> 00:03:23,616
needed to redeploy everything because it's a single executable component.

48
00:03:23,760 --> 00:03:27,072
And also additional challenges are

49
00:03:27,248 --> 00:03:30,920
bringing to developers to maintain this single huge database

50
00:03:30,952 --> 00:03:34,752
without well established rules. And basically

51
00:03:34,808 --> 00:03:38,520
these rules are not very well established and it's really complicated.

52
00:03:38,672 --> 00:03:42,560
And also another problem which is quite obvious, technologies and vendors

53
00:03:42,632 --> 00:03:46,234
coupling, which is quite, quite obvious because you cannot just

54
00:03:46,274 --> 00:03:50,074
have several languages in one monolithic application because it

55
00:03:50,114 --> 00:03:53,374
technically has different runtimes and so on, so forth.

56
00:03:54,354 --> 00:03:57,714
If we'll move further, we can take a look on the opposite

57
00:03:57,754 --> 00:04:01,650
one. This is a microservice architecture, where microservice architecture

58
00:04:01,722 --> 00:04:04,818
is an architecture style and one of the variants of

59
00:04:04,866 --> 00:04:08,874
service oriented architecture. We structure the system as a collection

60
00:04:08,914 --> 00:04:12,610
of loosely complete services. For instance, in the

61
00:04:12,642 --> 00:04:15,578
same example, companies accounts,

62
00:04:15,666 --> 00:04:19,770
customers and user interface are represented as separate processes

63
00:04:19,882 --> 00:04:22,574
deployed on several nodes.

64
00:04:23,474 --> 00:04:26,754
And all of these services has its own

65
00:04:26,794 --> 00:04:30,090
database. Time and time shared database is utilized,

66
00:04:30,282 --> 00:04:33,714
but that probably is a bad practice

67
00:04:33,754 --> 00:04:38,322
or anti pattern for microservices architecture. So in this case I demonstrated how the

68
00:04:38,418 --> 00:04:41,574
different and database for services utilized.

69
00:04:42,594 --> 00:04:46,050
So, but what do distributed systems gives

70
00:04:46,082 --> 00:04:49,602
us, to be honest, the main, I think,

71
00:04:49,658 --> 00:04:52,994
obvious fact that technically we can get

72
00:04:53,034 --> 00:04:57,362
some horizontal scalability traits of such system. You can scale database horizontally,

73
00:04:57,458 --> 00:05:02,034
you can scale your services horizontally, or technically any infrastructure component

74
00:05:02,074 --> 00:05:05,736
can be scaled horizontally. Just technically cloning that,

75
00:05:05,840 --> 00:05:09,204
putting some kind of load balancer between all of that and

76
00:05:09,944 --> 00:05:13,240
leave without a lot of pain. Another thing which is quite important

77
00:05:13,312 --> 00:05:16,800
is about high availability and fault tolerance. Because in

78
00:05:16,832 --> 00:05:20,552
this way, whenever you have several clones, you may organize some

79
00:05:20,648 --> 00:05:24,804
failure techniques will help you to avoid any downtimes

80
00:05:25,744 --> 00:05:29,416
in case of crashes or some memory leaks

81
00:05:29,560 --> 00:05:33,364
and services outages. Another thing is quite important

82
00:05:33,444 --> 00:05:36,724
about your geographic distribution. We all have

83
00:05:36,764 --> 00:05:40,140
customers all the world, in USA, in Europe,

84
00:05:40,212 --> 00:05:43,460
in Asia, and we also want to bring some best experience to our

85
00:05:43,492 --> 00:05:47,052
customers. That's why we'll be needed to anyway to distribute these

86
00:05:47,108 --> 00:05:50,700
services across the whole world and organize some more

87
00:05:50,732 --> 00:05:54,364
complicated techniques for data replication, how to connect all of that

88
00:05:54,484 --> 00:05:57,812
together. Another thing which is also

89
00:05:57,868 --> 00:06:01,276
quite obvious is about technology choice freedom. Because you can deliver

90
00:06:01,380 --> 00:06:05,080
one service with this tool and other service with another tool, you can

91
00:06:05,232 --> 00:06:08,792
try more. You can find the best solution for the current problem.

92
00:06:08,928 --> 00:06:12,304
You can scale it horizontally based on

93
00:06:12,344 --> 00:06:15,992
some special techniques, special solutions

94
00:06:16,128 --> 00:06:19,936
and so on, so forth. Another thing which is quite important

95
00:06:20,040 --> 00:06:23,368
to know and to note about

96
00:06:23,496 --> 00:06:26,840
easier architecture control. It really helps to

97
00:06:26,872 --> 00:06:30,082
control the system, because when there is only

98
00:06:30,138 --> 00:06:34,306
single component time and time rules are violated,

99
00:06:34,490 --> 00:06:38,418
everything is broken. You need to dig into this component, find some

100
00:06:38,466 --> 00:06:41,898
balance, make an order and so on, so forth. When this some kind

101
00:06:41,946 --> 00:06:45,894
of little bit isolated from each other, several small services,

102
00:06:46,194 --> 00:06:50,234
very likely these

103
00:06:50,354 --> 00:06:53,814
services will be not very messed with some changes

104
00:06:54,234 --> 00:06:58,484
from a big group of developers working on the same component.

105
00:06:59,064 --> 00:07:02,120
Any distributed system comes with its own challenges.

106
00:07:02,152 --> 00:07:05,264
We'll take a look right now. So before

107
00:07:05,304 --> 00:07:08,192
we take a look into challenges, I just want to talk a little bit about

108
00:07:08,248 --> 00:07:11,640
quality attributes we'd like to have in our system. So there are three

109
00:07:11,672 --> 00:07:15,256
main quality attributes which any system has at

110
00:07:15,280 --> 00:07:19,568
some level or another level. First of all, it's reliability.

111
00:07:19,736 --> 00:07:23,726
Reliability is a way to continue work correctly even when

112
00:07:23,790 --> 00:07:27,870
things go wrong, meaning to be fault tolerant or resilient.

113
00:07:28,022 --> 00:07:31,150
Even when a system is working reliably today, that doesn't

114
00:07:31,182 --> 00:07:34,654
mean it will necessarily work reliably in the future. One common reason

115
00:07:34,694 --> 00:07:38,230
for degradation usually is increased lots or perhaps the system

116
00:07:38,262 --> 00:07:42,582
has grown from 10,000 concurrent requests or users to

117
00:07:42,758 --> 00:07:46,054
100,000 current users, or run from 1 million

118
00:07:46,094 --> 00:07:49,414
to 10 million. Scalability is the term we used to

119
00:07:49,454 --> 00:07:53,154
describe a system's ability to handle just an increased load.

120
00:07:53,464 --> 00:07:57,144
And the final piece of that is about maintainability, is about

121
00:07:57,224 --> 00:08:01,008
making our life better. For the engineering and

122
00:08:01,056 --> 00:08:04,456
operations teams who need to work with the system, good and stable

123
00:08:04,520 --> 00:08:08,256
abstractions really help to reduce complexity overhead

124
00:08:08,400 --> 00:08:11,736
and make the system much, much easier to modify and

125
00:08:11,760 --> 00:08:15,032
adapt for new features. So this is the

126
00:08:15,048 --> 00:08:18,592
system example. I just want to show you of

127
00:08:18,608 --> 00:08:22,360
the system which has various components such as API gateways,

128
00:08:22,472 --> 00:08:26,376
memory caches, message queues, different databases,

129
00:08:26,480 --> 00:08:29,728
different microservices, full text search and so on, so forth.

130
00:08:29,776 --> 00:08:33,216
And all of that, in order to evolve, should be

131
00:08:33,240 --> 00:08:37,048
reliable, scalable and maintainable. So far it sounds easy,

132
00:08:37,096 --> 00:08:40,552
but this is a famous low and

133
00:08:40,568 --> 00:08:43,920
very realistic law, to be honest, and the best friend of

134
00:08:43,952 --> 00:08:47,648
distributed systems. In some formulations it's extended to

135
00:08:47,816 --> 00:08:51,764
anything that can go wrong will go wrong, and the worst

136
00:08:52,374 --> 00:08:55,758
possible time is it just because we're

137
00:08:55,806 --> 00:08:59,198
not really lucky or infrastructure is not. The reality

138
00:08:59,246 --> 00:09:02,982
is different. And now let's take a look at the real complexities

139
00:09:03,038 --> 00:09:06,910
with distributed system. What are the main troubles?

140
00:09:07,102 --> 00:09:10,302
First of all, networks are unreliable.

141
00:09:10,438 --> 00:09:13,474
We'll take a look on it deeper little bit later on.

142
00:09:14,014 --> 00:09:17,702
But just from a high level perspective, you cannot completely rely on networks because

143
00:09:17,798 --> 00:09:20,984
so many things are, are out of your control.

144
00:09:21,104 --> 00:09:24,464
You cannot say for sure that it will be working in 100% cases.

145
00:09:24,624 --> 00:09:28,120
Another thing which is quite important is about unreliable clocks

146
00:09:28,152 --> 00:09:31,984
and problems with time synchronization. Another thing which is

147
00:09:32,024 --> 00:09:35,344
also quite painful for us, it's about process pauses.

148
00:09:35,464 --> 00:09:39,368
And let's take a look deeper on this one. And I think many of

149
00:09:39,416 --> 00:09:43,432
you who was working with the systems having garbage collection

150
00:09:43,568 --> 00:09:46,820
probably was thinking about this problem as well. Another thing

151
00:09:46,852 --> 00:09:50,980
which quite obvious is about eventual consistency. Whenever we have some horizontal

152
00:09:51,052 --> 00:09:54,404
scalability of our database, we also have some follower,

153
00:09:54,564 --> 00:09:58,244
we have a leader, and whenever data is replicated from leader

154
00:09:58,284 --> 00:10:02,172
to followers there is some lag which technically may affect some

155
00:10:02,228 --> 00:10:06,196
systems, some logic, user experience and so on, so forth. It's not a problem

156
00:10:06,260 --> 00:10:10,204
from one perspective because it gives you a lot of availability

157
00:10:10,324 --> 00:10:13,492
benefits, but it can be a problem from a different perspective where you need to

158
00:10:13,508 --> 00:10:16,916
achieve really strong consistency. Another thing which

159
00:10:16,940 --> 00:10:19,424
is quite also important is about observability.

160
00:10:19,844 --> 00:10:23,156
Whenever we build a system on this huge scale with

161
00:10:23,180 --> 00:10:26,764
a lot of services, databases, queues, we need to understand how

162
00:10:26,844 --> 00:10:30,516
all of that is working. We need to have all insights about how

163
00:10:30,540 --> 00:10:34,228
it's working and how we can get everything about right

164
00:10:34,276 --> 00:10:38,572
now for that and how performance is tracked.

165
00:10:38,708 --> 00:10:42,476
Do we have any bottlenecks, errors and so on, so forth? This is a

166
00:10:42,500 --> 00:10:45,732
big and broad topic. We will take a look on some of

167
00:10:45,788 --> 00:10:49,756
these aspects which probably can be helpful in building such systems.

168
00:10:49,900 --> 00:10:52,384
And another one is about evolvability.

169
00:10:52,764 --> 00:10:56,900
It's some kind of extension of maintainability, but technically describes

170
00:10:57,052 --> 00:11:00,212
how to evolve the system or distribute the

171
00:11:00,268 --> 00:11:03,892
systems without a lot of problems and without a lot of

172
00:11:03,948 --> 00:11:07,436
pain, and how to make it really scalable for people, for engineers,

173
00:11:07,500 --> 00:11:10,724
for the business. And don't waste a lot of resources

174
00:11:10,804 --> 00:11:14,982
and be stuck because something it was really designed

175
00:11:15,038 --> 00:11:18,766
very well and very bad and it cannot be scaled due to some

176
00:11:18,870 --> 00:11:22,638
issue in the system technically. This topic is also very

177
00:11:22,686 --> 00:11:26,262
broad and we'll take a look on several very useful aspects you may

178
00:11:26,438 --> 00:11:30,446
probably find. And first of all,

179
00:11:30,590 --> 00:11:33,606
as I said, let's take a look on unreliable networks.

180
00:11:33,710 --> 00:11:37,278
So there are so many reasons why networks are not reliable.

181
00:11:37,366 --> 00:11:41,476
So there are a few reasons why so. First of all your request may have

182
00:11:41,500 --> 00:11:45,020
been lost and you cannot do nothing about that because

183
00:11:45,212 --> 00:11:48,324
this is a problem of network. Another thing, your request

184
00:11:48,364 --> 00:11:51,964
may be waiting in the queue and will be just delivered later on,

185
00:11:52,084 --> 00:11:56,508
and you also cannot completely rely that it will be processed immediately.

186
00:11:56,636 --> 00:11:59,292
The remote node may have failed,

187
00:11:59,428 --> 00:12:02,212
probably, perhaps it crashed or powers down,

188
00:12:02,308 --> 00:12:06,168
or some kind of memory leak happens and so on, so forth.

189
00:12:06,316 --> 00:12:10,080
Remote not have may have temporarily stopped responding.

190
00:12:10,232 --> 00:12:14,024
The same problem with the second

191
00:12:14,064 --> 00:12:18,008
I mentioned about waiting in the queue, but it does happen on the not

192
00:12:18,056 --> 00:12:21,416
being this case also, remote not may have processed

193
00:12:21,440 --> 00:12:24,848
your request, but the response has been lost on the network

194
00:12:24,936 --> 00:12:29,016
and you technically just don't know was it processed or it wasn't processed.

195
00:12:29,080 --> 00:12:32,452
What you need to do right now and so on, so forth. And technically the

196
00:12:32,468 --> 00:12:36,036
modnot may have even processed your request and again the response has

197
00:12:36,060 --> 00:12:40,324
been delayed. Now you still have to wait for a while to get response

198
00:12:40,364 --> 00:12:43,504
and make some logic on your side.

199
00:12:44,724 --> 00:12:48,228
So the obvious thing how to probably

200
00:12:48,276 --> 00:12:51,652
solve the problem of request loss is to apply tam out

201
00:12:51,708 --> 00:12:54,940
logic on the caller side. For example, if the caller doesn't

202
00:12:54,972 --> 00:12:58,500
receive response after some timeout, we can just throw an exception

203
00:12:58,532 --> 00:13:02,592
that shows an error to the user. In most cases probably would be okay

204
00:13:02,688 --> 00:13:06,744
mainly for the back office I guess, but for the customers and have availability systems

205
00:13:06,784 --> 00:13:10,264
when we don't want to have some poorer user experience,

206
00:13:10,424 --> 00:13:14,752
we want to make something to solve this problem. And obvious

207
00:13:14,808 --> 00:13:19,064
solution for this problem is to because

208
00:13:19,104 --> 00:13:22,680
the network issue is very very likely temporary. At scale we can

209
00:13:22,712 --> 00:13:26,528
just apply, or even scale we can just apply retry pattern.

210
00:13:26,656 --> 00:13:29,800
So if the response indicates that something

211
00:13:29,872 --> 00:13:32,790
goes wrong, we can just retry it after timeout.

212
00:13:32,822 --> 00:13:36,286
So technically if timeouts happened, we like it's a temporary issue problem

213
00:13:36,390 --> 00:13:40,126
and we can just apply retry to process it again.

214
00:13:40,310 --> 00:13:44,006
But what if the request technically was processed

215
00:13:44,030 --> 00:13:47,326
by the server and only the response was lost?

216
00:13:47,430 --> 00:13:51,110
So in this scenario, to be honest with tries may lead to severe

217
00:13:51,222 --> 00:13:54,142
consequences like several orders,

218
00:13:54,278 --> 00:13:56,990
payments, transactions and so on, so forth.

219
00:13:57,102 --> 00:14:00,356
And that experience will be not very nice for the

220
00:14:00,380 --> 00:14:04,252
users. So the technique

221
00:14:04,348 --> 00:14:07,500
to avoid this problem, we can call it,

222
00:14:07,532 --> 00:14:11,212
or technically called so edompinac

223
00:14:11,228 --> 00:14:14,780
is a concept. We're doing the same thing multiple times has

224
00:14:14,812 --> 00:14:17,852
the same effect as doing just once.

225
00:14:18,028 --> 00:14:21,564
To achieve exactly one semantics, we can leverage a solution

226
00:14:21,604 --> 00:14:24,836
that attaches, especially in the potency key to the

227
00:14:24,860 --> 00:14:28,612
request. So after trying the same request with the identical

228
00:14:28,668 --> 00:14:32,534
identity key, the the server will verify that the request with such key

229
00:14:32,574 --> 00:14:35,806
has already been processed and we will simply return the

230
00:14:35,830 --> 00:14:39,278
previous response. Such any number of retries with the same key

231
00:14:39,326 --> 00:14:42,678
won't affect system behavior negatively,

232
00:14:42,806 --> 00:14:46,262
and that technique helps in most of the cases. And this

233
00:14:46,278 --> 00:14:49,966
is one of the main techniques you have to bear in mind while building

234
00:14:50,030 --> 00:14:53,910
distributed systems. Another pattern which

235
00:14:53,982 --> 00:14:57,510
might be quite useful in preventing overloading and completely

236
00:14:57,582 --> 00:15:01,104
crushing the server in case of failover, outage or

237
00:15:01,144 --> 00:15:04,792
temporary issues is circuit breaker.

238
00:15:04,888 --> 00:15:08,824
Circuit breaker acts as a proxy to prevent the calling system,

239
00:15:08,944 --> 00:15:12,920
which is probably under maintenance, likely will fail

240
00:15:13,032 --> 00:15:16,504
or heavily failing right now. So there are so many reasons

241
00:15:16,544 --> 00:15:20,296
why it can go wrong, memory leak, bug in the code, or external dependency

242
00:15:20,360 --> 00:15:24,216
which are faulted. So in such scenarios it's better to fail

243
00:15:24,320 --> 00:15:27,952
fast rather than risk of getting cascading fails

244
00:15:28,008 --> 00:15:31,594
over the system. So technically, circuit breaker represents some

245
00:15:31,634 --> 00:15:35,466
intermediate component which is checking based on some specific conditions

246
00:15:35,570 --> 00:15:39,802
and just switching it off and on when it's allowed

247
00:15:39,898 --> 00:15:43,546
based on some specific condition. Concurrency is one

248
00:15:43,570 --> 00:15:47,546
of the most intricate challenges in distributed systems. Concurrency means

249
00:15:47,610 --> 00:15:51,098
that multiple computations are happening at the same time.

250
00:15:51,186 --> 00:15:54,498
So what happens if we try to update the account balance at the same

251
00:15:54,546 --> 00:15:58,194
time from different operations? If there is no defensive

252
00:15:58,274 --> 00:16:01,578
mechanism applied, race conditions very likely will

253
00:16:01,626 --> 00:16:04,954
happen, which will lead to lost rights and data

254
00:16:05,034 --> 00:16:08,786
inconsistencies. In this example, you can see the top region are trying to update the

255
00:16:08,810 --> 00:16:12,226
account balance, and since they are concurrently running these operations,

256
00:16:12,410 --> 00:16:16,610
the last one is winning, which led to serious issues.

257
00:16:16,802 --> 00:16:20,730
And to avoid this problem, there are several techniques can

258
00:16:20,802 --> 00:16:24,604
be used. So before diving

259
00:16:24,644 --> 00:16:28,220
into the problem solution, let's take a look at what

260
00:16:28,292 --> 00:16:31,556
ASIP means. So the ACES acronym stands

261
00:16:31,580 --> 00:16:35,100
for atomicity, consistency, isolation and durability.

262
00:16:35,292 --> 00:16:38,756
All of the popular SQL databases implement these properties,

263
00:16:38,900 --> 00:16:42,412
but what do they mean? So atomicity specifies

264
00:16:42,508 --> 00:16:46,196
that the operation will be either completely executed

265
00:16:46,340 --> 00:16:49,560
or entirely failed. No matter at what stage it happens.

266
00:16:49,672 --> 00:16:53,088
It allows us to be ensured that another thread

267
00:16:53,136 --> 00:16:56,824
cannot see the half finished result of the operation. In very simple

268
00:16:56,864 --> 00:17:01,072
terms, consistency means that all invariants are

269
00:17:01,248 --> 00:17:04,936
defined and will be satisfied before successfully committing a

270
00:17:04,960 --> 00:17:08,336
transaction and changing the state of the database or

271
00:17:08,360 --> 00:17:11,724
just specific table or particular part of the data store.

272
00:17:12,024 --> 00:17:15,480
Whether isolation in the sense of faceit

273
00:17:15,552 --> 00:17:18,960
means that concurrently executing transactions are isolated from

274
00:17:18,992 --> 00:17:22,672
each other. This is a serializable isolation level, which is the strictest

275
00:17:22,768 --> 00:17:26,720
one to process all transactions sequentially. But another

276
00:17:26,792 --> 00:17:30,360
level named snapshot isolation. In popular databases like SQL

277
00:17:30,392 --> 00:17:34,072
Server, MySQL and Postgres, SQL and I

278
00:17:34,088 --> 00:17:37,632
believe many others is mainly widely used and we will talk about it

279
00:17:37,648 --> 00:17:41,216
very soon. So durability promises that once transaction

280
00:17:41,240 --> 00:17:45,556
is committed, all the data is stored safely. So that

281
00:17:45,580 --> 00:17:49,012
is very important point for us to be sure that we

282
00:17:49,068 --> 00:17:52,372
sort everything without any losses. So returning back to

283
00:17:52,388 --> 00:17:55,540
the concurrency problem, so the most practical solution is utilized,

284
00:17:55,572 --> 00:17:59,064
snapshot isolation level, which most of the popular databases

285
00:18:00,204 --> 00:18:03,932
with ACET compliant database provide. So I

286
00:18:03,948 --> 00:18:07,180
just thought about that. So the key of this level is the database

287
00:18:07,212 --> 00:18:11,036
track records version and fails to commit transactions

288
00:18:11,180 --> 00:18:14,300
for once that were already modified outside of the

289
00:18:14,332 --> 00:18:18,188
current transaction. And for all read operations

290
00:18:18,276 --> 00:18:22,332
in a transaction, see a consistent version of the data. And as

291
00:18:22,348 --> 00:18:25,788
it was at the start of this transaction for write operations,

292
00:18:25,916 --> 00:18:29,788
changes made by transaction are not visible to other transactions

293
00:18:29,916 --> 00:18:33,188
until the initial transactions commit. And that helps a lot to be

294
00:18:33,196 --> 00:18:37,112
honest, simplifies so many things during development,

295
00:18:37,288 --> 00:18:41,080
and it also guarantees you that if you managed

296
00:18:41,112 --> 00:18:44,856
it properly, you won't be losing anything and you

297
00:18:44,880 --> 00:18:48,204
will be quite consistent in your database and your daytime.

298
00:18:49,304 --> 00:18:53,856
Another thing which is also quite important to say that most

299
00:18:53,880 --> 00:18:58,152
of the no SQL databases are not providing acid properties completely

300
00:18:58,208 --> 00:19:01,674
right now while choosing in favor of base.

301
00:19:02,214 --> 00:19:05,534
In such cases, the compare and say the

302
00:19:05,614 --> 00:19:09,382
strategy may be utilized. So for instance, where base means basically

303
00:19:09,438 --> 00:19:13,074
available soft state and eventually consistent,

304
00:19:14,254 --> 00:19:17,694
the purpose of this operation, I mean compare and set,

305
00:19:17,854 --> 00:19:20,950
is to avoid lost updates. Of course,

306
00:19:21,022 --> 00:19:24,638
by allowing an update to happen only if the value

307
00:19:24,726 --> 00:19:28,366
has not changed since your last read it. If the

308
00:19:28,390 --> 00:19:31,994
current value does not match with what you previously read,

309
00:19:32,374 --> 00:19:36,934
the update has no effect and the read modify write cycle must be retried.

310
00:19:37,054 --> 00:19:40,886
So for instance, Cassandra provides lightweight transactions approach which

311
00:19:40,910 --> 00:19:44,702
allows you to utilize various if if not exist if exist

312
00:19:44,758 --> 00:19:48,262
statements to prevent concurrency issues. So please bear

313
00:19:48,278 --> 00:19:51,670
in mind this strategy will be very helpful if you're utilizing knowledgeable

314
00:19:51,702 --> 00:19:55,422
database and you need to be aware about how to track how to

315
00:19:55,478 --> 00:19:59,814
avoid this problem with concurrency, which may lead to really problematic situations

316
00:19:59,854 --> 00:20:02,954
in your software and the business as well.

317
00:20:03,614 --> 00:20:07,150
Also, I think we can take a look on another pattern which

318
00:20:07,182 --> 00:20:10,766
may be quite helpful. So another technique that might be useful is

319
00:20:10,790 --> 00:20:14,126
called lease. Let's imagine we have some kind of resource we want to

320
00:20:14,150 --> 00:20:17,502
update exclusively, and the lease pattern means that we

321
00:20:17,518 --> 00:20:21,006
should first obtain the lease, which with an expiration period

322
00:20:21,070 --> 00:20:24,890
for this specific resource, then update the resource

323
00:20:24,962 --> 00:20:28,794
and finally return back the list. In case of failures or

324
00:20:28,914 --> 00:20:32,146
anything else, the list will be automatically expired,

325
00:20:32,330 --> 00:20:35,730
allowing another threat to access the resource.

326
00:20:35,922 --> 00:20:39,858
Also, this technique is very useful and it will help you.

327
00:20:40,026 --> 00:20:43,130
In most of the cases there is a risk of process

328
00:20:43,202 --> 00:20:46,298
pauses and clock desynchronization, which may lead to issues with

329
00:20:46,346 --> 00:20:49,558
parallel resource access, and we'll take a look at it a

330
00:20:49,566 --> 00:20:52,926
little bit later on. So if you'll try to send a

331
00:20:52,950 --> 00:20:57,126
message during transaction, we'll find ourselves in the worst situation

332
00:20:57,310 --> 00:21:00,638
where for some reason transaction failed to commit. So technically,

333
00:21:00,726 --> 00:21:03,806
the dual write problem is a challenge that arises in

334
00:21:03,830 --> 00:21:08,214
distributed systems, mainly when dealing with multiple data sources

335
00:21:08,374 --> 00:21:12,142
or databases we need to capt and sync or just update.

336
00:21:12,238 --> 00:21:15,358
So in other words, let us imagine we need to store the new data in

337
00:21:15,366 --> 00:21:18,452
the database and then send some messages to the kafka. So since

338
00:21:18,508 --> 00:21:22,148
these two operations are not atomic, there is a

339
00:21:22,316 --> 00:21:25,276
chance of failure during the publishing of new messages.

340
00:21:25,420 --> 00:21:29,364
So based on this example, if we'll be trying

341
00:21:29,524 --> 00:21:33,020
to send some data, we will find really ourselves in the worst

342
00:21:33,052 --> 00:21:37,108
situation. For some reason transaction failed to commit, but external

343
00:21:37,156 --> 00:21:40,812
system like Kafka didn't get these messages,

344
00:21:40,908 --> 00:21:44,024
another system may be really disrupted.

345
00:21:44,524 --> 00:21:48,516
In this case, if we'll be trying to send the messages during transaction directly

346
00:21:48,540 --> 00:21:51,788
to Kafka and only then trying to commit transaction.

347
00:21:51,916 --> 00:21:55,372
We'll find ourselves in the worst situation when external

348
00:21:55,428 --> 00:21:58,652
systems are already notified but we

349
00:21:58,668 --> 00:22:01,996
didn't commit transaction on our site and depends

350
00:22:02,020 --> 00:22:05,516
on the business depends on the business and critical operation criticality of this

351
00:22:05,540 --> 00:22:09,120
operation. We may find ourselves in really, really interesting situations,

352
00:22:09,252 --> 00:22:12,960
very likely to lead to some very nice discussions with

353
00:22:12,992 --> 00:22:16,336
legals and trying to find some solution. But technically this is a really

354
00:22:16,360 --> 00:22:20,296
bad practice and you have to avoid this at any time and

355
00:22:20,400 --> 00:22:24,648
at any situation to not being involved in so many war rooms and solving

356
00:22:24,696 --> 00:22:28,464
problems. So one possible solution to implement

357
00:22:28,624 --> 00:22:32,168
and solve this problem is to implement transactional outbox. So this

358
00:22:32,216 --> 00:22:35,820
involves storing events in an outbox table within

359
00:22:35,852 --> 00:22:39,492
the same transaction at the operation itself. Because of

360
00:22:39,628 --> 00:22:43,644
atomicity, nothing will be stored in case of transaction failure.

361
00:22:43,804 --> 00:22:47,412
One more component needed here as well is relay

362
00:22:47,508 --> 00:22:51,300
which will be polling the outbox event table at some regular intervals

363
00:22:51,372 --> 00:22:55,788
and send the messages to destinations. Such approach allows

364
00:22:55,876 --> 00:22:58,852
to achieve at least one delivery guarantee.

365
00:22:58,988 --> 00:23:02,620
However, it's not a big problem since all the consumers must

366
00:23:02,652 --> 00:23:06,516
be independent anyway due to network possible failure and

367
00:23:06,540 --> 00:23:09,716
just network and reliability. An alternative

368
00:23:09,780 --> 00:23:13,548
solution instead of building custom transactional box is to utilize

369
00:23:13,596 --> 00:23:16,692
database, transactional lock and custom connectors to

370
00:23:16,708 --> 00:23:20,460
read directly from this lock and send these changes to exactly

371
00:23:20,532 --> 00:23:24,300
destinations. This approach has its own advantages and disadvantages.

372
00:23:24,412 --> 00:23:27,244
For instance, it requires to be coupled to database solutions,

373
00:23:27,364 --> 00:23:30,464
but that allows you to write less code in the application.

374
00:23:31,124 --> 00:23:34,524
Another problem we have to take a look. It's about

375
00:23:34,604 --> 00:23:38,146
unreliable clocks. The time tracking is one

376
00:23:38,170 --> 00:23:41,914
of the most important parts of any software or infrastructure,

377
00:23:41,954 --> 00:23:45,914
since you always need to track durations to enforce timeouts and

378
00:23:46,074 --> 00:23:49,570
expirations, or even gathering metrics to understand how your

379
00:23:49,602 --> 00:23:52,986
system operates. Unreliable clocks is one of the most tricky

380
00:23:53,010 --> 00:23:57,194
problems in distributed systems since time accuracy heavily depends

381
00:23:57,234 --> 00:24:01,002
on computer performance, since each machine has its own clock,

382
00:24:01,058 --> 00:24:04,432
which could be faster or slower than others.

383
00:24:04,618 --> 00:24:07,884
So there are two types of clocks used by computers, time of day

384
00:24:07,924 --> 00:24:11,100
and monotonic clocks. Timeofday returns date and

385
00:24:11,132 --> 00:24:15,540
time according to some calendar, which will lead to clocks desynchronization

386
00:24:15,732 --> 00:24:19,612
in case of if NTP server

387
00:24:19,668 --> 00:24:22,664
is desynchronized from our machine,

388
00:24:23,404 --> 00:24:26,700
monotonic clocks always move forward and that

389
00:24:26,812 --> 00:24:30,292
helps in many cases for calculating durations, so they can

390
00:24:30,308 --> 00:24:33,660
be really useful in such cases when we need to calculate how much time that

391
00:24:33,692 --> 00:24:37,462
required took on our machine and this monotonically

392
00:24:37,518 --> 00:24:41,214
increased value. But this value is unique per

393
00:24:41,254 --> 00:24:45,014
computer, so technically it can be used for multiple server comparison

394
00:24:45,094 --> 00:24:48,766
of date and time. Technically, there are not

395
00:24:48,790 --> 00:24:52,366
so many easy solutions to achieve very accurate clock

396
00:24:52,430 --> 00:24:55,806
synchronization. In most of the cases you don't need it, to be

397
00:24:55,830 --> 00:24:59,750
honest, but in situations where it's required by some specific regulations,

398
00:24:59,782 --> 00:25:03,466
where we are building very, very accurate system for

399
00:25:03,650 --> 00:25:07,394
probably tracking transactions or showing some charts

400
00:25:07,434 --> 00:25:10,970
with real time data, precision time protocol can be leveraged,

401
00:25:11,082 --> 00:25:15,154
which to balance requires a lot of huge investment of

402
00:25:15,194 --> 00:25:18,618
that. Now take a look on the cap theorem.

403
00:25:18,706 --> 00:25:22,226
And the cap theorem states that any distributed data

404
00:25:22,290 --> 00:25:26,134
store can satisfy only two of three requirements

405
00:25:26,544 --> 00:25:31,000
or guarantees in this case. So however, since network unreliability

406
00:25:31,072 --> 00:25:34,632
is not something you or someone else can significantly

407
00:25:34,688 --> 00:25:37,784
impact, in the case of network partitions,

408
00:25:37,864 --> 00:25:41,764
we have to choose between availability or consistency.

409
00:25:42,344 --> 00:25:46,088
So in this simple diagram, two clients read from different nodes,

410
00:25:46,176 --> 00:25:49,816
one from the primary node and another one from the follower.

411
00:25:49,920 --> 00:25:53,112
And replication is configured to update followers after the

412
00:25:53,128 --> 00:25:57,250
leader was being has been changed. But what happens

413
00:25:57,282 --> 00:26:00,214
if for some reason the leader stops responding?

414
00:26:00,834 --> 00:26:04,866
It could be a crash, network partitioning or

415
00:26:04,930 --> 00:26:08,346
any another issue. In highly available systems, a new

416
00:26:08,370 --> 00:26:12,018
leader has to be assigned. But how do we choose

417
00:26:12,066 --> 00:26:15,058
between existing followers? To solve this problem,

418
00:26:15,226 --> 00:26:18,778
we need to use some kind of distributed consensus

419
00:26:18,866 --> 00:26:22,686
algorithm. But before we look at it, let's review several major

420
00:26:22,750 --> 00:26:25,950
consistency types. There are two main classes

421
00:26:25,982 --> 00:26:29,646
of consistency used to describe guarantees, weak consistency

422
00:26:29,830 --> 00:26:34,126
or eventually consistency, and another

423
00:26:34,190 --> 00:26:38,134
one is strong consistency. Eventually consistency means that the data will be synchronized

424
00:26:38,214 --> 00:26:41,990
on all the followers after some times if you will

425
00:26:42,142 --> 00:26:46,634
stop making changes on the leader talking about strong consistency,

426
00:26:46,994 --> 00:26:50,930
I think the better to explain it in this way where

427
00:26:51,042 --> 00:26:55,010
a linearizable system, as soon as one client successfully completes

428
00:26:55,042 --> 00:26:58,834
a write, all clients reading from the database might

429
00:26:58,874 --> 00:27:02,530
be able to see the value just written. So this is a

430
00:27:02,642 --> 00:27:06,354
quote from Marcin Klatman book about

431
00:27:06,434 --> 00:27:10,178
designing data intensive applications and it perfectly describes

432
00:27:10,226 --> 00:27:14,484
what it is strong consistency. This is a synonym of for linear usability,

433
00:27:15,384 --> 00:27:19,444
how we can decide and how we can solve this problem with

434
00:27:20,744 --> 00:27:23,764
selecting leader by having existing followers.

435
00:27:24,424 --> 00:27:28,072
So returning back to the problem I just mentioned about,

436
00:27:28,168 --> 00:27:32,264
when leader has crashed, there is need to select or elect

437
00:27:32,344 --> 00:27:35,696
a new leader. So this problem at first glance probably looks

438
00:27:35,720 --> 00:27:39,518
easy, but in reality there are so many conditions and trade offs

439
00:27:39,696 --> 00:27:43,522
that have to be taken into account when selecting the appropriate

440
00:27:43,578 --> 00:27:47,654
approach. Per rough protocol, if followers

441
00:27:48,234 --> 00:27:51,922
do not receive data or heartbeat from the leader for a specified

442
00:27:51,978 --> 00:27:55,178
period of time, then a new leader election process begins.

443
00:27:55,346 --> 00:27:58,906
So distributed consensus is an algorithm for getting

444
00:27:58,970 --> 00:28:03,146
notes agree on something. So in our case agree on a new leader.

445
00:28:03,290 --> 00:28:07,310
So when these leaders election processes started,

446
00:28:07,482 --> 00:28:11,054
raft specifies a lot of instructions and techniques,

447
00:28:11,094 --> 00:28:14,886
how to achieve the consensus in case of network partitions

448
00:28:14,950 --> 00:28:17,710
as well. So technically each replication unit,

449
00:28:17,782 --> 00:28:21,542
it can be monolith, write node or multiple

450
00:28:21,598 --> 00:28:24,846
shards. It really depends on the infrastructure is associated with a set

451
00:28:24,870 --> 00:28:28,942
of rough logs and OS processes that maintains

452
00:28:28,998 --> 00:28:33,022
the logs and replicates changes from leader to followers. So the

453
00:28:33,038 --> 00:28:36,786
rough protocol guarantees that receive lock records in

454
00:28:36,810 --> 00:28:40,394
the same order they are generated by the leader. A user

455
00:28:40,434 --> 00:28:44,282
transaction is committed on the leader as soon as the half of the follower

456
00:28:44,338 --> 00:28:47,610
acknowledges the receipt of the commit record and writes

457
00:28:47,642 --> 00:28:51,426
it to the raft log. So technically, raft specifies how we

458
00:28:51,450 --> 00:28:54,962
can really implement this distributed consensus and how we

459
00:28:54,978 --> 00:28:58,690
can solve this problem of selecting a new leader. There are many other

460
00:28:58,762 --> 00:29:02,506
algorithms, like Paxos in

461
00:29:02,530 --> 00:29:06,610
many others. So in this case I just highlighted that

462
00:29:06,802 --> 00:29:10,346
raft is just an example. But destiny with the consensus algorithm is

463
00:29:10,370 --> 00:29:13,722
very very important. And without it, it's impossible to solve the problem

464
00:29:13,778 --> 00:29:16,614
of highly available software.

465
00:29:17,514 --> 00:29:21,410
And it has a lot of much more complexities. Let's imagine we have several

466
00:29:21,482 --> 00:29:24,658
data centers and we need to organize some kind of replication.

467
00:29:24,826 --> 00:29:28,674
One is located in USA, another located in

468
00:29:28,714 --> 00:29:32,552
Asia, and we don't want to ask users to make write

469
00:29:32,608 --> 00:29:36,832
requests from Asia to USA. We like to have several data

470
00:29:36,888 --> 00:29:40,576
centers which can accept rights. To achieve that

471
00:29:40,600 --> 00:29:44,080
we need to have some kind of multi leader application and apply some

472
00:29:44,112 --> 00:29:48,176
conflicts resolution strategy which will help us to achieve consistent way

473
00:29:48,240 --> 00:29:51,680
of the data. Also ordering is quite

474
00:29:51,752 --> 00:29:55,664
problematic thing, so it requires

475
00:29:55,744 --> 00:29:59,298
to generating some monotonically increasing numbers which will be

476
00:29:59,346 --> 00:30:03,094
assigned to any message in this replication specific mechanism.

477
00:30:03,594 --> 00:30:06,914
So let's take a look on eventual consistency.

478
00:30:07,074 --> 00:30:10,154
So this is a classic example of eventual consistency,

479
00:30:10,234 --> 00:30:14,034
which demonstrates situations when the user sees the absence of

480
00:30:14,074 --> 00:30:16,254
new jack created transactions.

481
00:30:17,034 --> 00:30:20,226
So in this case, first of all we're trying to

482
00:30:20,290 --> 00:30:24,026
insert some data in the leader node. Then of course it's replicated to the

483
00:30:24,050 --> 00:30:27,858
follower. And whenever we do a second request to the follower

484
00:30:27,986 --> 00:30:31,434
during two eventual consistency aspects, we probably may

485
00:30:31,474 --> 00:30:35,354
found that the transaction was not even created and be really

486
00:30:35,394 --> 00:30:38,890
really confused. So in this case probably it's not very critical because

487
00:30:39,042 --> 00:30:42,730
eventually it will be synchronized. But in

488
00:30:42,762 --> 00:30:45,602
more complicated scenarios when we need to make some logic,

489
00:30:45,658 --> 00:30:48,954
for instance, we cannot finance the company if it was added

490
00:30:48,994 --> 00:30:52,538
in some sanctions list. We cannot rely on eventual consistency

491
00:30:52,586 --> 00:30:55,754
because it may lead to some problems in the business.

492
00:30:56,494 --> 00:30:59,838
And one of the possible solutions to apply here,

493
00:30:59,886 --> 00:31:04,022
and it's probably one, it's quite effective and simple strategies,

494
00:31:04,078 --> 00:31:07,614
is to read from the follower by the user who just saved new data

495
00:31:07,654 --> 00:31:11,342
to avoiding this replication lag. And it helps in most of

496
00:31:11,358 --> 00:31:15,078
the cases. For more complicated scenarios, of course it's better take a

497
00:31:15,086 --> 00:31:18,230
look on the cases and trying to define the best strategy how to

498
00:31:18,262 --> 00:31:22,014
solve that. Another thing is about process spouses

499
00:31:22,054 --> 00:31:25,794
and let's take a look on this. So this is a quite dangerous problem and

500
00:31:25,954 --> 00:31:29,482
that may be even garbage collection stop the world situation

501
00:31:29,578 --> 00:31:33,850
happened, or virtual machine suspension or context switch

502
00:31:33,962 --> 00:31:37,794
took more time than we expected, or some

503
00:31:37,834 --> 00:31:41,642
delay in the network that led to unexpected

504
00:31:41,738 --> 00:31:45,610
pauses of the system which affected the logic. So if such thing

505
00:31:45,722 --> 00:31:49,730
happens under the cured list, for instance, we discussed

506
00:31:49,802 --> 00:31:53,436
this couple of slides ago, there's a chance to

507
00:31:53,460 --> 00:31:57,904
access the same resource twice, which may lead to severe consequences.

508
00:31:58,204 --> 00:32:02,228
To overcome this, there's a safety technique

509
00:32:02,276 --> 00:32:05,196
called fencing that could be leveraged. So technically,

510
00:32:05,260 --> 00:32:08,756
before updating the resource, we need to secure the lease as

511
00:32:08,780 --> 00:32:12,140
we did it previously, but without just occurring.

512
00:32:12,172 --> 00:32:15,868
The lease would like to get some kind of token and this token

513
00:32:15,956 --> 00:32:20,044
will be used on the data store to prevent updating the data

514
00:32:20,124 --> 00:32:24,288
or resource in case of such talking was already processed

515
00:32:24,336 --> 00:32:27,952
or it was already expired. But technically it means like we need to generate a

516
00:32:27,968 --> 00:32:31,856
monotonic increasing token. Of course we can attach some key here

517
00:32:32,000 --> 00:32:35,344
and whenever we update the resource, we check that the

518
00:32:35,384 --> 00:32:39,176
current token is the last token from the sequence. If some,

519
00:32:39,240 --> 00:32:42,004
if some another token in the past was already,

520
00:32:42,784 --> 00:32:46,484
if the time was already processed or this token was

521
00:32:47,184 --> 00:32:51,222
expired will be needed to throw an exception and solve this problem

522
00:32:51,328 --> 00:32:54,874
on the data storage. But probably it's better to take a

523
00:32:55,374 --> 00:32:59,810
look at the problem as well. Like whenever one client secured the lease for

524
00:32:59,842 --> 00:33:03,226
some reason garbage collection stop, the world may happen

525
00:33:03,290 --> 00:33:06,938
and we will be delaying until the lease expires.

526
00:33:06,986 --> 00:33:10,698
Another client will be getting the same lease under the same key

527
00:33:10,826 --> 00:33:14,626
and also will be started doing some operation and maybe the client

528
00:33:14,730 --> 00:33:18,174
to finish the operation. But when

529
00:33:18,254 --> 00:33:22,054
client one slapped out, it will be also

530
00:33:22,134 --> 00:33:25,766
trying to update the storage because it was already under release.

531
00:33:25,950 --> 00:33:29,942
And in this case we may probably have some issues and

532
00:33:30,078 --> 00:33:33,214
failures. So in this case, this fencing technique to

533
00:33:33,254 --> 00:33:36,678
having some monotonically increasing token and checking that

534
00:33:36,726 --> 00:33:40,238
there is no such token in there processed in the

535
00:33:40,326 --> 00:33:44,374
past, but this sequential number should

536
00:33:44,414 --> 00:33:47,638
be applied to this problem. It probably may be

537
00:33:47,726 --> 00:33:51,150
some theoretical problem, but it may happen.

538
00:33:51,262 --> 00:33:55,206
And it's better to bear in mind for really, really critical operations when we

539
00:33:55,270 --> 00:33:59,622
want to avoid anything related to transaction processing

540
00:33:59,798 --> 00:34:03,422
and so on, so forth. So as

541
00:34:03,438 --> 00:34:07,398
I said in the beginning, observability is one of the most important things in distributed

542
00:34:07,446 --> 00:34:10,526
systems. So in this example, you can see a distributed

543
00:34:10,550 --> 00:34:15,190
operation that spans transactions across multiple components

544
00:34:15,302 --> 00:34:18,734
and we need an easy mechanism for getting this information to

545
00:34:18,774 --> 00:34:22,246
effectively determine bottlenecks and have full visibility or

546
00:34:22,310 --> 00:34:26,046
executed attributed operation. The best

547
00:34:26,150 --> 00:34:29,390
thing to do that, or probably one of the best thing, is distributed

548
00:34:29,422 --> 00:34:33,862
tracing is an approach for tracking this distributed operation across multiple

549
00:34:33,918 --> 00:34:36,234
microservices nodes components.

550
00:34:37,334 --> 00:34:41,222
The key idea is to attach a unique destroyed operation id to

551
00:34:41,238 --> 00:34:45,148
the initial request, then span it down the road

552
00:34:45,276 --> 00:34:49,388
and finally collect and aggregate this unique id to build analytical

553
00:34:49,436 --> 00:34:51,424
visualization of the whole.

554
00:34:53,764 --> 00:34:56,956
For instance, Jager keeps track of all connections and

555
00:34:56,980 --> 00:35:00,180
provides charts and graphs to visualize request

556
00:35:00,252 --> 00:35:04,028
path in the application. So technically it has multiple

557
00:35:04,076 --> 00:35:08,304
components which is providing collecting of these destabilization ids

558
00:35:08,734 --> 00:35:12,318
and building some visualization to effectively take a

559
00:35:12,326 --> 00:35:16,070
look into operation and understanding the bottlenecks, the problems

560
00:35:16,142 --> 00:35:19,886
and so on, so forth. This is a visualization example

561
00:35:19,950 --> 00:35:23,414
which helps to analyze the bottlenecks and probably

562
00:35:23,454 --> 00:35:26,966
just to understand where my request is slow, how I can

563
00:35:27,030 --> 00:35:30,374
find the point in the system where I need to apply

564
00:35:30,414 --> 00:35:33,950
some optimization techniques and so on, so forth. It's very useful and

565
00:35:33,982 --> 00:35:37,506
helpful whenever I need to debug complicated scenarios and to

566
00:35:37,530 --> 00:35:40,614
be sure that you are not missing something very important.

567
00:35:42,034 --> 00:35:45,874
Another approach that may help to improve the systems observability is

568
00:35:45,914 --> 00:35:49,586
orchestration techniques rather than choreography. Orchestration in

569
00:35:49,610 --> 00:35:53,090
software engineering involves a central controller or orchestration

570
00:35:53,162 --> 00:35:57,254
engine that manages the interaction integration between services.

571
00:35:57,634 --> 00:36:00,922
Choreography refers to a decentralized approach

572
00:36:01,058 --> 00:36:04,098
where services interact based on navans. In this

573
00:36:04,146 --> 00:36:07,974
model, each service knows when to execute its operation and

574
00:36:08,274 --> 00:36:11,970
with whom to interact without needing to central point

575
00:36:12,042 --> 00:36:15,682
of control. Orchestration is usually the profitable approach,

576
00:36:15,738 --> 00:36:19,906
to be honest, for controlling growing complexity due to increased visibility

577
00:36:20,090 --> 00:36:23,794
characteristics and just having a place for controlling.

578
00:36:23,834 --> 00:36:27,890
Of course, it must be highly available because it can become a

579
00:36:27,922 --> 00:36:31,474
bottleneck or single point of failure. So high availability

580
00:36:31,554 --> 00:36:35,070
techniques must be applied in place to avoid any

581
00:36:35,102 --> 00:36:39,246
problems. So let's take a look about evolvability

582
00:36:39,350 --> 00:36:43,182
and cybernetics principles. So, in the context of software engineering,

583
00:36:43,238 --> 00:36:47,630
cybernetics can be defined as a study or science and

584
00:36:47,822 --> 00:36:51,294
application of feedback loops, control systems and

585
00:36:51,334 --> 00:36:55,726
communication processes within software development and operational environments.

586
00:36:55,830 --> 00:37:00,238
It focuses on how systems, software, hardware processes

587
00:37:00,286 --> 00:37:03,798
anything. Human interactions as well, can be designed and managed to achieve

588
00:37:03,846 --> 00:37:07,926
desired goals through self regulation. Adapting and learning.

589
00:37:08,070 --> 00:37:11,230
Cybernetics and software engineering emphasizes

590
00:37:11,342 --> 00:37:15,462
creating systems that can adjust to changes,

591
00:37:15,598 --> 00:37:19,158
learn from many interactions, and improve over time,

592
00:37:19,246 --> 00:37:22,710
ensuring reliability, efficiency and resilience. To be honest,

593
00:37:22,822 --> 00:37:26,814
that thing explains what we are doing as a software engineers

594
00:37:26,854 --> 00:37:30,900
or architects to manage the really complicated evolving

595
00:37:30,972 --> 00:37:34,548
of the system. So speaking about several

596
00:37:34,676 --> 00:37:37,716
principles of that, the first of all is systems thinking.

597
00:37:37,820 --> 00:37:41,244
So this concept focuses on the system as a whole rather

598
00:37:41,284 --> 00:37:45,044
than individual parts. So in software engineering, it technically

599
00:37:45,124 --> 00:37:48,252
means that all the parts

600
00:37:48,268 --> 00:37:51,836
of a pair are really important, how they interact with each other. And we need

601
00:37:51,860 --> 00:37:55,304
to have a holistic view about how it's working together,

602
00:37:55,734 --> 00:37:59,502
how it's evolving, how many dependencies we have, having some kind of dependency

603
00:37:59,558 --> 00:38:03,150
review over time and check all that. And design decisions are

604
00:38:03,182 --> 00:38:07,102
made with this understanding applied

605
00:38:07,198 --> 00:38:10,894
to the system itself. Without just considering one component,

606
00:38:10,934 --> 00:38:13,854
we're considering the system as a whole thing.

607
00:38:13,974 --> 00:38:17,254
So, example, when service b handles an event published by service

608
00:38:17,334 --> 00:38:21,278
a for the outcome does not really affect service

609
00:38:21,406 --> 00:38:25,176
a directly. However, the overall result of the

610
00:38:25,200 --> 00:38:28,840
operation is a significant problem to the system

611
00:38:28,912 --> 00:38:32,504
at the whole piece. So another thing is

612
00:38:32,544 --> 00:38:36,192
feedback loops. This is a core concept, awareness of cybernetics,

613
00:38:36,248 --> 00:38:39,464
and this involves implementing or just

614
00:38:39,504 --> 00:38:42,984
having feedback loops to understand how to control

615
00:38:43,104 --> 00:38:46,640
and stabilize the system. So in software architecture and systems,

616
00:38:46,672 --> 00:38:51,018
feedback loops may be represented in various forms like having retrospectives,

617
00:38:51,186 --> 00:38:54,418
having monitoring system performance, user feedback through

618
00:38:54,466 --> 00:38:58,378
different questionnaires and so on, so forth, and also having mechanism

619
00:38:58,506 --> 00:39:01,258
like continuous delivery,

620
00:39:01,306 --> 00:39:04,674
continuous integration and deployment to get feedback

621
00:39:04,754 --> 00:39:08,698
earlier and making decisions about how to make it better. So this

622
00:39:08,826 --> 00:39:12,242
principle is one of the important principles in the software engineering from

623
00:39:12,298 --> 00:39:16,474
cybernetics perspective. So another thing is about adaptability and

624
00:39:16,514 --> 00:39:19,864
learning. So cybernetics Pro promotes the idea that

625
00:39:19,904 --> 00:39:23,672
systems should be capable of adopting the changes. It also relates to

626
00:39:23,728 --> 00:39:27,784
maintainability and vulvability. And that means that

627
00:39:27,824 --> 00:39:31,536
we need to apply different techniques to evolve our system

628
00:39:31,600 --> 00:39:35,528
without lots of obstacles, and having specific

629
00:39:35,576 --> 00:39:39,408
techniques for incorporating inside the system to get some feedback

630
00:39:39,456 --> 00:39:43,352
as well, and learning from our mistakes from our existing metrics.

631
00:39:43,408 --> 00:39:45,884
So technically this is evolution of feedback loops.

632
00:39:46,734 --> 00:39:50,070
Talking about this stuff related to goal oriented

633
00:39:50,102 --> 00:39:54,150
design, this is one more principle in cybernetics. And cybernetic systems

634
00:39:54,222 --> 00:39:57,534
are often defined by their goals. So in context

635
00:39:57,574 --> 00:40:01,686
of software architecture, this means that the system should be designed

636
00:40:01,710 --> 00:40:04,758
with clear objectives in mind. Domain driven

637
00:40:04,806 --> 00:40:08,350
design is a software development methodology that focuses

638
00:40:08,382 --> 00:40:12,350
on building a domain model with a deep comprehensive understanding

639
00:40:12,382 --> 00:40:16,214
of domain processes and rules. So technically

640
00:40:16,294 --> 00:40:19,982
it helps to build the software which is aligned with the real

641
00:40:20,038 --> 00:40:24,398
business domains and business requirements drives architecture

642
00:40:24,446 --> 00:40:27,702
decisions like in this case, we can see a lot of

643
00:40:27,838 --> 00:40:31,550
different contexts or domains like one

644
00:40:31,582 --> 00:40:35,270
is related to accounting, another one for underwriting fraud profile analysis

645
00:40:35,302 --> 00:40:39,042
on the boarding. So we're technically designing this system which is solving these real

646
00:40:39,098 --> 00:40:42,594
problems from the domain. I believe everyone,

647
00:40:42,674 --> 00:40:46,494
to be honest, is familiar with such pattern or anti pattern

648
00:40:46,834 --> 00:40:50,174
and was involved in building such software.

649
00:40:50,594 --> 00:40:54,162
This is experience everyone had. And I believe

650
00:40:54,298 --> 00:40:57,930
that very helpful to have this experience because it

651
00:40:57,962 --> 00:41:00,930
helps you to understand how you shouldn't build the system.

652
00:41:01,042 --> 00:41:04,412
So this pattern, or again, anti pattern drives,

653
00:41:04,468 --> 00:41:08,412
building system drives without any architecture there

654
00:41:08,428 --> 00:41:12,564
is no rules, boundaries, strategy and how to control inevitable

655
00:41:12,644 --> 00:41:16,428
growing complexity. After some time it becomes really

656
00:41:16,476 --> 00:41:19,836
painful thing and system becomes unmaintainable.

657
00:41:19,940 --> 00:41:24,196
And any change leads to so many painful problems

658
00:41:24,300 --> 00:41:27,844
and of course reducing the revenue or

659
00:41:27,884 --> 00:41:31,466
just improving, increasing a lot of bugs and failures to the system.

660
00:41:31,650 --> 00:41:35,174
So we can apply some techniques to building software with different

661
00:41:35,674 --> 00:41:39,378
types and can really help. So system organized

662
00:41:39,466 --> 00:41:43,666
or in hierarchies or subsystem and

663
00:41:43,730 --> 00:41:47,178
software system often have a hierarchical structure with high level

664
00:41:47,226 --> 00:41:50,474
models, depending on lower level models for functionality.

665
00:41:50,594 --> 00:41:54,442
So this hierarchical decomposition helps really to manage

666
00:41:54,498 --> 00:41:58,104
complexity by breaking down the system into more manageable

667
00:41:58,224 --> 00:42:01,552
parts. But to be able to understand it better we need

668
00:42:01,568 --> 00:42:05,124
to take a look on something which is quite important. So first of all,

669
00:42:05,784 --> 00:42:09,536
it's a quite popular opinion that all microservices are

670
00:42:09,560 --> 00:42:12,960
the same. So technically from the deployment perspective they are the

671
00:42:12,992 --> 00:42:16,848
same. Everything is either virtual

672
00:42:16,896 --> 00:42:20,752
machine deployed or just a docker image and deployed as

673
00:42:20,768 --> 00:42:24,508
a docker container. But perspective

674
00:42:24,556 --> 00:42:27,784
of the system and perspective of the

675
00:42:28,724 --> 00:42:32,292
making a decision. We need to have some service types to make the

676
00:42:32,308 --> 00:42:35,884
decision much much simpler and simplify the thinking

677
00:42:35,924 --> 00:42:39,364
process. We can have multiple types of

678
00:42:39,524 --> 00:42:43,380
services which very likely will be having absolutely different

679
00:42:43,452 --> 00:42:47,364
rules and constraints. Like for instance, we have several services which are

680
00:42:47,484 --> 00:42:50,290
necessary to communicate with the external world.

681
00:42:50,362 --> 00:42:53,922
Like I need to communicate with some front end application which

682
00:42:53,938 --> 00:42:57,610
is technically run in the browser of the user, and I need to accept some

683
00:42:57,682 --> 00:43:01,294
request and move it in the central of the system.

684
00:43:02,074 --> 00:43:06,266
And in this case I defined about six types of services

685
00:43:06,330 --> 00:43:09,698
which may be very very useful. The first type is web application

686
00:43:09,746 --> 00:43:13,934
or mobile application, public API. Technically this is a service which is used by

687
00:43:14,354 --> 00:43:15,414
customers.

688
00:43:16,804 --> 00:43:21,044
Mainly it's started or run in browser,

689
00:43:21,084 --> 00:43:24,764
or it's a desktop application or mobile application. Another thing,

690
00:43:24,844 --> 00:43:28,164
another layer is back and front. I think this is a place where

691
00:43:28,204 --> 00:43:31,452
all incoming requests are coming. Then we are rotating this request

692
00:43:31,508 --> 00:43:34,900
down into the system. Then the third layer is

693
00:43:34,932 --> 00:43:38,364
about product workflows. Whenever we build a system which has some

694
00:43:38,404 --> 00:43:42,656
products, we need to structure some business logic which is really specific

695
00:43:42,740 --> 00:43:46,000
and detailed this product, and we cannot just generalize this

696
00:43:46,032 --> 00:43:49,324
stuff because it's a product specific, this is a unique of this product.

697
00:43:49,744 --> 00:43:53,512
Then we also have some reusable components we need to help,

698
00:43:53,648 --> 00:43:56,944
we need to utilize in our domain, and likely

699
00:43:57,064 --> 00:44:00,480
that we'll be having this components because all the

700
00:44:00,512 --> 00:44:04,304
time we are using some, we are resolving some domain

701
00:44:04,344 --> 00:44:07,752
problems. We need to utilize in several components. For instance,

702
00:44:07,808 --> 00:44:11,344
maybe kyc kybap be anti fraud, or maybe some just payment

703
00:44:11,384 --> 00:44:14,752
service internally which will be helping us to send money and

704
00:44:14,848 --> 00:44:18,104
getting money back. Another thing is about gateways

705
00:44:18,144 --> 00:44:21,464
and adapters. All the time we need to get some data

706
00:44:21,504 --> 00:44:24,800
from external system, we need to integrate with something from the external world.

707
00:44:24,872 --> 00:44:28,216
And in achieving that we need to call this service and the service will be

708
00:44:28,240 --> 00:44:31,864
responsible for getting this responsibility to take some

709
00:44:31,904 --> 00:44:35,706
data or send some data to outside in the bottom of that

710
00:44:35,840 --> 00:44:39,462
always have some kind of platform components. It of course related

711
00:44:39,518 --> 00:44:43,558
to identity access management, some core library configuration monitoring,

712
00:44:43,606 --> 00:44:47,430
even workflow mechanism for managing really complicated distributed

713
00:44:47,462 --> 00:44:50,742
workflows. Another very, very important thing

714
00:44:50,798 --> 00:44:55,038
that industrial systems and

715
00:44:55,166 --> 00:44:58,302
at scale we're starting to solve more and more problems

716
00:44:58,358 --> 00:45:01,894
and we're starting to having some domain areas, some macro

717
00:45:01,934 --> 00:45:05,434
domain areas, and in this areas we can collect the services

718
00:45:06,004 --> 00:45:10,180
or even concepts or even problems together and hide that

719
00:45:10,332 --> 00:45:14,676
behind some domain macro API which will be implemented

720
00:45:14,740 --> 00:45:18,124
or represented as a central point to communicate

721
00:45:18,164 --> 00:45:22,104
with this subdomain. It technically helps on the scale to control

722
00:45:23,444 --> 00:45:27,492
growing of a lot of microservices and be able to manage

723
00:45:27,548 --> 00:45:29,904
this really, really effectively.

724
00:45:30,564 --> 00:45:34,436
Talking about Sre SRe is a site reliability engineering.

725
00:45:34,500 --> 00:45:38,652
This is a foundation framework built by Google really

726
00:45:38,708 --> 00:45:42,244
many years ago. And the core principles of SRE

727
00:45:42,324 --> 00:45:45,732
is about first of all, embrace risk. We need to really understand the risk

728
00:45:45,788 --> 00:45:49,684
where the critical operation, whether it's not critical operation, where we need to understand that

729
00:45:49,724 --> 00:45:53,756
this is really mission critical for the business and we need to apply all the

730
00:45:53,780 --> 00:45:57,484
reliable techniques. If for some reason this technique is not

731
00:45:57,524 --> 00:46:00,824
very necessary or these functionality is not very

732
00:46:01,124 --> 00:46:04,120
critical, probably we don't need to have it at all.

733
00:46:04,252 --> 00:46:08,440
Probably we don't need to put a lot of resources and making more

734
00:46:08,472 --> 00:46:11,044
and more tasks, putting a lot of requirements,

735
00:46:11,424 --> 00:46:15,128
metrics, monitoring and so on, so forth. So technically it means like

736
00:46:15,176 --> 00:46:18,320
we need to understand the risk and we need to manage the risk. We need

737
00:46:18,352 --> 00:46:21,776
to count it from this perspective because of course a deal state

738
00:46:21,880 --> 00:46:25,296
where everything is covered by test everything, but it's

739
00:46:25,320 --> 00:46:29,120
impossible to achieve. So we need to prioritize where we want to put our

740
00:46:29,152 --> 00:46:32,826
resource based on what we have right now. Now another thing is about

741
00:46:32,930 --> 00:46:36,146
SLA, Slo and SLi to define system reliability.

742
00:46:36,330 --> 00:46:40,450
SLA is a service level agreement, SLO service level objective

743
00:46:40,482 --> 00:46:43,778
and SLI service level indicator. In simple terms,

744
00:46:43,906 --> 00:46:47,298
SLA is our agreement with a customer about what we

745
00:46:47,346 --> 00:46:51,266
promised from our system. SLA is an agreement inside the

746
00:46:51,290 --> 00:46:55,250
team. What we are promised to achieve as a team and

747
00:46:55,282 --> 00:46:59,864
SLI is about technique or just measure how

748
00:46:59,904 --> 00:47:03,120
to calculate our SLO and SLA's.

749
00:47:03,232 --> 00:47:06,712
And these techniques or these principles are very

750
00:47:06,768 --> 00:47:10,200
helpful to define the reliability of the system in

751
00:47:10,232 --> 00:47:13,984
order to communicate with the business about how reliable are

752
00:47:14,024 --> 00:47:18,044
we. Another thing is about automate manual work. We need to

753
00:47:18,664 --> 00:47:22,056
ideally to avoid any manual work as possible. Sometimes,

754
00:47:22,080 --> 00:47:25,808
of course it's not really feasible because automation will cost

755
00:47:25,856 --> 00:47:29,306
more. So we need to be wise about when we make such decision.

756
00:47:29,450 --> 00:47:33,274
But ideally we need to automate manual work. Repetitive work and

757
00:47:33,314 --> 00:47:37,066
trying to avoid any kind of complicated work

758
00:47:37,210 --> 00:47:40,802
have to be done by person or

759
00:47:40,898 --> 00:47:44,554
by engineer, or so on, so forth. Another thing is about

760
00:47:44,594 --> 00:47:48,594
monitor everything. Monitor everything means like we need to have visibility

761
00:47:48,714 --> 00:47:51,986
over everything and discuss with like we can implement some kind

762
00:47:52,010 --> 00:47:55,436
of distributed tracing. We need to put some metrics in place to

763
00:47:55,460 --> 00:47:59,196
calculate and measure performance of our microservices

764
00:47:59,300 --> 00:48:02,036
or systems or any infrastructure components.

765
00:48:02,180 --> 00:48:05,484
And the last thing is about simplify as much as you could

766
00:48:05,524 --> 00:48:08,860
do, because any complex system tends to fail

767
00:48:08,932 --> 00:48:12,396
because it just has a lot of components which may

768
00:48:12,580 --> 00:48:16,044
really fail and you not always have control

769
00:48:16,124 --> 00:48:19,904
over everything. So if you can simplify, choose the more

770
00:48:19,944 --> 00:48:22,884
simplified approach, it will help you over time.

771
00:48:23,504 --> 00:48:28,080
So take a look on some things which is quite important about infrastructure

772
00:48:28,112 --> 00:48:31,568
as code. This is a principle which states that we need to define

773
00:48:31,616 --> 00:48:35,560
our infrastructure as a code. Like I'm a developer writing some

774
00:48:35,712 --> 00:48:39,768
new microservice, the same thing I will be using for our infrastructure.

775
00:48:39,896 --> 00:48:43,744
One of the possible solutions is to having some tools

776
00:48:43,904 --> 00:48:47,308
which can allow us to define some templates of resources,

777
00:48:47,436 --> 00:48:51,580
some definitions of these resources, some scripts which will be updating infrastructure.

778
00:48:51,692 --> 00:48:54,812
And this is the following, the ultimate manual

779
00:48:54,868 --> 00:48:58,664
work guideline. So we can use some kind of terraform solution to

780
00:48:59,244 --> 00:49:03,084
write by engineers that will be validated and then applied to our

781
00:49:03,124 --> 00:49:07,012
infrastructure. And of course it will be versioned in some

782
00:49:07,108 --> 00:49:10,824
control like git, mercurial and so on, so forth.

783
00:49:12,504 --> 00:49:16,472
The thing about child engineering is

784
00:49:16,528 --> 00:49:19,044
also quite important for testing as well.

785
00:49:19,504 --> 00:49:23,800
So one technique we can utilize to achieve a

786
00:49:23,832 --> 00:49:27,256
safety of our system is Jobson, a special

787
00:49:27,320 --> 00:49:31,200
tool and framework developed by Kyle

788
00:49:31,232 --> 00:49:35,456
Kingsbury to analyze the safety and consistency of distributed database

789
00:49:35,520 --> 00:49:38,936
stores and systems under various conditions,

790
00:49:39,040 --> 00:49:42,552
particularly focusing on how system behave under network partitions

791
00:49:42,648 --> 00:49:46,304
and other types of failures. And technically, Jemson provides

792
00:49:46,344 --> 00:49:49,888
some, some very useful things like fault injection. It introduces

793
00:49:49,936 --> 00:49:53,600
specific faults in distributed systems and it looks how

794
00:49:53,712 --> 00:49:57,564
the system will behave under these conditions and what will be

795
00:49:58,104 --> 00:50:02,016
happening when a system is broken and so on, so forth. It includes network

796
00:50:02,080 --> 00:50:05,864
partitions where communication between nodes is really

797
00:50:05,984 --> 00:50:10,600
affected and we need to understand how our system will solve

798
00:50:10,632 --> 00:50:14,312
this problem. Another thing about operations testing test

799
00:50:14,368 --> 00:50:17,472
various default scenarios for read writes from leaders

800
00:50:17,568 --> 00:50:21,720
followers and understanding how fast it works and how

801
00:50:21,752 --> 00:50:25,168
our consistency is organized. And of course in currency. This is a

802
00:50:25,176 --> 00:50:29,072
very very important problem we need to cover is about how our system behave

803
00:50:29,168 --> 00:50:34,014
over constrained to simulate real world usage scenarios

804
00:50:34,094 --> 00:50:37,502
and just become more real and understand

805
00:50:37,598 --> 00:50:41,430
the impact of such thing. Finally, I'd like to talk about simplicity

806
00:50:41,462 --> 00:50:45,414
and measuring complexity. So it's really complicated to measure complexity,

807
00:50:45,494 --> 00:50:49,942
but complex system that works in a specific

808
00:50:49,998 --> 00:50:53,950
way can be measured somehow. And a complex system that works is invariably

809
00:50:54,022 --> 00:50:57,634
found to have evolved from a simple system that worked and

810
00:50:57,674 --> 00:51:00,698
that happens all the time. And this is a galls law

811
00:51:00,746 --> 00:51:04,642
about this stuff. And we need to measure complexity whenever

812
00:51:04,698 --> 00:51:08,314
we have possibility for that. So we can apply several techniques to measure this

813
00:51:08,354 --> 00:51:11,498
stuff. We can utilize some cyclomatic complexity which

814
00:51:11,546 --> 00:51:15,666
helps to track how complicated function you have right now and how

815
00:51:15,690 --> 00:51:19,570
you can make it better and much simpler to understand by other developers.

816
00:51:19,722 --> 00:51:23,026
Another thing which is quite important to track is about time

817
00:51:23,090 --> 00:51:27,192
to train. So whenever a new engineer is onboarding,

818
00:51:27,328 --> 00:51:30,528
you need to understand how much time you need to spend to train this

819
00:51:30,576 --> 00:51:34,480
guy to work with this specific solution or stuff. Probably that's not

820
00:51:34,512 --> 00:51:37,960
very great thing you want to have, but that's a very important thing to

821
00:51:37,992 --> 00:51:41,536
track and also explanation time. If you have

822
00:51:41,560 --> 00:51:45,160
very complicated domain and it takes really a lot of time to

823
00:51:45,192 --> 00:51:49,240
explain some details, probably you need to apply some techniques

824
00:51:49,272 --> 00:51:53,458
to simplify this and make it really easy for people

825
00:51:53,626 --> 00:51:57,282
to be on board with and starting to understand the details and techniques

826
00:51:57,298 --> 00:52:00,762
of this domain. So in conclusion, I just want

827
00:52:00,778 --> 00:52:04,858
to say that I covered several techniques related to destroyed

828
00:52:04,906 --> 00:52:07,954
systems. Destroyed systems is a really, really nice

829
00:52:08,034 --> 00:52:11,922
thing. We want to have to achieve highly available and scalable

830
00:52:11,978 --> 00:52:15,362
software, but it brings a lot of complexities we need

831
00:52:15,378 --> 00:52:18,692
to bear in mind, and we cannot just avoid that. We need, need to really

832
00:52:18,868 --> 00:52:22,780
solving this stuff. We need to understand the drawbacks and

833
00:52:22,812 --> 00:52:26,076
how to apply best practices to solve this problem. Thank you for

834
00:52:26,100 --> 00:52:28,196
attending this talk. Looking forward to.

