1
00:00:20,600 --> 00:00:24,006
Hi everyone, welcome to Con 42 Golang. I hope you had

2
00:00:24,030 --> 00:00:26,798
a great time with all the talks until now. I am Yash and today I

3
00:00:26,806 --> 00:00:30,294
am extremely excited to be talking about profile guided optimization

4
00:00:30,374 --> 00:00:33,678
and how it can boost your code and your compiler. So moving

5
00:00:33,726 --> 00:00:37,062
on, let me first of all introduce myself. I am Yash and currently

6
00:00:37,118 --> 00:00:40,782
I am working as a software engineer at Red Hat. I am also pursuing a

7
00:00:40,798 --> 00:00:44,246
masters at University of Waterloo in Canada and

8
00:00:44,310 --> 00:00:47,038
around my work I deal with Openshift, kubernetes,

9
00:00:47,126 --> 00:00:50,714
Golang, containers, docker, cloud, native stuff.

10
00:00:51,724 --> 00:00:55,412
Apart from that, in my free time, whenever I get some time I

11
00:00:55,428 --> 00:00:58,748
do open source dev. I love to do long distance running and I like to

12
00:00:58,756 --> 00:01:02,116
just build stuff. So moving on, let's first of all talk

13
00:01:02,140 --> 00:01:06,304
about compilation, right? So computers, as you all know, don't understand

14
00:01:07,044 --> 00:01:09,984
Golang, python, Java, all that stuff.

15
00:01:10,324 --> 00:01:13,964
Computers only know zeros and ones, that's the binary language.

16
00:01:14,124 --> 00:01:17,572
So there is, there should be an entity which translates a

17
00:01:17,588 --> 00:01:21,302
Golang code to zeros and ones so that the computer can execute

18
00:01:21,318 --> 00:01:24,838
it. And that entity, as most of you would know it,

19
00:01:25,006 --> 00:01:28,526
is called the compiler as shown in the image below. You can see

20
00:01:28,550 --> 00:01:32,286
that on the left hand side there's a Golang code printing hello world and there's

21
00:01:32,310 --> 00:01:35,822
some compiler magic happening which produces the machine code which is

22
00:01:35,838 --> 00:01:39,366
the binary or assembly, and that is fed to the linker to

23
00:01:39,390 --> 00:01:42,686
produce the actual binary or the executable which

24
00:01:42,710 --> 00:01:45,694
you execute to print hello World. Across this talk,

25
00:01:45,734 --> 00:01:49,476
we won't talk much about the linker, we would care and talk more about

26
00:01:49,500 --> 00:01:53,164
the compiler. So let's move on and let's dive a little bit into

27
00:01:53,244 --> 00:01:57,148
the magic inside the compiler. Magic. So compiler is an

28
00:01:57,236 --> 00:02:01,252
amazing piece of tech. I mean it's doing so many things so gracefully and

29
00:02:01,268 --> 00:02:04,396
beautifully, so behind the scenes it does a bunch

30
00:02:04,420 --> 00:02:07,876
of steps. Firstly, it breaks down the code into a bunch of tokens through

31
00:02:07,900 --> 00:02:11,836
the means of a lexer. That's that process is tokenization and

32
00:02:11,860 --> 00:02:15,516
then that is fed and a tree is created out of

33
00:02:15,540 --> 00:02:19,076
that set of tokens and that tree is called abstract syntax tree,

34
00:02:19,100 --> 00:02:22,596
which is a representation of your code. That tree then goes

35
00:02:22,620 --> 00:02:26,116
through a phase of semantic analysis to determine whether the

36
00:02:26,140 --> 00:02:30,316
type checking is happening correctly, whether a code is correct, all that stuff.

37
00:02:30,460 --> 00:02:34,332
And after that a code is converted into a platform

38
00:02:34,388 --> 00:02:37,468
independent representation called IR, which is intermediate

39
00:02:37,516 --> 00:02:41,412
representation, and that is fed to the backend of the compiler

40
00:02:41,548 --> 00:02:44,724
where a bunch of optimizations happening happen and a lot of

41
00:02:44,764 --> 00:02:48,724
magic happens, which by the way, we will dive further into the talk.

42
00:02:48,844 --> 00:02:52,412
And finally, this optimized IR is fed to

43
00:02:52,428 --> 00:02:56,864
the machine code generator which actually produces the machine code and

44
00:02:57,444 --> 00:03:00,344
yeah, feeds it to the linker to generate the executable.

45
00:03:01,044 --> 00:03:04,324
So let's talk about the optimizations. Sorry, the magic we care

46
00:03:04,364 --> 00:03:07,866
about which is the compiler optimizations. So the thing

47
00:03:07,890 --> 00:03:11,234
is that the code which you write, the code which I write it

48
00:03:11,274 --> 00:03:14,594
might not be the most efficient piece of code behind the scenes,

49
00:03:14,754 --> 00:03:18,306
meaning that there can be a bunch of efficient optimizations

50
00:03:18,330 --> 00:03:21,490
which can be made to it, and compiler takes care

51
00:03:21,522 --> 00:03:24,930
of it for us. So if I give you an example, imagine your

52
00:03:24,962 --> 00:03:29,130
code has an if statement which will never be executed. For example,

53
00:03:29,202 --> 00:03:32,402
it says if false print hello world.

54
00:03:32,538 --> 00:03:36,100
Now you and I, we both know that this if statement is

55
00:03:36,132 --> 00:03:39,772
absolutely useless. And the compiler also is smart enough

56
00:03:39,908 --> 00:03:43,476
to go through these lines of code and see that hey, this if statement

57
00:03:43,540 --> 00:03:47,420
would never execute, so let's not even include it during the compilation.

58
00:03:47,612 --> 00:03:51,420
So in this manner, the compiler actually ends up ignoring

59
00:03:51,452 --> 00:03:55,524
this small chunk of code and ends up producing a much lighter executable

60
00:03:55,564 --> 00:03:59,700
or binary. Now this was one trivial example, but the compiler actually

61
00:03:59,892 --> 00:04:03,316
like applies a bunch of crazy optimizations, and the ultimate

62
00:04:03,380 --> 00:04:06,836
benefit is that you end up with the lower executable, sorry, lower size

63
00:04:06,860 --> 00:04:10,436
of the executable. There are lesser number of instructions and code

64
00:04:10,460 --> 00:04:14,104
jumps happening in runtime because of lower number of instructions.

65
00:04:14,564 --> 00:04:18,364
And also the compiler can optimize your code during

66
00:04:18,404 --> 00:04:22,276
the compilation on the basis of the underlying hardware on top of which your code

67
00:04:22,300 --> 00:04:25,836
would run. For example, if your code is doing a lot of GPU programming and

68
00:04:25,860 --> 00:04:29,356
the hardware on top of which the compiler is running

69
00:04:29,500 --> 00:04:32,608
is also based out of a lot of GPU hardware,

70
00:04:32,756 --> 00:04:36,016
then the compiler can really make use of

71
00:04:36,040 --> 00:04:40,224
this information and optimize your code accordingly. With the help of the

72
00:04:40,344 --> 00:04:43,872
this power of compiler and optimizations, you can actually

73
00:04:43,928 --> 00:04:47,432
write a very clean piece of in a very beautiful and readable piece of code

74
00:04:47,528 --> 00:04:50,912
in an abstracted manner, and you won't have to be at

75
00:04:50,928 --> 00:04:54,744
the performance over it because the compiler would deconstruct these abstractions

76
00:04:54,824 --> 00:04:58,240
behind the scenes. So let me give you some

77
00:04:58,272 --> 00:05:01,952
examples of these optimizations. One optimization is pre calculation of constants

78
00:05:02,008 --> 00:05:05,656
where if you have an expression like a is equal to two multiplied by

79
00:05:05,680 --> 00:05:09,124
three plus five, the compiler during the compile time itself

80
00:05:09,544 --> 00:05:13,176
can calculate this and say that hey, there is no need

81
00:05:13,200 --> 00:05:16,736
to compute this again and again in runtime. Let me just compute it and

82
00:05:16,760 --> 00:05:19,364
straightaway feed it in the compilation phase itself.

83
00:05:19,904 --> 00:05:23,328
Similarly, there's a process of loop unrolling where the for

84
00:05:23,376 --> 00:05:27,352
loop is unrolled further. So that in runtime, when the

85
00:05:27,368 --> 00:05:30,616
for loop runs, it does not have to check the condition phase of the

86
00:05:30,640 --> 00:05:34,224
for loop multiple times. That also saves in a bunch of cpu cycles.

87
00:05:34,344 --> 00:05:38,236
And finally, there's a an optimization called Dead Star elimination where a bunch

88
00:05:38,260 --> 00:05:41,224
of useless code is ignored.

89
00:05:41,724 --> 00:05:45,732
For example, in the right hand image you can see that the

90
00:05:45,748 --> 00:05:49,748
variable is constantly getting updated. But all of those updates

91
00:05:49,796 --> 00:05:53,252
don't matter because the last update is going to overwrite all

92
00:05:53,268 --> 00:05:56,628
of the previous updates. So the compiler is smart enough to notice

93
00:05:56,676 --> 00:06:00,612
this, and it ignores all the above two lines and only considers

94
00:06:00,628 --> 00:06:04,304
the last line during the compilation. So the compiler does a lot of

95
00:06:04,344 --> 00:06:08,032
these cool optimizations, but we are interested in

96
00:06:08,048 --> 00:06:10,944
one optimization, and that is inlining.

97
00:06:11,064 --> 00:06:14,204
And this talk is going to be centered around this optimization.

98
00:06:14,904 --> 00:06:18,624
So inlining originates from a problem. The problem

99
00:06:18,784 --> 00:06:22,040
is the act of calling a function. All of us

100
00:06:22,072 --> 00:06:25,592
write a code in a pretty functional manner. We define functions, we call those functions

101
00:06:25,648 --> 00:06:29,316
multiple times. The problem is that in runtime,

102
00:06:29,420 --> 00:06:32,932
the act of calling a function is a slow operation, because when

103
00:06:32,948 --> 00:06:36,612
you just call the function, when you just invoke the function, a bunch

104
00:06:36,628 --> 00:06:39,972
of stuff happens behind the scenes which adds up to the runtime over it.

105
00:06:40,068 --> 00:06:43,836
For example, a new stack is like dedicated for the scope

106
00:06:43,860 --> 00:06:47,300
of the function. The parameters are pushed onto that stack whenever the

107
00:06:47,332 --> 00:06:50,940
function is completed with the execution, the returned

108
00:06:50,972 --> 00:06:54,396
values are returned back to the caller. So all of these things add

109
00:06:54,420 --> 00:06:58,296
up to the performance overhead in runtime. So the what the compiler

110
00:06:58,320 --> 00:07:01,672
does is compiler performs this optimization called inlining,

111
00:07:01,808 --> 00:07:04,936
where the compiler takes the body and implementation of the

112
00:07:04,960 --> 00:07:08,024
function and just places it wherever

113
00:07:08,064 --> 00:07:11,312
that function is invoked. So if you look at the image below,

114
00:07:11,488 --> 00:07:15,296
we have a function called sum, which sums up two parameters,

115
00:07:15,480 --> 00:07:19,200
x and y, and we have a main function where we are invoking this function

116
00:07:19,232 --> 00:07:22,078
twice. So if we perform inlining here,

117
00:07:22,176 --> 00:07:25,610
the compiler can actually just strip off the definition of the sum

118
00:07:25,642 --> 00:07:28,850
function and just replace the invocations of the sum

119
00:07:28,882 --> 00:07:32,690
function with their actual implementation. So sum one

120
00:07:32,722 --> 00:07:35,970
comma two becomes one plus two, some two comma four becomes

121
00:07:36,002 --> 00:07:40,146
two plus four. And the beautiful part is that this

122
00:07:40,210 --> 00:07:43,618
newly inlined piece of code can be further optimized through,

123
00:07:43,666 --> 00:07:47,050
say, pre calculation of constants to actually know

124
00:07:47,082 --> 00:07:50,428
in the compilation phase itself that res one is equal to three,

125
00:07:50,546 --> 00:07:54,112
res two is equal to six. So from a runtime perspective, things become very much

126
00:07:54,168 --> 00:07:57,504
more efficient, and that ends up elimination, eliminating the functional

127
00:07:57,544 --> 00:08:00,544
colleagues as well. But we have a problem.

128
00:08:00,664 --> 00:08:03,976
Nothing is perfect in word. What if you have

129
00:08:04,000 --> 00:08:07,164
defined a function and it has too many invocations?

130
00:08:07,864 --> 00:08:11,432
If there are too many invocations, then the inlining process will be

131
00:08:11,448 --> 00:08:14,736
too much if we perform the inlining blindly, and that would lead to

132
00:08:14,760 --> 00:08:18,304
too many new lines of code. This means that the code size would

133
00:08:18,344 --> 00:08:21,724
increase massively and that would lead to a bloated binary,

134
00:08:22,104 --> 00:08:25,112
and that ends up having some bad consequences in the runtime.

135
00:08:25,248 --> 00:08:29,656
For example, if I really go into the depth, then uploaded

136
00:08:29,680 --> 00:08:33,464
binary, basically behind the scenes means more number of static instructions

137
00:08:33,584 --> 00:08:37,324
and the higher number of static instructions in a binary in an executable,

138
00:08:38,784 --> 00:08:43,168
the bigger the instruction caches, and that leads to a bad

139
00:08:43,296 --> 00:08:46,856
cache hit ratio or cache it rate, and that leads

140
00:08:46,880 --> 00:08:50,688
to instruction cache message, which is quite bad in runtime. Moreover, a bloated

141
00:08:50,736 --> 00:08:54,464
binary causes page faults and whatnot. And if you are running your binary on

142
00:08:54,544 --> 00:08:58,096
a small device like a raspberry PI or something, even thrashing

143
00:08:58,120 --> 00:09:01,800
could happen. So blindly inlining a piece of code can be a bad

144
00:09:01,832 --> 00:09:04,936
situation. I have shown that as an example on the right hand image as well,

145
00:09:04,960 --> 00:09:07,900
that inlining a 506 lines of code,

146
00:09:08,072 --> 00:09:11,744
uh, can result in a 1000 lines of code result.

147
00:09:12,324 --> 00:09:15,988
So we have a weird problem here. Now if we do less in line inlining,

148
00:09:16,036 --> 00:09:19,308
we have a bad runtime performance because of the overhead introduced

149
00:09:19,356 --> 00:09:22,916
due to those function calls. And if we do more inlining,

150
00:09:23,060 --> 00:09:26,532
then we have a bad runtime performance due to the bigger executable,

151
00:09:26,588 --> 00:09:29,764
the page faults, the instruction cache misses and whatnot.

152
00:09:29,924 --> 00:09:32,584
So what to do then? That is frustrating.

153
00:09:32,924 --> 00:09:35,904
Well, we just need the right amount of inlining.

154
00:09:36,204 --> 00:09:39,732
So what does it mean? Ideally, we want to inline

155
00:09:39,788 --> 00:09:42,972
only the hot functions and not inline the cold functions.

156
00:09:43,148 --> 00:09:47,156
So the hot functions are the one which happen to execute a lot more frequently

157
00:09:47,180 --> 00:09:50,756
in runtime, while the cold functions are the one which don't execute

158
00:09:50,780 --> 00:09:54,604
that often in runtime. So the benefit of inlining hot functions is that

159
00:09:54,644 --> 00:09:58,300
these functions would execute a lot of times in runtime. This means that if you

160
00:09:58,332 --> 00:10:01,844
inline these functions, then you end up avoiding the function

161
00:10:01,884 --> 00:10:05,732
call overhead in runtime. So this gives you the benefit, the runtime benefit of

162
00:10:05,748 --> 00:10:08,972
inlining. You don't need to inline the cold functions

163
00:10:09,028 --> 00:10:13,020
because they are not executing enough amount of time, so there's no

164
00:10:13,052 --> 00:10:16,300
benefit in lining them. So by avoiding to

165
00:10:16,332 --> 00:10:19,892
inlining the cold functions, you actually end up saving on the

166
00:10:19,908 --> 00:10:23,756
binary size and the instruction count, and save yourself from

167
00:10:23,900 --> 00:10:27,526
all those bad stuff associated with excessive inlining.

168
00:10:27,630 --> 00:10:31,342
So ultimately we just need a sprinkle of inlining. And the

169
00:10:31,358 --> 00:10:34,926
problem is that compilers don't know a lot. When a compiler is compiling your

170
00:10:34,950 --> 00:10:38,934
code, it's literally reading across your code, it does not know

171
00:10:39,094 --> 00:10:43,174
a lot more information. So. And just your written code

172
00:10:43,214 --> 00:10:46,834
is not enough to tell how frequently a function would execute in runtime.

173
00:10:47,254 --> 00:10:51,166
Clearly compilers in more info. And what if we

174
00:10:51,190 --> 00:10:54,500
have a solution? The solution is that what if the compilers could look

175
00:10:54,532 --> 00:10:58,012
at the application in runtime and learn from that application,

176
00:10:58,148 --> 00:11:00,864
learn that which functions are hot and which functions are called?

177
00:11:01,204 --> 00:11:04,852
Or in other words, from a more implementation perspective, what if

178
00:11:04,868 --> 00:11:08,860
your applications runs in runtime and some somehow

179
00:11:08,892 --> 00:11:12,804
you collect various numbers and metrics about its behavior in runtime

180
00:11:12,924 --> 00:11:16,636
and finally feed those numbers and behavior as an information

181
00:11:16,820 --> 00:11:20,010
to the compiler. Next time you compile our code.

182
00:11:20,202 --> 00:11:23,330
So this with the next time you compile our code, the compiler would know that

183
00:11:23,362 --> 00:11:27,050
hey, this function is hot and that function is called. So let me just

184
00:11:27,162 --> 00:11:30,674
inline this function. So this kind of looks like a feedback loop,

185
00:11:30,714 --> 00:11:33,882
right? And that brings us to feedback driven

186
00:11:33,938 --> 00:11:37,194
optimization. So as the name suggests, it just

187
00:11:37,234 --> 00:11:40,554
teaches the compilers how and where to optimize the code

188
00:11:40,714 --> 00:11:44,194
on the basis of a feedback. Now that feedback can be benchmarks,

189
00:11:44,274 --> 00:11:47,634
user traffic profiles, all that kind of stuff. That feedback

190
00:11:47,674 --> 00:11:51,626
ultimately tells the compiler that which function is hot, which function is cold,

191
00:11:51,810 --> 00:11:55,894
and ultimately the compiler just does the right amount of inlining.

192
00:11:56,834 --> 00:12:00,330
Now the early days of feedback driven optimization were kind of

193
00:12:00,362 --> 00:12:03,546
governed by instrumentation based process. So what

194
00:12:03,570 --> 00:12:06,874
is instrumentation? It's very simple. With instrumentation,

195
00:12:06,914 --> 00:12:10,298
what happens is that whenever a code is compiled by the compiler,

196
00:12:10,426 --> 00:12:14,200
the compiler actually injects extra lines of code within your

197
00:12:14,232 --> 00:12:17,888
code. And what are these extra lines of code? Some timers,

198
00:12:17,976 --> 00:12:21,176
some call counters, all that kind of stuff. And the purpose of these

199
00:12:21,200 --> 00:12:24,736
additional lines of code is to track and instrument the behavior of your

200
00:12:24,760 --> 00:12:28,200
code in runtime. So once your code is compiled with these extra

201
00:12:28,232 --> 00:12:31,896
lines of code, a bunch of benchmarks are run against your application or

202
00:12:31,920 --> 00:12:35,904
code. And through the means of these extra lines of code, while these benchmarks

203
00:12:35,944 --> 00:12:39,414
are running, a bunch of information gets instrumented and collected,

204
00:12:39,544 --> 00:12:43,722
and this information becomes the feedback for the next build of the compiler.

205
00:12:43,818 --> 00:12:47,774
This information ends up representing how your application is performing in runtime.

206
00:12:48,594 --> 00:12:52,762
Now this sounds good, but is it really that efficient or that reliable?

207
00:12:52,938 --> 00:12:56,450
Because the thing is that now with this whole process, the code becomes

208
00:12:56,482 --> 00:13:00,146
much more bloated with all those compiler introduced instrumentation.

209
00:13:00,330 --> 00:13:04,014
And we have an extra benchmarking step, which just like makes

210
00:13:04,474 --> 00:13:08,138
the entire process of compilation and building

211
00:13:08,266 --> 00:13:11,696
quite slower and boring. And the biggest problem is that what

212
00:13:11,720 --> 00:13:15,256
if the benchmarks don't even resemble the reality of how your code

213
00:13:15,280 --> 00:13:17,724
runs in production, for example, right?

214
00:13:18,744 --> 00:13:22,504
What if it's actually quite opposite. And the benchmarks

215
00:13:22,544 --> 00:13:26,544
might end up introducing some wrongfully assumed optimizations and

216
00:13:26,584 --> 00:13:29,272
that can cause some really bad performance degradation.

217
00:13:29,448 --> 00:13:33,720
So a potentially hot function might be perceived as a cold function by the benchmark,

218
00:13:33,872 --> 00:13:36,764
and your compiler would not optimize it rightfully.

219
00:13:37,374 --> 00:13:40,194
So what do you want? Let's talk first principles.

220
00:13:40,614 --> 00:13:44,286
We want faster build times. We want realistic runtime data instead

221
00:13:44,310 --> 00:13:48,158
of benchmarks pretending to be real. And finally, we want light

222
00:13:48,206 --> 00:13:51,954
and small executables with no extra lines of code due to instrumentation.

223
00:13:52,534 --> 00:13:57,206
If you hate benchmarks, don't use them. So what

224
00:13:57,230 --> 00:14:00,702
we can do is that instead of benchmarks we can actually use the actual

225
00:14:00,758 --> 00:14:04,482
behavior of your code as a feedback to your compiler. And the

226
00:14:04,498 --> 00:14:07,826
beauty is that if you do that, you have now become independent

227
00:14:07,850 --> 00:14:11,626
of the benchmarking process as well, with which now you can just compile our

228
00:14:11,650 --> 00:14:14,874
code and that's what you are done. You don't have to execute the benchmarks,

229
00:14:14,914 --> 00:14:19,170
and this gives you the faster build times. Also, this is more realistic because you're

230
00:14:19,202 --> 00:14:22,854
not relying on any pretentious benchmarks pretending to be real users.

231
00:14:23,434 --> 00:14:27,154
So how do you do that? To be specific,

232
00:14:27,194 --> 00:14:31,026
it's sample based profiling. So the beauty of profiling is that it tracks the runtime

233
00:14:31,050 --> 00:14:34,606
behavior of your code without needing those extra lines of

234
00:14:34,630 --> 00:14:37,926
code inserted during for the sake of

235
00:14:37,990 --> 00:14:41,630
instrumentation. And when I say profiling, I mean sample based

236
00:14:41,662 --> 00:14:44,854
profiling, because like if we go really technical,

237
00:14:44,894 --> 00:14:48,510
then instrumentation is also a type of profiling. So what I'm talking about here is

238
00:14:48,542 --> 00:14:51,838
actually sample based profiling. But across this talk,

239
00:14:51,886 --> 00:14:54,934
whenever I say profiling, just here, sample based profiling.

240
00:14:55,014 --> 00:14:58,550
Alright, so yeah, how does it work? It's pretty simple.

241
00:14:58,702 --> 00:15:02,450
Now, just because we don't have those additional extra lines of

242
00:15:02,482 --> 00:15:05,978
code in the, in our code during the compilation

243
00:15:06,026 --> 00:15:09,642
phase, we need some external entity to poke and monitor

244
00:15:09,698 --> 00:15:13,186
and profile your application. And this is where the kernel and enters

245
00:15:13,210 --> 00:15:16,514
into the picture. So imagine an application is running in user

246
00:15:16,554 --> 00:15:20,690
space. How does it get profiled? Well, the kernel has a beautiful component

247
00:15:20,722 --> 00:15:24,106
called Linux perf, and the Linux perf schedules a bunch

248
00:15:24,130 --> 00:15:27,428
of interrupt events with the cpu hardware,

249
00:15:27,596 --> 00:15:31,300
and that's that hardware inside the cpu ends up scheduling

250
00:15:31,332 --> 00:15:34,668
and triggering those interrupts. And whenever those interrupts are triggered,

251
00:15:34,796 --> 00:15:38,300
the interrupt handler captures them inside the kernel, and the

252
00:15:38,332 --> 00:15:42,156
interrupt handler correspondingly pokes your application in

253
00:15:42,180 --> 00:15:45,580
user space and gets the runtime data at that point

254
00:15:45,612 --> 00:15:48,932
in time. Now that runtime data includes instruction pointer and

255
00:15:48,948 --> 00:15:52,064
call stack, which is enough to tell which function

256
00:15:52,104 --> 00:15:55,312
is executing at that time, which parameters are allocated in

257
00:15:55,328 --> 00:15:58,816
that function, what is the memory footprint? What is the resource footprint

258
00:15:58,880 --> 00:16:02,032
of that function? So a bunch of good profileable data,

259
00:16:02,168 --> 00:16:05,336
right? And that's it. Once all of this data has

260
00:16:05,360 --> 00:16:08,512
been gotten, this data is stored somewhere to be

261
00:16:08,528 --> 00:16:11,564
ultimately used by, for whatever purposes.

262
00:16:12,744 --> 00:16:17,000
And how we leverage that into the feedback driven optimization

263
00:16:17,072 --> 00:16:20,714
situation, we get profile guided optimization.

264
00:16:21,894 --> 00:16:25,510
PGO is compiler optimizations, which are

265
00:16:25,542 --> 00:16:28,950
guided by the profiles of your code collected during its runtime.

266
00:16:29,102 --> 00:16:33,206
So again, the same thing. Compiler is going through a feedback loop

267
00:16:33,350 --> 00:16:36,726
of, you know, the application runs, collects runtime behavior and feeds that

268
00:16:36,750 --> 00:16:39,714
to the compiler, and compiler optimizes it accordingly.

269
00:16:40,134 --> 00:16:44,554
Here the feedback is just the profiles, sample based profiles collected during runtime,

270
00:16:44,994 --> 00:16:48,634
right? So now talk is deep, let's walk inside the code.

271
00:16:48,794 --> 00:16:52,746
So let's take an example. Let's say we have a very simple server which exposes

272
00:16:52,770 --> 00:16:56,554
a post endpoint and which is called slash render.

273
00:16:56,714 --> 00:17:01,106
And whenever you call the post endpoint where the body is the markdown file,

274
00:17:01,290 --> 00:17:04,730
you get a rendered markdown in response. So an HTML

275
00:17:04,882 --> 00:17:08,610
rendered markdown, let's say this server is running, serving a bunch of

276
00:17:08,642 --> 00:17:12,220
users out there, and you want to compile to optimize

277
00:17:12,292 --> 00:17:15,620
just rightfully to it, depending on the usage

278
00:17:15,652 --> 00:17:19,812
patterns. So first of all, let's just

279
00:17:19,988 --> 00:17:24,204
compile this code without PGO, without profile guide optimization,

280
00:17:24,364 --> 00:17:28,100
and see what are the inlining decisions it takes. So after compiling

281
00:17:28,132 --> 00:17:31,436
this code, you can see that it's performing a bunch of inlining. I mean,

282
00:17:31,460 --> 00:17:34,500
it's quite readable here, but if I expand it further,

283
00:17:34,612 --> 00:17:38,104
specifically these method calls, these function calls are getting inlined.

284
00:17:38,614 --> 00:17:42,366
But if you notice carefully, there are two places where this inlining

285
00:17:42,390 --> 00:17:45,902
is not happening. Now, the decision to not inline a

286
00:17:45,918 --> 00:17:49,678
function is a consequence of multiple factors. One reason

287
00:17:49,726 --> 00:17:53,230
is that if a function is a non leaf function, it's like imagine a tree

288
00:17:53,262 --> 00:17:56,718
where each function leads to a child node. Each function call leads to a child

289
00:17:56,766 --> 00:17:59,594
node. If a function is a non leaf function,

290
00:18:00,054 --> 00:18:03,862
there's a good chance it won't get in line. Not necessarily, but a good chance.

291
00:18:03,998 --> 00:18:07,262
So probably that might be the reason why these two functions are not getting in

292
00:18:07,278 --> 00:18:10,262
line. But if you really think about it,

293
00:18:10,398 --> 00:18:13,894
inlining could have been useful here because these functions happen

294
00:18:13,934 --> 00:18:17,358
to be a part of the render function, which get

295
00:18:17,406 --> 00:18:20,526
always executed whenever the API is hit, whenever that endpoint is

296
00:18:20,550 --> 00:18:23,814
hit. And imagine thousands of users hitting that API

297
00:18:23,854 --> 00:18:26,950
again and again and again the function call over

298
00:18:26,982 --> 00:18:30,474
it. The overhead associated with calling the function and invoking the,

299
00:18:30,954 --> 00:18:34,334
for example, IO dot read all function not getting inlander

300
00:18:34,714 --> 00:18:38,530
can be huge, so it would be quite nice to inline the

301
00:18:38,642 --> 00:18:41,054
I o readall function, for example,

302
00:18:41,514 --> 00:18:44,658
and accordingly leverage the runtime benefits of it.

303
00:18:44,786 --> 00:18:48,250
But again, this is a piece of information which only,

304
00:18:48,402 --> 00:18:51,906
which can be only known if you perform the code in runtime. Actually, if there

305
00:18:51,930 --> 00:18:54,970
are barely any users, then it doesn't make sense to inline this function,

306
00:18:55,002 --> 00:18:58,326
right? So, so let's proceed and let's run

307
00:18:58,350 --> 00:19:01,514
and profile the program. So in the first image I built the program,

308
00:19:01,814 --> 00:19:05,142
then I built the binary without PGO,

309
00:19:05,238 --> 00:19:08,278
and I exported the binary as main nopigo.

310
00:19:08,446 --> 00:19:11,974
I ran the no pgo binary, and then I ran the

311
00:19:12,014 --> 00:19:15,454
server by this binary, and then I ran a load

312
00:19:15,494 --> 00:19:18,910
test generator against this markdown. And once

313
00:19:18,942 --> 00:19:22,802
I started executing the load behind the scenes, I opened up a new terminal,

314
00:19:22,858 --> 00:19:26,810
new session, and I just started profiling the server

315
00:19:27,002 --> 00:19:30,650
for 30 seconds just to collect the data associated with it. And finally,

316
00:19:30,682 --> 00:19:33,866
when the new set of profiles were generated after the 30 seconds

317
00:19:33,890 --> 00:19:37,454
of profiling, I stored it in a file called default PGO.

318
00:19:37,994 --> 00:19:42,018
Now the so ultimately we ended up with these files like main dot

319
00:19:42,066 --> 00:19:45,858
PGO, which was the binary, roughly 8.5 megabits large,

320
00:19:46,026 --> 00:19:49,438
and we have now cpu, nope, go pprof of,

321
00:19:49,466 --> 00:19:52,870
or the default PGO files. Both of these are the same files. These represent the

322
00:19:52,902 --> 00:19:56,942
profile of our application running in runtime facing

323
00:19:56,998 --> 00:20:00,462
alleged user load. Now let's compile our program with

324
00:20:00,478 --> 00:20:03,926
PGO, and as soon as you notice the dash PGO

325
00:20:03,990 --> 00:20:07,630
auto flag here, this basically tells the Go tool chain to

326
00:20:07,782 --> 00:20:11,166
use the default PGO profile as a

327
00:20:11,190 --> 00:20:14,954
feedback to compile the code. So this default PGO

328
00:20:15,134 --> 00:20:18,194
actually told the Go tool chain that how, you know,

329
00:20:18,314 --> 00:20:22,074
how frequently the load was hitting the render function

330
00:20:22,114 --> 00:20:24,814
and the IO dot redole. And as you can see,

331
00:20:26,074 --> 00:20:29,754
Golang now finally decided, the compiler finally decided that

332
00:20:29,794 --> 00:20:33,194
it should inline the I O dot read all function as well, which wasn't getting

333
00:20:33,234 --> 00:20:36,746
inland previously, by the way. And, and to

334
00:20:36,770 --> 00:20:40,042
be honest, this is not that, I mean behind the scenes, a lot

335
00:20:40,058 --> 00:20:43,500
of other internal functions which might be getting called by internal

336
00:20:43,532 --> 00:20:47,604
libraries might be getting further in line because of this profile

337
00:20:47,644 --> 00:20:50,916
guided optimization. And we can actually confirm that if

338
00:20:50,940 --> 00:20:54,316
you list the both the binaries before and after PGO,

339
00:20:54,500 --> 00:20:58,636
you can see that the previous binary was smaller and the larger. And this newer

340
00:20:58,660 --> 00:21:02,100
one is larger in size, roughly, I mean,

341
00:21:02,132 --> 00:21:05,584
0.2 megabytes size larger. That's because

342
00:21:07,704 --> 00:21:11,176
the new binary has more amount of inlining into it for the sake

343
00:21:11,200 --> 00:21:14,512
of better runtime benefits. So of course its size is going to be larger as

344
00:21:14,528 --> 00:21:17,880
well, because with inlining the code size increases, which is getting in line,

345
00:21:17,992 --> 00:21:21,144
right? Because instead of function invocations into

346
00:21:21,184 --> 00:21:24,884
your binary present, in your binary, instead of function invocations,

347
00:21:25,224 --> 00:21:28,136
you would actually have the definitions of function,

348
00:21:28,240 --> 00:21:31,124
which wherever the functions are in line. Alright,

349
00:21:31,704 --> 00:21:35,518
so now let's load test the old and new binaries. Again,

350
00:21:35,606 --> 00:21:38,782
as a part of first step, I ran the old binary,

351
00:21:38,838 --> 00:21:42,954
which is the main nopigo, and I ran a bunch of benchmarks

352
00:21:43,894 --> 00:21:47,326
and I basically dumped those benchmarks into a text file,

353
00:21:47,430 --> 00:21:51,574
which is a Nopigo test file. I did the similar thing with the binary

354
00:21:51,614 --> 00:21:55,350
with PgO and I dumped its benchmark into a

355
00:21:55,382 --> 00:21:59,006
separate text file called withpego. Txt. So ultimately we have two

356
00:21:59,030 --> 00:22:02,106
txt files. One, one file contains the results

357
00:22:02,170 --> 00:22:05,274
of the benchmarks of old binary, and the other

358
00:22:05,314 --> 00:22:08,706
one contains the benchmarks of the new binary. And one

359
00:22:08,730 --> 00:22:11,986
thing to be noted here, there is no change of code in both these

360
00:22:12,010 --> 00:22:15,298
binaries. No change of code. You I did not change

361
00:22:15,346 --> 00:22:19,530
anything. So it is just the magic of profile guided optimization,

362
00:22:19,722 --> 00:22:23,218
which we are about to witness. So as you proceed further,

363
00:22:23,266 --> 00:22:26,806
if we use benchtack to compare all of these, both of these text files,

364
00:22:26,930 --> 00:22:30,238
you would actually notice that with PGO, the runtime,

365
00:22:30,286 --> 00:22:34,438
the average runtime actually reduced. Runtime performance actually reduced

366
00:22:34,486 --> 00:22:38,142
or improved by roughly 2%, 1.88%.

367
00:22:38,318 --> 00:22:42,086
Now, I know it's not much, but the thing is that

368
00:22:42,230 --> 00:22:45,702
we just simulated a very slight amount of load and this was

369
00:22:45,718 --> 00:22:49,118
a very random, simple application. But imagine a fully fledged server

370
00:22:49,166 --> 00:22:53,242
with thousands and millions of users. In that case, some serious

371
00:22:53,298 --> 00:22:57,090
optimizations can be performed and this, this small difference of

372
00:22:57,122 --> 00:23:00,690
2% can be substantiated further to even five

373
00:23:00,722 --> 00:23:04,162
or 10%. You never know, right? And the best part is

374
00:23:04,178 --> 00:23:07,474
that the amount of effort involved with this improvement was nothing,

375
00:23:07,514 --> 00:23:10,850
it was negligible. I did not have to change a single line of code and

376
00:23:10,882 --> 00:23:13,738
I got this benefit out of the box, right?

377
00:23:13,906 --> 00:23:17,906
So let's do one thing, let's get

378
00:23:17,930 --> 00:23:20,506
our hands dirty. I mean, slides and all are fine, but let's get our hands

379
00:23:20,530 --> 00:23:23,690
dirty and actually play with profile guided optimization, right?

380
00:23:23,722 --> 00:23:27,954
On terminal. Alright, so pop

381
00:23:27,994 --> 00:23:31,402
open my terminal and you can see that I have a bunch

382
00:23:31,418 --> 00:23:34,922
of files here. So, alright, so let

383
00:23:34,938 --> 00:23:37,682
me do one thing, let me clean up this stuff, let me just clean up

384
00:23:37,698 --> 00:23:43,474
this stuff here, let me clean up this stuff. And so

385
00:23:43,514 --> 00:23:46,186
I'll first of all clean up all the text files. We don't care about those

386
00:23:46,210 --> 00:23:49,574
results anymore. I'll clean up the old binaries.

387
00:23:50,614 --> 00:23:54,198
All right. Main with big.

388
00:23:54,246 --> 00:23:56,634
Oh, I'm just cleaning up all the old binaries.

389
00:23:57,174 --> 00:24:00,634
And let me clean up all the old profiles as well. All right,

390
00:24:03,334 --> 00:24:06,646
so we have nothing. Now we have just, and of course let me remove the

391
00:24:06,670 --> 00:24:08,394
default p o file as well.

392
00:24:10,054 --> 00:24:13,486
So we are at scratch now. Okay, we are at scratch. We don't have anything.

393
00:24:13,590 --> 00:24:17,662
Now let's first build our code with nothing, with the,

394
00:24:17,838 --> 00:24:20,874
let's call it main. With, sorry, main,

395
00:24:21,494 --> 00:24:24,794
no pgo. And we are compiling the

396
00:24:25,974 --> 00:24:29,566
main door go file. Simple. Now with this we have

397
00:24:29,590 --> 00:24:32,598
compiled a code and if I run this,

398
00:24:32,646 --> 00:24:37,742
sorry, if I run this as this

399
00:24:37,758 --> 00:24:40,862
is running. Alright, now let's go

400
00:24:40,878 --> 00:24:45,224
to separate terminal again into a separate session. And actually let's

401
00:24:45,844 --> 00:24:49,424
run the load test. Okay, let's run the load test

402
00:24:49,964 --> 00:24:54,184
now here the load test is running behind the scenes against an application. All right.

403
00:24:54,484 --> 00:24:57,908
Now if I go here and if I further start

404
00:24:57,956 --> 00:25:01,424
the profiling process, let's say 30 seconds.

405
00:25:02,284 --> 00:25:06,544
Now for 30 seconds I'm, I've started the compilation or profiling process.

406
00:25:06,964 --> 00:25:10,180
And with this profiling process, what's happening is that behind the scenes,

407
00:25:10,212 --> 00:25:14,360
my binary, my server is getting hit by requests

408
00:25:14,392 --> 00:25:18,680
by this load. This means that is actually facing real time user traffic

409
00:25:18,872 --> 00:25:21,576
and that's actually simulating real time user traffic.

410
00:25:21,720 --> 00:25:25,248
And the profiling process is capturing all of the information about, you know,

411
00:25:25,296 --> 00:25:28,552
at each and every time. What is the memory footprint, what is the

412
00:25:28,568 --> 00:25:32,144
cpu footprint, what is the resource footprint of the binary execution.

413
00:25:32,184 --> 00:25:35,920
So as you can see, we are done and we have the profile ready

414
00:25:35,952 --> 00:25:40,024
here. Let's just rename our profile to know

415
00:25:41,604 --> 00:25:45,460
to default P O because that's something which is recommended by the

416
00:25:45,492 --> 00:25:48,836
Go tool chain. Of course you can have a custom name as well, but just

417
00:25:48,860 --> 00:25:52,356
for the sake of convenience, I put it up here. Now let's do one thing.

418
00:25:52,540 --> 00:25:56,052
As you can see, we have the default p go here. Now let's compile it

419
00:25:56,068 --> 00:25:59,476
again main. If I do the

420
00:25:59,500 --> 00:26:02,784
compilation process again, this time let's

421
00:26:02,824 --> 00:26:06,216
call the output binary as main. With PGO, we're doing main

422
00:26:06,280 --> 00:26:09,344
go and we will introduce the PGO automatic

423
00:26:09,384 --> 00:26:14,064
flag. This basically tells the compiler to read

424
00:26:14,104 --> 00:26:17,536
the default PGO file as a feedback for performing the

425
00:26:17,560 --> 00:26:20,960
compilation. And let's see, it's definitely taking some more time as

426
00:26:20,992 --> 00:26:24,240
compared to last time because again, now it's just going through more

427
00:26:24,272 --> 00:26:28,224
amount of scanning through the profiles and accordingly taking the rightful

428
00:26:28,264 --> 00:26:30,044
amount of optimization decisions.

429
00:26:31,104 --> 00:26:34,124
And yeah, it's just waiting, it's building a lot of suspense.

430
00:26:35,784 --> 00:26:38,984
But as you can see, it's doing a lot of compilation and whatnot.

431
00:26:39,024 --> 00:26:42,248
And yeah, it's taking a

432
00:26:42,256 --> 00:26:46,444
lot of time. So behind the scenes.

433
00:26:47,144 --> 00:26:50,864
Yep, we have stopped all the stuff. So yeah, yeah,

434
00:26:50,904 --> 00:26:56,124
it's not built and if I do a version M dot

435
00:26:56,164 --> 00:26:59,564
with ego you can actually see that it did.

436
00:26:59,604 --> 00:27:02,988
Consider this default PGO file while doing the compilation.

437
00:27:03,076 --> 00:27:06,724
Alright, now let's run this main with PGO

438
00:27:06,764 --> 00:27:10,076
file. But before that let me show you something interesting as

439
00:27:10,100 --> 00:27:12,424
well. So let's look at the sizes.

440
00:27:13,204 --> 00:27:17,644
I want to do an ls. Why do list? Yeah, just notice

441
00:27:17,764 --> 00:27:21,012
the size of actually,

442
00:27:21,068 --> 00:27:24,342
let's do a grep main as well. Then notice how

443
00:27:24,358 --> 00:27:27,486
the size of the one with PGO is actually larger and the one without Pigo

444
00:27:27,510 --> 00:27:31,030
is smaller because the one with Pigo actually told the compiler

445
00:27:31,062 --> 00:27:34,318
to perform more inlining, which led to the increase in

446
00:27:34,326 --> 00:27:38,390
the binary size. And finally let's execute

447
00:27:38,422 --> 00:27:42,670
main with PGO. This time let's

448
00:27:42,702 --> 00:27:46,054
actually perform a bunch of. No, let's perform benchmarks.

449
00:27:46,094 --> 00:27:49,450
Let's do some benchmarking here. So I'll

450
00:27:49,482 --> 00:27:53,546
actually start the benchmarks here and

451
00:27:53,570 --> 00:27:56,770
these benchmarks while these benchmarks are happening, let me tell you what's happening. The benchmarks

452
00:27:56,802 --> 00:28:00,354
are running and they're basically firing a bunch of load against binary

453
00:28:00,394 --> 00:28:03,786
with PGO and they're storing the

454
00:28:03,810 --> 00:28:08,250
results. The benchmarks are storing the results inside this file called with

455
00:28:08,282 --> 00:28:11,794
Pigo Txt. So right now we are doing this with PGO.

456
00:28:11,954 --> 00:28:16,062
Once these benchmarks are done, we will do these ones without PGO

457
00:28:16,258 --> 00:28:20,350
and then we can use a tool called benchtat to compare

458
00:28:20,382 --> 00:28:24,134
our results. Alright, so let's wait for a few

459
00:28:24,174 --> 00:28:27,966
seconds and this should be done in anytime soon. And there's

460
00:28:27,990 --> 00:28:31,694
nothing running in the second terminal that the way we generated the load

461
00:28:31,734 --> 00:28:35,910
previously was only for the sake of generating profiles that said nothing else.

462
00:28:36,022 --> 00:28:38,934
Right now we are doing benchmarking. So we are doing it in a different manner.

463
00:28:39,014 --> 00:28:41,988
And as you can see the benchmarks are executed.

464
00:28:42,166 --> 00:28:45,656
Now let's close this binary and run

465
00:28:45,680 --> 00:28:49,324
the one without PGO. And as soon as we run here,

466
00:28:49,784 --> 00:28:53,204
let me just name this file as nope.

467
00:28:53,624 --> 00:28:57,840
And again, this time as well we are running but this time across

468
00:28:57,912 --> 00:29:01,432
these benchmarks we will snapshot these benchmarks in

469
00:29:01,448 --> 00:29:05,056
a separate file called not nope Exe. And as I said,

470
00:29:05,240 --> 00:29:08,886
we will separate out these files and then benchtat against them

471
00:29:09,040 --> 00:29:12,854
to compare the real numbers that how things happen in runtime.

472
00:29:13,434 --> 00:29:17,090
They might be slightly different from what I showed in the PPT because

473
00:29:17,202 --> 00:29:20,986
again the load is kind of non deterministic

474
00:29:21,010 --> 00:29:24,694
in that manner. But yeah, let's see how it, how it happens.

475
00:29:25,154 --> 00:29:28,134
So it should be done anytime soon.

476
00:29:28,754 --> 00:29:32,374
We have nothing here and the no PBo file is running here.

477
00:29:32,954 --> 00:29:36,606
Yep, it's done. Now if you do bend status,

478
00:29:37,106 --> 00:29:40,714
no pgo txt and with

479
00:29:40,754 --> 00:29:44,226
pgo txt you can see what a beautiful

480
00:29:44,330 --> 00:29:48,010
difference it is. The one with no PGO, your average time was

481
00:29:48,202 --> 00:29:52,186
309 microseconds. With PGO it was 301. This is

482
00:29:52,210 --> 00:29:54,454
even a higher significant difference,

483
00:29:54,794 --> 00:29:58,250
even even a more significant difference with PGO,

484
00:29:58,362 --> 00:30:01,626
which is 2.33 as compared to the slides. So let's go

485
00:30:01,650 --> 00:30:05,734
back to the slides and just make some concluding notes.

486
00:30:06,234 --> 00:30:09,450
So that's pretty much it. I guess we learned quite a lot and see

487
00:30:09,482 --> 00:30:12,866
how profile guided optimization can actually benefit us a lot. So we

488
00:30:12,890 --> 00:30:16,490
explored the process of compilation, how compilation can be made more

489
00:30:16,522 --> 00:30:19,930
effective with feedback driven way by feeding it some

490
00:30:19,962 --> 00:30:24,442
runtime data and our sampling profiles based. Profile guided optimization

491
00:30:24,578 --> 00:30:27,986
works even more effectively in our favor and

492
00:30:28,010 --> 00:30:31,352
we actually got a very practical perspective by

493
00:30:31,448 --> 00:30:34,872
getting our hands dirty by playing with profile guided optimization

494
00:30:35,048 --> 00:30:38,604
with a server like code resembling a real life scenario.

495
00:30:39,024 --> 00:30:42,056
And you can find all of these slides and all of the associated content

496
00:30:42,120 --> 00:30:46,524
at the link below in my GitHub under my GitHub profile and

497
00:30:46,824 --> 00:30:50,444
let me share the references. So there's a bunch of references I used

498
00:30:50,904 --> 00:30:54,492
to learn about this topic and to inherit the content of these slides.

499
00:30:54,688 --> 00:30:58,380
Of course, this was just meant like I just sat on the shoulders

500
00:30:58,412 --> 00:31:00,984
of these giants who implemented all of this cool stuff.

501
00:31:01,884 --> 00:31:05,516
Finally, feel free to connect with me. All of my handles are

502
00:31:05,580 --> 00:31:09,228
given here. And that's all folks. Thanks a

503
00:31:09,276 --> 00:31:12,704
lot for your time. I appreciate you giving me your time and

504
00:31:13,324 --> 00:31:17,380
attention for this, for attending this talk, and feel absolutely

505
00:31:17,412 --> 00:31:20,748
free to raise any questions or reach out to me whenever. Hope you have a

506
00:31:20,756 --> 00:31:21,012
great day.

