1
00:00:27,200 --> 00:00:30,590
Hello everyone, and welcome to this session on open telemetry,

2
00:00:30,662 --> 00:00:34,140
and more specifically the open telemetry collector.

3
00:00:34,332 --> 00:00:37,908
So, to give a brief outline, we'll start by giving an

4
00:00:37,956 --> 00:00:42,544
overview of the open telemetry project and its functionality.

5
00:00:43,244 --> 00:00:46,964
We will talk about the collector and its architecture,

6
00:00:47,124 --> 00:00:50,980
and from there we will go to the main section of this

7
00:00:51,092 --> 00:00:54,424
presentation, how to configure the collector.

8
00:00:54,764 --> 00:00:57,940
We'll have some live demos that show how to configure

9
00:00:58,012 --> 00:01:01,478
and deploy the collector, and the goal is

10
00:01:01,566 --> 00:01:04,838
to help you be able to configure the collector for

11
00:01:04,886 --> 00:01:08,670
whatever your use case is. But before we get too far,

12
00:01:08,782 --> 00:01:13,702
let's get started with some introductions a

13
00:01:13,718 --> 00:01:16,766
little bit more about me my name is Curtis Robert. I am

14
00:01:16,830 --> 00:01:19,982
a software engineer at Cisco. I've been working in the

15
00:01:19,998 --> 00:01:22,822
observability space for about two years now,

16
00:01:22,998 --> 00:01:26,830
and I'm currently an approver in the contrib distribution

17
00:01:26,902 --> 00:01:30,230
of the open telemetry collector. Next up,

18
00:01:30,302 --> 00:01:34,510
my co presenter Steve thanks Curtis.

19
00:01:34,702 --> 00:01:37,878
My name is Steve Flanders and I'm an engineering leader at Cisco.

20
00:01:38,006 --> 00:01:41,470
I've been involved with the opencensus and open telemetry project since

21
00:01:41,502 --> 00:01:45,150
the very beginning and have more than a decade of experience in the monitoring

22
00:01:45,182 --> 00:01:48,790
and observability space, including leading log initiatives

23
00:01:48,822 --> 00:01:52,406
at vmware distributed tracing with omniscient, which is now the Splunk

24
00:01:52,430 --> 00:01:56,140
APM product, and now Metrics is part of Splunk infrastructure

25
00:01:56,172 --> 00:01:59,988
monitoring. I'm currently writing a book about open symmetry, and I'm excited

26
00:02:00,036 --> 00:02:01,904
for it to launch later on this year.

27
00:02:03,284 --> 00:02:06,732
All right, so let's talk a little bit about what open symmetry

28
00:02:06,788 --> 00:02:11,004
is first before jumping into our topic. So open symmetry

29
00:02:11,044 --> 00:02:14,252
is an observability framework and it's a toolkit. But at the end

30
00:02:14,268 --> 00:02:17,956
of the day, it really provides three building blocks. It provides

31
00:02:17,980 --> 00:02:21,700
a specification which is basically an open standard on which everything else

32
00:02:21,732 --> 00:02:25,204
is built. And this is what really allows and empowers open

33
00:02:25,244 --> 00:02:28,538
symmetry to be an open standard. On top of it, you have

34
00:02:28,586 --> 00:02:31,778
instrumentation libraries, which is a way across different

35
00:02:31,826 --> 00:02:35,162
languages to instrument an application, generating traces,

36
00:02:35,258 --> 00:02:39,234
metrics, logs, and additional signals, as they're called in open symmetry.

37
00:02:39,354 --> 00:02:43,042
And then there's a data collection component known as the open Symmetry collector,

38
00:02:43,138 --> 00:02:46,546
though there are other collector components as well. The collector allows

39
00:02:46,570 --> 00:02:49,946
you to ingest, process and export telemetry data,

40
00:02:50,050 --> 00:02:52,294
and we'll be talking more about that soon.

41
00:02:52,934 --> 00:02:56,326
Now, why does open telemetry matter at the end of the

42
00:02:56,350 --> 00:02:59,518
day? There are several kind of factors at play here. One is

43
00:02:59,566 --> 00:03:02,998
it is a single open standard that anyone can adopt, whether it

44
00:03:03,006 --> 00:03:05,718
is an end user, another open source project,

45
00:03:05,846 --> 00:03:09,390
or even vendors and its vendor agnostic, which means

46
00:03:09,422 --> 00:03:13,278
it's not going to lock you into one particular observability platform at

47
00:03:13,286 --> 00:03:16,354
the end of the day, it gives you flexibility and choice.

48
00:03:16,734 --> 00:03:20,382
Second, opensymmetry really empowers you to have data portability

49
00:03:20,478 --> 00:03:25,044
and control. If you want to send your data to multiple different observability backends,

50
00:03:25,124 --> 00:03:28,316
or if you want to change where the observability data is going, you have the

51
00:03:28,340 --> 00:03:31,780
flexibility of doing that with open telemetry. In addition,

52
00:03:31,892 --> 00:03:35,052
you control your data. So if you want to, for example,

53
00:03:35,108 --> 00:03:38,364
provide like crud metadata operations, you can do

54
00:03:38,404 --> 00:03:42,748
that with the open telemetry components that are available now. Open telemetry

55
00:03:42,796 --> 00:03:46,972
is the second most active project in the CNCF, behind only

56
00:03:47,028 --> 00:03:50,292
Kubernetes, which I think really speaks to the fact that this is a problem

57
00:03:50,348 --> 00:03:54,156
that needs to be solved, and it's adopted by a wide range

58
00:03:54,180 --> 00:03:57,770
of end users, but also contribute to the project, and it's supported

59
00:03:57,802 --> 00:04:00,922
by every major vendor today. In addition, there are a

60
00:04:00,938 --> 00:04:04,914
variety of open source projects that also have open telemetry integration,

61
00:04:05,034 --> 00:04:09,054
including Kubernetes, Istio, Prometheus and Jaeger.

62
00:04:09,394 --> 00:04:12,594
Now let me tell you a little bit more about the open telemetry collector,

63
00:04:12,634 --> 00:04:15,786
which is kind of the primary focus of this talk. The collector,

64
00:04:15,850 --> 00:04:19,250
as kind of mentioned, is a single binary that can be

65
00:04:19,282 --> 00:04:22,498
used to ingest, process and export telemetry

66
00:04:22,546 --> 00:04:25,786
data. It actually supports all signal data. So you can

67
00:04:25,810 --> 00:04:29,392
deploy a single agent or gateway instance in order to

68
00:04:29,408 --> 00:04:32,720
collect traces, metrics and logs. And it's very

69
00:04:32,792 --> 00:04:36,832
flexible through its pipeline configuration, which we'll be demonstrating here

70
00:04:36,888 --> 00:04:40,752
shortly. Now, if you think about a reference architecture,

71
00:04:40,848 --> 00:04:44,608
the two most common ways in which the collector would be deployed is

72
00:04:44,656 --> 00:04:48,496
as an agent that can actually be with the application itself as

73
00:04:48,520 --> 00:04:51,920
a sidecar, or if you're in like kubernetes as a daemon set.

74
00:04:51,952 --> 00:04:55,096
So you can do like one per host, even if there are multiple applications on

75
00:04:55,120 --> 00:04:58,812
that host. That's very common because it really offloads responsibilities

76
00:04:58,908 --> 00:05:02,660
from the application itself, the instrumentation, allowing the agent to

77
00:05:02,692 --> 00:05:05,932
kind of perform operations like buffer and retry logic.

78
00:05:06,108 --> 00:05:09,332
Or it could be deployed as its own standalone instance,

79
00:05:09,388 --> 00:05:12,932
typically referred to as a gateway. This is usually clustered,

80
00:05:12,988 --> 00:05:16,156
so there's usually multiple of these instances, and this can act

81
00:05:16,180 --> 00:05:19,500
as kind of a data aggregation or filtering layer. For example,

82
00:05:19,532 --> 00:05:22,806
if you wanted to do tail based sampling, you would do it with the

83
00:05:22,870 --> 00:05:26,118
gateway instance. Or if you wanted to collect from say

84
00:05:26,166 --> 00:05:29,846
the Kubernetes API, where you only want to do that once again, you could

85
00:05:29,870 --> 00:05:33,246
do that with a single gateway instance. So it's possible to

86
00:05:33,270 --> 00:05:37,150
have your OTL instrumentation send to a locally running collector

87
00:05:37,222 --> 00:05:40,838
agent. It could have that collector agent sending to a gateway cluster,

88
00:05:40,926 --> 00:05:44,726
and you can have that gateway cluster sending to one or more backends.

89
00:05:44,830 --> 00:05:48,350
Now of course the collector is entirely optional. You don't have to

90
00:05:48,382 --> 00:05:52,374
deploy it in either of these ways. In fact, you can have instrumentation send directly

91
00:05:52,414 --> 00:05:56,092
to your observability backend if you want. Again, the whole point

92
00:05:56,148 --> 00:05:59,972
here that opensymmetry is providing is flexibility and choice every

93
00:06:00,028 --> 00:06:03,836
step of the way. All right, now the collector is a go

94
00:06:03,900 --> 00:06:07,676
binary or it's written in go and it has different packaging that's available.

95
00:06:07,820 --> 00:06:12,340
So you can for example deploy this very easily on Linux, windows or macOS.

96
00:06:12,492 --> 00:06:16,260
In addition to this, it works very well in containerized environments and docker

97
00:06:16,292 --> 00:06:19,540
containers are provided. And there's even packaging

98
00:06:19,612 --> 00:06:23,458
specific for kubernetes for the more cloud native workloads as well.

99
00:06:23,636 --> 00:06:27,134
Now if you want to create your own again, you could do that, right?

100
00:06:27,174 --> 00:06:31,022
The building blocks are in place, so if you need other packaging options,

101
00:06:31,118 --> 00:06:34,662
they are available. Perhaps more important though is

102
00:06:34,678 --> 00:06:39,030
that the collector has this notion of distributions. A distribution

103
00:06:39,142 --> 00:06:42,590
is a specific set of components that are bundled

104
00:06:42,662 --> 00:06:46,374
with the collector itself. If you think of the open symmetry

105
00:06:46,454 --> 00:06:49,896
repositories which are hosted on GitHub, the collector has

106
00:06:49,960 --> 00:06:53,832
two main repositories. The first one is core. It is located

107
00:06:53,888 --> 00:06:57,272
at Opentelimetry collector and that has all the

108
00:06:57,328 --> 00:07:01,248
core components that are required for a pure open telemetry environment.

109
00:07:01,416 --> 00:07:05,304
For example, OTLP, which is the format in which all

110
00:07:05,344 --> 00:07:09,056
data is transformed into an open telemetry, has a receiver

111
00:07:09,120 --> 00:07:12,536
and exporter available in the core distribution.

112
00:07:12,720 --> 00:07:16,544
Now the contrib repository, which is available at open telemetry

113
00:07:16,704 --> 00:07:20,448
collector contrib. This actually has a wide range

114
00:07:20,496 --> 00:07:23,798
of receivers and processors and exporters. For example,

115
00:07:23,846 --> 00:07:27,894
this is where commercial vendors or even open source vendors would

116
00:07:27,934 --> 00:07:31,822
have their receiver or exporter components available.

117
00:07:31,998 --> 00:07:35,654
In addition, it contains functionality that you may or may not need.

118
00:07:35,774 --> 00:07:38,798
For example, if you want to do crud metadata operations,

119
00:07:38,926 --> 00:07:42,886
that would be a processor and it exists in the contrib repository.

120
00:07:42,990 --> 00:07:46,766
So it's very likely that you need some amount of components from core and contrib.

121
00:07:46,870 --> 00:07:50,610
At the end of the day, another packaging model that's available is,

122
00:07:50,682 --> 00:07:54,482
in terms of distribution at least is Kubernetes. And in the case of Kubernetes,

123
00:07:54,538 --> 00:07:58,178
it's actually packaged in helm. So there's a helm chart available, there's even a

124
00:07:58,186 --> 00:08:02,098
helm operator. And that packaging ensures that you have the components

125
00:08:02,146 --> 00:08:05,922
required to collect data from everything that Kubernetes has

126
00:08:05,938 --> 00:08:09,586
to provide. So there are actually specific receivers and processors that are

127
00:08:09,610 --> 00:08:13,186
enabled for you there. Now, the great part here is that there's actually

128
00:08:13,210 --> 00:08:16,914
a utility available that allows you to create custom distributions

129
00:08:16,994 --> 00:08:20,186
of the collector if you need it. So if you want to pick and choose

130
00:08:20,250 --> 00:08:24,106
which one of these components you're pulling in, you can do that. That is great

131
00:08:24,170 --> 00:08:27,810
for mature production environments where you're looking to really reduce the

132
00:08:27,842 --> 00:08:30,922
surface area of things that could be configured. And it gives you a

133
00:08:30,938 --> 00:08:34,094
lot more control of the collector instance itself.

134
00:08:35,314 --> 00:08:39,034
Now let's jump right into the configuration because that's the primary

135
00:08:39,074 --> 00:08:42,122
part of what we want to talk about today. I want to show it first

136
00:08:42,218 --> 00:08:44,706
and then I'll be turning it over to Curtis to give us a nice demo

137
00:08:44,730 --> 00:08:48,250
of how this all ends up playing out. So to do that,

138
00:08:48,282 --> 00:08:51,514
I have to talk about the components a little bit more in depth.

139
00:08:51,634 --> 00:08:55,066
So receivers are how you get data into the collector.

140
00:08:55,170 --> 00:08:58,538
These can be push or pull based. That means, for example,

141
00:08:58,586 --> 00:09:02,338
if you're familiar with tracing, trace data is often pushed

142
00:09:02,386 --> 00:09:05,626
from the application to a collector. That would be a push

143
00:09:05,690 --> 00:09:09,548
based receiver. Now, in the case of metrics, if you're familiar with Prometheus,

144
00:09:09,666 --> 00:09:13,256
it's very common that some endpoint is actually being scraped. The application

145
00:09:13,360 --> 00:09:16,768
metrics are being scraped themselves. That would be an example of pull based

146
00:09:16,816 --> 00:09:21,124
data collection. Both those are supported. There's a wide range of receivers.

147
00:09:21,424 --> 00:09:25,272
Processors are what you do with the data after you've collected it.

148
00:09:25,368 --> 00:09:28,728
So if we go back to like tail based sampling or crud made metadata

149
00:09:28,776 --> 00:09:32,232
operations, those would both be processors. And you have

150
00:09:32,248 --> 00:09:35,524
a lot of control there on how you want the data to be processed.

151
00:09:35,844 --> 00:09:39,252
Then you have exporters how you get data out of the collector.

152
00:09:39,348 --> 00:09:43,148
Again, this can be push or pull based, depending on the configuration,

153
00:09:43,316 --> 00:09:47,324
and then extensions. Extensions are usually things that don't touch

154
00:09:47,364 --> 00:09:51,588
the data themselves. Two very good examples. One would be like health check

155
00:09:51,636 --> 00:09:55,236
information for the collector instance itself. That's available as an extension.

156
00:09:55,340 --> 00:09:58,804
Or let's say you want to do something like authentication or

157
00:09:58,844 --> 00:10:02,686
service discovery for your receivers or even your x exporters.

158
00:10:02,830 --> 00:10:05,934
Those things are usually available as extensions as well.

159
00:10:06,094 --> 00:10:10,214
The newest component available is this notion of a connector. A connector

160
00:10:10,254 --> 00:10:13,958
is unique in that it's both a receiver and an exporter.

161
00:10:14,046 --> 00:10:17,566
It actually allows you to reprocess information either the same or

162
00:10:17,590 --> 00:10:21,566
different after it's gone through a pipeline. In the collector itself,

163
00:10:21,670 --> 00:10:24,910
a little bit more of an advanced use case, you typically wouldn't get be starting

164
00:10:24,942 --> 00:10:28,558
with a connector component, but it is available for use

165
00:10:28,606 --> 00:10:32,562
as well to ensure that you have everything that you need to fully process data

166
00:10:32,618 --> 00:10:36,842
in your environment. Now the collector is configured through YAML,

167
00:10:36,898 --> 00:10:40,330
very, very common in cloud native environments, for example in kubernetes.

168
00:10:40,442 --> 00:10:43,890
And it's really a two step process. So all of

169
00:10:43,922 --> 00:10:47,322
those components I talked about, they have to be defined and

170
00:10:47,378 --> 00:10:51,346
configured in this YAML. So on the top, top half of the example

171
00:10:51,370 --> 00:10:55,354
that you see on the screen, you will see things like receivers, processors and

172
00:10:55,394 --> 00:10:59,212
exporters being defined and you can define more than one of them.

173
00:10:59,268 --> 00:11:02,700
So for example, if we look at receivers for a second, you'll see that there's

174
00:11:02,732 --> 00:11:06,564
a host receiver that's being configured and that allows you to collect metrics

175
00:11:06,644 --> 00:11:10,316
and you can see that it has a custom configuration where certain scrapers are

176
00:11:10,340 --> 00:11:14,148
being defined and then you have an OTLP receiver

177
00:11:14,196 --> 00:11:17,588
that's defined. And because there's no additional configuration there,

178
00:11:17,676 --> 00:11:20,820
what that actually means is it's taking the default configuration

179
00:11:20,932 --> 00:11:25,022
that's available for the OTLP receiver today. Now just

180
00:11:25,078 --> 00:11:29,118
defining these and configuring them doesn't actually enable it.

181
00:11:29,206 --> 00:11:33,310
So the next step here is actually to define what's called a service pipeline

182
00:11:33,382 --> 00:11:37,158
in the collector and service pipelines are signal specific.

183
00:11:37,286 --> 00:11:40,598
So you'll see examples here of both metrics and traces.

184
00:11:40,766 --> 00:11:44,350
And that's where you actually define like which receivers, processors and

185
00:11:44,382 --> 00:11:47,526
exporters or even connectors are at play here.

186
00:11:47,630 --> 00:11:50,638
So if we look at the metrics example for a sec a second, it's going

187
00:11:50,646 --> 00:11:53,858
to take that host receiver, it's going to do some batch processing

188
00:11:53,906 --> 00:11:57,226
and then it's going to export it via Prometheus. You can see a similar

189
00:11:57,290 --> 00:12:00,930
example for traces. Now what's really interesting here is that

190
00:12:00,962 --> 00:12:04,450
there's flexibility in that you can receive in one format.

191
00:12:04,522 --> 00:12:08,194
Let's say we're looking at traces here, we're receiving an OTLP

192
00:12:08,274 --> 00:12:11,602
format, but we're actually exporting in zipkin format.

193
00:12:11,738 --> 00:12:15,018
That's what allows the collector to actually be vendor agnostic.

194
00:12:15,106 --> 00:12:18,930
It can receive data in one format and export it in a different

195
00:12:19,002 --> 00:12:22,618
format. And in addition you can have multiple processors. So if

196
00:12:22,626 --> 00:12:26,832
you want to perform multiple different pieces, pieces of processing before this data is exported,

197
00:12:26,928 --> 00:12:30,568
that's totally possible as well. The order of processors

198
00:12:30,656 --> 00:12:34,568
actually matters. So it's actually done in the order in which it's defined

199
00:12:34,656 --> 00:12:38,288
in the list. And that's important because maybe you want to sample

200
00:12:38,336 --> 00:12:41,640
before you filter or do crud metadata operations, or maybe

201
00:12:41,672 --> 00:12:44,968
you want to actually do crud metadata operations before you sample.

202
00:12:45,096 --> 00:12:48,680
So depending on your business requirements there, there's a lot of flexibility and

203
00:12:48,712 --> 00:12:52,092
choice every step of the way. And worth noting is actually the host

204
00:12:52,148 --> 00:12:54,860
metrics receiver. So that's just a typo here.

205
00:12:55,012 --> 00:12:59,812
But yeah, these receivers are all defined within the GitHub repositories.

206
00:12:59,948 --> 00:13:03,372
So let's go ahead and look at those GitHub repositories. Where am I

207
00:13:03,388 --> 00:13:06,348
going to find out what all these configuration options are and how I can define

208
00:13:06,396 --> 00:13:09,428
them? We're going to go to the GitHub repo. That's why I talked to you

209
00:13:09,436 --> 00:13:14,484
a little bit about those distributions. Here we are in the contrib repository

210
00:13:14,564 --> 00:13:17,964
and we're actually looking in the receiver folder and you'll see that there

211
00:13:18,004 --> 00:13:21,796
are actually multiple receivers defined. So if you're not

212
00:13:21,860 --> 00:13:25,252
using the contrib distribution, if you're using core, you would have

213
00:13:25,268 --> 00:13:28,724
to use a different repository here you drop the hyphen

214
00:13:28,844 --> 00:13:32,116
contrib at the end. And then if you didn't want to do receivers,

215
00:13:32,180 --> 00:13:35,484
you want to do processors or exporters or connectors, you would change

216
00:13:35,524 --> 00:13:39,228
to that particular folder and you would see everything that's available for

217
00:13:39,276 --> 00:13:42,676
that component set. Now let's go ahead and open up

218
00:13:42,700 --> 00:13:46,356
one of those. So here I am going back to the core repository. So we're

219
00:13:46,380 --> 00:13:49,292
in open symmetry collector, we're underneath receivers.

220
00:13:49,428 --> 00:13:53,148
We're looking at the OTLP receiver and you'll see it has some great

221
00:13:53,196 --> 00:13:56,700
getting started information. It gives you an example of that configuration file.

222
00:13:56,732 --> 00:14:00,068
At the end of the day it shows you what options are defined

223
00:14:00,156 --> 00:14:03,916
and configurable as well as what their default options are.

224
00:14:04,020 --> 00:14:07,596
And there's even advanced configuration options with links to that as well.

225
00:14:07,700 --> 00:14:11,452
So again, kind of rich getting started information. All of it's available in the GitHub

226
00:14:11,508 --> 00:14:15,508
repo. It's actually not in the open telemetry documentation site

227
00:14:15,556 --> 00:14:19,050
on open telemetry IO today. That will likely change in

228
00:14:19,082 --> 00:14:23,066
the future. Now there's a few other ways you can actually configure

229
00:14:23,090 --> 00:14:26,650
the collector as well. For example, you can use command line arguments.

230
00:14:26,762 --> 00:14:30,682
So if you want to define this as you're starting the open symmetry collector binary,

231
00:14:30,738 --> 00:14:34,242
you can do that. You can do more advanced things like within

232
00:14:34,298 --> 00:14:38,266
this YAML configuration, you can actually define environmental variables that'll

233
00:14:38,290 --> 00:14:41,482
get expanded for you automatically, providing a lot more

234
00:14:41,498 --> 00:14:44,680
of a dynamic configuration if you need it. And then there's

235
00:14:44,792 --> 00:14:48,128
a newer part of the open telemetry project called the open

236
00:14:48,176 --> 00:14:51,312
telemetry transformation Language, or OTTL.

237
00:14:51,448 --> 00:14:55,056
It's actually a DSL or a domain specific language that

238
00:14:55,080 --> 00:14:58,608
opentelemetry is creating and adopting, which will make it much

239
00:14:58,656 --> 00:15:02,720
easier and a much more standard way to define configuration going forward

240
00:15:02,792 --> 00:15:05,848
across the project that is experimental or in development

241
00:15:05,936 --> 00:15:09,616
mode today, but it's actively being developed and getting richer

242
00:15:09,640 --> 00:15:12,880
and richer by the day. Okay, so instead of

243
00:15:12,912 --> 00:15:15,976
me kind of showing you through two slides, I think it's way more

244
00:15:16,000 --> 00:15:19,072
powerful to actually see a demo of this live. So I'm going to turn it

245
00:15:19,088 --> 00:15:22,848
back over to Curtis and he's going to show you exactly how you can configure

246
00:15:22,896 --> 00:15:24,404
and use the collector.

247
00:15:27,544 --> 00:15:30,672
All right, thank you, Steve. So now that

248
00:15:30,688 --> 00:15:34,112
we have a basic and general understanding of

249
00:15:34,128 --> 00:15:37,956
the telemetry collector, how it works and its components,

250
00:15:38,100 --> 00:15:41,636
let's jump into a demo. So for the sake

251
00:15:41,660 --> 00:15:44,868
of simplicity, I'm going to be focusing on just

252
00:15:44,916 --> 00:15:48,664
running the collector directly on my local system.

253
00:15:49,524 --> 00:15:52,964
So as Steve mentioned earlier, we come to the

254
00:15:53,004 --> 00:15:57,716
releases distribution of our collector and

255
00:15:57,900 --> 00:16:01,730
we find the binary for our

256
00:16:01,852 --> 00:16:05,630
own system. I'm running on Mac and

257
00:16:05,822 --> 00:16:09,454
AMD processor architecture, so I just

258
00:16:09,494 --> 00:16:13,526
downloaded this file to my local system and I can run

259
00:16:13,550 --> 00:16:17,110
it from there. So I'll pull up the command line and I

260
00:16:17,142 --> 00:16:21,194
will get started with a basic configuration.

261
00:16:21,814 --> 00:16:25,714
So to run the collector locally, you simply

262
00:16:26,014 --> 00:16:28,234
specify a config file,

263
00:16:29,334 --> 00:16:33,234
config and point it to your.

264
00:16:35,894 --> 00:16:40,034
In our demo, we'll start with a

265
00:16:40,574 --> 00:16:44,314
very basic configuration and we'll just build from there.

266
00:16:44,694 --> 00:16:48,174
So as Steve mentioned earlier, the collector

267
00:16:48,214 --> 00:16:52,942
is made up of different components. We have the primary

268
00:16:53,038 --> 00:16:56,942
types are receivers, processors, and exporters. So in

269
00:16:56,958 --> 00:17:00,366
our basic ground up demo, we'll just

270
00:17:00,390 --> 00:17:04,270
start with the receiver and an exporter. And Steve

271
00:17:04,302 --> 00:17:07,446
mentioned this already, but the host metrics receiver

272
00:17:07,550 --> 00:17:10,950
is a good place to start. It just scrapes your local

273
00:17:11,022 --> 00:17:14,750
system for data and sends

274
00:17:14,782 --> 00:17:16,674
it through your metrics pipeline.

275
00:17:18,094 --> 00:17:21,390
So in the reading here, we have information about how to

276
00:17:21,422 --> 00:17:25,129
configure the host metrics receiver. One thing to

277
00:17:25,161 --> 00:17:29,225
know is the collection interval, how frequently you want to scrape

278
00:17:29,249 --> 00:17:34,017
your local system for data, and then also take

279
00:17:34,065 --> 00:17:37,793
note the scrapers. So since I'm on Mac,

280
00:17:37,873 --> 00:17:41,933
some of them are not compatible. Cpu disk

281
00:17:42,593 --> 00:17:46,801
don't work on Mac. So for our demo we'll

282
00:17:46,857 --> 00:17:48,253
just use memory.

283
00:17:49,984 --> 00:17:53,872
So we'll come back to our configuration file and

284
00:17:53,968 --> 00:17:58,164
we'll specify receivers post metrics

285
00:17:59,064 --> 00:18:02,884
collection interval. I will say 10 seconds.

286
00:18:04,704 --> 00:18:08,568
And then for scrapers, as I said, we'll just use memory

287
00:18:08,616 --> 00:18:12,632
for now. As far as exporters

288
00:18:12,688 --> 00:18:16,204
go, I'm going to use the debug exporter.

289
00:18:16,684 --> 00:18:20,172
It's always a good place to start. Just to make sure that data

290
00:18:20,228 --> 00:18:22,544
is coming through as expected.

291
00:18:23,404 --> 00:18:27,780
So we'll define a debug exporter.

292
00:18:27,972 --> 00:18:31,972
And verbosity is good to just say detailed

293
00:18:32,068 --> 00:18:35,740
to start out so that you get the output

294
00:18:35,772 --> 00:18:39,692
that you can. The debug exporter just

295
00:18:39,748 --> 00:18:43,196
sends all the data that's coming through your pipeline to your

296
00:18:43,300 --> 00:18:46,556
standard error logger. So we

297
00:18:46,580 --> 00:18:50,252
have a receiver, we have an exporter. The next step, we have to

298
00:18:50,308 --> 00:18:53,548
define a metrics pipeline that the

299
00:18:53,596 --> 00:18:57,500
metrics will actually flow through. So it goes

300
00:18:57,532 --> 00:19:01,144
under service pipelines

301
00:19:02,604 --> 00:19:06,076
and then metrics. You can have

302
00:19:06,100 --> 00:19:08,908
multiple metrics pipelines. You can have logs,

303
00:19:08,956 --> 00:19:12,074
traces. For this demo,

304
00:19:12,114 --> 00:19:15,802
we'll just be doing metrics. So we want

305
00:19:15,898 --> 00:19:20,242
a receiver here, host metrics.

306
00:19:20,338 --> 00:19:22,694
As we define exporters,

307
00:19:24,354 --> 00:19:27,954
debug. Okay, you can define

308
00:19:27,994 --> 00:19:31,178
as many components as you want, receivers, processors,

309
00:19:31,226 --> 00:19:35,432
exporters, connectors, but if they're not a pipeline,

310
00:19:35,538 --> 00:19:39,244
they will not be used. So make sure to include any

311
00:19:39,284 --> 00:19:43,344
component that you want to use in the relevant pipeline.

312
00:19:43,924 --> 00:19:47,580
So now that we have a basic collector configuration

313
00:19:47,652 --> 00:19:51,380
set up, we can come over to our command line and

314
00:19:51,492 --> 00:19:53,544
we can run the collector.

315
00:19:57,284 --> 00:20:00,668
Okay, it started up successfully. I'll just stop

316
00:20:00,716 --> 00:20:04,816
the collector there. We can see the output,

317
00:20:05,000 --> 00:20:08,632
and we come over here and we see that we're getting our memory

318
00:20:08,808 --> 00:20:13,272
metric successfully. So the system

319
00:20:13,328 --> 00:20:16,856
dot memory dot usage shows the bytes of memory that

320
00:20:16,880 --> 00:20:20,128
are in use. And then we have different data points down here.

321
00:20:20,256 --> 00:20:23,568
So we have a data point for how many bytes

322
00:20:23,616 --> 00:20:26,672
are being used. We have a data point for how many bytes

323
00:20:26,688 --> 00:20:30,146
are free, and we have a data point for how many bytes

324
00:20:30,170 --> 00:20:32,214
of memory are inactive.

325
00:20:33,834 --> 00:20:37,434
So next, the demo, we can add a processor.

326
00:20:37,554 --> 00:20:40,174
And for my demo,

327
00:20:40,594 --> 00:20:43,994
the resource detection processor is a

328
00:20:44,034 --> 00:20:46,214
good place to start.

329
00:20:47,194 --> 00:20:50,922
This will be able to add resource attributes to

330
00:20:50,938 --> 00:20:54,336
the metrics flowing through your pipeline. And in

331
00:20:54,360 --> 00:20:58,264
my case, I want to use the system detector,

332
00:20:58,424 --> 00:21:02,984
which will add the host name of my computer

333
00:21:03,104 --> 00:21:06,496
and my OS type. So we'll come back over

334
00:21:06,520 --> 00:21:10,504
to the configuration file. We will specify a

335
00:21:10,544 --> 00:21:12,844
resource detection processor,

336
00:21:13,784 --> 00:21:17,296
and we will use the system

337
00:21:17,480 --> 00:21:20,988
detector. So what this

338
00:21:21,076 --> 00:21:25,624
allows us to do is to filter metrics

339
00:21:25,924 --> 00:21:28,940
based on where they're coming from.

340
00:21:29,092 --> 00:21:33,500
So in whatever backend you're using, you can see

341
00:21:33,692 --> 00:21:37,684
all the metrics from a specific device or

342
00:21:37,764 --> 00:21:41,580
environment, and you can see if something's

343
00:21:41,612 --> 00:21:45,408
going wrong or if something's abnormal. So we want to

344
00:21:45,416 --> 00:21:49,284
make sure that we add our resource detection processor to the pipeline.

345
00:21:50,464 --> 00:21:54,440
We'll save it. We will run the collector

346
00:21:54,472 --> 00:21:57,624
again. Oh,

347
00:21:57,704 --> 00:21:58,804
I messed up.

348
00:22:06,704 --> 00:22:09,404
Invalid key detector.

349
00:22:10,204 --> 00:22:12,104
Okay, it needs to be plural.

350
00:22:19,044 --> 00:22:22,544
All right, so now we see our metrics are still coming through.

351
00:22:22,884 --> 00:22:26,748
But now the resource detection processor

352
00:22:26,916 --> 00:22:30,388
has detected my local os and it's detected

353
00:22:30,476 --> 00:22:33,824
my machine's hostname. So now any metrics

354
00:22:34,124 --> 00:22:38,028
coming through the processor and its pipeline will

355
00:22:38,076 --> 00:22:41,820
have the system information attached

356
00:22:41,852 --> 00:22:44,916
to these metrics. From here,

357
00:22:45,060 --> 00:22:48,644
I think the next step is to send our data to

358
00:22:48,684 --> 00:22:52,652
somewhere more useful. So I will use

359
00:22:52,708 --> 00:22:57,236
Prometheus for this. So we can look

360
00:22:57,260 --> 00:23:01,476
at the Prometheus exporter. It allows us to

361
00:23:01,660 --> 00:23:05,760
expose metrics at a specific endpoint that can then be scraped

362
00:23:05,932 --> 00:23:08,164
by our Prometheus environment.

363
00:23:09,864 --> 00:23:13,124
So I will define a Prometheus exporter.

364
00:23:13,744 --> 00:23:18,208
We have to set an endpoint and

365
00:23:18,296 --> 00:23:23,644
I will use localhost 8889.

366
00:23:24,944 --> 00:23:28,760
This is the default port for

367
00:23:28,792 --> 00:23:32,784
Prometheus. And I have set up my Prometheus environment

368
00:23:32,904 --> 00:23:35,644
to scrape the endpoint.

369
00:23:36,704 --> 00:23:40,616
And then the other configuration option that is helpful

370
00:23:40,680 --> 00:23:44,124
here is resource to telemetry conversion.

371
00:23:44,824 --> 00:23:48,488
This says to add resource attributes as

372
00:23:48,616 --> 00:23:51,696
labels to Prometheus metrics. This will make

373
00:23:51,720 --> 00:23:55,404
sure that we still have our host information

374
00:23:55,824 --> 00:23:58,164
attached to Prometheus metrics.

375
00:23:58,924 --> 00:24:02,504
So resource to telemetry

376
00:24:03,004 --> 00:24:07,424
conversion enabled.

377
00:24:07,724 --> 00:24:11,556
True. And then we have to add Prometheus

378
00:24:11,620 --> 00:24:17,308
to our pipeline and

379
00:24:17,396 --> 00:24:20,700
we can run the collector again and we'll check

380
00:24:20,732 --> 00:24:23,744
to see if Prometheus is getting our metrics properly.

381
00:24:28,064 --> 00:24:32,392
All right, let's start it up properly here and

382
00:24:32,408 --> 00:24:36,656
we'll go over to Prometheus to output and

383
00:24:36,680 --> 00:24:39,944
I'll start here to see all of the

384
00:24:40,104 --> 00:24:43,312
metrics that Prometheus is

385
00:24:43,408 --> 00:24:46,928
scraping and we'll see if the metric that we're looking for

386
00:24:46,976 --> 00:24:50,896
came through. So we see here

387
00:24:51,000 --> 00:24:54,392
system memory usage bytes. So we

388
00:24:54,408 --> 00:24:57,872
can notice that this metric is named differently than what we

389
00:24:57,888 --> 00:25:01,280
saw in the debug exporter. This is simply because

390
00:25:01,352 --> 00:25:05,288
of Prometheus's naming conventions being

391
00:25:05,336 --> 00:25:08,360
different than open telemetry. So the

392
00:25:08,392 --> 00:25:12,152
metric name gets converted to Prometheus format so

393
00:25:12,168 --> 00:25:15,384
we can come over to the Prometheus

394
00:25:15,424 --> 00:25:19,144
search and we can look up system memory

395
00:25:19,184 --> 00:25:23,814
usage bytes and we can see our data that is being

396
00:25:23,854 --> 00:25:26,974
scraped by Prometheus. A couple things to note.

397
00:25:27,054 --> 00:25:31,234
We can see that the hostname is coming through properly. So in Prometheus we could

398
00:25:33,734 --> 00:25:37,574
filter base name and also that the state is

399
00:25:37,614 --> 00:25:41,190
coming through properly. So Prometheus shows that we

400
00:25:41,222 --> 00:25:44,374
have memory coming through

401
00:25:44,454 --> 00:25:48,274
in different states and we can view the graph

402
00:25:48,854 --> 00:25:52,798
and see how that changes over time. So our

403
00:25:52,926 --> 00:25:56,614
free bytes are pretty low, we have

404
00:25:56,654 --> 00:26:01,074
inactive bytes and then we have used bytes.

405
00:26:03,654 --> 00:26:08,406
So from here we can add

406
00:26:08,470 --> 00:26:11,558
another processor and we can filter out data

407
00:26:11,606 --> 00:26:13,754
points that we may not be interested in.

408
00:26:14,154 --> 00:26:17,770
So this will help us reduce

409
00:26:17,922 --> 00:26:21,094
our resource usage and

410
00:26:21,914 --> 00:26:25,170
that way we don't have to ingest metrics that we

411
00:26:25,202 --> 00:26:28,954
don't care about. So for this demo, I think

412
00:26:29,034 --> 00:26:32,854
I will use the filter processor to

413
00:26:33,954 --> 00:26:37,454
filter out points that are not interesting.

414
00:26:37,874 --> 00:26:42,014
And in this case I'm just going to filter out inactive

415
00:26:42,154 --> 00:26:45,734
memory bytes. So the filter processor

416
00:26:45,774 --> 00:26:50,254
has the configuration options and we're just going to find

417
00:26:50,334 --> 00:26:54,110
an example that looks close to what we're

418
00:26:54,142 --> 00:26:57,390
looking for. So we have some ones that are close here.

419
00:26:57,422 --> 00:27:00,878
We just want to filter out data points based on metric name and

420
00:27:00,926 --> 00:27:05,274
attributes. So we'll define a new processor

421
00:27:06,414 --> 00:27:10,266
filter error

422
00:27:10,330 --> 00:27:15,306
mode ignore since this is just a demo and

423
00:27:15,330 --> 00:27:19,850
we're going to be filtering metrics and we're going to be filtering

424
00:27:20,042 --> 00:27:23,786
by data point. So here we're going to define the

425
00:27:23,810 --> 00:27:27,194
metric name. And note that this will

426
00:27:27,234 --> 00:27:31,854
be in the hotel naming scheme, not Prometheus.

427
00:27:32,394 --> 00:27:36,456
The metrics are converted to format in the exporter,

428
00:27:36,520 --> 00:27:40,520
so when they're coming through the processor, they're still in the

429
00:27:40,552 --> 00:27:44,320
hotel naming scheme. So the metric name

430
00:27:44,392 --> 00:27:46,564
system dot memory dot usage,

431
00:27:47,744 --> 00:27:51,000
and then the state that we're

432
00:27:51,032 --> 00:27:54,604
trying to filter out is inactive.

433
00:27:55,064 --> 00:28:00,624
So we'll filter by state inactive.

434
00:28:00,924 --> 00:28:04,780
So what this does is any data point that is in

435
00:28:04,812 --> 00:28:08,636
the system memory usage metric and has state

436
00:28:08,700 --> 00:28:12,464
inactive will be dropped by the filter processor.

437
00:28:13,204 --> 00:28:16,836
So we will restart the collector

438
00:28:16,900 --> 00:28:18,784
since we're changing the configuration.

439
00:28:24,004 --> 00:28:26,264
Oh, messed up the configuration again.

440
00:28:27,674 --> 00:28:29,934
Oh, inactive needs to be a string.

441
00:28:32,794 --> 00:28:33,654
All right.

442
00:28:37,314 --> 00:28:41,018
Okay, the collector has started successfully again. So let's go

443
00:28:41,026 --> 00:28:45,338
over to Prometheus to see if we're properly

444
00:28:45,506 --> 00:28:48,374
dropping inactive data points.

445
00:28:49,194 --> 00:28:53,114
So we will search again. All right,

446
00:28:53,654 --> 00:28:57,550
so we see here that there's a short break in

447
00:28:57,582 --> 00:29:00,718
metrics where the collector was down. We see

448
00:29:00,766 --> 00:29:04,194
that before when the collector was running, we had used,

449
00:29:04,534 --> 00:29:08,494
we had inactive and we had free memory

450
00:29:08,534 --> 00:29:12,174
bytes. But now that the collector has restarted and we're filtering

451
00:29:12,214 --> 00:29:14,834
out the inactive data points,

452
00:29:15,854 --> 00:29:19,910
we're not seeing those data points anymore. So the filter processor

453
00:29:20,062 --> 00:29:22,194
is working as expected.

454
00:29:23,774 --> 00:29:27,334
So now in our configuration, we've successfully

455
00:29:27,374 --> 00:29:31,206
added a receiver, multiple processors, and an exporter.

456
00:29:31,350 --> 00:29:35,054
We're sending our data to Prometheus. We're filtering out data

457
00:29:35,094 --> 00:29:38,806
points, maybe not useful for us, at least in this

458
00:29:38,830 --> 00:29:42,606
demo, and we are able to view our metrics

459
00:29:42,670 --> 00:29:46,374
in Prometheus. We wanted to share a

460
00:29:46,414 --> 00:29:49,798
more involved example of the power and capabilities

461
00:29:49,886 --> 00:29:54,188
of the collector, so we'll be using the official open telemetry

462
00:29:54,316 --> 00:29:57,660
demo. This will allow us to deploy the collector in a

463
00:29:57,692 --> 00:30:01,236
Kubernetes environment and to configure it and

464
00:30:01,300 --> 00:30:05,268
observe a website built from a microservices

465
00:30:05,356 --> 00:30:08,828
based architecture. And that's what the demo is.

466
00:30:08,956 --> 00:30:12,024
So let's view the architecture.

467
00:30:13,324 --> 00:30:17,304
So the demo is a website

468
00:30:17,734 --> 00:30:21,070
that is a store. So you can browse different

469
00:30:21,142 --> 00:30:24,454
products that are for sale, you can add them to your

470
00:30:24,494 --> 00:30:27,314
cart, you can check out,

471
00:30:28,174 --> 00:30:32,350
add your personal information like your address, credit card,

472
00:30:32,542 --> 00:30:35,870
and you can order. So as we

473
00:30:35,902 --> 00:30:40,014
see here, we have different microservices running that handle different parts

474
00:30:40,054 --> 00:30:43,334
of the website. And as the legend

475
00:30:43,414 --> 00:30:47,376
shows, these microservices are written in quite a

476
00:30:47,400 --> 00:30:51,192
few different languages. We have.net,

477
00:30:51,288 --> 00:30:54,352
c Go, Java,

478
00:30:54,408 --> 00:30:58,284
JavaScript, Python and many others.

479
00:30:58,864 --> 00:31:02,568
So for each of these microservices we

480
00:31:02,616 --> 00:31:07,444
have the languages SDKs

481
00:31:08,064 --> 00:31:12,434
instrument the microservices, either manually or automatically.

482
00:31:12,594 --> 00:31:16,354
So these microservices are instrumented to send their

483
00:31:16,434 --> 00:31:19,774
telemetry to the collector.

484
00:31:20,514 --> 00:31:23,930
So if we scroll down a little bit, we can see that the data is

485
00:31:23,962 --> 00:31:27,014
flowing from the demo to the collector.

486
00:31:28,114 --> 00:31:31,454
And there's a link here to the collector configuration.

487
00:31:33,074 --> 00:31:36,730
It's a bit hard to follow the flow of data

488
00:31:36,802 --> 00:31:39,738
in the collector if we're just looking at the YAML file.

489
00:31:39,906 --> 00:31:43,314
So for the sake of the demo, we can use Otel bin

490
00:31:43,434 --> 00:31:47,174
to visualize the flow of data.

491
00:31:48,434 --> 00:31:51,674
So in our pipelines we can see that we

492
00:31:51,714 --> 00:31:54,974
have traces, metrics and logs.

493
00:31:55,394 --> 00:31:57,814
In our first demo we only had metrics,

494
00:31:58,674 --> 00:32:02,654
so this will be helpful in showing us how other telemetry works as well.

495
00:32:03,634 --> 00:32:07,112
For traces, we see that they are coming

496
00:32:07,168 --> 00:32:10,600
through the OTLP receiver, which is

497
00:32:10,632 --> 00:32:13,044
defined here in the configuration file.

498
00:32:13,864 --> 00:32:17,684
We're able to receive traces over GrPC or HTTP

499
00:32:19,024 --> 00:32:23,364
and then these traces when they're coming from these different microservices

500
00:32:23,824 --> 00:32:28,044
will go through the batch processor and then be sent on to

501
00:32:28,344 --> 00:32:32,040
these exporters. We have the OTLP

502
00:32:32,112 --> 00:32:36,222
exporter, the debug and the span metrics

503
00:32:36,278 --> 00:32:40,278
connector. So the connector

504
00:32:40,366 --> 00:32:44,302
introduces a special use case and

505
00:32:44,318 --> 00:32:47,862
we can look at the spanmetrics

506
00:32:48,038 --> 00:32:51,394
connect or reaDMe to see what it does. But first,

507
00:32:52,014 --> 00:32:55,358
general overview of connectors. Connectors can be used

508
00:32:55,406 --> 00:33:00,946
to connect different pipelines telemetry.

509
00:33:01,130 --> 00:33:04,570
So here we see that the spanmetrics

510
00:33:04,602 --> 00:33:08,674
connector is an exporter in the traces pipeline, but then it's

511
00:33:08,714 --> 00:33:12,014
listed as a receiver in the metrics pipeline.

512
00:33:12,314 --> 00:33:15,498
So the spanmetrics connector is able to

513
00:33:15,626 --> 00:33:19,786
generate metrics from your traces and those metrics will

514
00:33:19,810 --> 00:33:21,374
go through the metrics pipeline.

515
00:33:24,574 --> 00:33:29,074
For more information, we can read the spanmetrics connector readme.

516
00:33:29,414 --> 00:33:32,782
So what it's doing is it's generating metrics

517
00:33:32,918 --> 00:33:36,310
based on the spans that flow through the

518
00:33:36,342 --> 00:33:40,510
collector. It's able to generate request count

519
00:33:40,622 --> 00:33:45,142
metrics and metrics for error counts and the duration

520
00:33:45,238 --> 00:33:48,438
of spans. So this allows you to search

521
00:33:48,486 --> 00:33:51,054
your data in different, different ways.

522
00:33:51,594 --> 00:33:55,454
You can view your traces and then you can view the same

523
00:33:56,434 --> 00:34:01,362
data just formatted differently and compiled

524
00:34:01,538 --> 00:34:05,698
in metric format. So coming

525
00:34:05,746 --> 00:34:10,334
back here to the configuration, we have the metrics pipeline.

526
00:34:13,074 --> 00:34:17,294
We have the HTTP check receiver,

527
00:34:17,674 --> 00:34:21,374
we have the redis receiver and the OTLP receiver as well.

528
00:34:21,874 --> 00:34:24,834
And all of these receivers are going through the batch processor,

529
00:34:24,874 --> 00:34:29,714
which then sends data to the OTLP HTTP

530
00:34:29,834 --> 00:34:32,974
exporter and the debug exporter.

531
00:34:35,794 --> 00:34:39,274
For logs. We have the OTLP receiver,

532
00:34:39,434 --> 00:34:43,098
the batch processor, and the open search and debug

533
00:34:43,186 --> 00:34:46,570
exporters. A couple things to note here.

534
00:34:46,722 --> 00:34:49,922
You're able to use the same component declaration

535
00:34:50,018 --> 00:34:53,934
in multiple pipelines. It's not a problem.

536
00:34:54,914 --> 00:34:58,378
Another thing, the order of

537
00:34:58,426 --> 00:35:01,754
receivers and exporters in your pipeline does not matter,

538
00:35:01,874 --> 00:35:04,534
but the order of your processors does matter.

539
00:35:04,994 --> 00:35:09,128
Your data is flowing through one processor after the next in order.

540
00:35:09,306 --> 00:35:11,544
So the order is very important.

541
00:35:16,084 --> 00:35:19,540
All right, coming back to our architecture from the exporters alone, it's hard

542
00:35:19,572 --> 00:35:23,164
to know where the data will end up going as far as your back

543
00:35:23,204 --> 00:35:27,124
end is concerned. So the architecture shows us what else

544
00:35:27,164 --> 00:35:30,428
the demo is spinning up. And the demo is

545
00:35:30,476 --> 00:35:35,472
spinning up a Prometheus

546
00:35:35,528 --> 00:35:39,432
instance, Grafana and Jaeger as the backends.

547
00:35:39,608 --> 00:35:43,000
So on the left we have metrics, and metrics will be

548
00:35:43,032 --> 00:35:46,524
going to Prometheus and Grafana,

549
00:35:48,304 --> 00:35:52,804
and our traces will be going to Jaeger and grafana.

550
00:35:56,784 --> 00:35:59,004
I've deployed in kubernetes.

551
00:36:00,524 --> 00:36:03,224
There's another help page to show how to do this.

552
00:36:03,844 --> 00:36:07,772
By default, there's a load generator running

553
00:36:07,868 --> 00:36:11,700
in your environment to mimic

554
00:36:11,852 --> 00:36:15,516
what a real load would be for your

555
00:36:15,620 --> 00:36:19,492
website or for the case of this demo, I have turned

556
00:36:19,508 --> 00:36:22,944
it off so that we can see our own usage information.

557
00:36:24,844 --> 00:36:28,294
So let's go to the website and take a look around.

558
00:36:29,874 --> 00:36:33,442
Okay, go shopping. All right, so here

559
00:36:33,458 --> 00:36:36,414
we have the different products available that we can buy.

560
00:36:37,554 --> 00:36:40,946
Comet book looks nice. Pick up a few

561
00:36:40,970 --> 00:36:44,650
of those and let's

562
00:36:44,682 --> 00:36:47,454
see what other items are recommended.

563
00:36:49,314 --> 00:36:52,538
Okay. The optical tube assembly

564
00:36:52,586 --> 00:36:56,070
looks expensive, so let's do that and we'll

565
00:36:56,102 --> 00:36:57,434
pick up a few.

566
00:37:01,734 --> 00:37:05,110
All right, now that our items are in the cart and

567
00:37:05,142 --> 00:37:08,726
we're ready to check out, we'll just confirm

568
00:37:08,830 --> 00:37:12,406
our information, make sure

569
00:37:12,430 --> 00:37:15,550
it all looks good, and we can place our

570
00:37:15,582 --> 00:37:18,870
order. So here we see our order is

571
00:37:18,902 --> 00:37:21,234
complete and everything looks good.

572
00:37:22,804 --> 00:37:26,524
So let's go see the telemetry that the microservices

573
00:37:26,644 --> 00:37:29,980
have generated

574
00:37:30,052 --> 00:37:33,584
from our instrumentation. So we can go to Jaeger.

575
00:37:34,284 --> 00:37:37,756
One view that is helpful. I'm sure other backends have this

576
00:37:37,780 --> 00:37:41,652
as well, but it's helpful to see the flow of

577
00:37:41,748 --> 00:37:45,324
requests through our architecture. So the traces coming

578
00:37:45,364 --> 00:37:49,000
into jaeger are able to generate this view that

579
00:37:49,032 --> 00:37:52,512
show us where a request goes when we're interacting with

580
00:37:52,528 --> 00:37:55,724
the website. So when we go to checkout,

581
00:37:56,184 --> 00:38:00,184
especially, we can see all the different services that the checkout

582
00:38:00,224 --> 00:38:04,764
service depends on. If we go to search,

583
00:38:05,224 --> 00:38:09,484
we can view traces for specific services.

584
00:38:10,184 --> 00:38:13,496
So I'm going to take a look at the front end service and

585
00:38:13,560 --> 00:38:17,764
find traces that have gone through the front end service.

586
00:38:19,904 --> 00:38:24,888
We're looking for the post operation because this will show us the

587
00:38:24,936 --> 00:38:28,008
different durations for checking out

588
00:38:28,136 --> 00:38:32,032
and buying items from our store. So we can see some services

589
00:38:32,128 --> 00:38:35,704
take quite a long time and some

590
00:38:35,744 --> 00:38:39,360
services are very fast. And this

591
00:38:39,392 --> 00:38:43,814
view helps us see another large

592
00:38:44,114 --> 00:38:47,954
value proposition of open telemetry and the collector as

593
00:38:47,994 --> 00:38:52,082
well, and that these microservices are written in very

594
00:38:52,138 --> 00:38:55,778
different technologies, different languages and so

595
00:38:55,826 --> 00:38:59,734
on. But with our instrumentation and sending

596
00:39:00,034 --> 00:39:03,546
traces through the collector, they can all be viewed in

597
00:39:03,570 --> 00:39:07,666
the same format and in the same chart to

598
00:39:07,810 --> 00:39:11,426
look the same. This helps

599
00:39:11,450 --> 00:39:15,034
a lot with being able to reduce our mean time

600
00:39:15,074 --> 00:39:18,974
to resolution for outages and to be able to search information

601
00:39:19,434 --> 00:39:20,894
in a uniform way.

602
00:39:24,554 --> 00:39:28,714
So from here we can go take a look at our Grafana

603
00:39:28,834 --> 00:39:32,094
instance and we see a few dashboards that are

604
00:39:33,074 --> 00:39:34,894
there for us automatically.

605
00:39:36,104 --> 00:39:40,376
So we can take a look at the demo dashboard and we

606
00:39:40,400 --> 00:39:44,284
can see, usually we can see different

607
00:39:44,904 --> 00:39:48,520
data coming in from the span metrics and there's another dashboard for

608
00:39:48,552 --> 00:39:52,448
that. So we'll take a look there. But we can see logs coming from

609
00:39:52,496 --> 00:39:57,224
the ad service. We can see that most are just informational

610
00:39:57,344 --> 00:40:00,484
severity, but we are getting a couple warnings.

611
00:40:02,204 --> 00:40:07,388
So if we're running into any issues, we can see logs

612
00:40:07,556 --> 00:40:11,412
that may point us in the right direction. We can see the

613
00:40:11,468 --> 00:40:15,260
number of each log message and the number of logs

614
00:40:15,292 --> 00:40:19,636
at each severity level. We also have information

615
00:40:19,780 --> 00:40:25,156
around the microservices and how

616
00:40:25,180 --> 00:40:26,864
they're working performance wise.

617
00:40:48,694 --> 00:40:51,886
All right, we're back. We'll take

618
00:40:51,910 --> 00:40:56,284
a look at other dashboards. We can go to the collector dashboard

619
00:40:59,744 --> 00:41:03,592
here. We can see the rate of data coming

620
00:41:03,648 --> 00:41:07,344
through the collector itself. So we can see how many spans

621
00:41:07,384 --> 00:41:10,608
are coming through, how many metrics and logs are coming through.

622
00:41:10,776 --> 00:41:14,364
We can see the general performance of the collector,

623
00:41:14,824 --> 00:41:18,884
we can see how much memory it's using and so on.

624
00:41:23,164 --> 00:41:26,664
And as mentioned earlier, we can come to the spanmetrics demo

625
00:41:27,324 --> 00:41:31,028
dashboard and this will show us the metrics that the spanmetrics

626
00:41:31,076 --> 00:41:35,012
connector is generating. So latency here

627
00:41:35,148 --> 00:41:38,332
shows how much latency there is,

628
00:41:38,468 --> 00:41:41,964
how long the spans take

629
00:41:42,124 --> 00:41:45,260
for each service. On the

630
00:41:45,332 --> 00:41:48,600
top right, we have how many requests per second

631
00:41:48,672 --> 00:41:51,684
each service is getting.

632
00:41:52,024 --> 00:41:55,320
You can see some are used much more than others and then

633
00:41:55,352 --> 00:41:59,608
we have error rates so we can see if any microservice

634
00:41:59,696 --> 00:42:02,644
is hitting a lot of errors.

635
00:42:10,424 --> 00:42:14,010
So that was a short overview of the open

636
00:42:14,042 --> 00:42:17,394
telemetry demo. We saw how to deploy in a

637
00:42:17,434 --> 00:42:20,610
Kubernetes environment with the collector. We saw the

638
00:42:20,642 --> 00:42:24,162
collector's configuration for receiving telemetry of

639
00:42:24,258 --> 00:42:28,454
traces, metrics and logs, and we were able to see how

640
00:42:28,754 --> 00:42:32,882
useful all this data can be in different back ends.

641
00:42:33,058 --> 00:42:36,914
We saw how we can detect outages quicker with

642
00:42:37,074 --> 00:42:40,758
traces and metrics, and we saw different ways

643
00:42:40,806 --> 00:42:44,034
to find the root cause of issues.

644
00:42:45,614 --> 00:42:49,670
We hope this gives more insight into the full potential

645
00:42:49,822 --> 00:42:52,942
and capabilities of opentelemetry and

646
00:42:52,998 --> 00:42:57,234
the collector as well. And we hope this was helpful in

647
00:42:58,694 --> 00:43:01,582
learning how to configure and use the collector.

648
00:43:01,758 --> 00:43:05,842
Thank you. That was a very cool demo. So as

649
00:43:05,858 --> 00:43:09,594
you can see, it's really easy to get started with the opensymmetry collector

650
00:43:09,674 --> 00:43:13,674
and it's extremely flexible and able to handle pretty much any use case

651
00:43:13,714 --> 00:43:17,554
that you can think of today. When it comes to data collection, there's a wide

652
00:43:17,634 --> 00:43:21,074
range of components out there. We didn't have time to cover all of them,

653
00:43:21,154 --> 00:43:24,530
but the basis basic process you follow across the board.

654
00:43:24,602 --> 00:43:29,026
Go find the GitHub repository that it's being hosted in. Go find the component itself.

655
00:43:29,130 --> 00:43:32,626
And there's rich read me information and examples there, even test data that

656
00:43:32,650 --> 00:43:36,748
you can use in order to kind of try out these components. And the components

657
00:43:36,796 --> 00:43:40,020
cover a wide range of environments or use cases today.

658
00:43:40,132 --> 00:43:43,796
So it's very, very feature rich and new ones are being added all the time.

659
00:43:43,900 --> 00:43:47,012
So if you're not seeing something that you need, go ahead and file a GitHub

660
00:43:47,068 --> 00:43:50,180
issue or feel free to come contribute to the project.

661
00:43:50,252 --> 00:43:53,660
We always welcome people to contribute here and kind of expand

662
00:43:53,732 --> 00:43:56,876
upon what is available. The goal is to provide as wide

663
00:43:56,900 --> 00:44:00,532
of coverage as possible. Now, the collector is very, very powerful,

664
00:44:00,588 --> 00:44:03,958
right? Single binary that can be deployed in a variety

665
00:44:04,006 --> 00:44:06,886
of different form factors that allows you to ingest,

666
00:44:06,990 --> 00:44:11,006
process and export your telemetry data. And it really gives you this

667
00:44:11,110 --> 00:44:14,654
vendor agnostic solution very easily can transform data

668
00:44:14,694 --> 00:44:19,174
from one format into another. And that's because under the covers it's using OTLP,

669
00:44:19,254 --> 00:44:22,830
right? The collector converts everything to OTLP and that allows us

670
00:44:22,862 --> 00:44:26,166
to do it in a standard way to perform different processing capabilities.

671
00:44:26,310 --> 00:44:30,342
Before that data is emitted from the collector instance. Now again,

672
00:44:30,398 --> 00:44:34,734
you don't have to use the collector, it's just available there to provide some capabilities

673
00:44:34,814 --> 00:44:38,374
that you may be looking to generically solve for. It can eliminate

674
00:44:38,414 --> 00:44:41,950
some of the proprietary aspects of using a vendor solution here,

675
00:44:42,062 --> 00:44:45,726
and it can even help you consolidate the number of agents or collectors that you're

676
00:44:45,750 --> 00:44:49,750
deploying because it can actually support multiple different signals for you.

677
00:44:49,862 --> 00:44:53,470
Now, if you're looking to learn more, there's plenty of documentation out there.

678
00:44:53,502 --> 00:44:57,208
We kind of showed you some on GitHub, but there's also the opensymmetry I O

679
00:44:57,256 --> 00:45:00,400
site which has some rich getting started information, how to like

680
00:45:00,512 --> 00:45:04,136
bootstrap the collector as an instance in different environments,

681
00:45:04,200 --> 00:45:07,752
as well as taking a look at the demo environment that Curtis was referring to.

682
00:45:07,848 --> 00:45:10,832
So definitely encourage you to kind of take a look at some of the material

683
00:45:10,888 --> 00:45:15,032
out there. And again, feel free to contribute and add more. The more documentation we

684
00:45:15,048 --> 00:45:18,664
can provide, the easier it will be for people to kind of get started.

685
00:45:18,824 --> 00:45:21,896
So thank you so much. We hope you enjoyed this talk. We hope you learned

686
00:45:21,920 --> 00:45:24,790
a few things about how the collector can be used in your environment environment.

687
00:45:24,902 --> 00:45:28,598
And if you have any questions, feel free to ping us on the CNCF

688
00:45:28,646 --> 00:45:31,534
slack instance. Curtis and I are both very active.

689
00:45:31,654 --> 00:45:33,414
Thank you so much and we'll be chatting soon.

