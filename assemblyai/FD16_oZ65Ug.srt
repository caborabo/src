1
00:00:20,570 --> 00:00:23,918
Hi, everyone. I'm Kareen. I'm going to do my introduction now just because I'm going

2
00:00:23,924 --> 00:00:27,478
to do my portion of the talk now. So nice to meet you

3
00:00:27,484 --> 00:00:30,386
guys. Let me share my screen so that way you can actually see a picture

4
00:00:30,418 --> 00:00:33,240
of me so you know what I look like. That's me.

5
00:00:33,610 --> 00:00:36,982
I'm Kareem Wallach. I have a consulting company called Project

6
00:00:37,036 --> 00:00:40,282
Elevate. I do developer relations, community stuff.

7
00:00:40,336 --> 00:00:44,006
It's basically all things that work with developer

8
00:00:44,038 --> 00:00:47,420
tools and the community. Fun people like you

9
00:00:48,670 --> 00:00:52,374
and. Yeah, I'm just going to talk about a portion of some of our presentation

10
00:00:52,422 --> 00:00:55,994
with Tim. So just to take it off, I'm just going to give some background

11
00:00:56,042 --> 00:00:59,546
about what is stream processing, because I don't know what people's

12
00:00:59,578 --> 00:01:03,694
experiences are, if they're aware of stream processing or streaming

13
00:01:03,822 --> 00:01:06,930
technologies in general. So I'm just going to give a quick overview.

14
00:01:08,150 --> 00:01:11,762
So stream processing is basically

15
00:01:11,896 --> 00:01:15,342
a approach dealing with data where you're.

16
00:01:15,406 --> 00:01:18,870
Well, there's two parts to it. The first is a streaming ingestion part.

17
00:01:18,940 --> 00:01:22,770
So streaming technologies, things like Kafka or red panda,

18
00:01:22,850 --> 00:01:26,742
where you are ingesting data as it's happening,

19
00:01:26,876 --> 00:01:30,418
instead of like batch or like more

20
00:01:30,444 --> 00:01:34,198
like batch kind of processing. Batch ingestion,

21
00:01:34,374 --> 00:01:37,626
this is more of like a continuous flow. So as the events happen, you are

22
00:01:37,648 --> 00:01:41,194
constantly ingesting the data. And then the processing side is you're doing something

23
00:01:41,232 --> 00:01:44,654
with that data as you're ingesting it. So it's kind of as real time

24
00:01:44,692 --> 00:01:48,158
as you can get. So this process

25
00:01:48,244 --> 00:01:51,706
of stream processing, there's a lot of processes of stream processing

26
00:01:51,738 --> 00:01:54,850
and processing. The data process of stream processing.

27
00:01:56,310 --> 00:02:00,238
So the process of bringing

28
00:02:00,254 --> 00:02:03,886
the data in as it's happening and then streaming kind of allows

29
00:02:03,918 --> 00:02:07,638
you to be able to do real time analysis on the data

30
00:02:07,724 --> 00:02:11,750
as you're bringing it in, and then transformations and computation

31
00:02:12,410 --> 00:02:15,782
before you're actually storing the data. So if you're almost

32
00:02:15,836 --> 00:02:19,610
thinking about it kind of in a philosophical way, the data

33
00:02:19,680 --> 00:02:23,478
is, as it's being ingested, you're able to make those transformations

34
00:02:23,654 --> 00:02:27,194
while it's in flight and then storing it

35
00:02:27,232 --> 00:02:31,006
or putting it into another stream processing system or something like that.

36
00:02:31,108 --> 00:02:34,254
But then there's a downstream side,

37
00:02:34,292 --> 00:02:37,998
too. So basically,

38
00:02:38,084 --> 00:02:42,062
stream processing is used to extract valuable insights in real

39
00:02:42,116 --> 00:02:45,774
time, things like detecting anomalies and trigger

40
00:02:45,822 --> 00:02:48,260
actions based on incoming data.

41
00:02:49,750 --> 00:02:53,554
I do go over some brief use cases here. There is a

42
00:02:53,592 --> 00:02:57,446
lot you can do with stream processing and real time

43
00:02:57,628 --> 00:03:01,622
kind of analytics and capabilities, including enabling your

44
00:03:01,676 --> 00:03:05,394
users to do more by providing them actionable

45
00:03:05,442 --> 00:03:09,420
insights, which is basically real time insights that then they can take action on.

46
00:03:10,350 --> 00:03:13,654
If you think about LinkedIn,

47
00:03:13,702 --> 00:03:17,606
for example, some of the real time stuff that they have implemented

48
00:03:17,638 --> 00:03:21,350
into their systems is if someone views your profile,

49
00:03:21,510 --> 00:03:25,354
a user of LinkedIn can then go in and take action.

50
00:03:25,402 --> 00:03:28,618
They can message that person, they can view their profile,

51
00:03:28,714 --> 00:03:32,574
whatever. They can take action based on that insight that they now

52
00:03:32,612 --> 00:03:36,174
have. There's a lot of cases like this where people are exposing

53
00:03:36,302 --> 00:03:40,226
real time insights and real

54
00:03:40,248 --> 00:03:43,406
time analytics. The insights are a little bit different. It's more of like a decision

55
00:03:43,438 --> 00:03:46,594
making capability. But a lot of organizations are doing this now too. So that's something

56
00:03:46,632 --> 00:03:50,434
to keep in mind. So there's different kinds of queries

57
00:03:50,482 --> 00:03:54,002
you can do inside stream processing. So we kind of talked about the streaming technologies

58
00:03:54,066 --> 00:03:58,194
first, where you're ingesting the data and you're able to do these transformations

59
00:03:58,242 --> 00:04:02,226
on it as it's in flight. Then you have the type of queries

60
00:04:02,338 --> 00:04:05,546
that you're able to do inside of stream processing as

61
00:04:05,568 --> 00:04:08,378
the data is in flight. So just to give a couple of examples, I'm going

62
00:04:08,384 --> 00:04:11,614
to give some examples of each. These are all listed here, so you can read

63
00:04:11,652 --> 00:04:15,722
along if you'd like. So the first one is filtering.

64
00:04:15,866 --> 00:04:19,454
This involves a selection of specific

65
00:04:19,572 --> 00:04:23,930
data from a stream based on predefined conditions.

66
00:04:24,010 --> 00:04:27,342
So an example of that is like filtering out log entries

67
00:04:27,406 --> 00:04:31,022
that contain errors that are needed for immediate attention.

68
00:04:31,086 --> 00:04:34,938
That's like a filtering example. So this is an example of queries and stream

69
00:04:34,974 --> 00:04:38,630
processing. Aggregation is

70
00:04:38,700 --> 00:04:42,406
involving of summarizing or reducing data over a

71
00:04:42,428 --> 00:04:45,718
certain time window or key. Some of these might be kind

72
00:04:45,724 --> 00:04:48,850
of straightforward. You might be already familiar with these types of queries,

73
00:04:49,010 --> 00:04:52,106
but this is all stuff that you can do in stream processing in a very

74
00:04:52,128 --> 00:04:56,310
real time kind of capability. This allows you to calculate the average temperature

75
00:04:56,390 --> 00:05:01,014
over the last ten minutes in a sensor stream joining

76
00:05:01,062 --> 00:05:04,362
streams. This one I think is just really cool.

77
00:05:04,416 --> 00:05:08,490
Like you're able to basically ingest streams from multiple sources

78
00:05:08,570 --> 00:05:12,458
and bring them together to kind of have a little bit more of

79
00:05:12,564 --> 00:05:16,450
a dimensional analyzed.

80
00:05:17,110 --> 00:05:21,202
So one of these examples is like combining user click data with

81
00:05:21,256 --> 00:05:23,650
product data to analyze user behavior.

82
00:05:24,630 --> 00:05:25,730
Windowing,

83
00:05:28,250 --> 00:05:31,362
they group data into fixed time intervals

84
00:05:31,426 --> 00:05:35,058
or Windows for analysis. It's windowing. I think it's

85
00:05:35,074 --> 00:05:38,726
pretty straightforward. So example of that is analyzed the total sales

86
00:05:38,908 --> 00:05:42,058
for each hour of the day over a week.

87
00:05:42,144 --> 00:05:44,730
So that's what an example of windowing would be.

88
00:05:44,880 --> 00:05:48,406
Pattern recognition. This is used a lot like with anomaly

89
00:05:48,438 --> 00:05:52,086
detection related stuff. It's like identifying a sequence or pattern of events

90
00:05:52,118 --> 00:05:55,914
in a data stream. This is where you can find anomalies

91
00:05:55,962 --> 00:05:59,626
and things like that. So example of this is detecting

92
00:05:59,658 --> 00:06:02,554
a series of login failures as a security breach.

93
00:06:02,602 --> 00:06:05,790
Attempt stateful processing.

94
00:06:06,290 --> 00:06:10,126
Maintain state information to analyze data over time, such as tracking user

95
00:06:10,158 --> 00:06:13,986
sessions. So example, this is monitoring user behavior and

96
00:06:14,008 --> 00:06:17,718
detecting when a user session is idle. So I'm going to talk a

97
00:06:17,724 --> 00:06:21,714
little bit about rising wave. This is a stream processing

98
00:06:21,762 --> 00:06:25,618
database. Rising wave, it's open source.

99
00:06:25,714 --> 00:06:29,598
It's a SQL distributed stream processing

100
00:06:29,634 --> 00:06:33,514
database. So essentially what it does, this is

101
00:06:33,552 --> 00:06:36,700
like a lot of just jargon, but I'll just explain it to you guys.

102
00:06:37,630 --> 00:06:41,514
All right, so basically it allows you to be able to pull in data as

103
00:06:41,552 --> 00:06:45,194
you're ingesting the stream. So it works with streaming technologies,

104
00:06:45,322 --> 00:06:48,446
which I'll show you, kind of dig a little bit deeper into this in a

105
00:06:48,468 --> 00:06:51,886
second, but you're able to pull this data in and then you

106
00:06:51,908 --> 00:06:55,954
can perform incremental computations as the data comes in

107
00:06:56,152 --> 00:07:00,558
and then update the results dynamically. So ingesting

108
00:07:00,574 --> 00:07:04,382
the data and doing transformations and queries

109
00:07:04,446 --> 00:07:08,278
on this data as you're bringing it in and then storing it

110
00:07:08,444 --> 00:07:10,950
in a downstream kind of capacity.

111
00:07:11,610 --> 00:07:14,742
So there's a downstream side.

112
00:07:14,796 --> 00:07:17,914
So it does have its own storage. So you can

113
00:07:17,952 --> 00:07:20,694
access the data in rising wave.

114
00:07:20,742 --> 00:07:25,340
Specifically the storage and the

115
00:07:25,710 --> 00:07:29,894
computation side of things are separate, so you can actually scale them out separately

116
00:07:29,942 --> 00:07:33,566
if you want. And you can also do downstream in another database or

117
00:07:33,588 --> 00:07:36,480
data warehouse or whatever if you want to do that.

118
00:07:36,850 --> 00:07:40,666
The benefits of having this downstream thing with rising wave is easily accessing

119
00:07:40,698 --> 00:07:44,500
this data that you just perform these transformations on.

120
00:07:45,670 --> 00:07:49,490
It is SQL based, which is really nice. If you know SQL, it's pretty easy

121
00:07:49,560 --> 00:07:53,150
to pick up. It's postgres compatible,

122
00:07:53,230 --> 00:07:56,662
which is also pretty great. It's like very

123
00:07:56,796 --> 00:08:00,034
easy. If you've ever dug into stream processing,

124
00:08:00,162 --> 00:08:05,000
there's some technologies out there that are pretty

125
00:08:06,010 --> 00:08:09,110
big builds. Like the learning curve is

126
00:08:09,180 --> 00:08:12,474
pretty challenging. So this kind of serves as really

127
00:08:12,592 --> 00:08:15,834
easy learning curve because it's all SQL and postgres. So if you know

128
00:08:15,872 --> 00:08:19,334
SQL and you're working with any kind of postgres compatible

129
00:08:19,382 --> 00:08:23,258
type of data stores or anything like that, then it's

130
00:08:23,274 --> 00:08:26,554
pretty straightforward to use. I mentioned the storage

131
00:08:26,602 --> 00:08:30,106
and computation are separate. It has incremental

132
00:08:30,138 --> 00:08:33,406
updates on materialized views. I didn't put

133
00:08:33,428 --> 00:08:36,446
that last part in there, but I think it's important. It's also built with rust.

134
00:08:36,558 --> 00:08:39,810
A lot of people love that because it's very performant.

135
00:08:41,110 --> 00:08:44,466
Okay, so the data stream process, this is kind of what I was

136
00:08:44,488 --> 00:08:48,166
like making jokes about the stream processing, the data stream process, the process of

137
00:08:48,188 --> 00:08:51,478
the data streaming and the data stream process is like this.

138
00:08:51,564 --> 00:08:55,458
So as I mentioned, rising wave allows

139
00:08:55,474 --> 00:08:59,100
you and this just stream processing systems in general.

140
00:08:59,630 --> 00:09:03,210
But I'm specifically speaking about rising wave. In this case, it allows you to

141
00:09:03,280 --> 00:09:07,066
accept data from a

142
00:09:07,168 --> 00:09:10,986
streaming source. So you do have to have something that

143
00:09:11,008 --> 00:09:14,394
allows you to be able to ingest this data in a real time fashion.

144
00:09:14,442 --> 00:09:18,186
So as an event happens, it's pulling into the data into rising

145
00:09:18,218 --> 00:09:21,630
wave. So you can use any like, so these are a bunch of popular

146
00:09:22,210 --> 00:09:25,678
streaming technologies like Kafka and Pulsar.

147
00:09:25,854 --> 00:09:29,342
There's also red panda. It's not listed on here, but that's also another really popular

148
00:09:29,406 --> 00:09:32,980
one, too, Amazon kinesis. So you can pull these

149
00:09:33,350 --> 00:09:37,122
streams and these pieces of data into

150
00:09:37,176 --> 00:09:40,566
rising wave, and then inside it has

151
00:09:40,588 --> 00:09:44,134
the ability to ingest it. You can do kind of a lot of those different

152
00:09:44,172 --> 00:09:47,486
kind of queries that I mentioned in the earlier slides of joining

153
00:09:47,538 --> 00:09:51,466
streams, filtering, aggregation transformations, whatever, and then have

154
00:09:51,488 --> 00:09:54,970
the downstream side where you can store it

155
00:09:55,120 --> 00:09:58,794
to either rising wave or other kind of data

156
00:09:58,832 --> 00:10:02,780
warehouses or data lakes or whatever kind of processing you're looking for.

157
00:10:03,710 --> 00:10:06,846
So here are some use cases and examples. I'm going to run through these really

158
00:10:06,868 --> 00:10:09,920
quickly just because I know there's a lot of material in this talk.

159
00:10:10,290 --> 00:10:13,786
But streaming ETL is basically the practice of extracting,

160
00:10:13,818 --> 00:10:17,250
transforming, and moving data within different systems.

161
00:10:17,830 --> 00:10:21,198
So it allows you to ingest the data from a wide range of data sources

162
00:10:21,294 --> 00:10:24,594
and transform that data in real time and

163
00:10:24,632 --> 00:10:27,750
then sync the process data to downstream systems.

164
00:10:28,810 --> 00:10:32,242
Real time analysis, this is kind of a popular

165
00:10:32,306 --> 00:10:35,814
trend now everyone's doing real time analytics. So real time

166
00:10:35,852 --> 00:10:39,880
analytics is basically the practice of analyzing data

167
00:10:40,570 --> 00:10:44,438
as it's generated. So as soon as something happens, you're analyzing

168
00:10:44,454 --> 00:10:48,566
it, you're running some kind of transformation before it's even stored. And that offers

169
00:10:48,598 --> 00:10:51,898
you the ability to be as real time as possible because you're literally transforming the

170
00:10:51,904 --> 00:10:55,226
data. One thing that I think

171
00:10:55,248 --> 00:10:57,886
that's just important to keep in mind is a lot of times, and I know

172
00:10:57,908 --> 00:11:01,486
this happened for me, and this is why I kind of said this. I said

173
00:11:01,508 --> 00:11:04,738
this before, I'm going to say it again, because when I first kind

174
00:11:04,744 --> 00:11:07,938
of learned about real time analytics in general, I think a

175
00:11:07,944 --> 00:11:12,542
lot of us just think about the generalized uses

176
00:11:12,606 --> 00:11:16,686
of analytics systems where it's like dashboards

177
00:11:16,718 --> 00:11:20,086
and things like that. The capabilities of what you can do with real

178
00:11:20,108 --> 00:11:23,490
time analytics go way beyond dashboards.

179
00:11:23,570 --> 00:11:26,790
And this is kind of what I was talking about those actionable insights before,

180
00:11:26,940 --> 00:11:30,838
that's like when you're offering your users

181
00:11:30,934 --> 00:11:34,886
insights or real time analytics to be able to make smarter

182
00:11:34,918 --> 00:11:38,554
or more effective decisions in real time. So things that

183
00:11:38,592 --> 00:11:41,962
have that need to know of what's happening right this very second,

184
00:11:42,096 --> 00:11:45,966
you can either use it internally and have a system process it like

185
00:11:45,988 --> 00:11:49,518
a machine that's like a llamalay detection related stuff. You can have a

186
00:11:49,524 --> 00:11:52,866
human process it or you could give it to

187
00:11:52,888 --> 00:11:56,290
your end users. And then people productize these things too,

188
00:11:56,360 --> 00:11:59,826
where they literally, people pay for premium services either for an

189
00:11:59,848 --> 00:12:04,740
entire software or for

190
00:12:05,590 --> 00:12:09,094
just certain real time analytic capabilities and things like that.

191
00:12:09,132 --> 00:12:12,294
So that's just something that I think is really important to keep in mind

192
00:12:12,332 --> 00:12:15,894
in terms of real time analytics. It provides business with

193
00:12:15,932 --> 00:12:19,020
valuable insights and actionable information real time.

194
00:12:19,710 --> 00:12:23,258
And it does provide competitive advantages by enabling business to

195
00:12:23,344 --> 00:12:27,254
respond quickly and changing market conditions. It also offers competitive

196
00:12:27,302 --> 00:12:30,538
advantages because if you use it for the actionable insights that

197
00:12:30,544 --> 00:12:34,106
I was kind of mentioning with your users, you can make your users

198
00:12:34,138 --> 00:12:38,254
happier and they could have a more positive experience using your technology if

199
00:12:38,292 --> 00:12:41,790
they have a more real time, insightful experience.

200
00:12:41,860 --> 00:12:46,754
I mean, everybody knows we don't have patients, and as

201
00:12:46,792 --> 00:12:50,546
time goes, we have less and less and less patients. Okay.

202
00:12:50,568 --> 00:12:54,206
And it could also be used to improve operational efficiency and reduce costs

203
00:12:54,398 --> 00:12:58,006
so you can use it internally as well. Another example,

204
00:12:58,108 --> 00:13:01,794
event driven applications, enables continuous

205
00:13:01,842 --> 00:13:05,394
event ingestion and processing, maintains context

206
00:13:05,442 --> 00:13:09,318
across multiple events, and performs sophisticated processing,

207
00:13:09,414 --> 00:13:12,970
and then detects complex patterns and correlations within

208
00:13:13,040 --> 00:13:17,178
a stream of events, and provides direct integration with

209
00:13:17,184 --> 00:13:20,730
a variety of upstream and downstream systems.

210
00:13:21,730 --> 00:13:24,730
So really quick rising wave architecture,

211
00:13:24,810 --> 00:13:27,946
kind of what it looks like. So first is you have a serving layer.

212
00:13:28,058 --> 00:13:32,714
This parses SQL queries and performs the planning and optimizations

213
00:13:32,762 --> 00:13:36,286
of the query jobs. Then you have a processing layer

214
00:13:36,398 --> 00:13:38,930
which performs all your computation.

215
00:13:39,990 --> 00:13:43,774
Then you have, I think I clicked too quickly, but it doesn't matter. Metadata management

216
00:13:43,822 --> 00:13:47,974
service. This manages the metadata of

217
00:13:48,012 --> 00:13:51,554
different nodes and then coordinates operations amongst these nodes.

218
00:13:51,602 --> 00:13:56,646
And then you have the storage layer that stores and retrieves data

219
00:13:56,748 --> 00:13:59,420
from object stores like storage like s three.

220
00:13:59,790 --> 00:14:03,340
And I think that's it. And then, Tim, I'll let you.

221
00:14:03,950 --> 00:14:07,466
Thanks. That was a really good introduction to a lot

222
00:14:07,488 --> 00:14:11,158
of complex topics there. And I'll take

223
00:14:11,184 --> 00:14:14,462
over from here. So we saw some really

224
00:14:14,516 --> 00:14:17,710
important things to see on how you're going to

225
00:14:17,860 --> 00:14:21,086
be general topics, how you can query it with

226
00:14:21,108 --> 00:14:24,482
something like rising wave. But let's cover a little

227
00:14:24,536 --> 00:14:27,746
bit more. I'm Tim Spann. I do

228
00:14:27,768 --> 00:14:31,394
a lot of streaming stuff I have some virtual meetups around the

229
00:14:31,432 --> 00:14:35,194
world. Put a newsletter out, you'll get all these slides.

230
00:14:35,342 --> 00:14:38,920
I don't want to spend too much time giving you 10,000

231
00:14:39,610 --> 00:14:42,870
overview things. I think the new thing to mention

232
00:14:42,940 --> 00:14:46,002
though is with generative AI,

233
00:14:46,066 --> 00:14:49,158
with these large language models and some of the smaller and

234
00:14:49,164 --> 00:14:52,682
mid sized language models, now you need a lot of data.

235
00:14:52,816 --> 00:14:56,614
You get that processed things like Apache,

236
00:14:56,662 --> 00:15:00,394
Nifi and Python let you get a lot of that data into

237
00:15:00,432 --> 00:15:03,806
the pipeline, whether it's going right to a model or

238
00:15:03,828 --> 00:15:07,486
using to a vector store. And then once you have it cleaned up,

239
00:15:07,508 --> 00:15:10,986
maybe I get the results from stock feeds,

240
00:15:11,098 --> 00:15:14,798
maybe I get it back from the model

241
00:15:14,884 --> 00:15:18,562
or some things stored in a vector store. I get that into

242
00:15:18,616 --> 00:15:21,906
something like Apache Kafka, which we have at Cloudera. And then

243
00:15:21,928 --> 00:15:25,670
I could share that with rising wave so they could do some real time

244
00:15:25,740 --> 00:15:28,982
analytic decisions on there. And maybe they

245
00:15:29,036 --> 00:15:32,966
put it into another Kafka topic which I could read later,

246
00:15:33,148 --> 00:15:35,350
run some additional processing.

247
00:15:35,930 --> 00:15:39,802
There is multiple levels of

248
00:15:39,856 --> 00:15:44,250
all of this, and a lot of it's enabled by Python.

249
00:15:44,670 --> 00:15:48,380
Python fits into many layers of this process,

250
00:15:48,830 --> 00:15:52,974
whether that's getting data into Kafka, and I'll show you that real quick,

251
00:15:53,172 --> 00:15:56,254
or it's being called by Nifi to do

252
00:15:56,292 --> 00:15:59,486
different processing for chunking up data,

253
00:15:59,588 --> 00:16:02,882
getting it into a vector store, running at the

254
00:16:02,936 --> 00:16:05,902
edge, working with different databases,

255
00:16:06,046 --> 00:16:09,700
lots out there. I have a number of articles on

256
00:16:10,150 --> 00:16:13,902
how to use that. Now we mentioned Apache Iceberg,

257
00:16:14,046 --> 00:16:16,886
again getting a lot into this one talk.

258
00:16:17,068 --> 00:16:21,138
But Iceberg is a very interesting table format

259
00:16:21,314 --> 00:16:25,394
that sits on top of usually apache parquet,

260
00:16:25,522 --> 00:16:29,402
could be another file format like Orc, but let's just keep it

261
00:16:29,456 --> 00:16:33,082
easy and say parquet. And there's tools built

262
00:16:33,136 --> 00:16:36,918
into the cloud to get data into these formats.

263
00:16:37,094 --> 00:16:41,214
And what's nice is having things in iceberg tables means

264
00:16:41,412 --> 00:16:44,480
everybody in every data store,

265
00:16:45,090 --> 00:16:48,286
any kind of query engine can access it.

266
00:16:48,388 --> 00:16:52,206
So it's very helpful. And I'll show you how to do that on

267
00:16:52,228 --> 00:16:55,538
the simple. So you could do that on your laptop just to try out.

268
00:16:55,624 --> 00:16:59,486
How can I get stuff into iceberg tables, into Kafka?

269
00:16:59,518 --> 00:17:03,266
Just using Python show you how easy that is. And then

270
00:17:03,288 --> 00:17:06,406
when you get into the enterprise level, all the governance and

271
00:17:06,428 --> 00:17:10,118
all the tools and encryption that you need in there are available.

272
00:17:10,284 --> 00:17:13,846
Don't have to learn anything new. Really easy

273
00:17:13,948 --> 00:17:18,390
to do that. Apache Nifi

274
00:17:18,830 --> 00:17:22,394
again, you can run it on your laptop or in

275
00:17:22,432 --> 00:17:26,490
production. Cloudera from Cloudera makes it easy and scalable

276
00:17:26,910 --> 00:17:30,234
and it's been around forever. So what's nice is

277
00:17:30,432 --> 00:17:33,582
it just doesn't die. It just keeps getting better and better.

278
00:17:33,716 --> 00:17:37,562
I've left servers running for months on an old laptop

279
00:17:37,626 --> 00:17:41,006
sitting somewhere. It just keeps running. And I show you a

280
00:17:41,028 --> 00:17:44,670
quick bit of that. And with the new version, which I have another

281
00:17:44,740 --> 00:17:48,706
talk on, we show you how we've made it so you can expand the

282
00:17:48,728 --> 00:17:51,906
tool and add your pipelines with Python. I show you

283
00:17:51,928 --> 00:17:55,298
a little bit of that today. Providence is important.

284
00:17:55,464 --> 00:17:58,774
So everything I run I know what happens. So if people

285
00:17:58,812 --> 00:18:02,262
go, how did I load this data? I could show you has

286
00:18:02,316 --> 00:18:06,242
a lot of pre built components to handle,

287
00:18:06,306 --> 00:18:10,666
lots of different formats for processing. This comes in really

288
00:18:10,768 --> 00:18:14,534
handy when you don't want to write your own boilerplate

289
00:18:14,582 --> 00:18:18,074
code to do everything. So do Python for the important

290
00:18:18,192 --> 00:18:21,870
stuff, complex processing,

291
00:18:22,930 --> 00:18:26,494
using some of the amazing libraries for vector stores and

292
00:18:26,532 --> 00:18:30,906
LLM, and all the easy stuff you could do with stocks.

293
00:18:30,938 --> 00:18:34,146
I'll show you some example code I have here so you could

294
00:18:34,168 --> 00:18:37,698
do that yourself really easy. We mentioned

295
00:18:37,784 --> 00:18:41,586
Kafka. Think of it as just an

296
00:18:41,768 --> 00:18:45,254
infinitely sized pipeline that I can put

297
00:18:45,292 --> 00:18:48,770
any data I want into, usually structured Json,

298
00:18:48,850 --> 00:18:52,662
avro protobuff, things like that.

299
00:18:52,796 --> 00:18:56,406
Get that in there and distribute to as many people who want it,

300
00:18:56,508 --> 00:18:59,494
which is really nice. I just have to get it in there.

301
00:18:59,612 --> 00:19:03,562
Then anyone who wants to subscribe to and can get to it and

302
00:19:03,616 --> 00:19:07,258
it scales out as big as you need to go. It is a

303
00:19:07,344 --> 00:19:11,022
nice buffer and pipeline to get my data to where it needs

304
00:19:11,076 --> 00:19:14,480
to go. But let's show you stuff actually running.

305
00:19:15,730 --> 00:19:20,106
You don't want to just see awesome looking generated

306
00:19:20,298 --> 00:19:23,842
graphics, but I'll show you one where I'm taking stuff

307
00:19:23,896 --> 00:19:27,950
from the fin hub stock API

308
00:19:28,110 --> 00:19:31,714
with Python. And what's nice there is it

309
00:19:31,752 --> 00:19:35,234
grabs websockets, push it to Kafka and

310
00:19:35,272 --> 00:19:38,390
do what I need to do with it. I can also

311
00:19:38,460 --> 00:19:43,250
process things like documents and different stock APIs,

312
00:19:43,410 --> 00:19:46,118
and I've got links to all the code here,

313
00:19:46,284 --> 00:19:49,258
so you can try that out on your own.

314
00:19:49,424 --> 00:19:52,460
But let's go through a couple of quick

315
00:19:52,830 --> 00:19:57,226
passes here and I'll first show you where the source code is.

316
00:19:57,328 --> 00:20:00,714
I've got a new one on grabbing

317
00:20:00,842 --> 00:20:05,802
stock data from Yahoo. Using the Yahoo finance

318
00:20:05,946 --> 00:20:10,334
library for Python. I am

319
00:20:10,452 --> 00:20:14,966
reading that and then as you can see here, I'm using Py

320
00:20:15,018 --> 00:20:18,450
iceberg to set up an iceberg catalog.

321
00:20:19,110 --> 00:20:22,882
Again, this is a small one, runs on your

322
00:20:23,016 --> 00:20:26,866
laptop using SQL lite, so you don't need a giant

323
00:20:27,058 --> 00:20:30,914
catalog or meta store out there, obviously not for production.

324
00:20:31,042 --> 00:20:34,994
And I've got a little mini instance

325
00:20:35,042 --> 00:20:38,406
running. So again, I don't have to pay for s three. Something to

326
00:20:38,428 --> 00:20:42,006
think about while you're learning, trying things out. Especially if you're

327
00:20:42,118 --> 00:20:46,154
coming from the Python world where you may not have

328
00:20:46,192 --> 00:20:49,786
access to a large data platform like Cloudera and you might have

329
00:20:49,808 --> 00:20:53,342
to just start learning on a laptop or whatever

330
00:20:53,396 --> 00:20:57,150
you have. So I'm just going through just

331
00:20:57,220 --> 00:21:00,560
calling as many of

332
00:21:01,570 --> 00:21:04,754
stock prices as I want, getting them back and

333
00:21:04,792 --> 00:21:08,978
sending them to Kafka. That's how easy it is to send from

334
00:21:09,064 --> 00:21:12,558
Python to Kafka. You set up a producer pointing

335
00:21:12,574 --> 00:21:16,690
to whatever servers you have. Again, for me, that's running in Docker.

336
00:21:16,850 --> 00:21:21,074
And then I send the message, give it a key that's

337
00:21:21,122 --> 00:21:25,400
gone, that's into Kafka. We'll show you that. And then over here

338
00:21:26,330 --> 00:21:30,022
I'm grabbing bunches of 1000 of those records

339
00:21:30,166 --> 00:21:33,046
and putting them into an iceberg table,

340
00:21:33,238 --> 00:21:36,842
appending that got a short sleep there.

341
00:21:36,976 --> 00:21:40,670
And every 1000 we refresh that just so

342
00:21:40,820 --> 00:21:45,194
the files aren't too small. They're still pretty small, but it's

343
00:21:45,242 --> 00:21:48,846
nice to keep them smaller than zero

344
00:21:48,948 --> 00:21:52,382
here. And then this is the data that's getting written to

345
00:21:52,436 --> 00:21:56,062
Kafka. It's Json, pretty straightforward

346
00:21:56,126 --> 00:21:59,394
and I can put as many as I want here and I could share

347
00:21:59,432 --> 00:22:02,814
it with whoever wants it. It could be a python user, a Java

348
00:22:02,862 --> 00:22:06,386
user, spark, rising wave,

349
00:22:06,578 --> 00:22:10,134
flink, nifi. Almost anybody can read

350
00:22:10,252 --> 00:22:14,054
Kafka, which is really nice. So we got that data in

351
00:22:14,092 --> 00:22:17,670
and on the other end I've got it into iceberg.

352
00:22:18,030 --> 00:22:21,466
This is the data. It's in parquet. The amazing

353
00:22:21,568 --> 00:22:24,918
part for iceberg is it creates this metadata

354
00:22:25,014 --> 00:22:28,726
that lets me do some amazing things like append,

355
00:22:28,918 --> 00:22:33,038
like change data, change field, do time travel.

356
00:22:33,204 --> 00:22:36,702
So I could say, what did the data look like two days

357
00:22:36,756 --> 00:22:40,000
ago, an hour ago? That is really cool.

358
00:22:40,370 --> 00:22:42,640
And that might be a feature you want.

359
00:22:43,730 --> 00:22:47,426
I'm going to send some more data into there. Let me show you

360
00:22:47,448 --> 00:22:50,626
some of this running. And I'm just running this like

361
00:22:50,648 --> 00:22:54,046
you saw in the code, just printing it out just to see that it's

362
00:22:54,078 --> 00:22:57,910
running. And then that is sending each record

363
00:22:57,980 --> 00:23:02,162
as it comes in into Kafka, which makes sense for Kafka.

364
00:23:02,226 --> 00:23:06,006
Kafka likes that sort of thing. As you get more

365
00:23:06,028 --> 00:23:09,500
and more data there and every thousand,

366
00:23:10,350 --> 00:23:14,234
I'm writing that to Parquet, which we

367
00:23:14,272 --> 00:23:17,980
can take a look at. There's the head.

368
00:23:18,430 --> 00:23:22,074
Just so we don't need a full query engine. I could just use the parquet

369
00:23:22,122 --> 00:23:24,480
tools to make sure those files look okay.

370
00:23:25,170 --> 00:23:29,086
And I could look at the header for that. You do whatever I

371
00:23:29,108 --> 00:23:32,282
want with those files, get a sample,

372
00:23:32,346 --> 00:23:36,190
query them, convert them. The command line tools

373
00:23:36,270 --> 00:23:39,794
are pretty good for that. And you can see, make sure the data

374
00:23:39,832 --> 00:23:43,490
is coming in and you can see we're pushing data

375
00:23:43,560 --> 00:23:47,080
out. We've got a couple of other ones. I mentioned

376
00:23:48,170 --> 00:23:51,926
that finance one, that is pretty easy for me to

377
00:23:51,948 --> 00:23:55,686
just run that and if

378
00:23:55,708 --> 00:23:59,094
I could spell properly and then I'd open websockets.

379
00:23:59,142 --> 00:24:03,110
You can see this one is super fast, python to websockets

380
00:24:03,190 --> 00:24:06,506
to Kafka, amazingly fast. And we

381
00:24:06,528 --> 00:24:10,314
just send that here and you can see that just the amount

382
00:24:10,352 --> 00:24:14,282
of rows just keeps piling up, really straightforward.

383
00:24:14,426 --> 00:24:17,946
And that one is documented here with the source codes.

384
00:24:17,978 --> 00:24:22,142
You can give that a try. And then another one I have is

385
00:24:22,196 --> 00:24:25,794
the same as that Iceberg one, but it is just sending it to

386
00:24:25,832 --> 00:24:29,634
Kafka, really straightforward there. And you can see that

387
00:24:29,672 --> 00:24:34,210
coming in last. I want to show you a couple of different things in Nifi,

388
00:24:34,550 --> 00:24:38,178
and one of them is I could read data from

389
00:24:38,344 --> 00:24:41,526
Slack. So here I put a slack message

390
00:24:41,628 --> 00:24:44,658
asking for the current stock price of IBM,

391
00:24:44,834 --> 00:24:48,678
and I've got a listener that's pushing it to Kafka.

392
00:24:48,854 --> 00:24:52,410
And then here if it is in the right

393
00:24:52,480 --> 00:24:56,426
channel, then I'm going to see if it actually

394
00:24:56,528 --> 00:25:00,218
asks about stocks. This one did. I'm going

395
00:25:00,224 --> 00:25:04,026
to use Python. I have a Python process

396
00:25:04,138 --> 00:25:08,558
to pull out the company name, and this is mentioned in that other

397
00:25:08,724 --> 00:25:11,854
talk. And then I'm going to use that to

398
00:25:11,892 --> 00:25:16,366
call Alpha Vantage to convert it into a stock title

399
00:25:16,478 --> 00:25:20,418
because your company name and your stock symbol usually

400
00:25:20,504 --> 00:25:24,482
not the same. And then if we got data back from that,

401
00:25:24,616 --> 00:25:28,390
I'm going to use Nifi to pull out the company name.

402
00:25:28,540 --> 00:25:31,974
And if there's multiple records, parse them, clean them up.

403
00:25:32,092 --> 00:25:35,720
Then I'm going to grab stock from twelve data for that,

404
00:25:36,410 --> 00:25:40,380
clean it up, add a timestamp and then

405
00:25:41,630 --> 00:25:45,338
push that to Kafka over here. Put that in

406
00:25:45,344 --> 00:25:49,178
a cache as well if we want to, and also trying to see if

407
00:25:49,184 --> 00:25:52,366
there's any wiki data for it. This is also a

408
00:25:52,388 --> 00:25:56,282
Python processor. If there's a wiki page for it, I download

409
00:25:56,346 --> 00:26:00,042
that as well. And we could push the results

410
00:26:00,106 --> 00:26:03,594
to slack, which again is nice. I can have Python

411
00:26:03,642 --> 00:26:07,726
do something, push it to Kafka, and then if I push it to slack,

412
00:26:07,838 --> 00:26:11,026
and then if I read it from Slack, maybe push it,

413
00:26:11,128 --> 00:26:13,010
execute some python,

414
00:26:14,070 --> 00:26:17,334
a cycle of what's going on. And you can see here

415
00:26:17,372 --> 00:26:21,030
I got some data back on this, there's some

416
00:26:21,100 --> 00:26:24,470
wiki information, plus the stock current

417
00:26:24,540 --> 00:26:27,480
value pretty easy to do that.

418
00:26:28,190 --> 00:26:32,582
There's a lot of different things I could do within Nifi

419
00:26:32,646 --> 00:26:35,498
with Python. Pretty easy.

420
00:26:35,664 --> 00:26:39,722
But just wanted to give you a short sample of that with

421
00:26:39,856 --> 00:26:44,522
stocks as well as over here, we've got stock

422
00:26:44,586 --> 00:26:48,026
news, but I've got a lot of records

423
00:26:48,058 --> 00:26:50,910
here. We could be sitting a while as it goes through all the news,

424
00:26:51,060 --> 00:26:53,440
but just to give you an example there,

425
00:26:54,150 --> 00:26:57,506
thank you for sitting through this talk.

426
00:26:57,688 --> 00:27:01,378
I hope you enjoy things. I know there was a

427
00:27:01,384 --> 00:27:04,802
lot in there. The source code is there. There's another talk. I have

428
00:27:04,856 --> 00:27:08,326
a little more in depth on Nifi and working with

429
00:27:08,348 --> 00:27:12,134
the various Python processors, so definitely check that

430
00:27:12,172 --> 00:27:15,862
one out. That is in this current list

431
00:27:15,916 --> 00:27:19,350
of comp 42 Python talks.

432
00:27:19,690 --> 00:27:23,766
So definitely check that out. Thank you for

433
00:27:23,868 --> 00:27:27,442
listening to my talk. If you have any questions, I'm on medium.

434
00:27:27,506 --> 00:27:29,510
I'm on GitHub, I'm on Twitter.

435
00:27:30,450 --> 00:27:34,410
You will find me. I am posting stuff everywhere.

436
00:27:34,570 --> 00:27:38,830
But thanks a lot for coming

437
00:27:38,900 --> 00:27:42,506
to our talk today. And Karen

438
00:27:42,538 --> 00:27:45,662
and me say thank you and we will talk to you again.

439
00:27:45,796 --> 00:27:46,220
Thank you.

