1
00:00:24,570 --> 00:00:28,150
Hello everyone. One welcome to my talk. The good, the bad,

2
00:00:28,220 --> 00:00:31,990
the native. I am Gregorio Palama and

3
00:00:32,140 --> 00:00:35,958
I will shortly introduce myself. I work

4
00:00:36,044 --> 00:00:39,378
as a DevOps and cloud engineer in Finwave.

5
00:00:39,554 --> 00:00:43,734
I am a Google Cloud innovator champion and I also am

6
00:00:43,772 --> 00:00:46,790
a community manager in GDG Pescara.

7
00:00:47,210 --> 00:00:51,230
You can find me on Twitter

8
00:00:51,570 --> 00:00:54,702
or on LinkedIn and here

9
00:00:54,756 --> 00:00:58,910
you can find the QR codes to my profiles.

10
00:00:59,410 --> 00:01:03,986
Okay, let's get into details and let's start with

11
00:01:04,008 --> 00:01:06,690
a question. What is cloud native?

12
00:01:07,750 --> 00:01:10,962
I used the definition that

13
00:01:11,016 --> 00:01:15,506
Priyanka Sharma, the CNCF general manager loves

14
00:01:15,538 --> 00:01:19,586
to use when she talks to people that is attached

15
00:01:19,618 --> 00:01:23,174
people or to people that is not touch at

16
00:01:23,212 --> 00:01:27,422
all. And she says that cloud native technologies,

17
00:01:27,506 --> 00:01:31,174
when engineers and software people utilize cloud computing

18
00:01:31,222 --> 00:01:35,130
to build touch, that's faster and more resilient,

19
00:01:35,550 --> 00:01:39,002
and they do that to meet customer demand really

20
00:01:39,056 --> 00:01:42,880
quickly. Well, what we can understand

21
00:01:43,970 --> 00:01:47,886
from these words is that cloud native technology

22
00:01:47,988 --> 00:01:52,442
is something that is related, strictly related

23
00:01:52,506 --> 00:01:55,120
to innovation, and that's important.

24
00:01:55,970 --> 00:01:59,182
Moreover, if we search about

25
00:01:59,236 --> 00:02:03,058
the definition of cloud native, we can find that

26
00:02:03,144 --> 00:02:07,106
the three major cloud platforms,

27
00:02:07,218 --> 00:02:09,830
public cloud platforms,

28
00:02:10,570 --> 00:02:16,294
offer a whole page to

29
00:02:16,412 --> 00:02:20,330
give a definition of what cloud

30
00:02:20,400 --> 00:02:23,914
native is. And even the links in

31
00:02:23,952 --> 00:02:28,122
this slide can show us how

32
00:02:28,176 --> 00:02:32,510
much Google, Amazon and Microsoft

33
00:02:33,250 --> 00:02:37,390
believe that it's important to give a strong definition

34
00:02:38,050 --> 00:02:40,560
to what this technology is.

35
00:02:41,090 --> 00:02:44,900
I use the definition that Google gives to

36
00:02:45,510 --> 00:02:49,134
cloud native to going further

37
00:02:49,262 --> 00:02:52,754
into details and to understand what

38
00:02:52,792 --> 00:02:56,342
we are addressing in things talk. First of all,

39
00:02:56,476 --> 00:02:59,510
we are talking about using microservices.

40
00:03:01,050 --> 00:03:04,870
Also, when we use microservices, we are using

41
00:03:04,940 --> 00:03:08,486
containers because containers,

42
00:03:08,598 --> 00:03:11,306
as CNCF says,

43
00:03:11,488 --> 00:03:15,180
are the building blocks of the cloud. As for now,

44
00:03:16,030 --> 00:03:20,034
of course, when we use containers, we have to orchestrate

45
00:03:20,102 --> 00:03:23,134
them. So again,

46
00:03:23,252 --> 00:03:26,382
containers are the building blocks of the cloud.

47
00:03:26,516 --> 00:03:30,682
But Kubernetes is, again, as CNCF

48
00:03:30,746 --> 00:03:34,114
says, Kubernetes is the

49
00:03:34,232 --> 00:03:38,094
operating system for the cloud. So we have to orchestrate

50
00:03:38,142 --> 00:03:41,710
containers in order to have a microservice solution

51
00:03:41,790 --> 00:03:44,814
in the cloud native world. Well,

52
00:03:44,952 --> 00:03:48,246
what does it mean in terms

53
00:03:48,348 --> 00:03:51,990
of cloud native efficiency?

54
00:03:52,570 --> 00:03:56,690
We have scalability. Cloud native architectures

55
00:03:56,770 --> 00:04:01,574
employ infrastructure automation, helping to eliminate

56
00:04:01,622 --> 00:04:04,730
downtime due to human error.

57
00:04:05,070 --> 00:04:09,206
We can balance load based on demand,

58
00:04:09,318 --> 00:04:13,562
allowing you us to optimize cost and performance

59
00:04:13,626 --> 00:04:17,840
better. What's important here is that we

60
00:04:18,450 --> 00:04:22,830
want to optimize cost and we want better performance.

61
00:04:23,510 --> 00:04:27,378
Lower cost? Well, we want better

62
00:04:27,464 --> 00:04:30,654
performance. But detailed

63
00:04:30,702 --> 00:04:34,740
optimized cost is referring to

64
00:04:36,010 --> 00:04:40,070
an objective that we want to reach to lower costs.

65
00:04:40,570 --> 00:04:44,722
And a streamlined software delivery process reduces

66
00:04:44,866 --> 00:04:48,150
the cost of delivering new updates and features.

67
00:04:49,070 --> 00:04:52,410
Cloud native application also allow

68
00:04:52,480 --> 00:04:57,046
us for sharing resources and on demand consumption.

69
00:04:57,238 --> 00:05:01,118
Well, that's something that is to reach

70
00:05:01,284 --> 00:05:04,414
when we are using microservices and when we

71
00:05:04,452 --> 00:05:08,282
are designing them. With sharing

72
00:05:08,346 --> 00:05:11,200
resources and on demand consumption in mind.

73
00:05:12,630 --> 00:05:15,922
We have something that is very important

74
00:05:16,056 --> 00:05:19,982
when we talk about cloud native iger availability.

75
00:05:20,126 --> 00:05:24,718
Cloud native architectures provide IG availability and

76
00:05:24,904 --> 00:05:29,346
reliability as they reduce operational complexity,

77
00:05:29,538 --> 00:05:33,266
simplify configuration changes, and offer auto

78
00:05:33,298 --> 00:05:37,138
scaling and self filling. Well, self filling

79
00:05:37,234 --> 00:05:41,094
is something that we design. Auto scaling we have

80
00:05:41,132 --> 00:05:44,970
to design it to inside our microservices.

81
00:05:46,110 --> 00:05:50,642
So what we can say is that cloud native applications

82
00:05:50,806 --> 00:05:54,778
make the most of modern infrastructure's dynamic

83
00:05:54,874 --> 00:05:58,346
distributed nature to achieve greater speed,

84
00:05:58,458 --> 00:06:01,934
agility, scalability, reliability and

85
00:06:01,972 --> 00:06:04,660
cost efficiency. Well,

86
00:06:05,110 --> 00:06:09,314
since things talk is about using an

87
00:06:09,352 --> 00:06:12,866
innovative technology to better

88
00:06:13,048 --> 00:06:17,350
design and build cloud native microservices,

89
00:06:17,930 --> 00:06:22,070
we want to concentrate on those

90
00:06:22,140 --> 00:06:25,938
words that in this slide are involved. So greater speed,

91
00:06:26,034 --> 00:06:29,706
scalability and cost efficiency because

92
00:06:29,808 --> 00:06:34,230
those words help us to concentrate

93
00:06:34,390 --> 00:06:37,590
in building better microservices

94
00:06:37,670 --> 00:06:41,934
that are available

95
00:06:42,132 --> 00:06:46,394
and are scalable and reduce

96
00:06:46,442 --> 00:06:50,446
costs and so on. What we want to achieve is to

97
00:06:50,468 --> 00:06:54,626
have a smaller memory footprint because that

98
00:06:54,808 --> 00:06:58,210
allow us to lower the cost and also

99
00:06:58,280 --> 00:07:02,558
because that allow us to share resources

100
00:07:02,734 --> 00:07:05,782
between our microservices. Also,

101
00:07:05,836 --> 00:07:10,278
we want to use less CPU and

102
00:07:10,444 --> 00:07:13,720
we also want a lower startup time.

103
00:07:14,250 --> 00:07:18,486
Let's get to the JVM microservices frameworks ecosystem

104
00:07:18,678 --> 00:07:22,780
and let's start with the three most used ones,

105
00:07:23,550 --> 00:07:26,410
Quarkus, Springboot and Micronaut.

106
00:07:26,910 --> 00:07:30,800
They are Java frameworks because

107
00:07:31,410 --> 00:07:35,162
they usually use Java

108
00:07:35,306 --> 00:07:39,082
to allow the development

109
00:07:39,146 --> 00:07:43,422
of our microservices. Well, we can use other languages

110
00:07:43,486 --> 00:07:46,820
too, but Java is the most common

111
00:07:47,190 --> 00:07:50,370
choice for those three frameworks.

112
00:07:50,950 --> 00:07:55,570
They are not the only one. For example, we have microprofile

113
00:07:56,970 --> 00:08:01,366
for example. We also have other languages because

114
00:08:01,468 --> 00:08:05,750
Java is not the only one that run on JVM.

115
00:08:06,170 --> 00:08:10,098
We have also scala for example with ACA,

116
00:08:10,194 --> 00:08:13,994
Lagom. Or we also have languages that

117
00:08:14,032 --> 00:08:17,530
are designed with cloud native in mind,

118
00:08:17,600 --> 00:08:21,834
such as ballerina, considering what we want to achieve.

119
00:08:21,962 --> 00:08:25,258
So a smaller memory footprint,

120
00:08:25,434 --> 00:08:30,506
less CPU consumption or faster

121
00:08:30,618 --> 00:08:34,190
startup. Those frameworks

122
00:08:34,270 --> 00:08:38,290
and every other one are equally

123
00:08:38,630 --> 00:08:42,418
we can consider them in an

124
00:08:42,504 --> 00:08:46,086
equally way. We will concentrate just

125
00:08:46,188 --> 00:08:50,150
on one of them just for simplicity

126
00:08:50,810 --> 00:08:55,430
and to demonstrate powerful

127
00:08:57,130 --> 00:09:00,914
innovation that are brought to the JVM

128
00:09:00,962 --> 00:09:04,314
ecosystem by gradm. Native image we will

129
00:09:04,352 --> 00:09:07,814
concentrate on Springboot and let's

130
00:09:07,862 --> 00:09:10,640
get to our demo project.

131
00:09:11,410 --> 00:09:14,110
I will switch to intellij.

132
00:09:14,530 --> 00:09:17,486
Ive created a very simple project.

133
00:09:17,668 --> 00:09:21,374
As you can see, we all just have a demo

134
00:09:21,412 --> 00:09:24,554
application and a demo controller.

135
00:09:24,682 --> 00:09:28,158
No services, no repositories nothing

136
00:09:28,244 --> 00:09:32,130
at all, just one endpoint with a greeting

137
00:09:32,630 --> 00:09:35,866
get mapping that will answer us nullodemo

138
00:09:35,918 --> 00:09:40,694
string and I wanted keep

139
00:09:40,732 --> 00:09:44,294
it simple because even if

140
00:09:44,332 --> 00:09:47,794
it is this simple, we will see how much

141
00:09:47,932 --> 00:09:52,140
gralvm native image will help us to

142
00:09:52,750 --> 00:09:57,050
optimize memory CPUs consumption.

143
00:09:57,790 --> 00:10:00,654
So let's start this application.

144
00:10:00,852 --> 00:10:04,190
It will take just a few seconds,

145
00:10:05,170 --> 00:10:07,920
compile it and to start it,

146
00:10:08,450 --> 00:10:12,442
I expect something like eight or

147
00:10:12,516 --> 00:10:16,370
9 seconds to start it and oh even

148
00:10:16,440 --> 00:10:18,180
less six.

149
00:10:20,870 --> 00:10:24,354
It's really fast to

150
00:10:24,392 --> 00:10:28,854
start, but not fast enough as

151
00:10:28,892 --> 00:10:32,614
we can see. We want to make sure that

152
00:10:32,652 --> 00:10:36,722
it works. So let's perform azure to our endpoint

153
00:10:36,786 --> 00:10:40,650
and we can see that the answer is hello

154
00:10:40,720 --> 00:10:43,978
demo. But we also want

155
00:10:44,064 --> 00:10:47,402
to see for example how much

156
00:10:47,456 --> 00:10:51,600
memory it is using. So we will

157
00:10:52,450 --> 00:10:56,560
the system to print us the

158
00:10:56,930 --> 00:11:00,160
resident set side that

159
00:11:00,850 --> 00:11:05,186
gives us measure of

160
00:11:05,288 --> 00:11:09,314
how much memory this process is using

161
00:11:09,512 --> 00:11:12,770
in this moment. And that is the

162
00:11:12,840 --> 00:11:16,246
real memory that this process is using. So you

163
00:11:16,268 --> 00:11:20,114
can see 49 KB

164
00:11:20,242 --> 00:11:23,686
more or less. It's easy, it's working.

165
00:11:23,868 --> 00:11:27,746
We have something that is not very big

166
00:11:27,868 --> 00:11:31,898
in memory and it starts in just

167
00:11:31,984 --> 00:11:35,370
a few seconds, 6 seconds.

168
00:11:36,830 --> 00:11:40,590
Let's get back to our

169
00:11:40,740 --> 00:11:44,286
presentation and let's see what we

170
00:11:44,308 --> 00:11:47,200
can do from here on.

171
00:11:47,890 --> 00:11:51,486
First of all, the talk is called the

172
00:11:51,508 --> 00:11:55,246
good the bad de native, referring to the

173
00:11:55,348 --> 00:11:59,214
movie the good the ugly from

174
00:11:59,252 --> 00:12:02,526
Ser Giuliana. And I imagined

175
00:12:02,638 --> 00:12:07,030
what one of the three characters

176
00:12:07,690 --> 00:12:10,918
could say after seeing

177
00:12:11,004 --> 00:12:15,234
this demo. And the character is angelize

178
00:12:15,282 --> 00:12:18,534
the bed. And he would say,

179
00:12:18,732 --> 00:12:21,894
well, those bastards out there want more

180
00:12:21,932 --> 00:12:24,758
memory and I have just a few resources.

181
00:12:24,934 --> 00:12:28,426
How do you think I can scale and lower the

182
00:12:28,448 --> 00:12:31,902
costs? Let's remember that we want to lower

183
00:12:31,956 --> 00:12:36,830
the costs and we want to scale and we want to share resources.

184
00:12:37,330 --> 00:12:40,318
So the more we say okay,

185
00:12:40,404 --> 00:12:44,322
it's good, the less we can scale and

186
00:12:44,456 --> 00:12:47,922
share resources. We want to make

187
00:12:47,976 --> 00:12:51,586
sure that there is a way to use

188
00:12:51,688 --> 00:12:55,542
less memory, less CPU, and to

189
00:12:55,676 --> 00:12:58,550
make our application stuff faster.

190
00:12:59,130 --> 00:13:03,910
So let's get into the details of gralvm.

191
00:13:04,410 --> 00:13:08,698
The JVM is an abstraction of an underlying actual

192
00:13:08,864 --> 00:13:13,222
machine that interprets the bytecode generated

193
00:13:13,286 --> 00:13:16,940
by the compilation of a code supported by the JVM itself.

194
00:13:17,550 --> 00:13:20,878
So what we can see

195
00:13:20,964 --> 00:13:25,498
here from this statement

196
00:13:25,674 --> 00:13:29,722
is that what makes Java portable

197
00:13:29,866 --> 00:13:33,474
is the JVM technology. It is

198
00:13:33,512 --> 00:13:36,980
where compile once runs, everywhere comes from.

199
00:13:37,350 --> 00:13:41,438
We compile once into a bytecode

200
00:13:41,614 --> 00:13:46,070
and the bytecode gets interpreted by the JVM.

201
00:13:46,570 --> 00:13:51,510
And a standard way

202
00:13:51,580 --> 00:13:55,430
that we can see of

203
00:13:55,580 --> 00:13:58,540
how a JVM works is this one.

204
00:13:58,990 --> 00:14:03,850
It is the hotspot JVM so the standard JVM.

205
00:14:04,350 --> 00:14:08,090
So we have our bytecode, that is something

206
00:14:08,160 --> 00:14:12,014
that is generated from our start code and the

207
00:14:12,052 --> 00:14:15,690
bytecode will be interpreted

208
00:14:15,770 --> 00:14:19,022
or compiled by adjusting some

209
00:14:19,076 --> 00:14:22,160
compiler in what? Well,

210
00:14:22,470 --> 00:14:27,090
in binary code. So the interpreter

211
00:14:27,430 --> 00:14:30,802
or the JIT compiler will transform our

212
00:14:30,856 --> 00:14:34,430
bytecode into something that our machine,

213
00:14:34,590 --> 00:14:40,246
so the real machine can execute well

214
00:14:40,348 --> 00:14:43,750
inside the JVM. We also have other

215
00:14:43,820 --> 00:14:47,560
components, they are very important.

216
00:14:48,010 --> 00:14:51,886
We have a garbage collector. We have thread

217
00:14:51,938 --> 00:14:55,290
management, we have memory management.

218
00:14:55,710 --> 00:14:59,862
We also have class loader and native method

219
00:14:59,926 --> 00:15:03,914
libraries. All of these allows

220
00:15:03,962 --> 00:15:07,674
us to create a JVM and allows

221
00:15:07,722 --> 00:15:11,850
us to use our bytecode,

222
00:15:12,010 --> 00:15:16,020
a single bytecode everywhere where

223
00:15:17,270 --> 00:15:21,106
the same JVM is present.

224
00:15:21,208 --> 00:15:24,510
So if we have a deep hotspot JVM

225
00:15:24,590 --> 00:15:28,470
on Linux or on macOS or on windows

226
00:15:28,810 --> 00:15:31,560
with the same bytecode, we can run.

227
00:15:32,330 --> 00:15:36,086
Well, let's get to the details. Because we

228
00:15:36,108 --> 00:15:39,510
said that the bytecode gets interpreted

229
00:15:39,590 --> 00:15:41,690
or compiled,

230
00:15:42,510 --> 00:15:46,294
that's not just about compiling

231
00:15:46,342 --> 00:15:50,278
it or interpreting it. The first

232
00:15:50,384 --> 00:15:53,934
way to get the

233
00:15:53,972 --> 00:15:58,282
bytecode and execute it in the underlying

234
00:15:58,346 --> 00:16:02,062
machine is to interpret it with

235
00:16:02,116 --> 00:16:05,970
the interpreter. It is very slow because

236
00:16:06,040 --> 00:16:10,370
it has to interpret line by line our bytecode,

237
00:16:11,750 --> 00:16:14,910
and while it interprets and

238
00:16:15,000 --> 00:16:19,394
execute our bytecode, it collects profiling

239
00:16:19,442 --> 00:16:23,000
information. It also has

240
00:16:23,770 --> 00:16:27,718
faster startup because well, we don't

241
00:16:27,734 --> 00:16:31,290
have to load into our memory anything at all.

242
00:16:31,440 --> 00:16:34,300
We are interpreting line by line,

243
00:16:34,670 --> 00:16:38,394
but that kind of operation line by

244
00:16:38,432 --> 00:16:42,062
line is very slow. The second option is

245
00:16:42,116 --> 00:16:45,374
the C one jit compiler. The C

246
00:16:45,412 --> 00:16:49,418
one compiles code when it gets frequently

247
00:16:49,514 --> 00:16:54,050
executed. So we have the first way

248
00:16:54,120 --> 00:16:58,414
to switch from the interpreter to the JIT

249
00:16:58,462 --> 00:17:02,610
compiler. When the code gets frequently executed,

250
00:17:03,030 --> 00:17:07,026
it stops being interpreted and it starts

251
00:17:07,138 --> 00:17:11,830
being compiled by the C one JIT compiled compiler.

252
00:17:12,730 --> 00:17:17,000
It also continue collecting profiling information

253
00:17:17,530 --> 00:17:21,100
and it has a facet warm up.

254
00:17:21,710 --> 00:17:25,274
We also have another JIT compiler, the C

255
00:17:25,312 --> 00:17:29,546
two. It starts compiling our

256
00:17:29,648 --> 00:17:33,646
bytecode and optimizing it when

257
00:17:33,668 --> 00:17:37,690
it is executed often enough and reaches

258
00:17:37,850 --> 00:17:41,674
certain thresholds. It uses

259
00:17:41,722 --> 00:17:45,742
the profile information that are collected

260
00:17:45,806 --> 00:17:49,598
by the interpreter and the C one compiler,

261
00:17:49,774 --> 00:17:53,118
and it has the high peak performance.

262
00:17:53,294 --> 00:17:58,194
So when we say we have to warm

263
00:17:58,242 --> 00:18:02,326
up our JVM, we are referring to this

264
00:18:02,428 --> 00:18:07,014
kind of compilation. We want to

265
00:18:07,212 --> 00:18:10,700
execute our code enough times

266
00:18:11,950 --> 00:18:15,786
and we want it to reach those

267
00:18:15,888 --> 00:18:19,770
certain thresholds because when it

268
00:18:19,840 --> 00:18:23,338
does, the C two JIT compiler

269
00:18:23,434 --> 00:18:27,114
optimize our code and compiles

270
00:18:27,162 --> 00:18:31,262
it after optimizing so we can have

271
00:18:31,316 --> 00:18:33,490
the high peak performance.

272
00:18:34,630 --> 00:18:38,798
Well, let's go GralvM

273
00:18:38,974 --> 00:18:42,642
and let's understand what

274
00:18:42,696 --> 00:18:45,990
kind of optimization it brings.

275
00:18:46,650 --> 00:18:50,374
It is a polyglot VM. What does

276
00:18:50,412 --> 00:18:54,502
it mean? It means that we can execute many

277
00:18:54,556 --> 00:18:58,886
languages, not just the JVM classic

278
00:18:58,998 --> 00:19:02,154
languages, but also languages that usually

279
00:19:02,272 --> 00:19:05,318
doesn't run on a Java virtual machine.

280
00:19:05,414 --> 00:19:09,414
So we can have Python, we can have Ruby JavaScript

281
00:19:09,462 --> 00:19:12,800
and so on. Gralvm also

282
00:19:13,410 --> 00:19:16,670
has a new compiler, a JIt compiler.

283
00:19:17,010 --> 00:19:20,398
So it is the gral compiler. It is

284
00:19:20,484 --> 00:19:24,558
a C two implementation. It has various

285
00:19:24,654 --> 00:19:28,660
optimization, a lot of them actually,

286
00:19:29,190 --> 00:19:32,462
and it removes unnecessary object allocation

287
00:19:32,526 --> 00:19:37,510
on the heap memory. Also we have native image

288
00:19:38,010 --> 00:19:41,638
it is not a JIT compiler, just in time

289
00:19:41,724 --> 00:19:45,542
compiler. It is ahead of

290
00:19:45,596 --> 00:19:49,900
time compiler. So it compiles everything

291
00:19:50,350 --> 00:19:54,982
before a hit, before it is executed.

292
00:19:55,126 --> 00:19:59,890
And the compilation must generate

293
00:19:59,990 --> 00:20:03,278
something because it is

294
00:20:03,364 --> 00:20:07,120
the kind of compiler that

295
00:20:08,690 --> 00:20:12,394
will use everything that it knows

296
00:20:12,522 --> 00:20:15,730
to gives us an executable file.

297
00:20:16,230 --> 00:20:19,438
And of course it compiles into native

298
00:20:19,534 --> 00:20:23,586
platform executable. Okay, so let's get

299
00:20:23,688 --> 00:20:26,974
into the differences between the oddspot

300
00:20:27,022 --> 00:20:30,482
VM and the gralvM. In the Oddspot VM

301
00:20:30,546 --> 00:20:34,166
we have the oddspot VM as a

302
00:20:34,268 --> 00:20:37,518
Java virtual machine. We have the compiler

303
00:20:37,554 --> 00:20:40,874
interface, and we have the C

304
00:20:40,912 --> 00:20:44,726
one and C two just in time compiler.

305
00:20:44,918 --> 00:20:49,146
Well, in this scenario we can have

306
00:20:49,248 --> 00:20:53,146
something that is called tired compilation,

307
00:20:53,338 --> 00:20:56,382
and it is used when

308
00:20:56,516 --> 00:21:00,314
we start by interpreting

309
00:21:00,362 --> 00:21:03,966
our code, our bytecode. When our bytecode

310
00:21:03,998 --> 00:21:07,394
is said often

311
00:21:07,512 --> 00:21:11,374
enough, DC one compiler starts

312
00:21:11,422 --> 00:21:15,442
to compile it just in time, and when

313
00:21:15,496 --> 00:21:19,250
it is executed even more often

314
00:21:19,320 --> 00:21:23,378
enough, C two compiler starts compilation,

315
00:21:23,474 --> 00:21:26,758
optimizing it and compiling it. This is called

316
00:21:26,844 --> 00:21:30,774
tired compilation. The tired compilation

317
00:21:30,822 --> 00:21:33,670
is something we also have on GralVM,

318
00:21:33,750 --> 00:21:37,690
but on grav we have something that

319
00:21:37,760 --> 00:21:41,582
is slightly different.

320
00:21:41,716 --> 00:21:45,246
We have a gral compiler instead of

321
00:21:45,268 --> 00:21:49,806
a C two compiler. And instead of

322
00:21:49,908 --> 00:21:53,282
compiler interface for the C two,

323
00:21:53,416 --> 00:21:57,522
we have the JVM Ci for

324
00:21:57,576 --> 00:22:02,014
the gral compiler. So the JVM compiler interface

325
00:22:02,062 --> 00:22:05,990
is a new interface written in Java,

326
00:22:06,810 --> 00:22:10,486
the same for the gral compiler. So these

327
00:22:10,588 --> 00:22:14,550
two new components are totally written in Java.

328
00:22:15,050 --> 00:22:18,700
And this is something that should

329
00:22:19,070 --> 00:22:23,018
tell us something because, well, if it is

330
00:22:23,104 --> 00:22:26,442
written in Java it will also get

331
00:22:26,496 --> 00:22:30,378
compiled, and then in native

332
00:22:30,474 --> 00:22:34,670
code it means that the gral

333
00:22:35,090 --> 00:22:38,942
compiler itself and the JVM Ci will get

334
00:22:38,996 --> 00:22:42,958
compiled by the C one compiler and then by

335
00:22:43,044 --> 00:22:47,042
graph compiler itself. And if

336
00:22:47,096 --> 00:22:51,010
we start thinking about tired compilation,

337
00:22:51,430 --> 00:22:56,070
we can imagine that the more often our

338
00:22:56,140 --> 00:22:59,654
gral compiler gets used, the more

339
00:22:59,692 --> 00:23:03,830
it gets optimized by the tired compilation.

340
00:23:04,490 --> 00:23:08,730
And it also has the gral compiler

341
00:23:09,390 --> 00:23:13,500
optimization. If we

342
00:23:14,830 --> 00:23:18,246
compare it with the C two compiler,

343
00:23:18,278 --> 00:23:21,690
it has a lot of optimization.

344
00:23:22,110 --> 00:23:26,430
So the graph compiler will start

345
00:23:26,500 --> 00:23:31,210
producing very optimized native

346
00:23:31,370 --> 00:23:35,422
binary code. And this is something that

347
00:23:35,556 --> 00:23:38,850
already gives us a lot of optimization

348
00:23:39,430 --> 00:23:42,130
in terms of memory consumption.

349
00:23:42,790 --> 00:23:47,074
But this is not the only compiler

350
00:23:47,122 --> 00:23:50,610
that GralvM offer us. We also have a native

351
00:23:50,690 --> 00:23:54,774
image. Let's get into the details of

352
00:23:54,892 --> 00:23:58,378
how the ahead of time compilation works,

353
00:23:58,544 --> 00:24:01,210
and let's start from the inputs.

354
00:24:01,790 --> 00:24:05,034
Our application code. We also have the

355
00:24:05,072 --> 00:24:08,374
libraries that it uses and the JDK.

356
00:24:08,502 --> 00:24:11,662
Of course, in our demo application we add

357
00:24:11,716 --> 00:24:15,210
the demo application itself, the libraries.

358
00:24:15,290 --> 00:24:18,426
So for example spring and spring boot,

359
00:24:18,618 --> 00:24:22,378
and the JDK. Our demo application

360
00:24:22,564 --> 00:24:26,610
itself uses string, and string is

361
00:24:26,680 --> 00:24:28,290
inside the JDK.

362
00:24:29,350 --> 00:24:33,010
Okay, these are the input for our build

363
00:24:33,080 --> 00:24:37,574
phase and the build phase with the

364
00:24:37,692 --> 00:24:42,310
native image compiler, a loop of

365
00:24:42,460 --> 00:24:46,870
three different phases, a point to analysis,

366
00:24:47,450 --> 00:24:50,406
a run of initializations,

367
00:24:50,518 --> 00:24:54,538
and a heap snapshotting. While of

368
00:24:54,704 --> 00:24:59,046
all of this, the ahead of time compilation

369
00:24:59,158 --> 00:25:02,842
can't execute our application,

370
00:25:02,976 --> 00:25:06,638
so it has to perform a static analysis of

371
00:25:06,724 --> 00:25:10,638
our code to understand everything

372
00:25:10,724 --> 00:25:15,202
that it needs to be initialization before

373
00:25:15,336 --> 00:25:19,426
it gets executed, and everything

374
00:25:19,528 --> 00:25:23,362
that needs to be initialized will

375
00:25:23,416 --> 00:25:26,982
be created into the

376
00:25:27,036 --> 00:25:30,838
heap and get snapshotted. And after

377
00:25:30,924 --> 00:25:34,854
that, maybe if we go

378
00:25:34,892 --> 00:25:38,140
on and keep analyzing our code,

379
00:25:38,590 --> 00:25:43,658
we see that after

380
00:25:43,744 --> 00:25:48,122
our initialization we can go

381
00:25:48,176 --> 00:25:53,034
further on some executions.

382
00:25:53,162 --> 00:25:56,814
And so we start again performing the point to

383
00:25:56,852 --> 00:26:00,814
analysis because we can make sure we

384
00:26:00,932 --> 00:26:04,642
have analyzed everything that can be

385
00:26:04,696 --> 00:26:07,570
executed and will be executed.

386
00:26:08,390 --> 00:26:12,340
Also, we can see from this scheme that

387
00:26:13,750 --> 00:26:17,254
no oddspot VM at

388
00:26:17,292 --> 00:26:20,726
all. And that's something that is not okay,

389
00:26:20,828 --> 00:26:24,866
because the Oddspot VM and the gral

390
00:26:24,898 --> 00:26:28,554
VM performs other operations such as

391
00:26:28,672 --> 00:26:32,826
garbage collecting or class loading and so on. And this

392
00:26:32,928 --> 00:26:37,002
kind of operation needs to be executed when

393
00:26:37,056 --> 00:26:40,558
we perform an

394
00:26:40,644 --> 00:26:44,750
ahead of time compilation too. That's why when we

395
00:26:44,820 --> 00:26:48,174
use the native image compilation, we also

396
00:26:48,212 --> 00:26:51,834
have a substrate VM. The substrate

397
00:26:51,882 --> 00:26:55,714
VM is written in Java, and it

398
00:26:55,752 --> 00:26:59,506
is something that will be compiled together with our application

399
00:26:59,608 --> 00:27:03,170
and libraries and JDK, because it will

400
00:27:03,240 --> 00:27:06,934
be compiled against

401
00:27:07,132 --> 00:27:10,790
the target machine, against the target

402
00:27:11,210 --> 00:27:15,254
architecture, and it will be

403
00:27:15,452 --> 00:27:19,370
optimized by our native image compilation.

404
00:27:20,030 --> 00:27:23,610
We have the output after our

405
00:27:23,680 --> 00:27:27,114
build phase, and our output will

406
00:27:27,152 --> 00:27:32,430
be the native executable. The native executable

407
00:27:32,770 --> 00:27:36,702
will have a code in the text section that

408
00:27:36,756 --> 00:27:39,550
will come from the out compilation.

409
00:27:40,610 --> 00:27:44,542
Also, we have an image heap in that section,

410
00:27:44,606 --> 00:27:49,250
and it will come from our image heap writing

411
00:27:50,230 --> 00:27:54,290
well, let's get back to our demo project and

412
00:27:54,360 --> 00:27:58,040
let's start talking about native this time.

413
00:27:58,810 --> 00:28:02,246
What I will perform here is

414
00:28:02,348 --> 00:28:06,246
something that is slightly different.

415
00:28:06,348 --> 00:28:10,134
Let's stop our application, the JVM one,

416
00:28:10,332 --> 00:28:14,262
and let's perform a native compilation.

417
00:28:14,326 --> 00:28:17,754
I will use the native profile and

418
00:28:17,792 --> 00:28:22,734
I will ask my

419
00:28:22,772 --> 00:28:26,014
maven installation to perform a

420
00:28:26,052 --> 00:28:29,886
package command. So it will create

421
00:28:30,068 --> 00:28:34,442
everything that I will need to execute

422
00:28:34,586 --> 00:28:37,570
my application in native wave.

423
00:28:38,150 --> 00:28:41,700
Okay, let's get into the execution. It will

424
00:28:42,150 --> 00:28:46,262
takes a while because of all

425
00:28:46,316 --> 00:28:50,758
the build phase. So it will perform

426
00:28:50,924 --> 00:28:54,774
those three steps. The point to

427
00:28:54,812 --> 00:28:58,774
analysis. It will

428
00:28:58,812 --> 00:29:03,854
run in its alley sessions and it will perform snapshotting.

429
00:29:04,002 --> 00:29:07,754
And these three steps will

430
00:29:07,792 --> 00:29:11,198
get executed again and again and again

431
00:29:11,364 --> 00:29:14,750
until it will reach the end

432
00:29:14,820 --> 00:29:18,574
of the process when every other step or

433
00:29:18,612 --> 00:29:22,190
every other loop of our

434
00:29:22,260 --> 00:29:26,290
phase step will add nothing more.

435
00:29:26,440 --> 00:29:29,602
So it will stop and start generating the

436
00:29:29,656 --> 00:29:33,854
native executable. Our compilation has finished,

437
00:29:33,982 --> 00:29:37,522
so let's get into its details.

438
00:29:37,666 --> 00:29:41,430
We can see that it has performed analysis

439
00:29:41,850 --> 00:29:45,778
and it has identified all the reachable

440
00:29:45,874 --> 00:29:48,490
types and fields and methods.

441
00:29:49,550 --> 00:29:54,166
It has building and inlining

442
00:29:54,278 --> 00:29:58,074
compilation and in the end

443
00:29:58,192 --> 00:30:00,830
it created an image.

444
00:30:01,730 --> 00:30:04,862
You can see that it will give

445
00:30:04,916 --> 00:30:09,002
us information on the top ten origins

446
00:30:09,066 --> 00:30:12,750
of the code and of the top

447
00:30:12,820 --> 00:30:16,562
object types in image. It will

448
00:30:16,616 --> 00:30:19,934
create an executable and the executable

449
00:30:19,982 --> 00:30:22,660
is this one target demo.

450
00:30:23,270 --> 00:30:27,410
Okay, we can see that target is

451
00:30:27,480 --> 00:30:31,414
an executable. What we want to try

452
00:30:31,452 --> 00:30:35,640
to do is to execute it. So let's get to

453
00:30:36,570 --> 00:30:40,154
execute it and we will see that it will get

454
00:30:40,192 --> 00:30:44,700
executed just as our application,

455
00:30:45,950 --> 00:30:49,580
that user, the JVM user to work.

456
00:30:50,290 --> 00:30:54,302
The first thing that we can observe is the

457
00:30:54,436 --> 00:30:58,638
starting time. We had 6.6

458
00:30:58,804 --> 00:31:02,458
seconds for the application that was running

459
00:31:02,564 --> 00:31:07,634
in the JVM. We have 0.8

460
00:31:07,752 --> 00:31:11,154
seconds for this kind of application.

461
00:31:11,272 --> 00:31:14,914
It is the same application, but what

462
00:31:15,032 --> 00:31:18,678
has changed is that this time

463
00:31:18,764 --> 00:31:22,166
we have a native compiled application.

464
00:31:22,348 --> 00:31:24,630
So node JVM,

465
00:31:25,210 --> 00:31:29,558
everything has been compiled into native

466
00:31:29,654 --> 00:31:33,242
code. Well, let's get sure that it

467
00:31:33,296 --> 00:31:37,094
works. So we had a greeting

468
00:31:37,222 --> 00:31:40,654
endpoint, so we will check

469
00:31:40,772 --> 00:31:44,750
if it will give us the same result. And it

470
00:31:44,820 --> 00:31:48,954
just says hello demo. And let's

471
00:31:49,002 --> 00:31:52,930
get into the detail of process

472
00:31:53,000 --> 00:31:56,930
ID. So let's see how much memory

473
00:31:57,830 --> 00:32:01,394
using this time. You can see

474
00:32:01,432 --> 00:32:05,570
that this time the memory is less

475
00:32:05,720 --> 00:32:09,986
of when we use the JVM.

476
00:32:10,178 --> 00:32:13,718
Well, the demo application

477
00:32:13,884 --> 00:32:17,506
is very simple, so we

478
00:32:17,548 --> 00:32:21,194
will not see a lot less of

479
00:32:21,312 --> 00:32:25,020
resident memory that will be used.

480
00:32:25,390 --> 00:32:28,938
But what we can see is that

481
00:32:29,024 --> 00:32:32,394
it is just less memory that when

482
00:32:32,432 --> 00:32:34,250
we use the JVM,

483
00:32:35,490 --> 00:32:40,382
also combined with a very

484
00:32:40,436 --> 00:32:45,554
small startup time and a

485
00:32:45,592 --> 00:32:49,170
CPU consumption that is very optimized,

486
00:32:49,670 --> 00:32:53,298
we can say that this time

487
00:32:53,464 --> 00:32:57,846
our application is definitely more

488
00:32:58,028 --> 00:33:01,238
scalable, offers us more

489
00:33:01,404 --> 00:33:05,606
opportunities to scale application because it

490
00:33:05,628 --> 00:33:09,322
will use less resources and those

491
00:33:09,376 --> 00:33:12,554
resources are shared between our

492
00:33:12,672 --> 00:33:16,170
microservices and instance of microservices.

493
00:33:16,590 --> 00:33:20,090
Okay, let's get back to our presentation.

494
00:33:21,090 --> 00:33:24,894
Let's get back to our objectives. We wanted to

495
00:33:24,932 --> 00:33:28,302
achieve a smaller memory footprint and

496
00:33:28,356 --> 00:33:32,266
we also wanted to have less CPU consumption

497
00:33:32,378 --> 00:33:34,260
and a lower startup time.

498
00:33:35,350 --> 00:33:38,706
With our demo application, it's difficult to

499
00:33:38,728 --> 00:33:43,186
see the less CPU consumption just because the application is very

500
00:33:43,288 --> 00:33:47,266
simple, but we have seen the smaller memory footprint

501
00:33:47,458 --> 00:33:51,158
and the lower startup time that is very

502
00:33:51,324 --> 00:33:54,518
lower. We are talking about

503
00:33:54,684 --> 00:33:59,610
6 seconds against 0.8

504
00:33:59,680 --> 00:34:03,514
seconds. Well, all of things

505
00:34:03,632 --> 00:34:07,914
achievement helps us to have

506
00:34:07,952 --> 00:34:11,282
a better scalability, to lower the costs

507
00:34:11,366 --> 00:34:14,926
and to have higher availability. That's because we

508
00:34:14,948 --> 00:34:18,586
are using less resources.

509
00:34:18,778 --> 00:34:22,838
So if we are using less resources, we can scale

510
00:34:22,954 --> 00:34:27,262
more using the same nodes

511
00:34:27,406 --> 00:34:31,860
in our cluster. And if we can scale more,

512
00:34:33,110 --> 00:34:36,994
of course we have higher availability. And that's

513
00:34:37,042 --> 00:34:40,614
not just about scaling, it's about

514
00:34:40,732 --> 00:34:43,926
using our resources in a better way because,

515
00:34:44,108 --> 00:34:49,722
well, we are starting our service in 0.8

516
00:34:49,776 --> 00:34:52,694
seconds, not in 6 seconds.

517
00:34:52,822 --> 00:34:56,554
So we don't have to wait 6

518
00:34:56,672 --> 00:35:00,430
seconds before our application can

519
00:35:00,500 --> 00:35:02,750
serve our users.

520
00:35:03,650 --> 00:35:08,094
It's just at least almost

521
00:35:08,292 --> 00:35:12,378
immediate. We can start after zero

522
00:35:12,564 --> 00:35:16,660
pains, 8 seconds, and it will

523
00:35:17,830 --> 00:35:21,380
allow us to have just a few

524
00:35:22,310 --> 00:35:27,250
replicas of our same microservice

525
00:35:27,330 --> 00:35:30,630
to serve our users without having

526
00:35:30,700 --> 00:35:34,390
them to wait until the single

527
00:35:34,460 --> 00:35:38,154
replica is ready to serve them. And of

528
00:35:38,192 --> 00:35:41,980
course all of these will lower the cost.

529
00:35:42,830 --> 00:35:45,994
Getting back to the movie of

530
00:35:46,032 --> 00:35:49,898
Sergio Leone, we could imagine Blondie the good

531
00:35:49,984 --> 00:35:53,290
saying, I will sleep peacefully

532
00:35:53,450 --> 00:35:56,670
because I know that the native is watching over

533
00:35:56,740 --> 00:36:01,182
me. Okay, we've seen the good

534
00:36:01,316 --> 00:36:04,750
things of the native image process,

535
00:36:04,900 --> 00:36:08,866
but we also have a native building

536
00:36:08,968 --> 00:36:12,114
drawbacks. First of all,

537
00:36:12,312 --> 00:36:15,766
lot of time and more resources to build

538
00:36:15,868 --> 00:36:19,240
our application. We've seen that

539
00:36:20,330 --> 00:36:23,942
when I started the application using

540
00:36:23,996 --> 00:36:27,946
the JVM, I didn't have to stop the recording because,

541
00:36:28,048 --> 00:36:31,402
well, it takes just a few seconds to

542
00:36:31,456 --> 00:36:36,250
build the application using

543
00:36:36,320 --> 00:36:40,810
the JVM, so generating the bytecode,

544
00:36:41,730 --> 00:36:45,214
but it will require a lot of time and a lot of

545
00:36:45,252 --> 00:36:49,214
resources to perform the static analysis and to

546
00:36:49,252 --> 00:36:52,806
create the native executable.

547
00:36:52,938 --> 00:36:55,330
So this is the first drawback.

548
00:36:55,830 --> 00:36:59,934
Also, native image generates metadata

549
00:37:00,062 --> 00:37:02,370
performing static analysis.

550
00:37:03,270 --> 00:37:06,930
And that static analysis is under

551
00:37:07,000 --> 00:37:11,250
a closed world assumption because we are not executing

552
00:37:11,410 --> 00:37:15,302
our application and everything that it

553
00:37:15,356 --> 00:37:19,050
collects, those reachable

554
00:37:19,790 --> 00:37:22,460
methods and fields and so on,

555
00:37:22,990 --> 00:37:27,850
those are information that are gatorade with

556
00:37:27,920 --> 00:37:31,914
a static analysis. So some dynamic

557
00:37:31,962 --> 00:37:34,750
features require additional configuration.

558
00:37:35,170 --> 00:37:38,990
Of course they can somehow

559
00:37:40,290 --> 00:37:44,034
be investigated and find out by

560
00:37:44,072 --> 00:37:47,534
the static analysis. But the dynamic

561
00:37:47,582 --> 00:37:51,470
features such as reflection and dynamic proxying

562
00:37:51,630 --> 00:37:55,334
are not so easy to be found by

563
00:37:55,372 --> 00:37:58,566
the static analysis. So we will reach a

564
00:37:58,588 --> 00:38:02,582
point to analysis where no more

565
00:38:02,636 --> 00:38:06,294
data can be collected. And to

566
00:38:06,332 --> 00:38:10,490
be sure that everything is collected we have to manually

567
00:38:12,670 --> 00:38:16,540
create those metadata and

568
00:38:16,990 --> 00:38:20,250
give them to the native image generation.

569
00:38:21,730 --> 00:38:25,294
For example, go back to our

570
00:38:25,492 --> 00:38:29,722
demo application and see slightly

571
00:38:29,786 --> 00:38:33,106
different example.

572
00:38:33,208 --> 00:38:36,660
We have a DTO, it's a record with a name.

573
00:38:37,030 --> 00:38:40,402
And our demo controller will

574
00:38:40,456 --> 00:38:44,082
have a get mapping just like before

575
00:38:44,216 --> 00:38:47,334
but also post mapping. And in the

576
00:38:47,372 --> 00:38:51,094
post mapping we will have a

577
00:38:51,132 --> 00:38:55,014
request body and the hello demo that we

578
00:38:55,052 --> 00:38:59,142
had before. This time we'll say hello

579
00:38:59,276 --> 00:39:03,210
and we'll concert the name that we

580
00:39:03,360 --> 00:39:07,146
give in input. Let's compile this

581
00:39:07,248 --> 00:39:10,542
and let's see what kind of metadata and

582
00:39:10,596 --> 00:39:12,670
what metadata it generates.

583
00:39:14,130 --> 00:39:18,800
Okay, again, we have our native application,

584
00:39:19,890 --> 00:39:24,738
we can start it. We have 0.9

585
00:39:24,824 --> 00:39:28,930
seconds. And what we will do is

586
00:39:29,080 --> 00:39:32,658
just test that the get mapping is working

587
00:39:32,744 --> 00:39:36,086
well and that well

588
00:39:36,188 --> 00:39:39,560
let's test the post mapping too.

589
00:39:40,090 --> 00:39:44,214
And okay, it is saying hello Gregorio because

590
00:39:44,412 --> 00:39:48,870
the name that we gave

591
00:39:48,940 --> 00:39:52,390
the endpoint is Gregorio.

592
00:39:52,830 --> 00:39:56,300
Okay, what we want to see here is not

593
00:39:57,550 --> 00:40:01,440
the memory footprint part.

594
00:40:02,210 --> 00:40:07,390
And let's see that all

595
00:40:07,460 --> 00:40:12,202
of things application has been processed

596
00:40:12,266 --> 00:40:16,818
and analyzed with some

597
00:40:16,984 --> 00:40:20,542
steps that produce metadata.

598
00:40:20,606 --> 00:40:24,082
And for example one of them

599
00:40:24,216 --> 00:40:28,274
is this one. The GralvM

600
00:40:28,322 --> 00:40:31,922
reachability metadata is a folder that contains

601
00:40:31,986 --> 00:40:35,330
information that are provided by the libraries

602
00:40:35,410 --> 00:40:38,954
that we are using. As we can see there

603
00:40:38,992 --> 00:40:43,014
is no reference to our package

604
00:40:43,062 --> 00:40:45,660
application example.

605
00:40:46,510 --> 00:40:50,794
So these are metadata that are provided

606
00:40:50,842 --> 00:40:54,478
by the libraries that we are using.

607
00:40:54,644 --> 00:40:58,350
What is referring

608
00:40:58,690 --> 00:41:03,234
to our application is inside this folder because

609
00:41:03,432 --> 00:41:06,914
spring provided a plugin to

610
00:41:06,952 --> 00:41:11,950
the Gralvm native image compiler that helps

611
00:41:12,110 --> 00:41:16,102
the compiler understand the reachability of

612
00:41:16,156 --> 00:41:20,280
the application and everything inside the application

613
00:41:20,650 --> 00:41:24,786
together with the information about the reachability

614
00:41:24,898 --> 00:41:27,800
of spring framework two.

615
00:41:28,590 --> 00:41:31,914
And the first thing that we will

616
00:41:31,952 --> 00:41:35,770
see is these folder resources.

617
00:41:36,110 --> 00:41:40,830
We can see that we have a native image properties

618
00:41:41,250 --> 00:41:44,782
file, but we also have a resource and

619
00:41:44,836 --> 00:41:48,350
a reflect compilation. The resource

620
00:41:50,370 --> 00:41:53,458
will tell to native image that

621
00:41:53,544 --> 00:41:58,930
everything inside, for example meta

622
00:42:01,350 --> 00:42:05,006
has to be included inside

623
00:42:05,128 --> 00:42:09,586
the bundle, inside the native image that will be generated

624
00:42:09,778 --> 00:42:13,080
together with the application properties and so on.

625
00:42:13,850 --> 00:42:17,774
But we also have this file reflect config

626
00:42:17,922 --> 00:42:21,194
and we can also for example

627
00:42:21,312 --> 00:42:25,142
have other configuration

628
00:42:25,206 --> 00:42:29,242
files based on what we put inside

629
00:42:29,376 --> 00:42:33,470
of our application. We can see that the

630
00:42:33,540 --> 00:42:37,406
demo application has been processed and the

631
00:42:37,428 --> 00:42:41,342
information, the metadata that are

632
00:42:41,396 --> 00:42:44,980
generated tells to native image that

633
00:42:45,590 --> 00:42:49,540
it has to query all public methods because

634
00:42:50,790 --> 00:42:54,420
for certain they will be used.

635
00:42:54,970 --> 00:42:58,582
And for example, we can see that the demo

636
00:42:58,636 --> 00:43:02,290
controller will use the greeting

637
00:43:02,370 --> 00:43:07,030
DTO and the greeting DTO will

638
00:43:07,100 --> 00:43:11,146
be exposing the declared fields and

639
00:43:11,248 --> 00:43:14,570
the declared constructor.

640
00:43:15,470 --> 00:43:19,414
Well, also it will have a method

641
00:43:19,462 --> 00:43:24,640
that is the accessor to the name property.

642
00:43:25,650 --> 00:43:29,326
All of this is

643
00:43:29,428 --> 00:43:33,026
inside the resources folder, but we also have

644
00:43:33,048 --> 00:43:36,850
a sources folder with a example

645
00:43:37,000 --> 00:43:40,020
demo and inside of things.

646
00:43:41,590 --> 00:43:45,990
Things spring AOT plugin generated

647
00:43:46,410 --> 00:43:49,906
other information such as for example bin

648
00:43:49,938 --> 00:43:53,510
definitions or pin factory registration.

649
00:43:53,930 --> 00:43:57,950
These are the information that spring framework

650
00:43:58,130 --> 00:44:01,850
generates when it starts. So when we

651
00:44:01,920 --> 00:44:05,478
started it in the JVM mode,

652
00:44:05,654 --> 00:44:09,162
it generated all of this

653
00:44:09,216 --> 00:44:13,178
information and things. Information required

654
00:44:13,274 --> 00:44:17,326
those 6 seconds in

655
00:44:17,348 --> 00:44:21,418
the startup time together with all the initialization

656
00:44:21,514 --> 00:44:25,730
of the framework. Well, in this scenario,

657
00:44:26,230 --> 00:44:30,370
the plugin generated everything that

658
00:44:30,520 --> 00:44:34,162
will be needed to the ahead

659
00:44:34,216 --> 00:44:38,962
of time compilation to create snapshot

660
00:44:39,106 --> 00:44:42,946
that will include everything that spring

661
00:44:43,058 --> 00:44:46,950
would create in those 6.6

662
00:44:47,020 --> 00:44:51,238
seconds. So every initialization,

663
00:44:51,414 --> 00:44:54,940
everything that is related to

664
00:44:55,950 --> 00:44:59,754
bin proxying and so on is inside of

665
00:44:59,792 --> 00:45:03,600
these and will be used to generate the

666
00:45:04,770 --> 00:45:09,630
hip snapshotting and the native image.

667
00:45:10,130 --> 00:45:14,558
Okay, let's get back to the presentation.

668
00:45:14,654 --> 00:45:18,654
So we have dynamic features

669
00:45:18,702 --> 00:45:22,290
that will require additional compilation.

670
00:45:22,630 --> 00:45:24,370
In some cases,

671
00:45:25,370 --> 00:45:29,494
the native image will not be

672
00:45:29,532 --> 00:45:34,342
able to collect the metadata. And this

673
00:45:34,396 --> 00:45:37,626
happened when for example, we use a lot

674
00:45:37,648 --> 00:45:41,078
of reflection and dynamic proxying.

675
00:45:41,174 --> 00:45:44,906
Well, in this situation, in this kind of situation we

676
00:45:44,928 --> 00:45:48,714
have an agent, a tracing agent that can help

677
00:45:48,752 --> 00:45:52,794
us a lot because it will gather metadata

678
00:45:52,842 --> 00:45:56,842
for us and prepare compilation files such as reflect

679
00:45:56,906 --> 00:46:00,686
config JSON that we can use

680
00:46:00,868 --> 00:46:04,420
and provide manually to

681
00:46:05,350 --> 00:46:09,250
the compiler. It is very

682
00:46:09,320 --> 00:46:13,074
useful because it will collect everything

683
00:46:13,272 --> 00:46:16,966
based on an execution of our application. So the

684
00:46:16,988 --> 00:46:20,422
tracing agent can be used. When we start

685
00:46:20,476 --> 00:46:23,862
the application in the JVM, we can

686
00:46:23,916 --> 00:46:27,842
use the application. So for example, we could run some

687
00:46:27,996 --> 00:46:31,322
end to end tests because those

688
00:46:31,456 --> 00:46:36,022
are something that will simulate

689
00:46:36,166 --> 00:46:40,830
a real scenario. And this

690
00:46:40,900 --> 00:46:43,694
real scenario will generate everything,

691
00:46:43,812 --> 00:46:48,106
every metadata that will be used by the native image.

692
00:46:48,298 --> 00:46:52,758
If we don't provide this reachability metadata,

693
00:46:52,874 --> 00:46:57,038
what we can obtain is that the compilation

694
00:46:57,134 --> 00:47:01,170
will go smoothly, it will generate our

695
00:47:01,240 --> 00:47:05,060
executable, but when we execute it,

696
00:47:05,450 --> 00:47:09,734
when it reaches the point that is

697
00:47:09,852 --> 00:47:13,650
not generated using the right reachability

698
00:47:13,730 --> 00:47:17,014
metadata, it will give us an

699
00:47:17,052 --> 00:47:20,566
error telling us that the method

700
00:47:20,678 --> 00:47:24,250
or the class that is required is not

701
00:47:24,320 --> 00:47:27,850
found things is something similar

702
00:47:28,000 --> 00:47:31,102
to no

703
00:47:31,156 --> 00:47:35,150
method exception or something

704
00:47:35,300 --> 00:47:39,166
similar to it, not really an

705
00:47:39,188 --> 00:47:43,026
exception. It is an error that tells us that we

706
00:47:43,048 --> 00:47:46,642
didn't provide enough metadata for

707
00:47:46,696 --> 00:47:50,210
the native mage to

708
00:47:50,280 --> 00:47:53,630
create the right executable.

709
00:47:53,710 --> 00:47:59,394
So to include all of the methods

710
00:47:59,442 --> 00:48:03,778
and classes and so on that the native

711
00:48:03,874 --> 00:48:07,458
executable will need. We have to remember

712
00:48:07,564 --> 00:48:11,514
that things kind of compilation will generate a

713
00:48:11,632 --> 00:48:14,778
small executable and everything

714
00:48:14,864 --> 00:48:19,142
that is not rigid by the static

715
00:48:19,206 --> 00:48:23,050
analysis will not be included in the native

716
00:48:23,130 --> 00:48:26,400
executable because we don't need it,

717
00:48:27,810 --> 00:48:31,322
including it inside of the executable.

718
00:48:31,466 --> 00:48:35,454
We have to obtain

719
00:48:35,582 --> 00:48:39,602
a small footprint and a

720
00:48:39,656 --> 00:48:42,754
lower setup time, so we will use

721
00:48:42,872 --> 00:48:47,442
just what we will really need. Another drawback

722
00:48:47,586 --> 00:48:51,270
is that some libraries does not provide good enough

723
00:48:51,340 --> 00:48:55,320
reachability metadatas. This is something that

724
00:48:56,330 --> 00:48:59,306
GralvM team is working on a lot,

725
00:48:59,408 --> 00:49:03,830
together with everyone that builds frameworks

726
00:49:03,910 --> 00:49:08,422
and libraries. So every time a library doesn't

727
00:49:08,486 --> 00:49:11,590
provide reachability metadata,

728
00:49:11,750 --> 00:49:15,678
the granitem team will work together

729
00:49:15,764 --> 00:49:20,160
with the people that

730
00:49:21,010 --> 00:49:24,738
creates libraries to provide

731
00:49:24,904 --> 00:49:28,962
those metadatas. And this is something that is getting better

732
00:49:29,016 --> 00:49:33,566
and better. Also, some include

733
00:49:33,598 --> 00:49:37,170
the dependencies that we may need to manually

734
00:49:37,330 --> 00:49:40,902
exclude. For example two different

735
00:49:41,036 --> 00:49:45,110
dependencies that initialize two

736
00:49:45,180 --> 00:49:47,922
different login libraries. Well,

737
00:49:48,076 --> 00:49:52,122
things is something that we don't want

738
00:49:52,256 --> 00:49:57,398
to be included inside our native

739
00:49:57,494 --> 00:50:01,262
executable and this is something that maybe will

740
00:50:01,316 --> 00:50:05,546
generate an error because the login facade

741
00:50:05,738 --> 00:50:09,214
will not be able to choose which one implementation to

742
00:50:09,252 --> 00:50:12,934
use. So we will need to manually

743
00:50:13,082 --> 00:50:16,754
exclude one implementation. But it's just an

744
00:50:16,792 --> 00:50:20,002
example and there

745
00:50:20,056 --> 00:50:23,826
might be different cases of this

746
00:50:23,928 --> 00:50:27,830
kind. Also, there are some libraries that will need

747
00:50:27,900 --> 00:50:32,214
some changes, either in the library itself or

748
00:50:32,252 --> 00:50:36,210
in gralvium native mage compiler

749
00:50:36,370 --> 00:50:39,866
one example is aspect J. Aspect J as for

750
00:50:39,888 --> 00:50:42,938
now uses agents to perform,

751
00:50:43,024 --> 00:50:46,154
for example load time weaving. And this

752
00:50:46,192 --> 00:50:49,258
is something that can't be done, as for now

753
00:50:49,344 --> 00:50:52,938
inside the gradm native mage compiler.

754
00:50:53,114 --> 00:50:57,200
So either the native mage compiler will

755
00:50:57,730 --> 00:51:01,726
accept the agents or aspect j

756
00:51:01,828 --> 00:51:05,614
will be changed to, well perform load

757
00:51:05,662 --> 00:51:08,020
time weaving in a different way.

758
00:51:08,710 --> 00:51:12,350
Last, it is better to avoid shaded libraries.

759
00:51:12,430 --> 00:51:16,630
Shaded libraries are some libraries that uses

760
00:51:17,850 --> 00:51:22,294
dependencies or classes that has

761
00:51:22,332 --> 00:51:25,526
a name, but they change the

762
00:51:25,548 --> 00:51:29,482
name or the package of those

763
00:51:29,536 --> 00:51:33,660
classes. So this is the shading of

764
00:51:35,630 --> 00:51:39,778
the library performs on our classes.

765
00:51:39,974 --> 00:51:43,758
Well, this is something that doesn't work really

766
00:51:43,844 --> 00:51:47,418
good inside native

767
00:51:47,514 --> 00:51:51,690
executable. So it is better to avoid shaded libraries

768
00:51:51,850 --> 00:51:54,740
and to use the unshaded one.

769
00:51:55,510 --> 00:52:00,180
Well, this is my

770
00:52:01,350 --> 00:52:05,780
journey. It's a denative way of

771
00:52:06,470 --> 00:52:11,030
using a JVM framework

772
00:52:11,450 --> 00:52:15,570
with a GralvM native image compiling.

773
00:52:15,730 --> 00:52:19,258
And we are at the end

774
00:52:19,344 --> 00:52:22,826
of this talk, so let's get back to the

775
00:52:22,848 --> 00:52:26,460
movie and imagine what the

776
00:52:26,910 --> 00:52:30,006
well, ugly, but in this case

777
00:52:30,128 --> 00:52:34,090
the native will say. So Tuko

778
00:52:34,250 --> 00:52:37,914
in this case will say when you go native,

779
00:52:38,042 --> 00:52:41,662
you go native. So everything

780
00:52:41,796 --> 00:52:45,758
that Priyanka Sharma said about

781
00:52:45,924 --> 00:52:50,082
the cloud native technology should tell us

782
00:52:50,136 --> 00:52:52,450
that we have to innovate.

783
00:52:53,270 --> 00:52:57,560
If we innovate, we can think about

784
00:52:58,330 --> 00:53:02,134
using cloud computing to build touch that's faster and

785
00:53:02,172 --> 00:53:05,910
more resilient. So we said

786
00:53:05,980 --> 00:53:09,420
that we want to have

787
00:53:10,990 --> 00:53:14,586
less CPU consumption, lower the

788
00:53:14,608 --> 00:53:18,982
costs and have smaller memory footprint.

789
00:53:19,126 --> 00:53:22,462
Well, we can achieve it starting

790
00:53:22,596 --> 00:53:26,014
using ralbm native image we

791
00:53:26,052 --> 00:53:29,550
have to remember to use the tracing agent because

792
00:53:29,700 --> 00:53:33,634
it helps us a lot and save us

793
00:53:33,672 --> 00:53:37,310
a lot of time of manually

794
00:53:37,390 --> 00:53:41,314
providing those reachability metadata and trying and

795
00:53:41,352 --> 00:53:45,334
trying and trying over and over again. Well, just use

796
00:53:45,372 --> 00:53:49,270
the tracing agent because it does all the

797
00:53:49,420 --> 00:53:50,680
job for us.

798
00:53:52,090 --> 00:53:55,990
And test the native executable or

799
00:53:56,060 --> 00:53:58,070
perform native tests.

800
00:53:59,150 --> 00:54:03,594
The native executable is something that may

801
00:54:03,712 --> 00:54:08,406
have something that is not configured correctly.

802
00:54:08,518 --> 00:54:12,350
For example, the reachability metadata could need

803
00:54:12,500 --> 00:54:16,078
some improvement. So it's better after

804
00:54:16,244 --> 00:54:20,334
creating the native executable to test

805
00:54:20,532 --> 00:54:24,462
to perform tests. Maybe it's end

806
00:54:24,516 --> 00:54:28,818
to end tests are a great starting

807
00:54:28,904 --> 00:54:33,140
point and we have to test it because

808
00:54:33,750 --> 00:54:38,310
it's something that is definitely innovative.

809
00:54:39,130 --> 00:54:43,126
So we have to be sure that that

810
00:54:43,228 --> 00:54:47,078
kind of compilation added everything that is

811
00:54:47,164 --> 00:54:50,826
needed by our use cases. And since the

812
00:54:50,848 --> 00:54:54,300
end to end tests maps the

813
00:54:55,390 --> 00:54:59,180
real world scenario use case well,

814
00:54:59,710 --> 00:55:02,878
they are something that is really good

815
00:55:02,964 --> 00:55:06,234
to use to test the native executable

816
00:55:06,282 --> 00:55:09,978
pool and be sure that we included

817
00:55:10,074 --> 00:55:13,650
everything with our native image compilation.

818
00:55:14,310 --> 00:55:17,922
In this slide you can find some of

819
00:55:17,976 --> 00:55:21,426
the link that I found

820
00:55:21,528 --> 00:55:25,506
very useful starting from the native image

821
00:55:25,538 --> 00:55:29,346
documentation going to the metadata

822
00:55:29,378 --> 00:55:33,330
collection and the native image plugin

823
00:55:33,410 --> 00:55:37,090
of spring. And I also included

824
00:55:37,170 --> 00:55:40,870
the native image guide for Quarkus.

825
00:55:41,030 --> 00:55:44,618
Remember that everything that we've seen with

826
00:55:44,704 --> 00:55:49,898
the demo application using spring is exactly

827
00:55:50,064 --> 00:55:55,694
the same and is valid for every

828
00:55:55,812 --> 00:55:59,982
microservices framework built with

829
00:56:00,036 --> 00:56:03,230
a language on the JVM.

830
00:56:03,810 --> 00:56:07,760
And this is all so thank you for watching me.

831
00:56:08,210 --> 00:56:11,710
Please tell me if you have

832
00:56:11,780 --> 00:56:15,540
found all of things inspiring and useful and.

