1
00:00:27,160 --> 00:00:30,298
Hello and welcome. Today we're going to learn about how we can use

2
00:00:30,386 --> 00:00:33,786
influxDB v three with HibEMQ and

3
00:00:33,890 --> 00:00:37,426
quics, which is a data streaming platform, as well

4
00:00:37,490 --> 00:00:41,402
as hugging face to build an anomaly detection solution for

5
00:00:41,458 --> 00:00:45,362
some IoT data. And we'll also learn about how this solution in this architecture

6
00:00:45,418 --> 00:00:48,706
could be used for an industrial IoT use case because it's scalable.

7
00:00:48,890 --> 00:00:52,074
So specifically for the agenda today we'll be talking about what

8
00:00:52,114 --> 00:00:55,980
data pipelines are, what HivemQ is, and what influxDB is.

9
00:00:56,122 --> 00:00:59,736
Then we'll dive a little bit into AI and ML in data pipelines

10
00:00:59,800 --> 00:01:03,312
and the application there. Then we'll go into real world applications

11
00:01:03,368 --> 00:01:07,040
and industrial IoT, and we'll follow that

12
00:01:07,072 --> 00:01:10,688
with learning how we can build with HiveMQ and MQTT broker quic

13
00:01:10,736 --> 00:01:14,192
and inflowxDb. Finally, I'll finish this talk today,

14
00:01:14,368 --> 00:01:17,880
sharing some conclusions, answering some questions, or sharing

15
00:01:17,912 --> 00:01:21,056
some common questions and some source code with you so that you can

16
00:01:21,080 --> 00:01:24,502
go ahead and try this example on your own. So who

17
00:01:24,518 --> 00:01:28,014
am I? I'm Ana Eustotis Georgieu. I'm a developer advocate at Influx

18
00:01:28,054 --> 00:01:31,638
data. Influx data is the creator of influxdb, and I want

19
00:01:31,646 --> 00:01:34,070
to encourage you to connect with me on LinkedIn if you'd like to do so.

20
00:01:34,102 --> 00:01:38,094
And I encourage you to ask me any questions that you have about this presentation,

21
00:01:38,214 --> 00:01:41,714
time series, data, data pipelines, IoT,

22
00:01:42,334 --> 00:01:45,670
etcetera. Come ask any questions that you have. I'd love to connect with you

23
00:01:45,742 --> 00:01:49,234
and help you on your time series and data pipeline journey.

24
00:01:49,844 --> 00:01:53,180
So what are data pipelines? If you're familiar with Kafka,

25
00:01:53,252 --> 00:01:57,116
then you're probably familiar with the source and sync model. Information is generated

26
00:01:57,180 --> 00:02:00,212
usually from a sensor, an application, or it could be a log,

27
00:02:00,348 --> 00:02:03,108
and it needs to make its way to where it needs to be consumed.

28
00:02:03,156 --> 00:02:06,580
And that could be applications, end users, controllers,

29
00:02:06,612 --> 00:02:10,092
etcetera. And when you have that data in a pipeline, what can you do

30
00:02:10,108 --> 00:02:13,508
with it? You can do things like normalize it, transform it.

31
00:02:13,556 --> 00:02:16,826
Maybe you need to standardize from a bunch of different types of protocols,

32
00:02:16,940 --> 00:02:20,110
and you might want to do this, all this in flight. So we'll

33
00:02:20,142 --> 00:02:23,654
talk about how we can use HiveMQ to build such a data

34
00:02:23,694 --> 00:02:28,238
pipeline and then store the data in influxdB. So HiveMQ

35
00:02:28,286 --> 00:02:31,878
is an MQTT broker, and like Kafka, it works on a pub sub

36
00:02:31,926 --> 00:02:35,782
model, and that's also similar to other MQT brokers as well.

37
00:02:35,918 --> 00:02:39,742
And it allows you to take information as a message to a topic that

38
00:02:39,758 --> 00:02:44,188
can be ephemeral, persistent, or shared amongst other consumers and producers.

39
00:02:44,366 --> 00:02:47,560
When you post data to that topic, other devices that subscribe to that

40
00:02:47,592 --> 00:02:50,880
topic can then access that data, process it, and write it to other

41
00:02:50,912 --> 00:02:54,496
databases like influxdb. So what

42
00:02:54,520 --> 00:02:58,072
else can you do with HivemQ? You can get data from a variety

43
00:02:58,128 --> 00:03:01,792
of different sources. You can use things like data hub and extensions

44
00:03:01,848 --> 00:03:05,324
to perform any transformations that you might need directly. With HiveMQ,

45
00:03:05,704 --> 00:03:08,176
you can also sync to a variety of databases,

46
00:03:08,240 --> 00:03:11,964
influxDB included, because they have their own MQTT connectors.

47
00:03:12,044 --> 00:03:14,984
But you can also pipe to other databases as well,

48
00:03:15,324 --> 00:03:19,300
and you can perform a bunch of ETL that you need. It also

49
00:03:19,372 --> 00:03:22,716
has t systems integrations. It can integrate

50
00:03:22,780 --> 00:03:25,924
very easily with streaming services like Kafka as well.

51
00:03:26,084 --> 00:03:29,652
And also you have a variety of deployment options. They understand

52
00:03:29,708 --> 00:03:33,292
that there's a need for flexibility in deployment, and so they offer

53
00:03:33,348 --> 00:03:36,814
HiveMQ cloud, which is fully managed and available on major

54
00:03:36,854 --> 00:03:39,926
cloud platforms, and HiveMQ self manage,

55
00:03:39,990 --> 00:03:43,990
which gives you control to specifically deploy your kubernetes and tailor

56
00:03:44,022 --> 00:03:47,110
your HiveMQ to your specific needs. So that's

57
00:03:47,142 --> 00:03:49,914
basically HiveMQ in a nutshell. Zoomed out.

58
00:03:50,534 --> 00:03:53,758
But before we understand how we can leverage HiveMQ to capture

59
00:03:53,806 --> 00:03:56,910
time series data, let's take a step back and talk about what

60
00:03:56,942 --> 00:04:00,718
time series data is in general. So, time series data

61
00:04:00,766 --> 00:04:04,330
is any data that has a timestamp associated with it.

62
00:04:04,462 --> 00:04:08,186
We typically think of stock market data as a

63
00:04:08,210 --> 00:04:11,714
prime example for time series data, and a thing

64
00:04:11,754 --> 00:04:15,098
that exists consistently across stock

65
00:04:15,146 --> 00:04:18,418
market data, or any other type of time series data, is that the single

66
00:04:18,466 --> 00:04:21,602
value is not usually that interesting. What you're

67
00:04:21,658 --> 00:04:25,014
usually interested in with time series data is the trend of

68
00:04:25,354 --> 00:04:29,242
data over time, or the stock value over time, because that

69
00:04:29,258 --> 00:04:31,666
lets you know whether or not you should be buying or selling, or whether or

70
00:04:31,690 --> 00:04:35,284
not there's a problem on your manufacturing floor.

71
00:04:35,434 --> 00:04:38,680
But time series data comes from a variety of

72
00:04:38,712 --> 00:04:42,368
different sources. And when we think about the types of time

73
00:04:42,416 --> 00:04:46,240
series data, we usually like to think of them in terms of two

74
00:04:46,272 --> 00:04:50,088
categories, and that's metrics and events. And metrics are predictable.

75
00:04:50,176 --> 00:04:53,648
They will occur at the same interval. So we can think of pulling,

76
00:04:53,776 --> 00:04:57,520
for example, a vibration sensor, and reading that vibration

77
00:04:57,592 --> 00:05:01,404
data every second from, let's say, something like an accelerometer.

78
00:05:01,884 --> 00:05:05,396
Meanwhile, events are unpredictable, and we cannot derive when

79
00:05:05,420 --> 00:05:08,812
an event will occur. So in the healthcare space, your cardiovascular,

80
00:05:08,868 --> 00:05:12,652
your heart rate would be your metric. And if you have a cardiovascular

81
00:05:12,708 --> 00:05:15,624
event like a heart attack or Afib, that is an event.

82
00:05:17,724 --> 00:05:20,404
We can also think of things like machine fault alerts.

83
00:05:20,564 --> 00:05:24,260
We don't know when a next machine fault will register,

84
00:05:24,412 --> 00:05:27,620
but we can store it when it does. However, one thing that's

85
00:05:27,652 --> 00:05:30,924
interesting about metrics and events is that through aggregation we can

86
00:05:31,044 --> 00:05:34,592
transform any event into a metric. So think about if we did a

87
00:05:34,608 --> 00:05:37,776
daily count of how many machine faults occurred. In this

88
00:05:37,800 --> 00:05:40,656
way we have a metric that's either going to be zero or more, but we'll

89
00:05:40,680 --> 00:05:44,640
at least know that we'll get one reading a day. And this is something that

90
00:05:44,672 --> 00:05:48,448
time series databases are also very good at doing. So what is

91
00:05:48,456 --> 00:05:51,688
a time series database? It has four components. The first

92
00:05:51,776 --> 00:05:55,048
is that it stores timestamp data. Every data point

93
00:05:55,096 --> 00:05:58,788
in a time series database is associated with a timestamp,

94
00:05:58,976 --> 00:06:01,932
and it should allow you to query in time order.

95
00:06:02,108 --> 00:06:05,476
The second is that it accommodates for really high write throughput.

96
00:06:05,540 --> 00:06:08,980
So in most cases you're going to have really high volumes

97
00:06:09,012 --> 00:06:12,260
of batch data or real time streams from multiple endpoints.

98
00:06:12,372 --> 00:06:15,876
So think of something like 1000 sensors, or maybe sensors with

99
00:06:15,900 --> 00:06:19,324
really high throughput, like for example, back to that vibration sensor.

100
00:06:19,364 --> 00:06:23,292
Example, industrial vibration sensors can give

101
00:06:23,388 --> 00:06:27,024
or create up to 10 khz/second so that's

102
00:06:27,104 --> 00:06:31,240
10,000 points every second. And you need to be able to write that data very

103
00:06:31,352 --> 00:06:34,816
easily and with performance security in mind.

104
00:06:34,960 --> 00:06:38,616
And you also want to make sure that it's not going to impact your query

105
00:06:38,680 --> 00:06:42,244
rate. So that brings us to the next part of

106
00:06:42,624 --> 00:06:45,968
a time series database, which is being able to query your data

107
00:06:46,016 --> 00:06:49,096
efficiently, especially over long time ranges, because what is

108
00:06:49,120 --> 00:06:52,728
the value of being able to accommodate really high write throughput

109
00:06:52,776 --> 00:06:55,878
if you can't perform efficient queries on that data subsequently?

110
00:06:56,016 --> 00:06:58,898
So that's another component of time series databases.

111
00:06:59,066 --> 00:07:02,586
And then the last but not least, you want to consider scalability and

112
00:07:02,610 --> 00:07:06,322
performance. You want to have access to scalable architecture where

113
00:07:06,338 --> 00:07:10,666
you can scale horizontally to handle increased load, often across distributed

114
00:07:10,690 --> 00:07:14,330
clusters of machines, again, to accommodate the really high write throughput

115
00:07:14,362 --> 00:07:17,626
that is typical for a lot of time series use cases,

116
00:07:17,810 --> 00:07:21,258
whether that's in the virtual world or the physical world,

117
00:07:21,306 --> 00:07:24,926
like in IoT. So what is influxdB?

118
00:07:25,030 --> 00:07:28,926
InfluxdB is a time series database platform. At its

119
00:07:28,950 --> 00:07:32,710
heart, influxdb v three is written on top of the Apache

120
00:07:32,742 --> 00:07:36,662
ecosystem, so it leverages Apache datafusion, which is

121
00:07:36,678 --> 00:07:40,634
a query execution framework. It also leverages Apache Arrow,

122
00:07:41,014 --> 00:07:44,950
which is the in memory columnar data format. And then

123
00:07:45,062 --> 00:07:48,562
the durable file format is also columnar, and that's parquet.

124
00:07:48,718 --> 00:07:52,322
And influxdbv three is really a complete new rewrite

125
00:07:52,418 --> 00:07:56,266
of the storage engine in the platform. And this was done to

126
00:07:56,330 --> 00:08:00,146
really facilitate a couple key motivations or key pushes

127
00:08:00,210 --> 00:08:04,374
design choices, and these ones were accommodate

128
00:08:04,834 --> 00:08:08,106
near unlimited dimensionality or cardinality so that

129
00:08:08,130 --> 00:08:11,978
you don't have to worry about how you're indexing your points.

130
00:08:12,146 --> 00:08:15,226
So you don't have to worry about that anymore. In the past, you have to

131
00:08:15,250 --> 00:08:18,664
worry about what you wanted to identify as metadata and

132
00:08:18,704 --> 00:08:22,552
fields or tags and fields to make sure that you didn't have

133
00:08:22,608 --> 00:08:26,136
runaway cardinality. But that's no longer a concern. And we also wanted to

134
00:08:26,160 --> 00:08:29,600
increase interoperability. We wanted to be able to have people

135
00:08:29,712 --> 00:08:33,440
hopefully eventually be able to query parquet directly from influxd, have better

136
00:08:33,472 --> 00:08:36,364
interoperability with a bunch of machine learning tools and libraries.

137
00:08:36,664 --> 00:08:40,496
We also, as a part of using data fusion and Arrow,

138
00:08:40,640 --> 00:08:44,358
have the ability to contribute ODBC and JDCB

139
00:08:44,406 --> 00:08:47,870
drivers to increase interoperability with business

140
00:08:47,982 --> 00:08:52,182
analytics tools and business intelligence tools like Tableau for example.

141
00:08:52,318 --> 00:08:56,062
In general, any other companies that's also leveraging Arrow and

142
00:08:56,238 --> 00:08:59,494
Arrow flight to transport arrow over network interface means

143
00:08:59,534 --> 00:09:03,062
that we can really easily plug in with those other systems. So the

144
00:09:03,078 --> 00:09:07,006
idea is to help you avoid vendor locked in and to allow you to build

145
00:09:07,190 --> 00:09:10,686
the solution that best suits your needs

146
00:09:10,750 --> 00:09:14,596
with the individual tools that, that you really need to solve

147
00:09:14,620 --> 00:09:18,324
your problem. And this presentation today is an example of that.

148
00:09:18,364 --> 00:09:21,524
How we're cherry picking HiveMQ with quics,

149
00:09:21,564 --> 00:09:25,904
with hugging face to build this industrial IoT anomaly detection example.

150
00:09:26,404 --> 00:09:30,684
So yeah, we're all about integrating data pipelines in application architectures

151
00:09:30,804 --> 00:09:33,900
today for this demo and this example,

152
00:09:34,012 --> 00:09:37,676
HibiMQ, is how we're going to collect all of our data from our different sensors.

153
00:09:37,820 --> 00:09:41,460
We're going to aggregate our data into one place where we can then bring our

154
00:09:41,492 --> 00:09:44,806
data to the rest of the pipeline. And then we'll use a sync to

155
00:09:44,830 --> 00:09:48,878
tap into the data from HivemQ and write that data directly to influxdb.

156
00:09:49,046 --> 00:09:52,074
Then we also have the ability to process and enrich our data,

157
00:09:53,014 --> 00:09:56,574
and we'll do that as well. Then we'll also use a visualization tool on top,

158
00:09:56,614 --> 00:09:59,966
like Grafana. You could also use something like Apache superset if you wanted

159
00:09:59,990 --> 00:10:04,166
to. And we'll showcase quics, which has support for influxDB,

160
00:10:04,230 --> 00:10:08,014
HiveMQ and MQTT. So how do we combine

161
00:10:08,054 --> 00:10:10,758
HivemQ and influxdb into one architecture?

162
00:10:10,846 --> 00:10:14,396
We are doing the data ingest from HiveMQ and the data processing,

163
00:10:14,460 --> 00:10:18,020
and then influxDB is what's underpinning the storage

164
00:10:18,052 --> 00:10:21,716
of this raw data and the enrichment and the anomaly

165
00:10:21,740 --> 00:10:24,908
detection, essentially. So we'll go into that all in a little bit.

166
00:10:25,036 --> 00:10:28,492
And let's talk about Quics so what we'll be doing with quics, what is

167
00:10:28,508 --> 00:10:31,276
it? First, Quics is a solution for building,

168
00:10:31,420 --> 00:10:35,396
deploying and monitoring event streaming applications, and it

169
00:10:35,420 --> 00:10:37,704
uses or leverages Kafka under the hood,

170
00:10:38,284 --> 00:10:42,416
but it offers this layer of abstraction with python based plugins.

171
00:10:42,540 --> 00:10:45,924
So you really don't need to know any kafka to use quiks. And you also

172
00:10:46,344 --> 00:10:50,192
don't necessarily even need to know Python because you can configure everything through

173
00:10:50,208 --> 00:10:54,136
the UI if you wanted to. You can also change any of the python

174
00:10:54,200 --> 00:10:57,712
that's running underneath any plugins or any components of

175
00:10:57,728 --> 00:11:01,752
your pipeline within clicks to make any changes that you might need and enrich

176
00:11:01,808 --> 00:11:05,320
any templates that they might have. So yeah, you can use either pre canned

177
00:11:05,352 --> 00:11:08,696
plugins from their code library, pre canned templates that are a

178
00:11:08,720 --> 00:11:12,564
series of plugins used together to modify and build your own data

179
00:11:12,604 --> 00:11:15,996
pipeline. And we'll also use quics to build an

180
00:11:16,020 --> 00:11:19,384
ML pipeline from scratch. With not a lot of effort.

181
00:11:20,004 --> 00:11:23,716
What kind of problems can we solve with an architecture like this? We're also using

182
00:11:23,820 --> 00:11:27,572
hugging face as well to deploy or to leverage a

183
00:11:27,668 --> 00:11:31,044
anomaly detection solution. But what kind of problems can we solve like this?

184
00:11:31,084 --> 00:11:35,188
Let's talk about a real world challenge. So this is an example

185
00:11:35,236 --> 00:11:38,922
where we have a company called Pack and go. And in

186
00:11:38,938 --> 00:11:42,370
this imaginary scenario, this packing company is having some issues.

187
00:11:42,442 --> 00:11:45,442
We don't know what the root cause is, but we're getting a lot of errors

188
00:11:45,498 --> 00:11:49,058
from our manufacturing floor. We don't know why a machine is failing,

189
00:11:49,106 --> 00:11:52,274
and we don't know how to identify what's causing the failure when it's

190
00:11:52,314 --> 00:11:56,034
running. Normally, the data exhibits a strong cyclical and seasonal pattern

191
00:11:56,194 --> 00:11:59,474
that has a very predictable pattern. However, when it starts to break

192
00:11:59,514 --> 00:12:02,242
down, it also exhibits a different known pattern.

193
00:12:02,378 --> 00:12:04,994
So the question becomes, how can we use HiVeMQ,

194
00:12:05,074 --> 00:12:08,610
MQTT and influxDB to automate identifying this

195
00:12:08,642 --> 00:12:12,160
pattern in anomalies. So the goal of this demo that I'm going to describe

196
00:12:12,192 --> 00:12:15,832
today is to highlight how we can perform this type of work for the specific

197
00:12:15,888 --> 00:12:19,152
use case and hopefully give you the confidence to try it for yourself so you

198
00:12:19,168 --> 00:12:22,884
can see how easily this could be adopted by someone on the floor.

199
00:12:23,544 --> 00:12:27,240
So here's our complete data pipeline architecture, and I'm going to

200
00:12:27,272 --> 00:12:30,424
continue to refer back to this so that we understand the different stages of

201
00:12:30,464 --> 00:12:34,192
data movement and transformation throughout this talk. So given this hypothetical

202
00:12:34,248 --> 00:12:37,296
problem, this would be our solution architecture and our

203
00:12:37,320 --> 00:12:41,208
data journey. Basically what we're going to be having is these three robot arms

204
00:12:41,256 --> 00:12:44,376
that are packing robots, and these robots are synced to Hive,

205
00:12:44,440 --> 00:12:48,352
the HivemQ broker. And we're going to use the quics MQTT

206
00:12:48,408 --> 00:12:51,520
client to ingest that data in real time and feed

207
00:12:51,552 --> 00:12:54,880
it to our quics destination influxdb plugin.

208
00:12:55,032 --> 00:12:58,640
Then we're going to write that data to a table called machine data

209
00:12:58,712 --> 00:13:01,900
with all of our raw and historical data in influxdb.

210
00:13:02,072 --> 00:13:05,404
From there, we'll use the Qwik source influxdb plugin to query

211
00:13:05,444 --> 00:13:08,588
the data back into quics. We'll run it through an ML model,

212
00:13:08,636 --> 00:13:12,500
and then we'll pass the results back into influxdb with a new

213
00:13:12,532 --> 00:13:16,092
table called ML results. Then we'll use Grafana

214
00:13:16,148 --> 00:13:20,140
to visualize the data, our ML results, and alert on these anomalies.

215
00:13:20,292 --> 00:13:23,964
And I'll talk more about hugging face in a second, but let's

216
00:13:24,004 --> 00:13:27,940
dive into some theory by first stripping it

217
00:13:27,972 --> 00:13:30,824
back and just talking about the data in just a little bit.

218
00:13:31,294 --> 00:13:34,942
So for this demo, we're actually going to be using a robot machine simulator

219
00:13:34,998 --> 00:13:38,174
rather than actual machine data because this is all

220
00:13:38,214 --> 00:13:42,734
containerized example that's available to you on GitHub. I'll share those resources

221
00:13:42,814 --> 00:13:46,166
at the end of this presentation. But basically, I don't want to

222
00:13:46,190 --> 00:13:49,846
have to make you connect hardware yourself to generate some machine data to run this

223
00:13:49,870 --> 00:13:53,062
example. So we have this robot machine stimulator,

224
00:13:53,238 --> 00:13:56,870
and essentially the robots are going to go through three steps.

225
00:13:56,942 --> 00:14:00,582
They create a machine, create a Hive MQ connection, and write a JSON payload to

226
00:14:00,598 --> 00:14:04,270
a tape to a topic. So the simulator spins up

227
00:14:04,422 --> 00:14:07,766
a thread per request machine. Each has its

228
00:14:07,790 --> 00:14:09,994
own independent connection to HiveMQ.

229
00:14:10,574 --> 00:14:13,974
So we will have the three robots and the JSON

230
00:14:14,054 --> 00:14:17,394
will contain a metadata payload which we can see here.

231
00:14:17,814 --> 00:14:22,006
And this contains information about the source. It also contains

232
00:14:22,190 --> 00:14:25,654
a data payload which contains actual sensor readings from

233
00:14:25,694 --> 00:14:29,146
our robot. So we see things like temperature, load, power and

234
00:14:29,170 --> 00:14:32,738
vibration. And as part of our metadata, we see machine

235
00:14:32,786 --> 00:14:36,746
id, barcode, and provider. We'll also write each robot's

236
00:14:36,770 --> 00:14:40,730
data to a child topic that matches their machine id under the

237
00:14:40,762 --> 00:14:44,378
parent topic machine in HiveMQ. So for example,

238
00:14:44,506 --> 00:14:47,970
this one machine that we have, machine one,

239
00:14:48,122 --> 00:14:50,814
the topic will be machine machine one.

240
00:14:51,314 --> 00:14:54,522
Yeah, that's pretty much all of the ingests that

241
00:14:54,538 --> 00:14:58,098
we need to know about for the robots. And whoops,

242
00:14:58,146 --> 00:15:01,946
this is also what the code looks like under the hood for the MQTT

243
00:15:02,010 --> 00:15:05,442
client. So essentially this is going to be a

244
00:15:05,458 --> 00:15:08,962
quick crash course on how to connect to Hive MQTT,

245
00:15:09,138 --> 00:15:12,514
on to Hive MQ with using a Python

246
00:15:12,554 --> 00:15:15,698
class for spinning up the MQTT publisher.

247
00:15:15,866 --> 00:15:19,002
So one cool thing about HiveMQ. Two is that I want to mention is that

248
00:15:19,018 --> 00:15:23,474
you can set up an insecure connection to HiveMQ's public broker,

249
00:15:24,054 --> 00:15:27,598
and you can set up an MQTT client and pass in the public

250
00:15:27,646 --> 00:15:31,110
broker credentials. And this is a great tool for just testing

251
00:15:31,142 --> 00:15:34,918
your client and HiveMQ connection. So to connect to HiveMQ

252
00:15:34,966 --> 00:15:38,086
Cloud, you will need a couple credentials. You'll need to set

253
00:15:38,110 --> 00:15:41,782
up authentication because it is natively secure.

254
00:15:41,838 --> 00:15:45,518
And you'll also need to set up an SSL certificate and a username and password.

255
00:15:45,686 --> 00:15:49,046
And this is also provided to you in the python onboarding

256
00:15:49,070 --> 00:15:52,166
in HiveMQ. So you don't really have to worry about digging through docs to figure

257
00:15:52,190 --> 00:15:56,110
out how to get these. But basically what we're doing here is

258
00:15:56,302 --> 00:15:59,806
after we set up that connection, then we

259
00:15:59,830 --> 00:16:03,542
can construct our topic and send our data with the client publish

260
00:16:03,598 --> 00:16:06,766
method. And I have two connection methods. Depending on the

261
00:16:06,790 --> 00:16:10,374
broker, we have the insecure that doesn't require an any SSL

262
00:16:10,454 --> 00:16:14,240
or the secure that requires setting the TSL certificate.

263
00:16:14,382 --> 00:16:18,332
It's also worth setting your version of to the default,

264
00:16:18,428 --> 00:16:21,684
to the right version that you want to use, which defaults to

265
00:16:21,764 --> 00:16:25,076
3.11. And lastly, at the bottom

266
00:16:25,100 --> 00:16:28,904
there is where we are actually constructing our topic and publishing to it.

267
00:16:29,444 --> 00:16:33,196
So at this point we are writing data to HiveMQ. And so

268
00:16:33,220 --> 00:16:37,380
the question becomes, how does quicz tap into this data stream?

269
00:16:37,492 --> 00:16:40,842
It's going to do so through the MQTT subscriber plugin that

270
00:16:40,858 --> 00:16:44,170
we can subscribe to our parent topic using the hashtag

271
00:16:44,202 --> 00:16:47,306
wildcard, which will use the same library we

272
00:16:47,330 --> 00:16:50,530
used to publish data. And we'll be subscribing

273
00:16:50,562 --> 00:16:54,506
to all three child topics with that wildcard, bringing in the JSON

274
00:16:54,650 --> 00:16:58,482
payload into quic. And then we'll parse that payload and then

275
00:16:58,538 --> 00:17:01,922
write it to the Qwik stream, which is really just a Kafka stream under the

276
00:17:01,938 --> 00:17:05,442
hood. But luckily, like I mentioned, the Qwiks interface abstracts, working with

277
00:17:05,458 --> 00:17:08,856
Kafka so you can just focus on your ETL and data science tasks.

278
00:17:09,010 --> 00:17:12,732
And while we're just doing this parsing of this payload here in

279
00:17:12,748 --> 00:17:16,500
quics, you could imagine if we were getting data from a variety

280
00:17:16,532 --> 00:17:20,332
of different sources with a lot of different protocols. This is Meyer. We might also

281
00:17:20,388 --> 00:17:24,668
perform some additional standardization of our data so that we can store

282
00:17:24,716 --> 00:17:28,180
all in one place and clean it as we would need. So yeah, so after

283
00:17:28,212 --> 00:17:32,028
we parse the JSON payload and write this directly onto the quickstream

284
00:17:32,076 --> 00:17:36,048
topic, which is essentially that Kafka topic, then we can apply

285
00:17:36,216 --> 00:17:39,160
other plugins, transformer plugins, destination plugins,

286
00:17:39,232 --> 00:17:42,984
etcetera. So that's what we're doing with quics. So let's talk about

287
00:17:43,024 --> 00:17:46,456
the data science side of industrial it and where we are

288
00:17:46,480 --> 00:17:51,176
with things. We mentioned that we have these three robots in this packing scenario,

289
00:17:51,360 --> 00:17:54,928
and we can see, for example, what the robots

290
00:17:54,976 --> 00:17:58,936
look like when they're performing normally. There's some evidence of some seasonality

291
00:17:59,000 --> 00:18:02,824
there, some, maybe some clear patterns that could be identified

292
00:18:02,944 --> 00:18:07,244
by decomposing those time series into their respective trend seasonality

293
00:18:08,024 --> 00:18:11,888
components. But we also see what the data

294
00:18:11,936 --> 00:18:15,192
looks like when we have an anomaly. And in an ideal

295
00:18:15,248 --> 00:18:18,784
world, our anomalous data would look something like that

296
00:18:18,824 --> 00:18:21,936
middle robot there, where we just have a sudden spike and the

297
00:18:21,960 --> 00:18:25,376
data looks completely different from our normal data, and something like

298
00:18:25,400 --> 00:18:28,732
a simple threshold indicate to us that we have an

299
00:18:28,748 --> 00:18:32,380
anomaly. However, the real world is usually not this easy.

300
00:18:32,532 --> 00:18:36,324
Realistically, we might also have our anomalous data

301
00:18:36,404 --> 00:18:39,964
presenting some sort of cyclical or seasonal pattern. It might be within

302
00:18:40,004 --> 00:18:43,260
the same standard deviation as our normal data. And so

303
00:18:43,332 --> 00:18:46,812
it becomes much harder to actually determine whether or not we have

304
00:18:46,828 --> 00:18:50,524
an anomaly than doing something as simple as analyzing maybe the standard

305
00:18:50,564 --> 00:18:53,344
deviation of our data or a threshold.

306
00:18:53,994 --> 00:18:57,826
In this case, we can use a more sophisticated method for anomaly

307
00:18:57,850 --> 00:19:01,706
detection, something like an artificial neural network, to help us solve our problem.

308
00:19:01,890 --> 00:19:04,854
Specifically, today we'll be employing an autoencoder.

309
00:19:05,514 --> 00:19:09,562
What is an autoencoder? It's an unsupervised machine learning technique.

310
00:19:09,698 --> 00:19:13,514
So what this means is, essentially, this is a type of machine learning

311
00:19:13,674 --> 00:19:17,410
technique that tries and learns patterns in our data without

312
00:19:17,562 --> 00:19:21,338
being provided any labels or prompts from the engineer,

313
00:19:21,426 --> 00:19:25,114
from me, for example, auto encoders were actually originally used

314
00:19:25,154 --> 00:19:28,818
for image compression, but it was found out that they work great for time series

315
00:19:28,866 --> 00:19:32,658
anomaly detection. So they've often been repurposed for this

316
00:19:32,706 --> 00:19:36,194
exact scenario. So how do autoencoders work?

317
00:19:36,234 --> 00:19:39,682
I'm going to briefly go over the idea. Basically,

318
00:19:39,818 --> 00:19:43,106
there's this input layer, and let me just take

319
00:19:43,130 --> 00:19:47,382
a step back. Let's imagine, for example, that we are trying to convince

320
00:19:47,558 --> 00:19:50,646
our robot to learn a dance, and we

321
00:19:50,670 --> 00:19:54,550
want this autoencoder to be able to describe this dance and

322
00:19:54,702 --> 00:19:58,046
understand it so that we can teach that robot a dance.

323
00:19:58,190 --> 00:20:01,742
We use this analogy just because, essentially, whether or

324
00:20:01,758 --> 00:20:05,582
not we're looking at vibration or temperature

325
00:20:05,678 --> 00:20:08,894
or pressure or whatever other components

326
00:20:08,934 --> 00:20:12,012
we might have for monitoring any sort

327
00:20:12,028 --> 00:20:15,756
of machine on a machine floor, all these components tell us

328
00:20:15,820 --> 00:20:19,092
how a robot is moving or how its health is. We'll just use

329
00:20:19,108 --> 00:20:22,556
the dance analogy, because I also am a dancer and I love dance. So any

330
00:20:22,580 --> 00:20:25,732
tense I get. So let's go back to the input layer of an

331
00:20:25,748 --> 00:20:29,316
autoencoder. An input layer is basically like telling the

332
00:20:29,340 --> 00:20:32,628
robot that it's going to learn a dance made out of specific dance steps.

333
00:20:32,756 --> 00:20:36,464
And then we have a sequence layer. And the sequence layer is

334
00:20:37,104 --> 00:20:40,456
not a standard case layer, but if we want to imagine

335
00:20:40,560 --> 00:20:44,432
what it might do, we can think of it as the time where we're

336
00:20:44,448 --> 00:20:47,736
going to prepare the sequence of the dance steps that we're going to need to

337
00:20:47,760 --> 00:20:51,752
be learned over a certain number of beats or timestamps. It's just mapping out what

338
00:20:51,768 --> 00:20:55,184
the whole dance is going to look like. And then we have an LSTM layer

339
00:20:55,224 --> 00:20:58,480
or a long, short term memory layer. And this layer acts a lot

340
00:20:58,512 --> 00:21:01,624
like the robot's memory. As the robot is going to watch the dance, it's going

341
00:21:02,124 --> 00:21:06,248
to use these layers to remember the sequence. And the first LSTM

342
00:21:06,296 --> 00:21:10,240
layer with the 16 units could be seen as a robot focusing

343
00:21:10,272 --> 00:21:13,800
on remembering the main parts of the dance. And then the second LSTM

344
00:21:13,832 --> 00:21:17,720
layer with four units is like trying to remember the key moments or the

345
00:21:17,752 --> 00:21:21,416
key movements that define the dance's style, not just the steps,

346
00:21:21,480 --> 00:21:25,144
but specifically how those individual shapes

347
00:21:25,184 --> 00:21:28,744
look. And within a time series context, to maybe make that a little bit more

348
00:21:28,784 --> 00:21:32,658
simpler, maybe we know that we have some seasonal patterns

349
00:21:32,706 --> 00:21:35,962
happening. That might be the first 16 layers. And then the second layer

350
00:21:35,978 --> 00:21:39,610
is a little bit more specific, like, how does the arch of those layers look

351
00:21:39,642 --> 00:21:43,362
like? And then we have an encoding layer. And this essentially

352
00:21:43,458 --> 00:21:47,242
back to the dance analogy. The dance moves are encoded into a

353
00:21:47,258 --> 00:21:50,786
simpler form. So we can imagine the robot now has a compressed memory of

354
00:21:50,810 --> 00:21:54,894
this dance, focusing on the most important moves, which is the output of the second

355
00:21:55,074 --> 00:21:58,862
LSTM layer. And this is also similar to how we learn. We first

356
00:21:59,038 --> 00:22:02,182
absorb a big picture of something. Then we have the opportunity to focus on

357
00:22:02,198 --> 00:22:05,350
the details and understand and tune the details. And then when we

358
00:22:05,382 --> 00:22:09,254
start to really put something into our own body or really learn it,

359
00:22:09,334 --> 00:22:12,990
we do only usually think of key moments that help prompt

360
00:22:13,022 --> 00:22:15,674
us onto the next, while the rest becomes muscle memory.

361
00:22:16,094 --> 00:22:19,606
And then we have a repeat vector. And this is like the robot getting ready

362
00:22:19,630 --> 00:22:23,118
to recreate the dance. It takes the core memory of the dance and prepares

363
00:22:23,166 --> 00:22:26,792
it to expand it back into the full sequence. And then

364
00:22:26,808 --> 00:22:30,400
we have the decoding layer. The next LSTM layers are the robot

365
00:22:30,432 --> 00:22:33,368
trying to recall the full dance from its core memories.

366
00:22:33,496 --> 00:22:36,584
And it starts with the essential moves and then builds up the details until

367
00:22:36,624 --> 00:22:40,312
it has the full sequence. And finally, we have the time distributed dance

368
00:22:40,368 --> 00:22:43,640
layer. And this is like the robot refining each moves,

369
00:22:43,672 --> 00:22:47,920
ensuring that each step in the dance is sharp and matches the original

370
00:22:47,992 --> 00:22:51,436
as closely as possible. Now that we understand how

371
00:22:51,460 --> 00:22:55,944
autoencoders work in theory, let's talk about how it can help us to detect anomalies.

372
00:22:56,404 --> 00:23:00,124
In order to understand that, we need to talk about the mean squared errors.

373
00:23:00,164 --> 00:23:03,676
The mean squared error is a way to define the

374
00:23:03,700 --> 00:23:08,332
difference between our actual data and our predicted data and

375
00:23:08,428 --> 00:23:10,864
determine how well our predicted data is.

376
00:23:11,884 --> 00:23:15,460
So the predicted data will be made by the autoencoder,

377
00:23:15,612 --> 00:23:19,340
and the MSA represents a reconstruction error.

378
00:23:19,452 --> 00:23:23,276
It'll measure the distance, basically between our actual data

379
00:23:23,340 --> 00:23:26,956
and our predicted or reconstructed data. Our reconstructed

380
00:23:26,980 --> 00:23:30,196
data should be really similar to our normal data. And so if we have

381
00:23:30,220 --> 00:23:33,412
a high MSc, that means that we have a deviation

382
00:23:33,508 --> 00:23:37,184
from our predicted or reconstructed data,

383
00:23:37,764 --> 00:23:41,220
and therefore we must be experiencing something out of the ordinary.

384
00:23:41,292 --> 00:23:45,026
So high errors indicate anomalies. So let's talk about

385
00:23:45,170 --> 00:23:48,642
some real world challenges with going operational in general,

386
00:23:48,818 --> 00:23:52,330
because we've talked about how to build a pipeline, how to incorporate

387
00:23:52,362 --> 00:23:55,778
some machine learning. But let's specifically talk about

388
00:23:55,906 --> 00:23:58,906
some of the issues melding these worlds together,

389
00:23:59,050 --> 00:24:02,066
because you can have really good data scientists and you can have people that are

390
00:24:02,090 --> 00:24:05,842
really good at building models. But one true challenge is

391
00:24:05,898 --> 00:24:09,306
bringing those models into production. Jupyter and

392
00:24:09,330 --> 00:24:13,526
Keras Jupyter notebooks being one of the primary tools that data scientists work in,

393
00:24:13,650 --> 00:24:17,006
work in a completely different way than the way that we want to run the

394
00:24:17,030 --> 00:24:21,446
models in production and deploy it. So we have our model, our autoencoder,

395
00:24:21,510 --> 00:24:24,846
how do we actually deploy it within our solution and monitor the results?

396
00:24:25,030 --> 00:24:28,558
This is one of the hardest parts of building an AI driven solution. It's like

397
00:24:28,686 --> 00:24:31,302
how do we take a miracle pill that was created out of the lab,

398
00:24:31,358 --> 00:24:34,966
which works specifically in a controlled environment, and bring it to production for

399
00:24:34,990 --> 00:24:38,790
use in everyday life and a lot of time. This is where the bottleneck

400
00:24:38,822 --> 00:24:42,136
is. So we have great machine learning models being developed

401
00:24:42,160 --> 00:24:45,312
by excellent data scientists, but it takes forever for

402
00:24:45,328 --> 00:24:48,856
them to reach production and actually deliver value. And this is where hugging

403
00:24:48,880 --> 00:24:52,640
face comes in. So hugging face is going to be your central repository for

404
00:24:52,672 --> 00:24:56,736
storing, evaluating and deploying models along with

405
00:24:56,760 --> 00:25:00,600
the datasets. So imagine at git on steroids, you upload your model

406
00:25:00,632 --> 00:25:04,160
there, and basically we are using

407
00:25:04,192 --> 00:25:07,472
an API to deploy and then access

408
00:25:07,528 --> 00:25:10,916
our model in our data pipeline. And Quics has an

409
00:25:10,940 --> 00:25:14,388
integration as a native plugin directly with hugging face as well

410
00:25:14,516 --> 00:25:18,380
so your data scientists can stay in the realm of their Jupyter notebooks and

411
00:25:18,492 --> 00:25:21,932
just focus on their expertise. Push their models to

412
00:25:21,948 --> 00:25:25,452
hugging face, and then your data engineers can incorporate those models and deploy them

413
00:25:25,508 --> 00:25:28,996
with incremental testing. So the cool thing about this demo is that

414
00:25:29,020 --> 00:25:32,372
you can generate anomalies also in real time, and then you

415
00:25:32,388 --> 00:25:35,580
can also pick up on those with the auto encoder, adding a

416
00:25:35,612 --> 00:25:39,292
tag that labels the data as anomalous or not, along with the MSE

417
00:25:39,388 --> 00:25:43,350
percentage. This example that's highlighted here also includes

418
00:25:43,382 --> 00:25:45,994
the Grafana visualization that I mentioned earlier.

419
00:25:46,334 --> 00:25:50,190
And we could, for example, use Grafana's powerful alerting tools

420
00:25:50,262 --> 00:25:53,406
to say, if I have a certain amount of anomalous points within a certain amount

421
00:25:53,430 --> 00:25:56,234
of time, then go ahead and actually alert on the data.

422
00:25:56,894 --> 00:26:00,646
So just to conclude here, this is how we use Quic's, influxdB,

423
00:26:00,710 --> 00:26:04,518
HivemQ and hugging face to operationalize anomaly detection

424
00:26:04,606 --> 00:26:07,936
in the Iot space. The cool thing about this stack is

425
00:26:07,960 --> 00:26:11,624
just how scalable it is too. Obviously, this demo is only operating

426
00:26:11,704 --> 00:26:15,392
on three types of robots with generated robot machine data,

427
00:26:15,488 --> 00:26:19,364
but we could easily operate on thousands of robots with this architecture.

428
00:26:20,064 --> 00:26:24,016
So what's next? Let's talk about some hypotheticals. We could imagine putting

429
00:26:24,080 --> 00:26:27,880
another labeling algorithm and actually labeling them as the type anomalies they

430
00:26:27,912 --> 00:26:31,464
are. So we can understand whether or not our machines are encountering bad

431
00:26:31,504 --> 00:26:34,294
bearings, shaky belts, an unexpected load,

432
00:26:34,334 --> 00:26:37,534
etcetera. And then operators could see in real time

433
00:26:37,694 --> 00:26:39,914
what the condition of these machines are.

434
00:26:40,414 --> 00:26:44,118
And of course, everyone's really excited about LLMs right now,

435
00:26:44,246 --> 00:26:47,670
for good reason, right? And as they get better and faster, we could even think

436
00:26:47,702 --> 00:26:51,318
about what else they could do. Maybe they could automate protocol conversion.

437
00:26:51,446 --> 00:26:55,134
Maybe they can translate machine language into human readable language.

438
00:26:55,294 --> 00:26:58,422
And maybe we could replace dashboards altogether. We don't have

439
00:26:58,438 --> 00:27:01,730
to look at dashboards because that's interpreted to us by LLMs.

440
00:27:01,902 --> 00:27:04,614
And instead we could provide insights into our environment.

441
00:27:06,514 --> 00:27:10,074
We could also think about maybe using AI to pass on

442
00:27:10,114 --> 00:27:13,538
expert knowledge. We could monitor how experts are troubleshooting

443
00:27:13,586 --> 00:27:17,098
problems, and how operators are troubleshooting problems, and create

444
00:27:17,146 --> 00:27:20,226
models based off of their domain knowledge in specific use cases,

445
00:27:20,290 --> 00:27:23,722
and how they respond to critical events. Then we

446
00:27:23,738 --> 00:27:26,946
could pass down this knowledge to new technicians and offer prompts to

447
00:27:26,970 --> 00:27:30,196
help them solve the problem. Perhaps when certain machines

448
00:27:30,260 --> 00:27:34,164
encounter certain issues, we also notice a correlation between that

449
00:27:34,204 --> 00:27:38,052
and a certain action, or, you know, access of

450
00:27:38,108 --> 00:27:41,644
particular documentation or protocols that could be automatically

451
00:27:41,684 --> 00:27:45,036
provided to help assist new technicians in

452
00:27:45,180 --> 00:27:49,172
them identifying what the root cause is. And eventually,

453
00:27:49,228 --> 00:27:52,916
maybe we would even have self defined digital twins where they're making the

454
00:27:52,940 --> 00:27:56,104
connections between the machines, the sensors, the applications,

455
00:27:56,424 --> 00:28:00,328
monitoring solutions, and they're making these connections to proactively monitor

456
00:28:00,376 --> 00:28:03,784
and solve problems. So you could imagine maybe

457
00:28:03,824 --> 00:28:07,376
being able to walk onto a factory floor and ask a machine how

458
00:28:07,400 --> 00:28:10,124
it's doing and just be able to be a machine doctor.

459
00:28:11,144 --> 00:28:15,016
So yeah, we can let our imagination run wild with the combination of

460
00:28:15,040 --> 00:28:19,404
all these solutions together. But what are the next step steps for you?

461
00:28:19,784 --> 00:28:22,982
The first is I want to encourage you to try this demo out yourself.

462
00:28:23,128 --> 00:28:26,538
All you need to do in order to try it is to create

463
00:28:26,626 --> 00:28:30,250
a qwiks free account and follow the URL here

464
00:28:30,282 --> 00:28:33,642
and clone it so you can get it up and running and

465
00:28:33,698 --> 00:28:36,574
a influxdb pre cloud tier.

466
00:28:37,554 --> 00:28:40,994
And you can go ahead and like I said, yeah, run this example

467
00:28:41,034 --> 00:28:44,490
yourself for free, or pick and choose the

468
00:28:44,562 --> 00:28:48,478
components from it that you want to use. I also want to encourage you

469
00:28:48,626 --> 00:28:53,126
to join and start up with influxdata.com,

470
00:28:53,190 --> 00:28:56,550
and that's where you can get the free tier of influx data. And I

471
00:28:56,582 --> 00:29:00,622
also want to encourage you to visit our community, our slack, and our forums

472
00:29:00,638 --> 00:29:03,726
at community dot influxdata.com to

473
00:29:03,750 --> 00:29:07,086
ask any questions that you might have about this presentation about

474
00:29:07,150 --> 00:29:10,430
IoT, machine learning, MQTT, etcetera.

475
00:29:10,582 --> 00:29:13,646
And last but not least, I also want to point you to influxdB University.

476
00:29:13,750 --> 00:29:17,554
So influxdb University is a free resource

477
00:29:17,594 --> 00:29:21,938
where you can get access to live and self paced courses

478
00:29:22,066 --> 00:29:25,466
on all things influxdb and even earn badges for some of them. So you can

479
00:29:25,490 --> 00:29:29,370
display those badges on your LinkedIn and also please access to

480
00:29:29,402 --> 00:29:32,986
documentation and self service content like blogs from

481
00:29:33,010 --> 00:29:36,370
influxdata as well so you can learn more about all of this information

482
00:29:36,442 --> 00:29:38,894
in detail in the format that works best for you.

483
00:29:39,474 --> 00:29:43,006
Also encourage you to join the Hivemq community as well if you have questions

484
00:29:43,030 --> 00:29:46,870
about Hivemq and the Qwiks community. We also have Qwiks engineers

485
00:29:46,942 --> 00:29:50,366
on our influx data community as well. So if you're doing anything

486
00:29:50,470 --> 00:29:54,022
with quics, they're happy to. So if you have any questions, please forward them there.

487
00:29:54,038 --> 00:29:54,806
I'd love to hear from.

