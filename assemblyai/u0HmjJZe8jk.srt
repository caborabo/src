1
00:00:20,650 --> 00:00:25,074
Hello and welcome. Today I want to talk about open source library quickstreams

2
00:00:25,162 --> 00:00:28,470
and how to use it to process streaming data in Python.

3
00:00:28,970 --> 00:00:32,406
I'm Tomas Neubauer, I'm CTO and co founder at

4
00:00:32,428 --> 00:00:35,814
Quicks and I want to give you a short insight into

5
00:00:35,852 --> 00:00:39,574
how I kind of get into stream processing and why

6
00:00:39,612 --> 00:00:43,606
we are doing this. So before Quix, I worked in

7
00:00:43,708 --> 00:00:47,734
McLaren and I was leading a team that was connecting f one cars

8
00:00:47,782 --> 00:00:51,914
to factory, basically connecting

9
00:00:51,962 --> 00:00:55,642
a track site to cloud so people can build real time decision

10
00:00:55,706 --> 00:00:59,440
insight in a factory without traveling around the world.

11
00:01:00,130 --> 00:01:03,586
And it was quite challenging use case because the

12
00:01:03,608 --> 00:01:07,810
amount of data from each car was quite huge, roughly 20

13
00:01:07,960 --> 00:01:11,326
million different numerical values per minute.

14
00:01:11,438 --> 00:01:14,722
And as a result, we quickly find out there is

15
00:01:14,776 --> 00:01:18,338
no way how we're going to be able to persist this and query

16
00:01:18,434 --> 00:01:22,134
at the same time with a database in the middle.

17
00:01:22,332 --> 00:01:25,846
And we were sort of forced to look around

18
00:01:25,948 --> 00:01:30,182
for different technology and we end up using streaming

19
00:01:30,246 --> 00:01:34,154
and Kafka broker and we successfully connected the

20
00:01:34,192 --> 00:01:38,540
data and we thought we have solved the problem. But really

21
00:01:38,990 --> 00:01:42,638
the bigger problem was to teach all different

22
00:01:42,724 --> 00:01:46,282
cross functional streams that mostly use Python

23
00:01:46,346 --> 00:01:50,366
and Maslab to leverage this data that we

24
00:01:50,388 --> 00:01:53,934
had now in the Kafka, because streaming is

25
00:01:53,972 --> 00:01:58,034
traditionally Java based ecosystem and

26
00:01:58,072 --> 00:02:01,746
all the tools and frameworks and engines are built in

27
00:02:01,768 --> 00:02:06,478
Java, and as a result it's quite hostile

28
00:02:06,574 --> 00:02:09,814
to Python engineers. So before

29
00:02:09,852 --> 00:02:12,854
we start, let me talk you through what we're going to cover today.

30
00:02:12,972 --> 00:02:15,906
So we're going to talk about different streams processing architectures,

31
00:02:16,018 --> 00:02:20,010
pros and cons of each. We're going to talk about Quickstream architecture

32
00:02:20,350 --> 00:02:23,990
and then to kind of demonstrate the theory in practice,

33
00:02:24,070 --> 00:02:27,226
I will build a real stream processing pipeline in

34
00:02:27,248 --> 00:02:30,926
visual studio code using quickstreams to kind of show you how it

35
00:02:30,948 --> 00:02:34,686
works. Let's talk about streams processing and why you actually want to use

36
00:02:34,708 --> 00:02:38,622
it, because here you get one kind of a gist what

37
00:02:38,756 --> 00:02:42,806
use case we were solving

38
00:02:42,938 --> 00:02:46,626
in Maclam. So generally you

39
00:02:46,648 --> 00:02:49,826
want to use streams processing if you're either getting

40
00:02:49,928 --> 00:02:53,346
lets of data or you want to

41
00:02:53,368 --> 00:02:57,606
react to that data fast, seconds, not minutes, hours or

42
00:02:57,628 --> 00:03:01,094
days. And if you say yes to

43
00:03:01,132 --> 00:03:04,998
at least one of the questions, well, this is probably why you

44
00:03:05,004 --> 00:03:08,074
want to use stream processing. If you say yes to both,

45
00:03:08,192 --> 00:03:13,066
well, it's perfect. So in

46
00:03:13,088 --> 00:03:16,346
the first case, you might want to preprocess your data before you land it to

47
00:03:16,368 --> 00:03:19,986
a storage that could be normalization of your schema,

48
00:03:20,118 --> 00:03:24,206
joining of data filtering, down sampling and

49
00:03:24,388 --> 00:03:27,134
any sort of data preparation in general,

50
00:03:27,332 --> 00:03:30,686
because dumping the raw data into a storage is

51
00:03:30,708 --> 00:03:33,790
expensive, decreasing the performance of your database,

52
00:03:34,130 --> 00:03:37,342
and it's going to just spread on a disk

53
00:03:37,406 --> 00:03:41,314
lots of data and you can't afford that

54
00:03:41,432 --> 00:03:44,786
if you have lots of data and input. If you have five messages per

55
00:03:44,808 --> 00:03:47,080
day, you probably don't care.

56
00:03:47,850 --> 00:03:51,478
Now second, if you want to react to data

57
00:03:51,564 --> 00:03:55,190
fast, regardless of the velocity of your data,

58
00:03:55,340 --> 00:03:59,050
so imagine you want to react

59
00:03:59,390 --> 00:04:02,806
pretty quickly if the nuclear reactor

60
00:04:02,838 --> 00:04:06,490
core is getting overheated or

61
00:04:06,560 --> 00:04:09,782
if you getting your food delivered in two minutes,

62
00:04:09,936 --> 00:04:13,550
you really want to get that information to the destination pretty

63
00:04:13,620 --> 00:04:17,594
fast. And this is where you want to use streams

64
00:04:17,642 --> 00:04:21,374
processing. Now with the stream processing you

65
00:04:21,412 --> 00:04:24,594
have basically two different architectures to

66
00:04:24,632 --> 00:04:28,094
choose from at the beginning. The one is to use client

67
00:04:28,142 --> 00:04:31,634
libraries from the broker of your choice. If you use

68
00:04:31,672 --> 00:04:35,998
Kafka, there are many different libraries.

69
00:04:36,174 --> 00:04:40,150
For example the confluent maintain library,

70
00:04:41,210 --> 00:04:43,160
which is really up to date,

71
00:04:43,930 --> 00:04:47,786
and you will get all the latest features there. And when

72
00:04:47,808 --> 00:04:51,910
you do that, you're basically combining Kafka with some microservice orchestration

73
00:04:52,070 --> 00:04:55,994
like kubernetes by yourself. The second is

74
00:04:56,032 --> 00:04:59,894
to opt for full fledged stream processing framework like fling

75
00:04:59,942 --> 00:05:03,502
or spark. At that moment you're combining that

76
00:05:03,556 --> 00:05:07,646
plus fling or spark and you're kind of adding a dimension to

77
00:05:07,668 --> 00:05:11,806
your architecture. And let's talk about those two options now.

78
00:05:11,988 --> 00:05:15,330
So the first

79
00:05:15,400 --> 00:05:19,086
is quite elegant

80
00:05:19,198 --> 00:05:22,706
for simple use cases where you're just processing one message at

81
00:05:22,728 --> 00:05:26,002
a time, so you don't really have any

82
00:05:26,056 --> 00:05:28,950
context, any sort of state between the messages.

83
00:05:29,290 --> 00:05:32,754
And at that point you are not dragging any JVM dependency.

84
00:05:32,882 --> 00:05:37,094
You have different languages to choose from, so it's quite great.

85
00:05:37,212 --> 00:05:40,940
But the moment you need staple processing, the moment you need

86
00:05:41,310 --> 00:05:45,146
rely on the value of the previous message and trust me, you will end

87
00:05:45,168 --> 00:05:47,100
up there pretty much all the time.

88
00:05:50,210 --> 00:05:53,550
It's going to be very difficult. It's one of the hardest computer

89
00:05:53,620 --> 00:05:57,146
science problem to solve this reliably

90
00:05:57,258 --> 00:06:00,270
at scale and residently against

91
00:06:00,340 --> 00:06:01,710
any hardware failures.

92
00:06:03,650 --> 00:06:07,154
So this is where you might be tempted to go

93
00:06:07,192 --> 00:06:10,562
to something like Flink or Spark. The problem

94
00:06:10,616 --> 00:06:14,478
is that the moment you do that, you're just getting lots

95
00:06:14,494 --> 00:06:18,058
of problems. So first you're dragging a huge Java dependency

96
00:06:18,094 --> 00:06:21,638
to your architecture. And second,

97
00:06:21,804 --> 00:06:26,086
it's not really typical way

98
00:06:26,108 --> 00:06:29,878
of programming because you actually not using Python or

99
00:06:29,964 --> 00:06:34,486
Java, you're actually using Flink DSL to orchestrate

100
00:06:34,518 --> 00:06:37,782
the engine to manipulate data in a scalable,

101
00:06:37,926 --> 00:06:41,814
resilient fashion, which means that it's a different mindset

102
00:06:41,862 --> 00:06:45,338
really, and it also brings some problems to a

103
00:06:45,344 --> 00:06:49,034
day to day developer's life. Like for example debugging so let's

104
00:06:49,082 --> 00:06:53,220
discuss this in a real example.

105
00:06:53,990 --> 00:06:58,094
So here you have Flink

106
00:06:58,142 --> 00:07:02,226
and Quickstream's code. They will do the same thing.

107
00:07:02,408 --> 00:07:06,278
Get the topic with the data of chat application

108
00:07:06,364 --> 00:07:07,960
where people having a different,

109
00:07:09,370 --> 00:07:12,994
they're chatting about different topics in different rooms and you want to calculate

110
00:07:13,042 --> 00:07:16,726
how many words per hour, per room was done.

111
00:07:16,908 --> 00:07:20,822
And here with fling, you're going to connect to a Kafka topic.

112
00:07:20,966 --> 00:07:25,014
Now you can see here, if you look closely, I'm referring

113
00:07:25,062 --> 00:07:28,300
to these two jar files. Well,

114
00:07:28,750 --> 00:07:32,746
putting apart that it's quite difficult

115
00:07:32,848 --> 00:07:36,270
to found the right jar file, to put it to right place,

116
00:07:36,420 --> 00:07:40,462
and to know that you have to also rely on another

117
00:07:40,516 --> 00:07:43,570
jar file. So it's a dependency chain.

118
00:07:43,990 --> 00:07:48,606
But then you are not connecting your code to Kafka,

119
00:07:48,798 --> 00:07:52,610
you're connecting fling to Kafka.

120
00:07:53,030 --> 00:07:56,754
So that's why here you have the SQL statement

121
00:07:56,882 --> 00:07:59,720
which is basically doing exactly that.

122
00:08:00,250 --> 00:08:03,400
And this is the first problem.

123
00:08:03,770 --> 00:08:07,170
Obviously you're relying on not the official Kafka client,

124
00:08:07,250 --> 00:08:09,930
but you're relying on this Fling connector.

125
00:08:10,670 --> 00:08:14,442
And b then

126
00:08:14,496 --> 00:08:17,818
when you want to do some operation, let's say that you want to

127
00:08:17,824 --> 00:08:20,540
calculate this number of words,

128
00:08:22,050 --> 00:08:25,306
because we need to use a custom logic

129
00:08:25,338 --> 00:08:29,086
here to count the word. We are not really going to do

130
00:08:29,108 --> 00:08:33,178
that in Python. Here we're

131
00:08:33,194 --> 00:08:37,022
going to build a python function that we're going to register in fling,

132
00:08:37,166 --> 00:08:40,770
and then we're going to reference the function in our DSL,

133
00:08:41,270 --> 00:08:45,214
which means that fling would basically communicate between Python

134
00:08:45,262 --> 00:08:48,886
and Java with the sockets. And every time we

135
00:08:48,908 --> 00:08:52,742
receive new message, the data would float both

136
00:08:52,796 --> 00:08:56,102
directions. Now I guess you already

137
00:08:56,156 --> 00:08:58,890
start to feel that this is not ideal.

138
00:08:59,390 --> 00:09:03,034
It's not ideal for performance, for developer experience,

139
00:09:03,232 --> 00:09:06,506
and not for debugging, because you simply can't put a

140
00:09:06,528 --> 00:09:09,366
pain point here in this method.

141
00:09:09,478 --> 00:09:12,570
It's not running in your computer, it's running in

142
00:09:12,640 --> 00:09:16,814
some node in fling. So this

143
00:09:16,852 --> 00:09:20,446
is on the other hand an example of quickstream doing exactly the

144
00:09:20,468 --> 00:09:23,742
same. It fits into one page, which is first

145
00:09:23,796 --> 00:09:27,074
plus, but b, this function is doing

146
00:09:27,112 --> 00:09:31,934
exactly the same thing as the previous one. It's running in the same runtime,

147
00:09:32,062 --> 00:09:35,700
which means I can do this,

148
00:09:36,470 --> 00:09:40,594
I can just put a breakpoint in that method and debug

149
00:09:40,642 --> 00:09:44,582
it, see what's happening. So imagine here, it could be not one line but

150
00:09:44,636 --> 00:09:48,262
hundredth line, it could be a couple of classes, could be quite complex

151
00:09:48,326 --> 00:09:52,138
engineering, and I can debug it in

152
00:09:52,144 --> 00:09:55,558
my iDe. And I don't

153
00:09:55,574 --> 00:09:59,638
have to rely on jar files. There's no Java, which means I will not get

154
00:09:59,824 --> 00:10:03,166
exceptions like this. This is my own exception. I got when

155
00:10:03,188 --> 00:10:06,922
I was having the right flink connector,

156
00:10:06,986 --> 00:10:10,750
but incorrect. Kafka client jar file.

157
00:10:12,130 --> 00:10:16,414
And then obviously this is not the greatest architecture

158
00:10:16,542 --> 00:10:19,710
of how to run your code. So you have the Java environment,

159
00:10:19,790 --> 00:10:23,122
python environment, they're connecting to each other. There's a difference between local

160
00:10:23,176 --> 00:10:26,150
and cluster deployment. My goodness.

161
00:10:27,210 --> 00:10:31,238
So what is our approach to stream processing? What I want to show you today

162
00:10:31,404 --> 00:10:35,286
is different approach. The combination of Python, Kafka and

163
00:10:35,308 --> 00:10:39,430
kubernetes, a library that will work nicely

164
00:10:39,510 --> 00:10:43,226
with Docker Kubernetes, graceful shutdowns and

165
00:10:43,328 --> 00:10:47,094
Kafka concept to deliver highly

166
00:10:47,142 --> 00:10:51,214
abstracted stream processing for Python developers so

167
00:10:51,252 --> 00:10:55,162
they can use the whole ecosystem of Pip at Conda

168
00:10:55,306 --> 00:10:58,078
and really use machine learning,

169
00:10:58,164 --> 00:11:01,422
math, physics, all of that without

170
00:11:01,556 --> 00:11:05,226
the drudgery of Java engines

171
00:11:05,418 --> 00:11:09,074
and use their favorite ide in

172
00:11:09,112 --> 00:11:12,494
building their pipelines. And our goals

173
00:11:12,542 --> 00:11:16,614
really is to build scalable and reliable system

174
00:11:16,812 --> 00:11:20,114
both on the transportation of the data side and compute

175
00:11:20,162 --> 00:11:23,430
side, which is extensible and correct.

176
00:11:23,580 --> 00:11:27,766
Correct means you can rely on the output regardless

177
00:11:27,798 --> 00:11:31,046
of the hardware failures, network outage,

178
00:11:31,238 --> 00:11:35,430
et cetera. So this is the basic architecture

179
00:11:35,590 --> 00:11:39,130
and you basically see that Kafka,

180
00:11:40,130 --> 00:11:44,474
where you have replication of data and horizontal scale

181
00:11:44,602 --> 00:11:48,762
is then being consumed by your compute side, where you have the containers

182
00:11:48,826 --> 00:11:52,240
running in a kubernetes, in a stateful set

183
00:11:52,690 --> 00:11:56,846
underpinned by the PVC, where you have your state backup by changelock

184
00:11:56,878 --> 00:12:00,978
topic. And when you need more compute you add more instances of your

185
00:12:01,064 --> 00:12:04,466
Python microservice. When you need more transportation of your

186
00:12:04,488 --> 00:12:07,906
data, you add more nodes in your broker. All it's

187
00:12:07,938 --> 00:12:11,766
good. And then is

188
00:12:11,948 --> 00:12:15,010
how we're going to access this library,

189
00:12:15,170 --> 00:12:18,378
what is going to be the API? And because

190
00:12:18,464 --> 00:12:21,914
Python is a huge community of

191
00:12:21,952 --> 00:12:25,882
developers and huge portion of them are

192
00:12:25,936 --> 00:12:30,050
using pandas for offline analysis in Jupyter notebooks,

193
00:12:30,150 --> 00:12:33,694
we have decided to create a library which

194
00:12:33,732 --> 00:12:37,950
would have interface like pandas dataframes. So you

195
00:12:38,020 --> 00:12:41,614
using the muscle memory that you learn on batch in

196
00:12:41,652 --> 00:12:45,330
streaming and you don't have to relearn new API.

197
00:12:45,750 --> 00:12:49,454
So today I want to show you this in action.

198
00:12:49,582 --> 00:12:53,630
And actually I want to kind of demonstrate the theory in practice

199
00:12:53,710 --> 00:12:57,266
by programming the crash detection microservice.

200
00:12:57,378 --> 00:13:00,726
So we're going to look at the sensor data from this app and

201
00:13:00,748 --> 00:13:04,038
we're going to create a

202
00:13:04,044 --> 00:13:07,862
threshold detection. If we go over a certain number of

203
00:13:07,916 --> 00:13:11,146
gforces applied to this phone, we're going

204
00:13:11,168 --> 00:13:14,746
to create an event of a crash. So let's get

205
00:13:14,768 --> 00:13:18,566
to it. So now we are in quicks.

206
00:13:18,598 --> 00:13:22,366
When I have my pipeline that I want to show you today. Here we

207
00:13:22,388 --> 00:13:26,410
have a web gateway which exposing an API

208
00:13:26,570 --> 00:13:30,750
which this application on my phone would use to send the telemetry

209
00:13:31,490 --> 00:13:35,442
into the pipeline. So I'm going to start the pipeline here.

210
00:13:35,576 --> 00:13:38,706
And you should see that this is basically getting green.

211
00:13:38,808 --> 00:13:42,754
Amazing. And if I go to Messages, I'm getting messages through

212
00:13:42,792 --> 00:13:46,246
this endpoint to the topic. If I get just

213
00:13:46,268 --> 00:13:50,738
one random one. Now you see that it's a payload

214
00:13:50,834 --> 00:13:56,200
in a JSON, not really that

215
00:13:56,570 --> 00:14:00,140
friendly for processing, but we're going to solve that in the next

216
00:14:00,510 --> 00:14:04,570
service. In this service we're taking this raw data and

217
00:14:04,640 --> 00:14:08,154
we converting it to much more

218
00:14:08,192 --> 00:14:10,880
tabular type of data like this.

219
00:14:11,410 --> 00:14:15,454
Cool. And then we have third service called

220
00:14:15,492 --> 00:14:19,214
features which I have prepared for kind

221
00:14:19,252 --> 00:14:22,898
of build something new here. So I will clone this

222
00:14:22,984 --> 00:14:27,250
local and go to visual studio code to get coding.

223
00:14:31,030 --> 00:14:34,366
So we are now in visual studio code when I'm running my dev container.

224
00:14:34,398 --> 00:14:37,230
So I have exactly the same environment as I would have in quicks when I

225
00:14:37,240 --> 00:14:40,950
deployed this to the pipeline. And now we're going to develop

226
00:14:41,020 --> 00:14:45,510
the code that would read the data from a sensor in a phone and

227
00:14:45,580 --> 00:14:48,674
look at the actual meter readout.

228
00:14:48,802 --> 00:14:53,110
And if we met some threshold we can create alert.

229
00:14:53,270 --> 00:14:57,194
So let's get to it. So I'm just going to do a python free

230
00:14:57,232 --> 00:15:00,410
main API to see what is in a topic.

231
00:15:03,250 --> 00:15:06,746
Great. So we have some data. So let's

232
00:15:06,778 --> 00:15:11,178
stop it. We see here the accelermeter,

233
00:15:11,354 --> 00:15:15,330
the dimension. So let's just make a filter

234
00:15:15,910 --> 00:15:19,714
where we make sure we're only getting rows where

235
00:15:19,752 --> 00:15:23,906
this is present. And then I

236
00:15:23,928 --> 00:15:27,734
see we have a timestamp in epoch from

237
00:15:27,772 --> 00:15:31,874
1970 nanoseconds. That's not really that readable

238
00:15:31,922 --> 00:15:35,160
for the development. So let's convert that

239
00:15:35,610 --> 00:15:38,710
to a string.

240
00:15:50,370 --> 00:15:51,120
It.

241
00:15:53,650 --> 00:15:57,146
So it requires seconds,

242
00:15:57,258 --> 00:16:01,440
this method. So I'm going to just divide it and then

243
00:16:03,510 --> 00:16:04,820
let's print it.

244
00:16:10,630 --> 00:16:14,386
Yeah, it would be probably better if

245
00:16:14,488 --> 00:16:16,070
we make this a string.

246
00:16:22,170 --> 00:16:25,462
Amazing. And a table like view is

247
00:16:25,516 --> 00:16:29,274
probably going to be a bit

248
00:16:29,312 --> 00:16:30,170
nicer.

249
00:16:38,910 --> 00:16:42,414
So you see all three dimensions and

250
00:16:42,452 --> 00:16:46,378
a timestamp which is current time. If I shake

251
00:16:46,474 --> 00:16:49,854
you see it's going that values up and

252
00:16:49,892 --> 00:16:53,860
down. So first thing we're going to do is calculate a new

253
00:16:55,430 --> 00:16:59,570
column which is going to be the

254
00:16:59,640 --> 00:17:00,770
accelerometer,

255
00:17:04,470 --> 00:17:08,662
absolute value from all dimensions. That will give us kind

256
00:17:08,716 --> 00:17:11,766
of a sense regardless of

257
00:17:11,788 --> 00:17:13,240
the rotation of the phone,

258
00:17:15,290 --> 00:17:19,046
how hard the forces are. And let's

259
00:17:19,078 --> 00:17:23,114
just print timestamp and

260
00:17:23,152 --> 00:17:24,330
this new column.

261
00:17:32,690 --> 00:17:35,998
All right, so if I take the phone into my hand. Oh great,

262
00:17:36,084 --> 00:17:39,646
we're getting high numbers. So that is

263
00:17:39,748 --> 00:17:43,506
almost where we want to be, but this will

264
00:17:43,528 --> 00:17:46,946
be susceptible to kind of bumps when you

265
00:17:46,968 --> 00:17:50,594
hit with your bike, a bump or you

266
00:17:50,632 --> 00:17:54,690
just get a random shaking

267
00:17:55,690 --> 00:17:59,682
of your phone, it would not really be a real crash.

268
00:17:59,826 --> 00:18:03,462
So let's look at let's say one or 2 seconds long

269
00:18:03,516 --> 00:18:07,538
window where we accumulate the forces applied.

270
00:18:07,634 --> 00:18:14,354
So if there is a continuous

271
00:18:14,402 --> 00:18:18,266
forces applied to the phone like you are falling from the rock or

272
00:18:18,288 --> 00:18:21,886
something, we're going to create an alert. So for that

273
00:18:21,908 --> 00:18:25,486
we're going to do a hopping window, so we're going to

274
00:18:25,508 --> 00:18:29,594
do extraction

275
00:18:29,722 --> 00:18:36,080
of this parameter first

276
00:18:40,850 --> 00:18:45,814
and then hopping window of at

277
00:18:45,852 --> 00:18:48,946
2 seconds and we're going to emit

278
00:18:48,978 --> 00:18:52,578
the row every 250 milliseconds

279
00:18:52,754 --> 00:18:55,926
and we're going to get a sum and

280
00:18:55,948 --> 00:18:59,450
we only want a final row of each window.

281
00:19:00,750 --> 00:19:02,300
So let's run it.

282
00:19:04,670 --> 00:19:08,186
Cool. Now obviously we

283
00:19:08,208 --> 00:19:11,934
get the start and of the window and the value to make

284
00:19:11,972 --> 00:19:16,000
it a bit nicer to read. Let's do the same transformation here

285
00:19:17,090 --> 00:19:24,990
with the startup.

286
00:19:25,350 --> 00:19:29,106
And now you can see that this data being generated four times per

287
00:19:29,128 --> 00:19:32,820
second. So that's great. And if I start

288
00:19:33,910 --> 00:19:37,254
doing some simulation of the crash, you see that we

289
00:19:37,292 --> 00:19:40,598
accumulate this value. So now we are kind of in a

290
00:19:40,604 --> 00:19:44,358
good position to just output this result

291
00:19:44,444 --> 00:19:48,678
to output topic like that and

292
00:19:48,844 --> 00:19:52,314
we can just deploy this to our pipeline and connect it to

293
00:19:52,352 --> 00:19:55,594
our APIs or front end.

294
00:19:55,792 --> 00:19:59,194
And so this is how you use Quickstream's library to

295
00:19:59,232 --> 00:20:03,006
process real time data in a broker. Now I hope you saw

296
00:20:03,188 --> 00:20:06,446
how simple is that? So give

297
00:20:06,468 --> 00:20:07,360
it a try.

298
00:20:10,290 --> 00:20:13,934
Here are the QR codes for GitHub. So you can try this

299
00:20:13,972 --> 00:20:16,826
library or look at the code of the library,

300
00:20:16,938 --> 00:20:20,666
or if you want to try to build a whole pipeline

301
00:20:20,698 --> 00:20:24,446
in the cloud, you can sign up for a free tier of our

302
00:20:24,468 --> 00:20:28,166
cloud plus form. I hope you enjoyed this talk. And if

303
00:20:28,188 --> 00:20:31,926
you have any questions, we have a community slack when you

304
00:20:31,948 --> 00:20:37,846
can discuss anything with me or my colleagues, and if

305
00:20:37,868 --> 00:20:41,522
you have any problems with our library when you're kicking the tires,

306
00:20:41,586 --> 00:20:45,318
just let us know, we won't block you. Thank you very much for

307
00:20:45,484 --> 00:20:48,706
listening to this presentation and enjoy the whole conference.

