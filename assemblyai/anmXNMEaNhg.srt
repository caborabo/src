1
00:00:20,840 --> 00:00:24,558
Hi everyone, and thanks for joining me today. I'm excited here to talk about

2
00:00:24,646 --> 00:00:28,540
large language models, especially large language models evaluations

3
00:00:28,652 --> 00:00:31,932
and how you can evaluate your model in a better way, or how you

4
00:00:31,948 --> 00:00:35,300
can use certain frameworks that are available to understand how

5
00:00:35,332 --> 00:00:39,064
a particular language model is performing for your specific use case.

6
00:00:39,404 --> 00:00:42,948
For those of you who don't know me, I'm Ashwin. I've been working

7
00:00:42,996 --> 00:00:47,204
in the field of machine learning, computer vision, and NLP for over three years,

8
00:00:47,284 --> 00:00:50,620
and I've also worked in some other domains throughout my

9
00:00:50,652 --> 00:00:55,124
career. Today I would like to dive into into a

10
00:00:55,164 --> 00:00:58,780
specific aspect of course evaluations, but also

11
00:00:58,852 --> 00:01:02,844
why they're necessary and how we can get to

12
00:01:02,924 --> 00:01:06,784
that particular evaluation metric. So, to get started,

13
00:01:07,404 --> 00:01:10,436
let me just quickly move the screen. Yep. Okay.

14
00:01:10,540 --> 00:01:14,356
Yeah. So, will the language model speak

15
00:01:14,380 --> 00:01:17,652
the truth? I guess that's the question that everyone's been asking.

16
00:01:17,788 --> 00:01:21,724
Everyone's been really concerned about the whole evaluation flow,

17
00:01:21,804 --> 00:01:25,104
or if we can trust these language models into

18
00:01:25,484 --> 00:01:29,428
telling us something that it definitely knows versus making things

19
00:01:29,476 --> 00:01:33,116
up and telling us something that even we are not sure whether this

20
00:01:33,140 --> 00:01:36,380
is the truth or not. So the bigger question that we probably tend

21
00:01:36,412 --> 00:01:40,044
to answer today is whether the language model is

22
00:01:40,084 --> 00:01:43,604
going to be truthful in answering the questions that we ask them or,

23
00:01:43,644 --> 00:01:46,264
you know, getting summaries out of it. So,

24
00:01:47,074 --> 00:01:50,738
moving ahead, we are jumping into

25
00:01:50,786 --> 00:01:54,170
a particular aspect is why we need these evaluations

26
00:01:54,282 --> 00:01:57,738
and why these particular evaluations are necessary. The reason being

27
00:01:57,786 --> 00:02:00,874
that these evaluations are important

28
00:02:00,994 --> 00:02:04,834
as a part of a measure of your overall large language

29
00:02:04,874 --> 00:02:08,634
model development workflow. So think about it on how.

30
00:02:08,754 --> 00:02:12,778
Think about it as how we can effectively manage or

31
00:02:12,826 --> 00:02:16,330
leverage these language models, while also making sure that

32
00:02:16,362 --> 00:02:20,090
we are not letting it lose completely and having a

33
00:02:20,122 --> 00:02:23,794
bad customer experience in general. So the

34
00:02:23,834 --> 00:02:27,538
three aspects that you need to take care of here is the management

35
00:02:27,626 --> 00:02:31,434
of these language models. Maybe you're using just a few APIs

36
00:02:31,474 --> 00:02:35,514
that are available online and getting the results and just publishing them

37
00:02:35,554 --> 00:02:38,970
to your users. Or maybe you're using, or maybe you're hosting

38
00:02:39,002 --> 00:02:42,404
your own models using frameworks like B LLM.

39
00:02:42,824 --> 00:02:46,360
I guess there's one more called Lama CPP or something like

40
00:02:46,392 --> 00:02:49,736
that. So these frameworks also allow you to

41
00:02:49,760 --> 00:02:53,528
host your own models and also

42
00:02:53,576 --> 00:02:56,840
probably understand how you can improve

43
00:02:56,952 --> 00:02:59,832
the overall performance of these language models.

44
00:03:00,008 --> 00:03:03,664
Understanding these models, measuring these models will ultimately lead

45
00:03:03,704 --> 00:03:07,772
to how you can improve these models, how you can understand on

46
00:03:07,908 --> 00:03:11,444
where these llms fall short, so that we can either refine

47
00:03:11,484 --> 00:03:15,604
our training data, we can start thinking about fine tuning these

48
00:03:15,644 --> 00:03:18,784
language models, maybe lower adapters or,

49
00:03:19,164 --> 00:03:23,588
you know, just in general testing out different language models or different specifically

50
00:03:23,636 --> 00:03:26,024
publicly available, fine tuned language models.

51
00:03:27,164 --> 00:03:30,724
Moving ahead, we are concerned

52
00:03:30,764 --> 00:03:34,132
about what kind of frameworks or what kind

53
00:03:34,188 --> 00:03:37,952
of measurement techniques,

54
00:03:38,008 --> 00:03:41,640
or maybe let's just call them frameworks,

55
00:03:41,752 --> 00:03:45,824
would help us in doing a particular thing.

56
00:03:45,944 --> 00:03:49,456
So we have three selection criteria, or let's

57
00:03:49,480 --> 00:03:52,928
say three evaluation criteria that we can think

58
00:03:52,976 --> 00:03:55,804
of, and we can jump right into that.

59
00:03:56,184 --> 00:03:59,496
But let's seeing that we can have task

60
00:03:59,560 --> 00:04:03,110
specific scores that measure the right

61
00:04:03,182 --> 00:04:07,318
outcome of a particular language model. Imagine a

62
00:04:07,446 --> 00:04:11,766
toolbox where you have specifically designed metrics

63
00:04:11,790 --> 00:04:15,574
or specifically designed items that you use to assess

64
00:04:15,654 --> 00:04:18,854
llms. And while when you consider the

65
00:04:18,894 --> 00:04:22,230
toolbox, your tools would be these three things that we've

66
00:04:22,262 --> 00:04:25,874
mentioned are the task specific scores that we talked about

67
00:04:26,534 --> 00:04:30,614
here. I must say that a one size fits all approach

68
00:04:30,734 --> 00:04:34,834
usually doesn't work. So the ideal framework

69
00:04:35,134 --> 00:04:38,550
will offer you the metrics tailored to your different tasks.

70
00:04:38,622 --> 00:04:41,942
To specific tasks, let's say question answering

71
00:04:41,998 --> 00:04:45,126
or summarization. It will help you,

72
00:04:45,190 --> 00:04:49,118
or it will allow you to evaluate these models on these

73
00:04:49,206 --> 00:04:52,526
specific tasks. The other thing that we need to consider

74
00:04:52,590 --> 00:04:55,674
for a good evaluation framework is the metric list.

75
00:04:56,964 --> 00:05:00,252
You know you have the toolbox, you know how you have the toolset.

76
00:05:00,348 --> 00:05:03,540
But if you don't have a reliable set of metrics that have been

77
00:05:03,572 --> 00:05:07,180
proven before, there is no way in really understanding what

78
00:05:07,252 --> 00:05:11,340
a particular score or what a particular metric

79
00:05:11,412 --> 00:05:14,716
will mean if it is presented to you in an aggregate

80
00:05:14,780 --> 00:05:18,788
or a abstracted manner. So having a good list of

81
00:05:18,836 --> 00:05:22,220
available metrics that you can easily implement is really crucial when

82
00:05:22,252 --> 00:05:26,092
deciding what framework or what library or what general research you're

83
00:05:26,108 --> 00:05:29,956
going to follow to evaluate your language models. And the third part is

84
00:05:30,060 --> 00:05:31,944
basically, really,

85
00:05:33,484 --> 00:05:36,836
we could discuss this broadly or narrowly, depending on what the

86
00:05:36,860 --> 00:05:40,252
context is. But the framework that we're going to use

87
00:05:40,308 --> 00:05:43,860
should be extensible, it should be maintainable,

88
00:05:43,972 --> 00:05:48,268
and of course it should provide you an ease of access to the

89
00:05:48,356 --> 00:05:52,346
underlying function or the underlying classes of it. The reason for this

90
00:05:52,410 --> 00:05:57,226
is because if the framework is adaptable to a particular task,

91
00:05:57,330 --> 00:06:01,330
that's public, but your task requires some specific understanding

92
00:06:01,362 --> 00:06:05,370
or specific knowledge, or a specific way of loading, maybe loading the model

93
00:06:05,402 --> 00:06:09,490
weights. Or maybe if you use a different tokenizer

94
00:06:09,562 --> 00:06:13,258
for tokenizing your particular request

95
00:06:13,306 --> 00:06:17,010
responses, then this becomes a really crucial

96
00:06:17,082 --> 00:06:20,874
aspect in determining whether a particular framework is going to be useful

97
00:06:20,914 --> 00:06:24,196
for you. So, for example, the LM eval,

98
00:06:24,300 --> 00:06:27,948
the LM evaluation harness framework, a really good framework,

99
00:06:27,996 --> 00:06:31,572
a really good design made to evaluate any

100
00:06:31,708 --> 00:06:35,060
model that you can think of, any supported support model

101
00:06:35,092 --> 00:06:39,564
that you can think of on public datasets. But while working

102
00:06:39,684 --> 00:06:43,332
on this particular framework, if you have a task that's

103
00:06:43,388 --> 00:06:46,744
specific to your needs, it's a really difficult,

104
00:06:48,214 --> 00:06:51,838
how do I say it's a really difficult way of implementing

105
00:06:51,886 --> 00:06:55,286
that in this particular library? It's not just about this particularly,

106
00:06:55,390 --> 00:06:59,446
it's in general, any library that has come up for

107
00:06:59,550 --> 00:07:03,358
evaluations which does, which kind of abstract these

108
00:07:03,526 --> 00:07:06,834
evaluation metrics for us. So,

109
00:07:07,294 --> 00:07:10,902
moving ahead, by incorporating all these elements, we can

110
00:07:10,958 --> 00:07:14,864
create a robust, or we can decide

111
00:07:14,944 --> 00:07:17,880
on what robust evaluation framework we can use.

112
00:07:17,992 --> 00:07:21,840
Now, let's dive deeper into the two main approaches

113
00:07:22,032 --> 00:07:25,688
for the LM evaluation and why they

114
00:07:25,736 --> 00:07:28,924
are necessary. As you can see in this particular diagram,

115
00:07:29,704 --> 00:07:33,240
you can see that the part where the human evaluation is

116
00:07:33,272 --> 00:07:36,880
concerned, it ranks really higher, as opposed

117
00:07:36,952 --> 00:07:40,692
to user testing, fine tuning

118
00:07:40,848 --> 00:07:44,756
and maybe public data sets, public benchmarks,

119
00:07:44,820 --> 00:07:48,076
auto evaluation. And the reason really being for that

120
00:07:48,140 --> 00:07:51,660
is the human evaluation aspect really focuses

121
00:07:51,732 --> 00:07:55,420
on multiple geographies, multiple languages, and the

122
00:07:55,452 --> 00:07:59,060
way people understand a particular response,

123
00:07:59,092 --> 00:08:02,668
a particular language, and that really determines a

124
00:08:02,756 --> 00:08:06,588
proper metric or a proper score for your language model to be evaluated

125
00:08:06,636 --> 00:08:10,982
upon. Because as we know, there's multiple dialects

126
00:08:10,998 --> 00:08:14,646
for a particular language, there's multiple people talking or

127
00:08:14,830 --> 00:08:19,114
using different style of grammar, that is not a common

128
00:08:19,534 --> 00:08:22,678
way of speaking or understanding things in their own

129
00:08:22,726 --> 00:08:26,006
countries. And that could really mean a

130
00:08:26,030 --> 00:08:29,526
lot of difference. When you're evaluating a framework, let's say,

131
00:08:29,550 --> 00:08:33,062
for someone in the US, versus if you're evaluating,

132
00:08:33,238 --> 00:08:36,670
sorry, if you're evaluating model on a specific response

133
00:08:36,702 --> 00:08:40,454
for, let's say, for someone in the US, versus for someone who's

134
00:08:40,614 --> 00:08:44,926
not a native english speaker, for them, understanding the context,

135
00:08:44,990 --> 00:08:48,302
understanding what's going on around a particular

136
00:08:48,398 --> 00:08:51,894
response may or may not differ, and they

137
00:08:51,934 --> 00:08:55,134
may or may not think that this particular answer suits them well.

138
00:08:55,254 --> 00:08:58,142
So having a human evaluation framework,

139
00:08:58,198 --> 00:09:01,646
that's drug geography, region specific, or, and of course,

140
00:09:01,710 --> 00:09:05,014
application specific, is a really important aspect

141
00:09:05,054 --> 00:09:09,054
of evaluating these models. Now, once we've talked about human

142
00:09:09,134 --> 00:09:12,262
evaluators and why

143
00:09:12,318 --> 00:09:15,526
those are necessary, we should also consider that it is not

144
00:09:15,590 --> 00:09:19,062
always possible to collect all of this feedback. And it

145
00:09:19,078 --> 00:09:23,474
is not also always possible to have your customers decide or determine

146
00:09:23,974 --> 00:09:27,582
whether a particular answer was good or bad. The customers are,

147
00:09:27,678 --> 00:09:30,910
the customers are really concerned about the value that

148
00:09:30,942 --> 00:09:36,030
your application or your use case is providing. So in

149
00:09:36,062 --> 00:09:40,354
general, we can think of these evaluation metrics.

150
00:09:41,214 --> 00:09:44,294
So in general, these evaluation metrics you can

151
00:09:44,334 --> 00:09:47,574
understand from two different perspectives. One is of course,

152
00:09:47,614 --> 00:09:51,390
the one we discussed, which is the human evaluator part, and one is

153
00:09:51,422 --> 00:09:55,190
the frameworks of the libraries that we'll be exploring in this talk,

154
00:09:55,342 --> 00:09:58,492
we can call them as auto evaluators or, you know,

155
00:09:58,638 --> 00:10:02,324
anything that's non human evaluator. And so

156
00:10:03,024 --> 00:10:06,544
previously or traditionally, the way these language models even came

157
00:10:06,584 --> 00:10:09,792
into being were based on a

158
00:10:09,848 --> 00:10:13,440
large number of data sets, a large number of text or

159
00:10:13,472 --> 00:10:17,000
material that was open, that was available publicly.

160
00:10:17,112 --> 00:10:20,832
And so overall, while these

161
00:10:20,888 --> 00:10:24,584
companies are use cases that are trying to test these models,

162
00:10:24,704 --> 00:10:28,312
they've made human evaluators a kind of a standard,

163
00:10:28,408 --> 00:10:32,688
or how do I say, kind of a stop

164
00:10:32,776 --> 00:10:36,304
in the loop on giving users a complete access

165
00:10:36,384 --> 00:10:41,664
to the language model use case, versus maybe

166
00:10:41,704 --> 00:10:45,144
releasing it as an experiment, or maybe releasing it as a beta,

167
00:10:45,184 --> 00:10:49,192
so that when people interact with it and people understand

168
00:10:49,288 --> 00:10:52,980
what's going on, and sometimes you get oh no, this answer is totally

169
00:10:53,012 --> 00:10:56,364
false, and then people will just bash you for whatever you've

170
00:10:56,404 --> 00:10:59,860
done and just give you negative remarks that in turn helps

171
00:11:00,012 --> 00:11:03,340
you in better serving the model or

172
00:11:03,372 --> 00:11:07,284
better evaluating on what went wrong and where. So the

173
00:11:07,324 --> 00:11:11,212
strengths really here are that humans can provide a nuanced

174
00:11:11,268 --> 00:11:15,244
feedback, judging whether an outcome is simply right or wrong

175
00:11:15,324 --> 00:11:18,836
based on their understanding, but also considering factors

176
00:11:18,900 --> 00:11:22,636
like creativity, coherence, and relevance to the task.

177
00:11:22,780 --> 00:11:26,544
Just as I mentioned before, the same thing that

178
00:11:27,204 --> 00:11:31,348
probably would be relevant or coherent or creative to a

179
00:11:31,476 --> 00:11:35,012
native english speaker wouldn't be the same as for a non native

180
00:11:35,068 --> 00:11:38,824
speaker, just because of how they understand the language.

181
00:11:39,844 --> 00:11:43,548
There is obviously many challenges with human evaluation,

182
00:11:43,636 --> 00:11:47,120
which has its own limitations. The choices, as I said, can be

183
00:11:47,152 --> 00:11:50,592
subjective, based on location and defining success

184
00:11:50,688 --> 00:11:54,544
matrix, only based on what someone from a particular place

185
00:11:54,624 --> 00:11:58,256
said is obviously challenging and may raise

186
00:11:58,320 --> 00:12:02,324
questions on how this came out to be. So, to this,

187
00:12:03,704 --> 00:12:07,512
adding to the fact that human evaluation will also take

188
00:12:07,568 --> 00:12:11,776
time, it is an iterative process and it is also expensive

189
00:12:11,880 --> 00:12:16,416
because you will probably be putting this into the

190
00:12:16,440 --> 00:12:19,840
hands of your potential customers, who probably would get

191
00:12:19,952 --> 00:12:23,792
maybe frustrated, who stop using this app, and then you have to convince

192
00:12:23,848 --> 00:12:27,176
them to use it and give feedback and whatever all

193
00:12:27,200 --> 00:12:30,600
this, that entire flow costs time,

194
00:12:30,752 --> 00:12:33,960
costs money, so we can move towards

195
00:12:34,152 --> 00:12:37,936
automatic evaluators. Now, when I say automatic evaluators,

196
00:12:38,040 --> 00:12:41,336
I don't necessarily mean that everything's happening by itself and

197
00:12:41,360 --> 00:12:44,704
you're just calling a simple function and everyone's happy,

198
00:12:44,744 --> 00:12:48,296
and all the language models have achieved nirvana or greatness.

199
00:12:48,400 --> 00:12:51,880
And whenever I say automatic evaluators, it really means that

200
00:12:51,912 --> 00:12:55,280
the toolbox that we set up with the three different criteria,

201
00:12:55,432 --> 00:12:59,120
that toolbox allows you to evaluate

202
00:12:59,152 --> 00:13:03,648
a particular large language model on its own while

203
00:13:03,776 --> 00:13:07,384
you're iterating over the use cases and the strengths.

204
00:13:07,424 --> 00:13:10,302
Basically, here are their fast, they're efficient,

205
00:13:10,398 --> 00:13:14,086
and they're objective. They assess how well specific parts

206
00:13:14,110 --> 00:13:17,966
of speech or output match your expectations based on the

207
00:13:17,990 --> 00:13:21,358
data sets that you have. And choices are based usually

208
00:13:21,446 --> 00:13:25,174
on known outcomes and well defined metrics. Now,

209
00:13:25,294 --> 00:13:28,750
how do these known outcomes come come into picture? It's the

210
00:13:28,782 --> 00:13:32,518
people who are actually serving these models determining what a

211
00:13:32,566 --> 00:13:36,038
particular output for a particular, let's say,

212
00:13:36,086 --> 00:13:40,508
question or a summary should be, so that whenever you

213
00:13:40,596 --> 00:13:44,252
are close to it, like for example, root scores,

214
00:13:44,348 --> 00:13:48,396
whenever you're really close to it, you think that this is probably a better understand

215
00:13:48,500 --> 00:13:52,188
the better understood answer rather than something that's completely

216
00:13:52,236 --> 00:13:55,724
made up. However, we can also cannot

217
00:13:55,764 --> 00:13:58,628
discount the limitations that they possess.

218
00:13:58,756 --> 00:14:02,284
They can't replicate what we talked about before,

219
00:14:02,364 --> 00:14:05,836
which was a great thing about human evaluator, as well

220
00:14:05,860 --> 00:14:09,506
as its limitation, is that the ability to

221
00:14:09,610 --> 00:14:13,774
understand the context and nuance of this in a

222
00:14:14,154 --> 00:14:17,610
overall sense of what a

223
00:14:17,642 --> 00:14:22,794
particular response should be, or what particular answer,

224
00:14:22,874 --> 00:14:26,426
or what particular future question can come

225
00:14:26,490 --> 00:14:29,442
over based on that particular answer. So,

226
00:14:29,498 --> 00:14:34,000
automatic evaluators, they do struggle with creativity and overall quality judgment,

227
00:14:34,122 --> 00:14:37,836
but they are a lot better evaluator in terms of metrics and

228
00:14:37,860 --> 00:14:41,372
in terms of, let's say, as I said,

229
00:14:41,428 --> 00:14:45,524
Google scores, n gram matching. So they somehow

230
00:14:45,684 --> 00:14:49,300
work well together. But the real, the golden

231
00:14:49,332 --> 00:14:52,860
chance, or the golden opportunity here is

232
00:14:52,892 --> 00:14:56,028
to mix these two human evaluators as well

233
00:14:56,036 --> 00:14:59,876
as the auto evaluators in having your entire flow

234
00:14:59,940 --> 00:15:03,936
is such a way that you leverage the best of

235
00:15:04,000 --> 00:15:08,200
these two and you also overcome the bad,

236
00:15:08,392 --> 00:15:12,440
I would not say bad parts, but the less best parts,

237
00:15:12,472 --> 00:15:15,712
I guess, from each of those.

238
00:15:15,848 --> 00:15:19,112
So I would the takeaway from overall, this is

239
00:15:19,168 --> 00:15:23,448
a collaborative option is probably better, and which

240
00:15:23,496 --> 00:15:27,304
one to choose really depends on your particular use case.

241
00:15:27,424 --> 00:15:30,776
And the ideal approach most likely often

242
00:15:30,880 --> 00:15:35,680
involves a combination where humans provide valuable

243
00:15:35,752 --> 00:15:39,544
feedback on whether this was right or wrong. And the auto evaluators

244
00:15:39,664 --> 00:15:43,488
often consistency in checking whether the answer

245
00:15:43,536 --> 00:15:47,096
that you're getting for the, let's say, for the same question

246
00:15:47,200 --> 00:15:51,264
or for requesting the same summary is always

247
00:15:51,424 --> 00:15:53,884
going to be somewhat similar.

248
00:15:55,434 --> 00:15:58,914
Let shift gears to what we are talking about when we were talking

249
00:15:58,954 --> 00:16:02,650
about whole evaluators, and let shift gears

250
00:16:02,722 --> 00:16:07,570
on understanding how do these two evaluators function,

251
00:16:07,642 --> 00:16:11,730
or how do these two evaluators work with different sorts

252
00:16:11,762 --> 00:16:15,374
of data sets. So we have two kind of

253
00:16:16,514 --> 00:16:19,624
data set divergence or data set paths here.

254
00:16:19,754 --> 00:16:23,356
One is using the public benchmarks that are already available. You don't have

255
00:16:23,380 --> 00:16:26,452
to do much, you just trust these benchmarks that are

256
00:16:26,468 --> 00:16:30,784
available probably on these public leaderboards,

257
00:16:31,604 --> 00:16:37,044
maybe hugging face or individual competitions.

258
00:16:37,204 --> 00:16:40,748
And the other way is using golden datasets.

259
00:16:40,796 --> 00:16:44,004
Now, whenever I say golden datasets, it doesn't

260
00:16:44,044 --> 00:16:47,628
mean that this is literally the gold standard. It just means

261
00:16:47,676 --> 00:16:52,180
that these are datasets or these are the values

262
00:16:52,212 --> 00:16:55,944
that you control and these are the values that are

263
00:16:56,444 --> 00:17:01,604
definitely, or I should say almost 90%

264
00:17:01,764 --> 00:17:05,180
true to be really effective in giving you an answer.

265
00:17:05,332 --> 00:17:08,636
So public benchmarks, these are predefined data sets. You can

266
00:17:08,660 --> 00:17:12,184
easily get that from hugging face. And they are more.

267
00:17:13,024 --> 00:17:16,584
The more the research that was put into creating these data

268
00:17:16,624 --> 00:17:20,464
sets, understanding or training a particular model to test on these data

269
00:17:20,504 --> 00:17:24,312
sets, the more fair assessment these public benchmarks

270
00:17:24,368 --> 00:17:27,824
will give you. And they will give you an understanding of the

271
00:17:27,864 --> 00:17:31,336
general capabilities of a model on different sorts of

272
00:17:31,360 --> 00:17:34,084
data sets, like how well a model is,

273
00:17:35,624 --> 00:17:39,284
how well a model performs with, let's say, something as

274
00:17:39,744 --> 00:17:42,608
something called, as maybe abstractive summarization,

275
00:17:42,696 --> 00:17:46,120
summarization, question answering, or even giving you

276
00:17:46,152 --> 00:17:51,184
answers to a particular code, debugging or explaining maybe

277
00:17:51,224 --> 00:17:55,064
programming concepts or scientific concepts, or, you know,

278
00:17:55,224 --> 00:17:59,144
any concepts in general. However, these public

279
00:17:59,224 --> 00:18:02,576
benchmarks, given the broad scope of what

280
00:18:02,640 --> 00:18:06,634
kind of data sets they usually have and the formats of these,

281
00:18:06,794 --> 00:18:10,578
they don't necessarily guarantee success, or they don't necessarily

282
00:18:10,626 --> 00:18:14,538
guarantee a yes or no answer whenever you're

283
00:18:14,586 --> 00:18:18,530
looking at a particular model and making a decision whether you should use that model

284
00:18:18,562 --> 00:18:21,986
or not, because your use case is really

285
00:18:22,090 --> 00:18:26,218
a specific use case that's coming out of the model, rather than people

286
00:18:26,306 --> 00:18:29,818
asking travel tips or money saving tips,

287
00:18:29,986 --> 00:18:34,156
which is what we usually see. The most popular use cases

288
00:18:34,220 --> 00:18:37,764
of these language models are so the takeaway on public benchmarks.

289
00:18:37,844 --> 00:18:41,044
They do offer valuable starting point, as in,

290
00:18:41,084 --> 00:18:45,104
they do offer something to give you a edge over,

291
00:18:45,564 --> 00:18:48,844
but they shouldn't be the sole measure of what you're trying

292
00:18:48,884 --> 00:18:53,332
to achieve and are trying to test based on the LLM's effectiveness.

293
00:18:53,508 --> 00:18:56,876
Moving ahead to the golden data sets, these data sets,

294
00:18:56,900 --> 00:19:00,162
as I already said, are tailored to your specific needs.

295
00:19:00,298 --> 00:19:03,962
You control what output you expect, you control

296
00:19:04,098 --> 00:19:07,802
what prompt, or let's say, what metrics or

297
00:19:07,978 --> 00:19:11,746
how to put it in a better way. You control what the exact

298
00:19:11,810 --> 00:19:15,554
result from the large language model that you're expecting

299
00:19:15,634 --> 00:19:19,434
is. Of course, you do not control what the large language model obviously

300
00:19:19,474 --> 00:19:22,810
gives you to a certain extent. But what you know is

301
00:19:22,842 --> 00:19:25,514
that if I refer this to particular,

302
00:19:27,174 --> 00:19:31,166
let's say, a particular sentence, I know, the more this sentence

303
00:19:31,230 --> 00:19:35,126
matches with what the LLM has given me. It's more

304
00:19:35,190 --> 00:19:38,726
accurate, or I would assume that it's more accurate and performs well

305
00:19:38,790 --> 00:19:41,114
on the task that I have made it to.

306
00:19:41,694 --> 00:19:45,014
So these, let's say golden data

307
00:19:45,054 --> 00:19:49,230
sets are used will allow you to evaluate how well an LLM

308
00:19:49,262 --> 00:19:53,018
performs of the tasks that matter most to you, rather than being

309
00:19:53,066 --> 00:19:56,282
a generic data set of maybe

310
00:19:56,418 --> 00:19:59,770
Reddit comments on maybe people just posting this, this,

311
00:19:59,802 --> 00:20:03,354
this in a chain that doesn't make sense to you anymore.

312
00:20:03,394 --> 00:20:06,946
But obviously this is a part of the

313
00:20:07,130 --> 00:20:09,854
whole training data set if they weren't clean before.

314
00:20:11,034 --> 00:20:14,314
So some examples of golden data

315
00:20:14,354 --> 00:20:17,810
sets would be for a, let's say for

316
00:20:17,842 --> 00:20:21,824
a use case, like checking semantic similarity of the

317
00:20:21,864 --> 00:20:24,968
content that was given out, or measuring perplexity,

318
00:20:25,056 --> 00:20:28,400
or as I said, root scores in summarization

319
00:20:28,472 --> 00:20:32,064
tasks so that you could understand how well a summary was generated,

320
00:20:32,104 --> 00:20:36,040
how short or how long that summary even was. And these are instrumental

321
00:20:36,072 --> 00:20:39,856
in building the rag based workflows,

322
00:20:39,880 --> 00:20:43,216
which is the retrieval augmented generation based workflows.

323
00:20:43,320 --> 00:20:47,336
And these will give you a really good idea on how your rag

324
00:20:47,400 --> 00:20:50,942
workflow should be, which is what

325
00:20:50,998 --> 00:20:54,142
we are evaluating or what we are

326
00:20:54,158 --> 00:20:57,574
understanding to evaluate here. So the most effective approach,

327
00:20:57,654 --> 00:21:01,126
again, leverage the as we discussed before,

328
00:21:01,190 --> 00:21:03,074
leverage the public.

329
00:21:05,654 --> 00:21:09,206
Leverage the public information, as well as leverage the

330
00:21:09,230 --> 00:21:12,390
data sets that you prepare because they both provide you a

331
00:21:12,422 --> 00:21:15,978
certain benchmark that you can then evaluate and iterate

332
00:21:16,026 --> 00:21:19,574
over. Moving ahead. Now that we understand

333
00:21:20,354 --> 00:21:23,850
what these evaluation methods resources are, what usually

334
00:21:23,922 --> 00:21:27,546
works, let's talk about applying this knowledge to the specific

335
00:21:27,610 --> 00:21:31,394
needs. As I said before, your use case is probably

336
00:21:31,474 --> 00:21:35,162
well defined. You're looking for a model that performs really well

337
00:21:35,218 --> 00:21:38,674
in a particular sector rather than a general model for.

338
00:21:38,794 --> 00:21:42,414
Let's assume this Chevrolet case where someone

339
00:21:43,554 --> 00:21:47,058
tried to ask the language model and try to get a brand

340
00:21:47,106 --> 00:21:49,654
new Chevrolet Tahoe for like $1.

341
00:21:50,394 --> 00:21:53,930
Let's assume that you are in the scientific research community and your

342
00:21:53,962 --> 00:21:57,114
model is really open to answering

343
00:21:57,194 --> 00:22:00,894
any and all sorts of questions that's not really

344
00:22:01,274 --> 00:22:05,090
good for you, that's not really conducive for you. And the

345
00:22:05,122 --> 00:22:08,756
evaluation metrics when they're actually in progress

346
00:22:08,820 --> 00:22:12,268
or work with these models. These will tell you how close people

347
00:22:12,316 --> 00:22:15,612
are talking about the use case,

348
00:22:15,668 --> 00:22:18,788
how close the topics are, and whether they really make

349
00:22:18,836 --> 00:22:22,252
sense, or whether the user feedback makes sense.

350
00:22:22,308 --> 00:22:25,740
So in general, you could consider

351
00:22:25,812 --> 00:22:29,988
this as do you need a particular use case to answer questions?

352
00:22:30,076 --> 00:22:33,610
Do you need to answer summaries? Or do you need particular

353
00:22:33,682 --> 00:22:36,210
use case to provide you citations,

354
00:22:36,322 --> 00:22:40,002
references, or just be a general language model

355
00:22:40,058 --> 00:22:43,786
that really answers the question without any external source

356
00:22:43,890 --> 00:22:47,338
and answers questions from its knowledge database.

357
00:22:47,466 --> 00:22:50,770
So the content that goes in and the content that

358
00:22:50,802 --> 00:22:54,626
comes basically, whatever the content that goes in and

359
00:22:54,650 --> 00:22:58,842
whatever the content that LLM interacts with, is it supporting

360
00:22:58,898 --> 00:23:01,906
a document library, a vector database,

361
00:23:02,010 --> 00:23:06,334
or just a vast collection of text data? That's how

362
00:23:07,714 --> 00:23:10,946
your use case will determine what the output,

363
00:23:11,010 --> 00:23:14,858
or how well defined your output should

364
00:23:14,906 --> 00:23:18,306
be. And eventually, once this is defined,

365
00:23:18,450 --> 00:23:21,810
you could really understand on how the model is performing.

366
00:23:21,922 --> 00:23:24,962
You could maybe involve some fine tuning for it.

367
00:23:25,058 --> 00:23:28,746
You could go on topic modeling and maybe restrict the model on

368
00:23:28,810 --> 00:23:30,804
answering, or even,

369
00:23:32,424 --> 00:23:35,696
or even getting the model to answer particular questions based on

370
00:23:35,720 --> 00:23:39,008
what your use case is. And it's also crucial to establish

371
00:23:39,056 --> 00:23:43,160
certain guardrails, certain input validation,

372
00:23:43,272 --> 00:23:47,204
certain output validation, so that you're not drifting into something that

373
00:23:47,704 --> 00:23:50,952
you shouldn't be doing or you shouldn't be talking about. Of course,

374
00:23:51,008 --> 00:23:53,968
prompt engineering or in general,

375
00:23:54,016 --> 00:23:57,454
defining the prompt is also somewhat an

376
00:23:57,574 --> 00:24:01,806
aspect of this particular flow. But once these metrics that

377
00:24:01,830 --> 00:24:05,198
we'll discuss now are defined, you really know

378
00:24:05,286 --> 00:24:09,062
how a particular language model is working for your

379
00:24:09,158 --> 00:24:12,534
particular flow. Moving ahead, let's dive

380
00:24:12,574 --> 00:24:16,190
into the nitty gritty, or, you know, the specifics of

381
00:24:16,222 --> 00:24:20,314
what we were talking about. How do we measure the effectiveness of a tailored

382
00:24:20,694 --> 00:24:24,196
large language model? We could first move

383
00:24:24,260 --> 00:24:27,464
to traditional metrics, the most familiar

384
00:24:28,084 --> 00:24:32,304
how these tools are. These tools are effective

385
00:24:32,964 --> 00:24:37,064
kind of outputs we outputs we get

386
00:24:37,604 --> 00:24:41,744
from flavors of rules of rule scores.

387
00:24:42,564 --> 00:24:47,804
More language

388
00:24:47,844 --> 00:24:51,484
language processing language processing approach.

389
00:24:54,744 --> 00:24:58,720
How well the LLM in n gram matches,

390
00:24:58,832 --> 00:25:02,344
or in other use cases where

391
00:25:02,384 --> 00:25:05,984
you check other scores, will help

392
00:25:06,024 --> 00:25:09,768
you understand on how better to how

393
00:25:09,816 --> 00:25:13,552
better your model is performing. Also, there's another particular

394
00:25:13,648 --> 00:25:17,850
metric that we should also be exploring is perplexity.

395
00:25:18,002 --> 00:25:21,594
It's a metric that takes a slightly different approach.

396
00:25:21,674 --> 00:25:24,922
Rather than having a well

397
00:25:24,978 --> 00:25:28,690
put dataset together, having a well put timeline together.

398
00:25:28,842 --> 00:25:32,826
It assesses the LLM's internal ability to predict the next word

399
00:25:32,970 --> 00:25:36,834
in a sequence. So it will tell you how confident a particular

400
00:25:36,914 --> 00:25:41,094
LLM was in predicting the next word, and a lower the score.

401
00:25:41,804 --> 00:25:45,284
Lower the overall score. It indicates that the LLM is more

402
00:25:45,324 --> 00:25:49,024
confident in its predictions and is less likely to generate

403
00:25:49,444 --> 00:25:52,972
surprising results, or even completely non

404
00:25:53,028 --> 00:25:56,780
essential results. So while these traditional metrics are available,

405
00:25:56,972 --> 00:26:00,140
things like regas

406
00:26:00,252 --> 00:26:04,444
or ragas, I should say, I don't know how it's

407
00:26:04,484 --> 00:26:08,356
actually pronounced, but things like ragas or regas, they are really

408
00:26:08,420 --> 00:26:12,618
good workflows to to treat your retrieval augmented

409
00:26:12,786 --> 00:26:16,378
generation pipelines. And the metrics like faithfulness

410
00:26:16,466 --> 00:26:19,786
or relevance or other aspects of

411
00:26:19,850 --> 00:26:24,202
this particular framework will allow you to understand more better on how

412
00:26:24,338 --> 00:26:27,490
your whole rag pipeline is working.

413
00:26:27,562 --> 00:26:31,298
And also another good framework

414
00:26:31,346 --> 00:26:35,194
is the QA Ul by Daniel. It also provides

415
00:26:35,274 --> 00:26:39,086
good insights and it checks whether a particular LLM

416
00:26:39,190 --> 00:26:42,750
captures the key concepts or the key information that

417
00:26:42,782 --> 00:26:47,662
was asked of it. So these are some of the I

418
00:26:47,678 --> 00:26:51,582
guess the next step on this, now that we have

419
00:26:51,638 --> 00:26:55,918
established a benchmark is what about llms evaluating

420
00:26:56,006 --> 00:26:58,514
llms? There's been some good research,

421
00:26:59,054 --> 00:27:02,766
some good findings based on whether an LLM can

422
00:27:02,830 --> 00:27:05,810
actually evaluate other LLM.

423
00:27:05,922 --> 00:27:10,374
And this is most likely a trial

424
00:27:11,594 --> 00:27:15,746
and error approach on making sure to be understand the

425
00:27:15,770 --> 00:27:18,874
use case and something like chatbot arena,

426
00:27:18,954 --> 00:27:22,858
which basically you could use the outputs generated from

427
00:27:22,906 --> 00:27:26,106
that from two different models and evaluate and see

428
00:27:26,170 --> 00:27:30,098
whether these two different models could compete with each other in generating

429
00:27:30,146 --> 00:27:34,062
the text. If you ask one model, hey, is this factually correct

430
00:27:34,118 --> 00:27:37,574
based on this answer, and you could iterate

431
00:27:37,614 --> 00:27:41,182
on that, I wouldn't put

432
00:27:41,278 --> 00:27:44,774
much stress on how efficient

433
00:27:44,854 --> 00:27:48,958
or how correct these metrics

434
00:27:49,006 --> 00:27:52,734
are, these judgment would be, but considering

435
00:27:52,774 --> 00:27:57,158
that most models were trained on somewhat similar data

436
00:27:57,206 --> 00:28:01,536
sets, you should expect that the results would be more subjective

437
00:28:01,680 --> 00:28:04,984
and will give you a good insight on how a model

438
00:28:05,024 --> 00:28:08,704
would perform against some other model. The other

439
00:28:08,784 --> 00:28:12,576
approach of this is definitely the metrics that

440
00:28:12,600 --> 00:28:16,400
we discussed. So the metrics that you

441
00:28:16,432 --> 00:28:19,960
choose, your own metrics, you compare those metrics, you have a

442
00:28:19,992 --> 00:28:23,216
definite set of iterative metrics that you've had so far.

443
00:28:23,280 --> 00:28:26,660
You have a data set that you can test on for, and you can

444
00:28:26,692 --> 00:28:30,804
really put that into a number or an understanding

445
00:28:30,844 --> 00:28:33,864
in a perspective that even your customers,

446
00:28:34,244 --> 00:28:37,540
people in your management, or people you're working

447
00:28:37,612 --> 00:28:41,916
with, or even the scientists that you're working with, really have

448
00:28:41,980 --> 00:28:45,292
a good benchmark on where they are right now

449
00:28:45,388 --> 00:28:47,584
and where they are looking to be.

450
00:28:48,724 --> 00:28:53,454
So the human touch, the ultimate judge, what human evaluation

451
00:28:53,614 --> 00:28:57,398
does, what a multifaceted approach will

452
00:28:57,446 --> 00:29:01,574
give you. And despite the rise of all of these automated

453
00:29:01,654 --> 00:29:05,630
metrics, I believe that we

454
00:29:05,662 --> 00:29:09,454
should be also looking at the human aspect of it and be

455
00:29:09,494 --> 00:29:12,982
really sure that we are closing the gap between what we

456
00:29:12,998 --> 00:29:16,398
are evaluating versus what is necessary.

457
00:29:16,446 --> 00:29:20,114
So that instead of chasing the next

458
00:29:20,194 --> 00:29:23,834
cool model or the next best model, the next

459
00:29:23,874 --> 00:29:27,634
public best model available, we really make an informed

460
00:29:27,674 --> 00:29:31,810
decision on whether we are falling behind just because

461
00:29:32,002 --> 00:29:35,882
there was a new model launch, or whether we are falling behind because

462
00:29:36,058 --> 00:29:39,330
the data set that we fine tuned on isn't that good.

463
00:29:39,402 --> 00:29:43,082
Or maybe we need more of that data set, or maybe we

464
00:29:43,098 --> 00:29:47,176
are good with whatever the metrics that we have so slowly

465
00:29:47,200 --> 00:29:51,376
iterating over that we'll get good at the game and we'll

466
00:29:51,400 --> 00:29:55,124
be able to clearly determine what is essentially required.

467
00:29:55,824 --> 00:29:59,616
Moving ahead. These are some available frameworks,

468
00:29:59,680 --> 00:30:03,064
Regas helm, which is a really good benchmark.

469
00:30:03,104 --> 00:30:07,080
Again, these benchmarks will tell you how a

470
00:30:07,112 --> 00:30:11,440
particular model was working on a particular large

471
00:30:11,512 --> 00:30:15,650
corpus of data sets. There's also langsmith by Lang chain,

472
00:30:15,762 --> 00:30:19,282
with and without the integration for the weights

473
00:30:19,298 --> 00:30:23,474
and biases for you to eval. There's OpenAI evals

474
00:30:23,554 --> 00:30:28,130
that gives you a good idea on what evaluation frameworks

475
00:30:28,162 --> 00:30:31,810
or what other evaluation metrics are available. There's deep eval,

476
00:30:31,922 --> 00:30:35,722
there's this LM evaluation harness that I've really grown

477
00:30:35,898 --> 00:30:39,614
kind of fond of. They do a really good job at

478
00:30:40,554 --> 00:30:44,082
allowing you to evaluate a particular model on a

479
00:30:44,098 --> 00:30:47,906
particular public dataset with really less overhead.

480
00:30:47,970 --> 00:30:51,978
But as I said, there's some

481
00:30:52,026 --> 00:30:55,854
overhead to making it work for your own custom use case.

482
00:30:56,234 --> 00:31:00,970
And that's where I found that using simply basic

483
00:31:01,042 --> 00:31:04,842
hugging face functions to load your data set to calculate

484
00:31:04,938 --> 00:31:09,516
scores is probably what is beats,

485
00:31:09,620 --> 00:31:13,332
or is more easier to use or is more efficient to use.

486
00:31:13,468 --> 00:31:17,684
So that's the frameworks that are available that

487
00:31:17,724 --> 00:31:21,244
we can try and test. And at the end, all you

488
00:31:21,284 --> 00:31:25,584
need is a test eval set,

489
00:31:26,164 --> 00:31:29,756
a multifaceted approach on how

490
00:31:29,820 --> 00:31:33,306
you can use the existing test eval

491
00:31:33,370 --> 00:31:37,154
set, or whatever the existing data set you have, or you

492
00:31:37,194 --> 00:31:40,762
will create, and into understanding really

493
00:31:40,818 --> 00:31:44,514
how you can develop a particular set of metrics

494
00:31:44,554 --> 00:31:47,866
around it. And you can use those. So the foundation is

495
00:31:47,890 --> 00:31:51,066
going to be your test set or evaluation set, and the

496
00:31:51,090 --> 00:31:54,322
properties, some which are internal to the model or which

497
00:31:54,338 --> 00:31:57,902
are internal to the framework that you're using, like perplexity, which can be

498
00:31:57,918 --> 00:32:01,598
called calculated from the outputs that you get, or any

499
00:32:01,646 --> 00:32:04,982
existing implementations for any

500
00:32:05,078 --> 00:32:08,606
particular public task, or any particular specific task that you're trying to

501
00:32:08,630 --> 00:32:11,838
look. And the benefits of this

502
00:32:11,926 --> 00:32:15,526
would be that you really control what sort of metrics

503
00:32:15,590 --> 00:32:19,526
are used in particular evaluation, what sort of metrics

504
00:32:19,550 --> 00:32:23,626
you can quantify your application against.

505
00:32:23,750 --> 00:32:26,930
And you can also have an established set

506
00:32:26,962 --> 00:32:30,794
of data sets that can help you over the long term in understanding

507
00:32:30,954 --> 00:32:34,522
how the model performed over a certain duration or

508
00:32:34,538 --> 00:32:38,434
a certain time, and what you can do more better after.

509
00:32:38,514 --> 00:32:41,562
Let's say you fine tuned the model multiple times,

510
00:32:41,618 --> 00:32:45,442
you've changed the model, so it gives you a good flow of all of

511
00:32:45,458 --> 00:32:48,690
the metrics that you can make the decisions

512
00:32:48,722 --> 00:32:52,638
on. I guess that's probably

513
00:32:52,726 --> 00:32:57,406
it. That's what I want to discuss. And I

514
00:32:57,430 --> 00:33:00,590
would in general say that adapting public libraries or

515
00:33:00,622 --> 00:33:04,438
public frameworks like hugging face eval or lmewal

516
00:33:04,486 --> 00:33:08,166
harness is a really good start at first to get

517
00:33:08,230 --> 00:33:11,674
any metrics like f one aggregated scores,

518
00:33:12,854 --> 00:33:17,014
blue scores or root scores, blue scores or root

519
00:33:17,054 --> 00:33:20,792
scores, or all of these scores, to decide

520
00:33:20,848 --> 00:33:24,376
or define a particular evaluation framework for

521
00:33:24,400 --> 00:33:27,768
a RNG based flow. And these will work seamlessly

522
00:33:27,856 --> 00:33:31,008
with your chosen data sets. Also, of course,

523
00:33:31,176 --> 00:33:34,496
including human evaluation, human flow into

524
00:33:34,560 --> 00:33:37,912
the application and comparing really on

525
00:33:38,048 --> 00:33:41,464
what human evaluation results are with

526
00:33:41,504 --> 00:33:45,144
what you're getting, as opposed to a certain particular

527
00:33:45,224 --> 00:33:48,740
benchmark, will be a overall

528
00:33:48,852 --> 00:33:52,284
good strategy to evaluate large language models

529
00:33:52,324 --> 00:33:55,596
on. In conclusion, we've really,

530
00:33:55,700 --> 00:33:59,116
in a really abstract manner, explored how

531
00:33:59,300 --> 00:34:03,252
you can evaluate why a particular evaluation type

532
00:34:03,308 --> 00:34:07,132
is necessary. And it's, and at the end, it is not just

533
00:34:07,188 --> 00:34:10,556
about whether you get a score out of it

534
00:34:10,580 --> 00:34:14,068
or not. It's not about establishing a particular metric and,

535
00:34:14,116 --> 00:34:17,224
you know, charting out six

536
00:34:17,304 --> 00:34:21,184
months of metric data and just

537
00:34:21,224 --> 00:34:24,600
saying that, hey, this is the model that works for me. It's about

538
00:34:24,672 --> 00:34:28,920
establishing a foundation so that whenever you're developing

539
00:34:28,952 --> 00:34:33,376
iteratively, whenever you're mark, you're managing

540
00:34:33,440 --> 00:34:37,232
these large language models so that you can continuously

541
00:34:37,288 --> 00:34:40,896
improve on the remarkable research that

542
00:34:40,920 --> 00:34:45,099
has already done into putting these large language models out public with

543
00:34:45,131 --> 00:34:48,555
their weights. And by leveraging these right metrics,

544
00:34:48,659 --> 00:34:51,875
you can now unlock the potential of these models

545
00:34:52,019 --> 00:34:55,667
in for your specific use case and determine

546
00:34:55,715 --> 00:34:59,427
whether a particular model works for you, or if it doesn't

547
00:34:59,475 --> 00:35:02,595
work for you, why does it not work for you?

548
00:35:02,699 --> 00:35:06,571
And in general, understanding or delivering

549
00:35:06,707 --> 00:35:10,954
real world benefits to the user. And as this field

550
00:35:11,374 --> 00:35:14,534
grows, there's new, more new research into

551
00:35:14,574 --> 00:35:18,766
it. We'd probably move ahead, or we've probably already moved

552
00:35:18,790 --> 00:35:22,510
ahead beyond these basic scores and we've gone into more

553
00:35:22,582 --> 00:35:25,034
a complex understanding of context,

554
00:35:26,254 --> 00:35:30,014
whether the particular model understands a particular context grammar

555
00:35:30,094 --> 00:35:34,318
and all of these ideas. And it

556
00:35:34,326 --> 00:35:36,714
is going to be really exciting on what more,

557
00:35:37,674 --> 00:35:41,346
what more metrics or frameworks that come up so

558
00:35:41,370 --> 00:35:44,694
that we could evaluate a large language model better.

559
00:35:45,394 --> 00:35:48,610
So yeah, that's it from me, and I

560
00:35:48,642 --> 00:35:51,826
hope you got a really good starting point. This was meant

561
00:35:51,850 --> 00:35:55,482
to be a good, this was meant to be just an introduction on

562
00:35:55,618 --> 00:35:59,394
what kind of frameworks are available and what you can do with those and which

563
00:35:59,434 --> 00:36:04,056
approaches work really well. So I hope you enjoyed

564
00:36:04,080 --> 00:36:07,768
this, you learned something, or maybe you confirmed

565
00:36:07,816 --> 00:36:11,864
something that you already knew, and I'd be happy to connect

566
00:36:11,944 --> 00:36:15,560
and explore more in depth given the time restraint

567
00:36:15,592 --> 00:36:18,680
that we have. And if you have any questions, or if you want to really

568
00:36:18,752 --> 00:36:22,752
go dive deep into any of these concepts, or why you

569
00:36:22,768 --> 00:36:26,728
should make a particular choice or what my previous experiences

570
00:36:26,776 --> 00:36:30,716
have been. You could connect with me on LinkedIn and we could discuss

571
00:36:30,780 --> 00:36:33,996
that as well. Again, thank you Khan 42 for giving

572
00:36:34,060 --> 00:36:38,164
me this platform explaining this. I've learned a lot while

573
00:36:38,284 --> 00:36:42,076
researching over this particular topic from my previous experience and

574
00:36:42,100 --> 00:36:43,244
I hope you guys learned too.

