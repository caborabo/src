1
00:00:21,160 --> 00:00:25,750
Hello and welcome. Today we will explore the fascinating

2
00:00:25,822 --> 00:00:29,394
world of security risks in large language models.

3
00:00:30,084 --> 00:00:33,652
Llms have introduced AI technologies to millions

4
00:00:33,668 --> 00:00:36,984
of people for both professional and personal usage.

5
00:00:37,324 --> 00:00:40,704
However, with great power comes great responsibility,

6
00:00:41,084 --> 00:00:44,964
so llms should be safe and secure. In this

7
00:00:45,004 --> 00:00:48,780
presentation, we will review the most important security

8
00:00:48,892 --> 00:00:52,532
risks for large language models. My name is

9
00:00:52,548 --> 00:00:55,964
Eugene, and I have been working in cybersecurity

10
00:00:56,044 --> 00:01:00,784
since 2008 and NGA safety since 2018.

11
00:01:01,674 --> 00:01:05,130
These days, I focus on commercializing research

12
00:01:05,282 --> 00:01:08,134
and transforming it into enterprise products,

13
00:01:08,834 --> 00:01:12,426
and during my journey I have made some industry

14
00:01:12,530 --> 00:01:16,498
contributions, including creating the MLsocos

15
00:01:16,546 --> 00:01:20,534
framework for integrating security into Amlops,

16
00:01:20,834 --> 00:01:24,882
founding the first startup in adversarial machine

17
00:01:24,938 --> 00:01:28,114
learning, and lately co authoring the

18
00:01:28,194 --> 00:01:30,934
Wasp top ten for LLM security.

19
00:01:31,924 --> 00:01:36,196
This presentation is based on my work as an AI security expert

20
00:01:36,260 --> 00:01:40,744
and a core team member of the LLM security team at OWASP.

21
00:01:41,284 --> 00:01:45,100
So today my goal is to give you a very quick

22
00:01:45,212 --> 00:01:48,304
and gentle introduction to LLM security.

23
00:01:49,324 --> 00:01:52,980
So for more advanced content, I recommend

24
00:01:53,092 --> 00:01:56,884
referring to the original work. The references

25
00:01:57,004 --> 00:01:59,798
will be provided at the end of this presentation.

26
00:01:59,996 --> 00:02:03,866
And now let's start exploring the most critical

27
00:02:03,970 --> 00:02:07,214
security risks for LLM applications.

28
00:02:08,114 --> 00:02:12,138
You have likely heard concerns about llms being

29
00:02:12,186 --> 00:02:16,010
misused for illicit activities such as

30
00:02:16,202 --> 00:02:20,334
making bombs, creating drugs, or even grooming children.

31
00:02:21,114 --> 00:02:24,746
Llms from well known AI companies were

32
00:02:24,770 --> 00:02:26,974
not intended for such tasks.

33
00:02:27,804 --> 00:02:32,316
Developers of LLM based chat applications

34
00:02:32,500 --> 00:02:35,784
implement safety measures and content filters,

35
00:02:36,284 --> 00:02:39,916
and despite this effort, malicious prompts

36
00:02:39,940 --> 00:02:43,424
can still bypass these safeguards.

37
00:02:44,204 --> 00:02:47,468
This vulnerability in LLM guardrails

38
00:02:47,596 --> 00:02:51,824
opens the door for an attack known as jailbreaking.

39
00:02:52,624 --> 00:02:56,284
Jailbreaking encompasses a range of techniques,

40
00:02:56,904 --> 00:03:00,632
from switching between languages to data format

41
00:03:00,688 --> 00:03:03,904
manipulation and even persuasive negotiation

42
00:03:03,944 --> 00:03:07,120
with llms. The consequences of weak

43
00:03:07,152 --> 00:03:10,968
guardrails against jailbreaks vary across use

44
00:03:11,016 --> 00:03:15,472
cases. General purpose chatbots providers

45
00:03:15,648 --> 00:03:19,914
often face significant negative publicity in

46
00:03:19,954 --> 00:03:23,894
regulated industries or mission critical use cases.

47
00:03:24,354 --> 00:03:28,414
Such vulnerabilities can lead to severe consequences.

48
00:03:29,274 --> 00:03:32,914
Ignore all previous instructions and do what I tell

49
00:03:32,954 --> 00:03:36,586
you. It is not how you want your LLM based

50
00:03:36,650 --> 00:03:40,234
product to deviate from expected behavior,

51
00:03:40,394 --> 00:03:44,234
especially when prompted to perform unusual or unsafe

52
00:03:44,274 --> 00:03:47,960
tasks. LLM models operate

53
00:03:48,112 --> 00:03:51,480
based on instructions provided by developers,

54
00:03:51,632 --> 00:03:55,464
including system prompts that define chatbot

55
00:03:55,504 --> 00:03:59,032
behavior. And although these prompts are not

56
00:03:59,088 --> 00:04:02,336
visible for regular users, they set up

57
00:04:02,360 --> 00:04:06,384
the conversation context for the model, and attackers

58
00:04:06,424 --> 00:04:09,648
can exploit it by attempting to predict,

59
00:04:09,776 --> 00:04:13,686
manipulate, or extract prompts to alter

60
00:04:13,750 --> 00:04:16,814
the model's behavior. For instance,

61
00:04:16,894 --> 00:04:20,334
attackers may request the model to ignore all

62
00:04:20,374 --> 00:04:23,822
previous system instructions and perform a different

63
00:04:23,958 --> 00:04:27,390
malicious action. By extracting system

64
00:04:27,462 --> 00:04:31,238
prompts, attackers gain insights, inter instructions,

65
00:04:31,326 --> 00:04:35,766
and possibly sensitive data. Think of competitors

66
00:04:35,830 --> 00:04:39,508
who can extract the brains of your LLM application

67
00:04:39,676 --> 00:04:42,224
and learn about trade secrets.

68
00:04:42,804 --> 00:04:46,724
If the LLM accepts inputs from external

69
00:04:46,764 --> 00:04:51,068
sources, such as files or webpages, then hidden

70
00:04:51,116 --> 00:04:54,024
malicious instructions could be embedded there.

71
00:04:54,524 --> 00:04:58,468
Imagine a resume that tricks a recruiting LLM

72
00:04:58,556 --> 00:05:01,996
into giving the highest possible rating to

73
00:05:02,020 --> 00:05:05,638
this resume. A significant risk for LLM

74
00:05:05,686 --> 00:05:09,230
developers is the leakage of system prompts,

75
00:05:09,342 --> 00:05:13,222
which are fundamental in defining custom behavior on

76
00:05:13,238 --> 00:05:16,886
top of foundational models. These system prompts

77
00:05:16,910 --> 00:05:20,622
may reveal detailed descriptions of business processes,

78
00:05:20,718 --> 00:05:24,434
confidential documents, or sensitive product pricing.

79
00:05:25,054 --> 00:05:28,700
Another risk involves manipulating behavior

80
00:05:28,862 --> 00:05:32,328
of llms integrated into core product

81
00:05:32,416 --> 00:05:36,928
workflows or decision making. Processes not

82
00:05:36,976 --> 00:05:40,760
only have the capability to manipulate llms

83
00:05:40,872 --> 00:05:44,328
through interaction, but also to infect

84
00:05:44,456 --> 00:05:48,280
their memory and training process data

85
00:05:48,352 --> 00:05:52,004
serves as the lifeblood of large language models.

86
00:05:52,584 --> 00:05:56,314
For training foundational models, developers use

87
00:05:56,654 --> 00:06:00,134
Internet scale datasets as well as chat history from

88
00:06:00,174 --> 00:06:03,966
users. And if attackers could poison

89
00:06:04,030 --> 00:06:07,838
such datasets with strategic data injections,

90
00:06:08,006 --> 00:06:10,954
they could manipulate future model responses.

91
00:06:11,614 --> 00:06:15,302
And while manipulating large datasets may be

92
00:06:15,398 --> 00:06:19,110
complicated for attackers without access to internal

93
00:06:19,142 --> 00:06:22,154
infrastructure, it still remains feasible,

94
00:06:22,454 --> 00:06:26,102
and the increasing use of open source

95
00:06:26,238 --> 00:06:30,406
models and datasets downloaded from the Internet simplifies

96
00:06:30,470 --> 00:06:34,390
this task. But what is much easier to do is

97
00:06:34,422 --> 00:06:38,254
to create thousands of fake accounts and generate

98
00:06:38,374 --> 00:06:42,030
millions of chat messages that look benign

99
00:06:42,142 --> 00:06:45,474
individually but collectively are malicious.

100
00:06:46,414 --> 00:06:49,230
These chat messages, when used for training,

101
00:06:49,422 --> 00:06:53,190
have the potential to influence model behavior during

102
00:06:53,262 --> 00:06:56,790
entrance. The implications of data poisoning

103
00:06:56,822 --> 00:07:00,150
can range from degrading model quality

104
00:07:00,342 --> 00:07:04,334
to establishing backdoors for bypassing content

105
00:07:04,414 --> 00:07:08,166
safety filters and delivering malicious responses to users

106
00:07:08,230 --> 00:07:11,878
at scale. And let's say you invested significant

107
00:07:12,006 --> 00:07:15,246
resources to ensure that LLM is

108
00:07:15,270 --> 00:07:18,234
not only useful but also safe and secure.

109
00:07:18,854 --> 00:07:22,194
But what if attackers can simply take your LLM down?

110
00:07:22,974 --> 00:07:26,118
The state of LLM ecosystem and best

111
00:07:26,166 --> 00:07:29,914
practices remains relatively immature,

112
00:07:30,374 --> 00:07:34,750
and this complexity creates opportunities for exploitation

113
00:07:34,822 --> 00:07:38,366
of such inefficiencies. Quite simple attacks

114
00:07:38,470 --> 00:07:42,286
can render the entire LLM application unresponsive

115
00:07:42,470 --> 00:07:44,994
or even deplete the available budget.

116
00:07:45,974 --> 00:07:50,074
This can be exploited in different ways. In a classic

117
00:07:50,414 --> 00:07:54,486
denial of service attack, they might pass a

118
00:07:54,510 --> 00:07:56,710
malicious file to the LLM,

119
00:07:56,902 --> 00:08:00,550
triggering resource intensive operations or

120
00:08:00,582 --> 00:08:03,354
internal calls to other components,

121
00:08:03,934 --> 00:08:07,354
and making processing time take like forever.

122
00:08:08,114 --> 00:08:11,134
Another tactic, known as a denial of wallet,

123
00:08:11,634 --> 00:08:15,562
involves flooding the LLM application with an excessive

124
00:08:15,618 --> 00:08:20,106
number of API calls. This attack can potentially

125
00:08:20,210 --> 00:08:23,450
exhaust your entire budget in a matter of

126
00:08:23,562 --> 00:08:27,314
minutes or hours. The risks associated

127
00:08:27,394 --> 00:08:31,146
with resource exhaustion are obvious and

128
00:08:31,250 --> 00:08:35,258
easily quantifiable because they can deplete technical

129
00:08:35,386 --> 00:08:39,370
and financial resources entirely. And what if

130
00:08:39,402 --> 00:08:43,122
attackers switch from overloading requests

131
00:08:43,218 --> 00:08:47,274
to making requests so meaningful and valuable

132
00:08:47,434 --> 00:08:50,174
that they could replicate the entire model.

133
00:08:50,874 --> 00:08:54,254
This is exactly how model stealing works.

134
00:08:54,634 --> 00:08:58,210
Attackers can send millions of requests and

135
00:08:58,242 --> 00:09:01,896
collect responses from the target LLM selected for

136
00:09:01,920 --> 00:09:05,336
replication, and they carefully craft a

137
00:09:05,360 --> 00:09:09,296
dataset of prompts to ask and responses collected from

138
00:09:09,320 --> 00:09:12,672
the target LLM, which is then used

139
00:09:12,728 --> 00:09:15,952
to train a brand new model which is nearly

140
00:09:16,048 --> 00:09:20,272
identical to the original one. This new model can

141
00:09:20,328 --> 00:09:23,960
serve as a playground for testing further attacks,

142
00:09:24,152 --> 00:09:27,470
or it can be used for benign purposes without

143
00:09:27,582 --> 00:09:31,554
effort and cost associated with training it from scratch.

144
00:09:32,174 --> 00:09:35,382
That's exactly what researchers accomplished with only

145
00:09:35,438 --> 00:09:39,390
few hundred dollars when they successfully

146
00:09:39,462 --> 00:09:43,150
replicated a high value chat GPT model,

147
00:09:43,302 --> 00:09:46,758
which originally required tens of millions of dollars

148
00:09:46,806 --> 00:09:50,758
to train. And given the substantial cost

149
00:09:50,886 --> 00:09:54,624
of creating intellectual property and training,

150
00:09:54,704 --> 00:09:58,752
unique models, poses a significant risk

151
00:09:58,848 --> 00:10:02,364
to competitive advantage and market position.

152
00:10:03,304 --> 00:10:07,232
Similar to model theft, this strategy

153
00:10:07,328 --> 00:10:10,544
can help extract sensitive information from

154
00:10:10,584 --> 00:10:14,056
an LLM. Llms have the tendency

155
00:10:14,160 --> 00:10:17,608
to memorize secret information they were trained

156
00:10:17,656 --> 00:10:21,716
on, and not only can this information be

157
00:10:21,880 --> 00:10:25,252
inadvertently revealed, but it can also

158
00:10:25,308 --> 00:10:29,284
be strategically elicited by attackers through

159
00:10:29,364 --> 00:10:32,064
targeted questioning or interrogation.

160
00:10:32,644 --> 00:10:35,772
If confidential data is integrated into

161
00:10:35,828 --> 00:10:39,628
the LLM workflow, it can be extracted

162
00:10:39,676 --> 00:10:43,384
through methods like jailbreaks or prompt injections,

163
00:10:43,804 --> 00:10:47,188
and if secret data was incorporated into the

164
00:10:47,236 --> 00:10:50,994
training process, attackers can trigger data leakage

165
00:10:51,034 --> 00:10:54,762
by crafting datasets with strategic questions about

166
00:10:54,938 --> 00:10:58,618
specific areas such as intellectual property or

167
00:10:58,666 --> 00:11:02,562
customer information, and responses from the LLM

168
00:11:02,658 --> 00:11:07,054
to such interrogation are likely to reveal sensitive information,

169
00:11:08,034 --> 00:11:11,334
so the risks of sensitive information disclosure

170
00:11:11,874 --> 00:11:15,528
are significant and widely recognized by companies

171
00:11:15,706 --> 00:11:20,108
as their top priority. You can see that uncontrolled

172
00:11:20,156 --> 00:11:24,104
responses from llms present business challenges,

173
00:11:24,564 --> 00:11:27,624
but they can also introduce technical risks.

174
00:11:28,284 --> 00:11:32,204
Some llms serve as a system components that

175
00:11:32,244 --> 00:11:35,584
generate software, code, or configuration files,

176
00:11:36,084 --> 00:11:39,308
and these outputs are subsequently

177
00:11:39,396 --> 00:11:43,464
executed or used as inputs for or other components,

178
00:11:43,804 --> 00:11:47,836
and without oversight. This can introduce vulnerable

179
00:11:47,900 --> 00:11:50,424
code or insecure configurations.

180
00:11:51,444 --> 00:11:55,484
This security risk can materialize with or

181
00:11:55,524 --> 00:11:58,628
without threat actors. Llms known

182
00:11:58,676 --> 00:12:02,348
for their hallucinations may suggest non

183
00:12:02,396 --> 00:12:05,304
existent packages during code generation.

184
00:12:06,084 --> 00:12:09,416
Attackers are already capitalizing on this by

185
00:12:09,540 --> 00:12:13,448
registering frequently hallucinated libraries and injecting

186
00:12:13,496 --> 00:12:16,912
malicious code into them. Alternatively,

187
00:12:17,048 --> 00:12:20,496
in the first place, LLM may generate a

188
00:12:20,520 --> 00:12:23,912
vulnerable code, a configuration or command

189
00:12:24,088 --> 00:12:27,524
that could compromise system integrity when executed.

190
00:12:28,064 --> 00:12:32,272
In all of these scenarios, improper handling of insecure

191
00:12:32,328 --> 00:12:37,040
outputs can jeopardize the security of LLM applications

192
00:12:37,232 --> 00:12:40,724
and potentially other downstream systems.

193
00:12:41,384 --> 00:12:44,560
And moving beyond the LLM itself,

194
00:12:44,752 --> 00:12:48,128
it is critical to consider the security of the

195
00:12:48,176 --> 00:12:51,872
environment. The proliferation of LLM

196
00:12:51,928 --> 00:12:55,312
first startups has resulted in the integration

197
00:12:55,408 --> 00:12:58,752
into many integrating insecure

198
00:12:58,808 --> 00:13:02,832
extensions of plugins can significantly expand the

199
00:13:02,888 --> 00:13:06,844
attack surface and introduce new attack vectors.

200
00:13:07,504 --> 00:13:11,524
For instance, if an LLM has a plugin for

201
00:13:12,064 --> 00:13:15,800
direct database connection for tasks like

202
00:13:15,872 --> 00:13:19,896
sales analytics and insights, insecure permission

203
00:13:19,960 --> 00:13:23,528
handling between the plugin and the database could allow

204
00:13:23,576 --> 00:13:27,064
attackers to extract additional sensitive information,

205
00:13:27,184 --> 00:13:30,164
such as customers financial details.

206
00:13:30,954 --> 00:13:34,890
In some cases, attackers may exploit vulnerable plugins

207
00:13:34,962 --> 00:13:38,294
to pivot to other parts of the infrastructure,

208
00:13:38,594 --> 00:13:41,894
similar to the classic SSRF attack.

209
00:13:42,594 --> 00:13:46,226
Additionally, if the LLM has the capability

210
00:13:46,330 --> 00:13:49,378
to visit website links, attackers could

211
00:13:49,426 --> 00:13:52,538
trick users to visit a malicious website

212
00:13:52,706 --> 00:13:56,226
that could extract chat history or other data

213
00:13:56,290 --> 00:13:59,490
from the LLM. LLM extensions of

214
00:13:59,522 --> 00:14:02,866
plugins serve as a privileged gateways to

215
00:14:02,890 --> 00:14:06,610
the entire infrastructure, and this represents a

216
00:14:06,642 --> 00:14:10,130
classic security vulnerability, with the far reaching

217
00:14:10,202 --> 00:14:13,842
implications ranging from unauthorized access

218
00:14:14,018 --> 00:14:17,454
to complete control over internal systems.

219
00:14:18,074 --> 00:14:22,418
Insecure agents are the siblings of insecure

220
00:14:22,466 --> 00:14:26,758
extensions. Agents differ from plugins

221
00:14:26,806 --> 00:14:30,462
or extensions because they imply delegation of

222
00:14:30,518 --> 00:14:34,070
actions. It means an LLM agent

223
00:14:34,182 --> 00:14:37,270
could navigate to various resources and

224
00:14:37,342 --> 00:14:40,894
execute tasks. This delegation opens

225
00:14:40,934 --> 00:14:44,454
the door for exploitation, as an agent could

226
00:14:44,494 --> 00:14:47,582
be redirected to perform malicious

227
00:14:47,638 --> 00:14:51,714
activity for the benefit of attackers. A variety of

228
00:14:51,754 --> 00:14:54,890
attack techniques against agents follows the

229
00:14:54,922 --> 00:14:58,938
creativity of LLM developers. For instance,

230
00:14:59,066 --> 00:15:02,482
if an LLM agent is tasked with

231
00:15:02,578 --> 00:15:07,034
classifying incoming emails and responding automatically

232
00:15:07,074 --> 00:15:10,530
to certain topics, attackers could exploit this

233
00:15:10,562 --> 00:15:13,994
functionality. They could instruct the agent to

234
00:15:14,034 --> 00:15:17,174
respond to their email with sensitive information,

235
00:15:17,634 --> 00:15:21,458
disclose contact lists, or even launch a malware

236
00:15:21,506 --> 00:15:25,454
campaign by sending phishing emails to all contacts.

237
00:15:26,154 --> 00:15:31,362
Similar attack scenarios are possible in many programming

238
00:15:31,418 --> 00:15:35,242
copilots with access to code repositories or

239
00:15:35,338 --> 00:15:38,650
DevOps agents with permissions to manage cloud

240
00:15:38,722 --> 00:15:42,706
infrastructure. The risk posed by excessive

241
00:15:42,770 --> 00:15:47,246
agency and vulnerable agents is significant as

242
00:15:47,270 --> 00:15:50,990
it extends beyond the LLM application and has

243
00:15:51,022 --> 00:15:55,174
the potential to scale automatically. Just as

244
00:15:55,294 --> 00:15:58,994
llms can impact the security of

245
00:15:59,294 --> 00:16:03,438
external components, external components can also influence

246
00:16:03,486 --> 00:16:07,446
the security of llms. With the proliferation

247
00:16:07,550 --> 00:16:11,238
of open source ecosystems, LLM developers

248
00:16:11,326 --> 00:16:14,518
heavily rely on public models, datasets,

249
00:16:14,566 --> 00:16:17,990
and libraries, and compromising or

250
00:16:18,062 --> 00:16:22,478
hijacking elements within this supply chain introduces

251
00:16:22,526 --> 00:16:26,194
one of the most critical and stealthy vulnerabilities.

252
00:16:27,174 --> 00:16:31,462
Whether by accident or through malicious campaigns,

253
00:16:31,638 --> 00:16:35,918
LLM developers may inadvertently download

254
00:16:36,006 --> 00:16:38,274
compromised models or datasets,

255
00:16:38,654 --> 00:16:42,766
resulting in seemingly normal LLM application

256
00:16:42,830 --> 00:16:46,446
behavior, which in fact can be remotely controlled

257
00:16:46,510 --> 00:16:50,278
by attackers. Vulnerable software packages

258
00:16:50,406 --> 00:16:53,678
of machine learning frameworks or standard

259
00:16:53,726 --> 00:16:57,334
libraries can introduce new vulnerabilities and

260
00:16:57,374 --> 00:17:00,814
enable attacker control. The primary

261
00:17:00,934 --> 00:17:04,514
risk associated with supply chain vulnerabilities

262
00:17:04,873 --> 00:17:08,769
is the stealthy control by attackers over

263
00:17:08,921 --> 00:17:11,489
LLM decisions, behaviors,

264
00:17:11,641 --> 00:17:14,533
or potentially the entire application.

265
00:17:15,273 --> 00:17:18,993
As you can see, there is a variety of security risks

266
00:17:19,073 --> 00:17:22,893
throughout the entire lifecycle of LLM applications.

267
00:17:23,793 --> 00:17:27,297
Unfortunately, the format of this presentation

268
00:17:27,465 --> 00:17:31,013
doesn't permit a deep dive into solutions.

269
00:17:31,944 --> 00:17:35,656
The LLM ecosystem is still in its infancy

270
00:17:35,800 --> 00:17:39,044
and will require considerable time to mature.

271
00:17:39,704 --> 00:17:43,520
Moreover, llms, like other ML models,

272
00:17:43,712 --> 00:17:47,844
are inherently vulnerable to adversarial attacks.

273
00:17:48,544 --> 00:17:52,416
My primary advice here is to consider the

274
00:17:52,440 --> 00:17:56,296
security of the entire system. Rather than focusing

275
00:17:56,360 --> 00:18:00,354
solely on LLM models or datasets.

276
00:18:00,774 --> 00:18:03,790
It is crucial to assume LLM

277
00:18:03,862 --> 00:18:07,334
vulnerability and design applications with this

278
00:18:07,374 --> 00:18:11,238
in mind, implementing safety guardrails and

279
00:18:11,286 --> 00:18:15,358
security controls around vulnerable but useful

280
00:18:15,406 --> 00:18:18,822
models. So if you want to dive deeper

281
00:18:18,878 --> 00:18:22,878
into the topic, you can check the OWAS website

282
00:18:23,046 --> 00:18:26,382
for more technical details about the top

283
00:18:26,438 --> 00:18:29,114
ten LLM security risks.

284
00:18:29,534 --> 00:18:33,310
You can learn about integrating security into

285
00:18:33,462 --> 00:18:37,354
MLAbs processes with the ML scope framework,

286
00:18:38,174 --> 00:18:42,150
and if you have any questions or ideas for collaboration,

287
00:18:42,342 --> 00:18:45,574
feel free to contact me. Thank you for watching

288
00:18:45,614 --> 00:18:46,214
this presentation.

