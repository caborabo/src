1
00:00:20,720 --> 00:00:24,142
Hi, are you ready to answer the question that everyone's a

2
00:00:24,158 --> 00:00:27,402
bit too afraid to ask? Let's find out how to

3
00:00:27,418 --> 00:00:29,774
use chat GPT without getting caught.

4
00:00:30,314 --> 00:00:34,066
And to do that, we'll start from scratch. So you don't need to know

5
00:00:34,090 --> 00:00:38,170
like any programming or nothing at all. We're just gonna try

6
00:00:38,202 --> 00:00:42,074
to understand how the systems to attack AI generated text

7
00:00:42,154 --> 00:00:45,834
work, which is actually the key to

8
00:00:45,874 --> 00:00:49,330
know how to get around them. So, are you ready

9
00:00:49,362 --> 00:00:52,598
to uncover the mysteries? Thanks a lot for

10
00:00:52,646 --> 00:00:53,434
joining.

11
00:00:56,254 --> 00:00:59,414
We're gonna have three parts in the talk,

12
00:00:59,494 --> 00:01:03,230
but I'd say that if

13
00:01:03,262 --> 00:01:06,674
you really want to get into like,

14
00:01:07,174 --> 00:01:10,794
the interesting part, so that's the real answers,

15
00:01:11,294 --> 00:01:15,374
you could just skip to this part here. So the circumvention

16
00:01:15,494 --> 00:01:19,530
that's gonna come around like minute 2020 something.

17
00:01:19,682 --> 00:01:23,234
So if you want to just get answers

18
00:01:23,274 --> 00:01:26,810
and that's it, you can just skip to that

19
00:01:26,842 --> 00:01:29,854
part. But if you want to understand everything,

20
00:01:30,434 --> 00:01:33,538
I'd say stick with me for a little bit,

21
00:01:33,666 --> 00:01:38,130
and we are going to be understanding how

22
00:01:38,242 --> 00:01:42,414
the detection works, which explains the rest of the presentation,

23
00:01:44,054 --> 00:01:47,374
up to you. But I really suggest that you stick for

24
00:01:47,414 --> 00:01:49,874
the first 20 minutes, kinda.

25
00:01:51,134 --> 00:01:54,714
And the first of the things that we need to know is,

26
00:01:55,094 --> 00:01:58,430
what are llMs? So, like,

27
00:01:58,462 --> 00:02:01,870
there's lots of ways to, like, explain this thing, right?

28
00:02:02,022 --> 00:02:04,754
But in the context of this presentation,

29
00:02:05,174 --> 00:02:08,462
we just care about the fact that LLMs are

30
00:02:08,558 --> 00:02:12,310
text predictors. So it's like, you know, like when you're writing

31
00:02:12,342 --> 00:02:16,374
like a text for someone or something like that, like, your keyboard

32
00:02:16,414 --> 00:02:20,438
is like predicting the next word all of the time. So it's just

33
00:02:20,486 --> 00:02:24,790
literally that. So another lamb just completes the text

34
00:02:24,942 --> 00:02:28,254
all the time. Something like this, you see, like,

35
00:02:28,294 --> 00:02:31,622
it's writing the next word or

36
00:02:31,758 --> 00:02:34,554
the next part of the word all the time.

37
00:02:35,394 --> 00:02:40,354
That's an LLM. That's what we need to know here. And the

38
00:02:40,394 --> 00:02:44,214
only difference between like, chatgpt or something

39
00:02:44,714 --> 00:02:48,706
like bar to, etcetera, and your

40
00:02:48,770 --> 00:02:51,194
phone is that chat GPT is, let's say,

41
00:02:51,274 --> 00:02:54,458
intelligent. Like it's a big system, lots of parameters,

42
00:02:54,506 --> 00:02:57,842
etcetera. And your phone is something small,

43
00:02:57,938 --> 00:03:00,738
something less intelligent.

44
00:03:00,906 --> 00:03:04,412
But the way that they work is the same. They just write the

45
00:03:04,428 --> 00:03:08,148
next word all the time. So that's what

46
00:03:08,196 --> 00:03:12,244
we need to know in this presentation, and that

47
00:03:12,284 --> 00:03:16,184
will help us to understand the common techniques that exist

48
00:03:16,484 --> 00:03:20,468
to detect generated text. And we're going to be speaking

49
00:03:20,516 --> 00:03:24,292
about four techniques, but actually just three.

50
00:03:24,348 --> 00:03:28,550
You'll see why. And the first of them is something super simple.

51
00:03:28,582 --> 00:03:32,390
So, like, I'm just gonna really go over this

52
00:03:32,422 --> 00:03:36,142
part very quickly, the first thing is a classifier.

53
00:03:36,198 --> 00:03:39,694
So classifier is just the thing that tells us, yes, it's human,

54
00:03:39,854 --> 00:03:43,126
or yes, it's generated. No, it's not generated. So it's

55
00:03:43,150 --> 00:03:46,910
human. How do we do that? We just give

56
00:03:46,982 --> 00:03:50,590
lots of examples to the system, and we hope that the system will

57
00:03:50,622 --> 00:03:54,224
just know somehow how

58
00:03:54,264 --> 00:03:57,804
to, well, figure out if a text is generated or not.

59
00:03:58,464 --> 00:04:02,264
There's an interesting system that was

60
00:04:02,304 --> 00:04:06,404
created by OpenAI to detect generated text.

61
00:04:07,024 --> 00:04:10,856
And if you go to their website, you see here, like, this is the OpenAI

62
00:04:10,920 --> 00:04:16,048
website, and in January 2023,

63
00:04:16,096 --> 00:04:20,412
so that's last year they created,

64
00:04:20,468 --> 00:04:22,744
or, well, they launched this website.

65
00:04:23,724 --> 00:04:26,704
That just gives us an error right now.

66
00:04:27,044 --> 00:04:30,660
So why am I showing you the website then?

67
00:04:30,852 --> 00:04:34,904
It's because, like, they discontinued the system because they said

68
00:04:35,244 --> 00:04:38,748
our classifier is not reliable. People are,

69
00:04:38,796 --> 00:04:42,644
like, trusting our system, but actually, like, it doesn't

70
00:04:42,724 --> 00:04:47,036
really work. So that's one of the things about classifiers.

71
00:04:47,140 --> 00:04:52,188
They are not. We should be aware

72
00:04:52,236 --> 00:04:56,308
that they tend to, well, make mistakes.

73
00:04:56,396 --> 00:04:59,784
And specifically the OpenAI one,

74
00:05:00,564 --> 00:05:04,652
it's not very good. That's why they just retired the

75
00:05:04,708 --> 00:05:08,916
thing. But it also raises some questions.

76
00:05:09,020 --> 00:05:12,366
Like, you're OpenAI. You created a chatbot

77
00:05:12,460 --> 00:05:15,754
that writes answers and people are using it,

78
00:05:15,874 --> 00:05:19,290
but at the same time, you're not able to create a

79
00:05:19,322 --> 00:05:22,466
system that really works for detecting the

80
00:05:22,490 --> 00:05:26,138
things that you are creating. That's more about ethics and

81
00:05:26,266 --> 00:05:29,254
those things that I just don't have time to get into, but,

82
00:05:29,794 --> 00:05:33,614
well, it raises questions. Right? I don't know,

83
00:05:34,474 --> 00:05:37,882
that's just things to speak about in a different

84
00:05:37,938 --> 00:05:42,204
talk. But I'd love to mention another

85
00:05:42,284 --> 00:05:46,388
thing, which is Ghostbuster. Ghostbuster is just another system

86
00:05:46,436 --> 00:05:50,012
to detect generated text. You see, like, I get some score,

87
00:05:50,068 --> 00:05:54,268
etcetera. It's better than the OpenAI

88
00:05:54,316 --> 00:05:57,484
classifier, which could be accessed anyway. Like, it's on

89
00:05:57,524 --> 00:06:01,964
hugging face. So, like, if you search on Google hangface, OpenAI classifier

90
00:06:02,004 --> 00:06:05,972
or something like that, you could still use the system. But this one

91
00:06:05,988 --> 00:06:09,064
that I'm showing you right now, Ghostbuster,

92
00:06:10,204 --> 00:06:14,004
better. It's just better because they just take

93
00:06:14,164 --> 00:06:17,984
lots of different metrics to train normal classifier.

94
00:06:20,404 --> 00:06:24,220
Well, it's just come out pretty recently, some months

95
00:06:24,252 --> 00:06:27,452
ago. It's state of the art. So I'd say if you

96
00:06:27,468 --> 00:06:32,012
want to use a classifier, this is the one you should

97
00:06:32,068 --> 00:06:35,804
probably consider. And it's a good

98
00:06:35,844 --> 00:06:39,876
system, works fairly well. And yeah,

99
00:06:39,900 --> 00:06:43,716
it's state of the art. So that's four classifiers. The first of

100
00:06:43,740 --> 00:06:46,356
the systems that I wanted to, like,

101
00:06:46,420 --> 00:06:49,116
just have a note on.

102
00:06:49,300 --> 00:06:52,956
And just for the sake of time, let me

103
00:06:53,020 --> 00:06:56,588
move on to the second type of systems,

104
00:06:56,636 --> 00:07:00,140
which is the second type of analysis

105
00:07:00,332 --> 00:07:04,344
that is a thing that we call black box analysis.

106
00:07:05,124 --> 00:07:08,420
However, as I said before, we're going to be

107
00:07:08,452 --> 00:07:12,348
speaking about not four but three systems.

108
00:07:12,396 --> 00:07:16,004
And that's the reason why we're not going to be really

109
00:07:16,044 --> 00:07:19,020
speaking about black box analysis.

110
00:07:19,172 --> 00:07:22,404
And that's just because this type of analysis

111
00:07:22,444 --> 00:07:24,184
is not very effective today.

112
00:07:25,354 --> 00:07:28,450
It's kind of outdated, you could imagine, like,

113
00:07:28,522 --> 00:07:32,498
it's not the best tool that we could use

114
00:07:32,666 --> 00:07:36,586
right now for detecting generated text. So let me just

115
00:07:36,770 --> 00:07:40,170
skip over this part to speak about

116
00:07:40,282 --> 00:07:43,378
white box analysis. And, well,

117
00:07:43,426 --> 00:07:46,730
to speak about white box analysis, I think that the best

118
00:07:46,762 --> 00:07:50,066
thing that I could do is just show you how it

119
00:07:50,090 --> 00:07:53,540
works. So let me show you

120
00:07:53,612 --> 00:07:56,464
this tool that's called GLTR, okay?

121
00:07:57,044 --> 00:08:00,584
That's a website. You can search for it,

122
00:08:00,884 --> 00:08:04,716
and it's this one here. So here

123
00:08:04,740 --> 00:08:08,044
we are, GLTR. I'm gonna

124
00:08:08,164 --> 00:08:12,224
pick like, yeah, like a sample text.

125
00:08:13,084 --> 00:08:18,708
For example, this text, it was written by GPT-2 and

126
00:08:18,836 --> 00:08:22,516
what we see here is that we have an

127
00:08:22,540 --> 00:08:26,224
analysis of the distribution of the probabilities

128
00:08:26,724 --> 00:08:30,140
of each of the words or parts of words

129
00:08:30,252 --> 00:08:33,784
in the text. So for example,

130
00:08:34,404 --> 00:08:39,148
if you look into this

131
00:08:39,196 --> 00:08:42,724
word. So I've been a gamer for over ten

132
00:08:42,804 --> 00:08:46,054
years during. And then we ask

133
00:08:46,094 --> 00:08:50,614
the system, what do you think comes after this

134
00:08:50,654 --> 00:08:54,838
word here? The system tells us, I think

135
00:08:54,886 --> 00:08:59,166
that probably the next word is that if

136
00:08:59,190 --> 00:09:02,814
it's not that, then it should be those this my,

137
00:09:02,974 --> 00:09:06,046
the, et cetera, et cetera. So it's giving

138
00:09:06,110 --> 00:09:10,100
us a distribution of probabilities of

139
00:09:10,132 --> 00:09:14,524
the words that the system thinks that should come after the

140
00:09:14,564 --> 00:09:17,884
first bits here. And that is

141
00:09:17,924 --> 00:09:21,532
the, well, it's a great way to see

142
00:09:21,548 --> 00:09:25,004
if the system would write this text. So if we see

143
00:09:25,044 --> 00:09:29,020
lots of green things as here, it means that the system would

144
00:09:29,092 --> 00:09:32,396
have written this text because like all

145
00:09:32,420 --> 00:09:35,624
of the words are things that the system,

146
00:09:35,984 --> 00:09:40,016
things that should go here. And that

147
00:09:40,120 --> 00:09:43,944
is something very different from a human text. Because if we

148
00:09:44,104 --> 00:09:47,392
take an example from a real text,

149
00:09:47,568 --> 00:09:50,968
we see that the system is very

150
00:09:51,056 --> 00:09:55,440
surprised to see things like learned facts

151
00:09:55,512 --> 00:09:59,600
or represents or structure. Gan here,

152
00:09:59,792 --> 00:10:03,794
those are words that GPT-2 would have never written.

153
00:10:04,494 --> 00:10:07,314
If we see a text with like lots of colors,

154
00:10:08,014 --> 00:10:10,234
probably the text is human.

155
00:10:10,974 --> 00:10:14,606
Now we're going to take a look at like an evolution of

156
00:10:14,630 --> 00:10:18,014
this thing that is called the tagpt. And that

157
00:10:18,054 --> 00:10:21,758
thing is like probably the number one result that comes up. Like, if you just

158
00:10:21,806 --> 00:10:25,470
search for the tagged AI generated text, something like

159
00:10:25,502 --> 00:10:28,894
that, on Google. What's the thing that,

160
00:10:29,234 --> 00:10:32,134
or like, the twist that detect GPT does?

161
00:10:33,514 --> 00:10:38,042
I love to use an example to explain that thing. So when

162
00:10:38,058 --> 00:10:41,082
I talk, I tend to use like,

163
00:10:41,218 --> 00:10:44,418
a lot, like, like that word. I really use it,

164
00:10:44,466 --> 00:10:47,778
like a lot. So if you see a text

165
00:10:47,826 --> 00:10:51,874
that contains, like, a lot of likes, probably it's

166
00:10:51,914 --> 00:10:55,758
my text, right? Like you would say that the probability

167
00:10:55,886 --> 00:10:59,434
that that text is mine is very high.

168
00:10:59,974 --> 00:11:03,742
But if we take a text that I

169
00:11:03,918 --> 00:11:07,254
spoke or wrote containing

170
00:11:07,374 --> 00:11:11,134
lots of likes, and we change the

171
00:11:11,254 --> 00:11:15,126
likes in the text by another word. So like, for example, we take the likes

172
00:11:15,150 --> 00:11:18,494
and we just write okay,

173
00:11:18,574 --> 00:11:21,776
instead, or like another word, right? Like we just

174
00:11:21,870 --> 00:11:24,988
change the word. Then we can

175
00:11:25,116 --> 00:11:28,580
do this thing that's called, well, just a perturbation. And we

176
00:11:28,612 --> 00:11:31,996
take the score of the perturbation and the

177
00:11:32,020 --> 00:11:35,748
original text. So the original text, the text with

178
00:11:35,796 --> 00:11:40,068
lots of likes, will have a super high score

179
00:11:40,236 --> 00:11:42,384
for being my text.

180
00:11:43,164 --> 00:11:46,372
However, the rewritten text, so the text that

181
00:11:46,428 --> 00:11:49,940
doesn't have any likes probably will have

182
00:11:49,972 --> 00:11:53,668
a super low score. So the detector will

183
00:11:53,716 --> 00:11:57,148
not think that the system is sorry that the text

184
00:11:57,236 --> 00:12:00,692
is my text. So what we do is

185
00:12:00,748 --> 00:12:04,468
we take the scores of the original text and the written text,

186
00:12:04,596 --> 00:12:07,908
and then we compare. The question here is,

187
00:12:08,036 --> 00:12:11,996
was the original text much more likely to be my

188
00:12:12,060 --> 00:12:15,846
text compared to the written version?

189
00:12:16,030 --> 00:12:19,694
So if the answer is yes, then we conclude that yes,

190
00:12:19,774 --> 00:12:22,394
probably the original text was my text.

191
00:12:23,534 --> 00:12:27,594
And what we do with GPT, or any of

192
00:12:27,934 --> 00:12:31,710
like, any LLM, really, is that we just

193
00:12:31,862 --> 00:12:36,254
take like a text, we do

194
00:12:36,294 --> 00:12:39,706
some modifications to the text, so we perturb the text,

195
00:12:39,830 --> 00:12:43,594
and then we take the score of the original text

196
00:12:43,634 --> 00:12:47,674
and the perturbations, and we compare again if the

197
00:12:47,714 --> 00:12:51,538
text originally was written by an LLM, and then

198
00:12:51,586 --> 00:12:54,970
we rewrite the text, and it doesn't look like it's being written by

199
00:12:55,002 --> 00:12:58,738
an LLM. That means that the original text was

200
00:12:58,786 --> 00:13:02,426
written by an LLM. So that's

201
00:13:02,450 --> 00:13:06,374
how the text GPT works. Very interesting idea, very smart.

202
00:13:06,714 --> 00:13:09,970
And it's one of the state of the art systems as

203
00:13:10,002 --> 00:13:14,106
well, a really, really good system. So that's

204
00:13:14,130 --> 00:13:17,762
attached GPT, but it has a problem. And it's a

205
00:13:17,778 --> 00:13:20,734
problem. I love this name, the capybara problem.

206
00:13:21,074 --> 00:13:24,850
I didn't come up with it, but I think it's a great name

207
00:13:24,962 --> 00:13:28,642
to describe something that is really important, which is that

208
00:13:28,778 --> 00:13:31,994
if I go to chat GPT and I ask chatgpt

209
00:13:32,154 --> 00:13:34,074
about something like,

210
00:13:35,454 --> 00:13:39,342
rather strange, something that people don't usually

211
00:13:39,438 --> 00:13:43,714
speak about. So in this case like capybara that is like an astrophysicist.

212
00:13:44,094 --> 00:13:47,462
If I ask Judge GPT about the thing, of course

213
00:13:47,518 --> 00:13:51,798
I will get an answer about capybara that is an astrophysicist.

214
00:13:51,966 --> 00:13:55,046
It makes sense, right? Because I just asked about this thing.

215
00:13:55,150 --> 00:13:59,234
So we have conditioned probability here

216
00:13:59,884 --> 00:14:03,180
for this thing being the

217
00:14:03,212 --> 00:14:06,628
answer of my query. It makes sense.

218
00:14:06,716 --> 00:14:10,812
The likelihood of this text knowing that I just asked about

219
00:14:10,868 --> 00:14:14,532
this is very high. It's probable

220
00:14:14,588 --> 00:14:18,020
that I would get this answer when I see

221
00:14:18,052 --> 00:14:21,876
a text in isolation. So, like, for example, if I see a high school report

222
00:14:21,940 --> 00:14:25,476
or something like that, I never get to see the prompt of the

223
00:14:25,500 --> 00:14:29,744
user, so I don't know what the person asked. Chat GPT.

224
00:14:30,044 --> 00:14:33,284
Which means that if I just see a text without any

225
00:14:33,324 --> 00:14:36,464
context, it can be very surprising.

226
00:14:37,484 --> 00:14:41,076
I knew where it was coming from. So I understand that

227
00:14:41,220 --> 00:14:44,196
this thing is likely. So I can say,

228
00:14:44,260 --> 00:14:48,340
oh, it was written by an LLM, because an LLM would likely write

229
00:14:48,372 --> 00:14:51,812
this thing when it's being asked to write about a capybara that

230
00:14:51,828 --> 00:14:55,212
is an astrophysicist. But if I just see this text and

231
00:14:55,228 --> 00:14:57,904
I don't know that it's been written by chatgpt,

232
00:14:58,574 --> 00:15:02,262
then I would say, oh, no, an LLM would never write about a capybara that

233
00:15:02,278 --> 00:15:04,874
is an astrophysicist, because, like, it's,

234
00:15:05,214 --> 00:15:08,350
well, a super weird idea. And, like, an LLM

235
00:15:08,382 --> 00:15:11,334
would never imagine that this thing could be,

236
00:15:11,414 --> 00:15:15,222
like, a real thing, you know? So it's

237
00:15:15,318 --> 00:15:19,350
very surprising to someone that doesn't know the context.

238
00:15:19,502 --> 00:15:23,102
That's the capybara problem, and it's a hard problem

239
00:15:23,158 --> 00:15:26,592
to solve. Well, a group of

240
00:15:26,768 --> 00:15:30,184
scientists tried to solve pretty recently, like,

241
00:15:30,224 --> 00:15:33,960
some months ago, and they came up with formula

242
00:15:33,992 --> 00:15:37,472
that I'm not gonna get into, but the idea

243
00:15:37,528 --> 00:15:41,232
is just to measure, well,

244
00:15:41,328 --> 00:15:44,616
an idea that I think is very interesting, which is that, as we

245
00:15:44,640 --> 00:15:47,920
just saw, like, 30 seconds ago, something can be

246
00:15:47,952 --> 00:15:51,900
surprising. So if something can be surprising, what we

247
00:15:51,932 --> 00:15:55,284
do is that we normalize the thing by

248
00:15:55,324 --> 00:15:58,904
the expected surprise of an LlM on that text.

249
00:15:59,644 --> 00:16:02,964
That can sound confusing. So I'm just going to give you an example so

250
00:16:03,004 --> 00:16:05,424
we understand, what's this thing?

251
00:16:06,204 --> 00:16:10,060
Let's take a text, right? Like, the capybara text. That's confusing

252
00:16:10,132 --> 00:16:13,252
or, like, that's surprising.

253
00:16:13,428 --> 00:16:17,504
We wouldn't expect to, like, see this text just coming

254
00:16:17,544 --> 00:16:21,528
out of, like, NLM. So the perplexity,

255
00:16:21,616 --> 00:16:23,444
the surprise, the.

256
00:16:24,304 --> 00:16:28,284
Just, like, the likelihood of that text,

257
00:16:29,224 --> 00:16:33,256
or in, well, the unlikelihoodness of that text is

258
00:16:33,280 --> 00:16:36,488
very high. So we wouldn't expect to see this

259
00:16:36,536 --> 00:16:39,616
text. But why would

260
00:16:39,720 --> 00:16:42,004
we not expect to see that text?

261
00:16:42,924 --> 00:16:45,988
There's actually two parts of something being

262
00:16:46,156 --> 00:16:49,900
unlikely. The first thing is that in

263
00:16:49,932 --> 00:16:53,564
this case, this text is unlikely because it's unusual. So, like,

264
00:16:53,604 --> 00:16:56,532
the topic is something very creative,

265
00:16:56,668 --> 00:17:00,244
so we wouldn't expect to see this thing. But then

266
00:17:00,324 --> 00:17:05,212
another source of perplexity surprise is

267
00:17:05,308 --> 00:17:09,769
that the text is written by a human person. So when we write as humans,

268
00:17:09,891 --> 00:17:14,165
we use, like, some words that wouldn't be the

269
00:17:14,309 --> 00:17:18,405
top one choice for GPT, for example. GPT will

270
00:17:18,429 --> 00:17:22,353
be more surprised to see a human text than a machine text.

271
00:17:23,573 --> 00:17:27,669
And what we try to do is that we try to isolate these two

272
00:17:27,781 --> 00:17:31,613
parts of surprise. So we try to

273
00:17:31,773 --> 00:17:35,405
remove the part of the perplexity that

274
00:17:35,429 --> 00:17:39,104
just comes from the fact that the text is unusual.

275
00:17:39,964 --> 00:17:43,684
We remove this part. And that's what I

276
00:17:43,724 --> 00:17:47,628
said before, right? Like, we normalize by the expected

277
00:17:47,716 --> 00:17:51,300
surprise of an LLM on that text. We remove the

278
00:17:51,332 --> 00:17:54,740
part of a text being surprising because it's unusual.

279
00:17:54,892 --> 00:17:58,236
And if we get to do that, what we're left with

280
00:17:58,300 --> 00:18:01,748
is that we have a measure of how

281
00:18:01,796 --> 00:18:04,886
likely is the text to be written by an AI.

282
00:18:04,990 --> 00:18:09,234
So that's what we do. That's a formula.

283
00:18:09,854 --> 00:18:13,350
We get a measure, and the measure that we get

284
00:18:13,422 --> 00:18:16,478
is a much better measure of how

285
00:18:16,566 --> 00:18:20,554
is the this, how much is this text likely

286
00:18:20,894 --> 00:18:23,354
to have been written by an LLM.

287
00:18:25,374 --> 00:18:30,418
There's, well, a ratio that they came up with experimentally.

288
00:18:30,586 --> 00:18:34,402
0.85. If we get like a higher

289
00:18:34,458 --> 00:18:37,894
number, it's human. If we get a lower number, then it's AI.

290
00:18:38,234 --> 00:18:42,170
It's a great idea, really. It's a great idea. And this system can

291
00:18:42,202 --> 00:18:45,978
be tried online. I'm just not going to show it because it's basically

292
00:18:46,026 --> 00:18:49,850
the same thing as the other systems that we just saw. But it's

293
00:18:49,882 --> 00:18:53,154
a very smart idea and a very smart

294
00:18:53,194 --> 00:18:56,540
way of trying to get around the problem of

295
00:18:56,732 --> 00:19:00,820
the capybara problem. So that's binoculars.

296
00:19:00,972 --> 00:19:03,772
Really great system, came out very recently,

297
00:19:03,868 --> 00:19:07,516
so a really great system to

298
00:19:07,540 --> 00:19:11,380
try. And the fourth technique, and the

299
00:19:11,412 --> 00:19:14,588
last one I'm going to be talking about is watermarking.

300
00:19:14,676 --> 00:19:17,024
So what's watermarking?

301
00:19:18,124 --> 00:19:21,284
Watermarking is just like something that we embed into a text,

302
00:19:21,324 --> 00:19:24,714
right? Like we put a watermark on something and that thing

303
00:19:24,794 --> 00:19:28,734
sticks into the thing, but we can't actually see it.

304
00:19:29,074 --> 00:19:32,850
So when we do watermarking with the text, we do

305
00:19:33,002 --> 00:19:36,546
a thing that's called a system of two lists,

306
00:19:36,650 --> 00:19:40,134
a red list and a green list.

307
00:19:40,714 --> 00:19:44,482
And to imagine how it works, just imagine that

308
00:19:44,538 --> 00:19:48,122
I have a dictionary here and I start highlighting all of the

309
00:19:48,138 --> 00:19:51,530
words in the dictionary. So, like, the first word is red, second word

310
00:19:51,562 --> 00:19:55,956
is green. The third word is red. The fourth word is green.

311
00:19:56,100 --> 00:20:00,492
And after doing that with all of the words in the dictionary, I have

312
00:20:00,628 --> 00:20:04,228
notion of which word is red

313
00:20:04,276 --> 00:20:07,764
and which word is green. Right. I have two lists. One list is

314
00:20:07,844 --> 00:20:11,644
my red words and the other list is my green

315
00:20:11,684 --> 00:20:15,252
words. So what I do after that is that I

316
00:20:15,268 --> 00:20:19,112
just tell the system, you can never write any

317
00:20:19,128 --> 00:20:22,736
of the red words. If you write a red word, that's banned.

318
00:20:22,800 --> 00:20:26,776
So you can't choose those words in your distribution of probabilities

319
00:20:26,920 --> 00:20:30,204
that we saw before, you can choose those words.

320
00:20:31,024 --> 00:20:34,520
That works great until we get to words that go together.

321
00:20:34,592 --> 00:20:38,284
So, for example, Barack Obama, that thing has to go together, right?

322
00:20:38,784 --> 00:20:42,216
So what we do to solve this problem is that we just do a thing

323
00:20:42,240 --> 00:20:45,722
that's called weak water marking in contrast

324
00:20:45,818 --> 00:20:49,642
to strong water marking. And basically

325
00:20:49,698 --> 00:20:52,970
the idea here is that if you have two words that are,

326
00:20:53,002 --> 00:20:55,694
like, 99% likely to go together,

327
00:20:56,034 --> 00:21:00,174
then you don't apply this thing of the red and green list.

328
00:21:01,394 --> 00:21:04,714
If you have, like, a set of possibilities that

329
00:21:04,754 --> 00:21:07,894
look likely, then you do apply the watermark.

330
00:21:08,554 --> 00:21:12,066
That's a great technique, and it's especially great because it gets rid

331
00:21:12,090 --> 00:21:15,390
of the capybara problem. If we see a text and we

332
00:21:15,422 --> 00:21:18,434
see lots of red words, we know it's human.

333
00:21:19,094 --> 00:21:22,182
If it doesn't have any red words, it's very likely

334
00:21:22,238 --> 00:21:25,486
to have been written by an AI, by an

335
00:21:25,510 --> 00:21:28,034
AI that was using the watermark.

336
00:21:28,654 --> 00:21:32,834
So it's a great technique, really very interesting.

337
00:21:33,494 --> 00:21:37,234
And those are the four techniques that will allow us to understand

338
00:21:37,934 --> 00:21:41,206
how the detection of AI texture works.

339
00:21:41,270 --> 00:21:45,278
But. But I'd love to give you some

340
00:21:45,406 --> 00:21:49,366
little tips on how to attack the thing just

341
00:21:49,510 --> 00:21:53,194
using your common sense. I'd say

342
00:21:54,294 --> 00:21:57,902
one of the things that we should be aware of

343
00:21:57,958 --> 00:22:02,110
is that looking into the types of words doesn't

344
00:22:02,142 --> 00:22:05,982
really work well. Llms tend

345
00:22:05,998 --> 00:22:09,758
to write kind of similarly to humans, so that thing doesn't really

346
00:22:09,806 --> 00:22:13,014
work. But if you think about other things, like your

347
00:22:13,054 --> 00:22:15,942
writing style, if I'm, I don't know,

348
00:22:15,958 --> 00:22:20,462
let's say I'm 15, I'm supposed to write like a

349
00:22:20,478 --> 00:22:23,390
teenager, right? Like, I'm not supposed to write like a PhD.

350
00:22:23,542 --> 00:22:26,958
So your writing style can give some hints of that person

351
00:22:27,046 --> 00:22:30,594
using, like, a chatbot to write the text.

352
00:22:31,214 --> 00:22:34,734
Also dialect. If I'm american, I don't write

353
00:22:34,774 --> 00:22:38,354
as a british person, so I wouldn't use british

354
00:22:38,394 --> 00:22:41,650
words. Also, typos sometimes,

355
00:22:41,682 --> 00:22:45,214
like, can give you, like, a hint of, okay, that person like

356
00:22:45,914 --> 00:22:48,946
was writing text or of course, like they could

357
00:22:48,970 --> 00:22:52,458
just like do it on purpose. But like, these are just

358
00:22:52,506 --> 00:22:56,334
ideas, you know, hints, things that could help you to see,

359
00:22:56,794 --> 00:23:00,094
like, well, like to get ideas.

360
00:23:00,994 --> 00:23:04,582
Also hallucinations. If you see

361
00:23:04,718 --> 00:23:08,502
a hallucination, that thing is really the best hint that you can get.

362
00:23:08,558 --> 00:23:12,710
Like something that just doesn't look real, like something that you know is

363
00:23:12,742 --> 00:23:16,006
false. There's different types of

364
00:23:16,030 --> 00:23:20,034
them, but try to specially look out for things that

365
00:23:20,374 --> 00:23:24,062
just don't make sense at all or facts that you know

366
00:23:24,118 --> 00:23:27,350
are not true or bad math. So, like,

367
00:23:27,382 --> 00:23:30,084
when you're doing math and like, the result is just incorrect,

368
00:23:30,254 --> 00:23:33,824
those are very evident types of

369
00:23:33,904 --> 00:23:37,164
hallucinations. And it really can give you a great

370
00:23:37,544 --> 00:23:40,444
hint on this text has been generated.

371
00:23:41,784 --> 00:23:46,364
However, human annotators

372
00:23:47,624 --> 00:23:51,744
are horrible, are really bad at detecting generated text.

373
00:23:51,824 --> 00:23:55,124
So you should really keep that in the back of your head.

374
00:23:55,884 --> 00:23:58,584
We are very bad at detecting generated text.

375
00:23:59,244 --> 00:24:02,184
So we can try to do it. But yeah,

376
00:24:02,804 --> 00:24:06,344
will it work? Kind of complicated.

377
00:24:07,644 --> 00:24:11,068
This is the real, real important part,

378
00:24:11,236 --> 00:24:14,172
which is how to get around the systems.

379
00:24:14,268 --> 00:24:18,044
So what can we do to just evade the

380
00:24:18,084 --> 00:24:22,394
detectors of generated text? We've just seeing

381
00:24:22,434 --> 00:24:26,414
how they work. So now it will make sense that we

382
00:24:26,874 --> 00:24:29,970
just need to do a very specific thing to get

383
00:24:30,002 --> 00:24:34,138
around them. Right. And if I ask you what's

384
00:24:34,186 --> 00:24:37,650
the main idea that comes to your head when like,

385
00:24:37,722 --> 00:24:39,934
you try to get around them?

386
00:24:40,514 --> 00:24:44,562
Probably you'll tell me that what you have to do is

387
00:24:44,738 --> 00:24:48,114
paraphrasing or rewriting text or

388
00:24:48,154 --> 00:24:52,366
changing the words. So if you do

389
00:24:52,390 --> 00:24:56,286
this thing, if you just like take a text, change the

390
00:24:56,310 --> 00:25:00,926
words of the text, then you will arrive

391
00:25:00,990 --> 00:25:04,966
to a text. If you do it well, if you paraphrase well,

392
00:25:05,150 --> 00:25:08,614
you will arrive to a text where it can't be

393
00:25:08,774 --> 00:25:13,166
spotted anymore. This thing can be done manually.

394
00:25:13,310 --> 00:25:16,840
You can just rewrite the text yourself or you can use paraphrasers.

395
00:25:16,982 --> 00:25:20,380
We'll look into that in a second. But the idea here

396
00:25:20,412 --> 00:25:24,428
is that if you paraphrase, you can really

397
00:25:24,476 --> 00:25:28,060
evade the detectors. In fact, how much

398
00:25:28,092 --> 00:25:31,348
can you evade them? Well,

399
00:25:31,516 --> 00:25:35,724
here I have a table where we had a system that

400
00:25:35,764 --> 00:25:39,116
was detecting text with a 90% accuracy,

401
00:25:39,180 --> 00:25:41,664
which is a lot, right? It's really great.

402
00:25:41,964 --> 00:25:46,394
But my question is, how much do you think it drops when

403
00:25:46,434 --> 00:25:48,094
we paraphrase? So,

404
00:25:49,874 --> 00:25:53,002
like, yeah, like how much can we, like,

405
00:25:53,058 --> 00:25:56,922
reduce the accuracy of this detector that was working really

406
00:25:56,978 --> 00:26:01,014
well with like, well, a set of texts

407
00:26:03,554 --> 00:26:07,410
drops a lot. It drops so much that it basically

408
00:26:07,562 --> 00:26:11,210
becomes a coin. So a flip of a coin has a 50%

409
00:26:11,282 --> 00:26:13,534
chance of being correct. Right.

410
00:26:14,594 --> 00:26:18,854
If we like paraphrase the text,

411
00:26:19,674 --> 00:26:23,266
we make the detector behave as the flip

412
00:26:23,290 --> 00:26:27,114
of a coin. So it means that it's not effective at all. It means that

413
00:26:27,274 --> 00:26:30,986
paraphrasing or rewriting is really, really the

414
00:26:31,010 --> 00:26:34,654
best thing that we can do. And it's really effective as a technique.

415
00:26:35,474 --> 00:26:38,976
We need to do it very well to

416
00:26:39,000 --> 00:26:43,200
be able to actually evade the detectors because it's not just changing five words,

417
00:26:43,272 --> 00:26:46,456
it's really rewriting the phrases. But if we do it

418
00:26:46,480 --> 00:26:49,952
well, we can really evade those

419
00:26:50,008 --> 00:26:54,304
systems. And now I'm going to give you some tools that you could consider

420
00:26:54,384 --> 00:26:56,964
using. If you want to rewrite your texts,

421
00:26:58,344 --> 00:27:01,880
you can just search for them in Google. Really, there's new tools all the

422
00:27:01,912 --> 00:27:04,798
time. So you can just search paraphrase,

423
00:27:04,846 --> 00:27:08,566
text, AI, something like that on Google. And like you will probably

424
00:27:08,750 --> 00:27:12,350
like just find all of these tools that I'm just gonna give you.

425
00:27:12,382 --> 00:27:15,674
But if you want some like names of tools.

426
00:27:16,294 --> 00:27:19,354
Yeah, Grammarly is a good tool. Auto writer,

427
00:27:21,414 --> 00:27:25,182
go copy. There's really like

428
00:27:25,238 --> 00:27:28,782
lots of them and deeper. The one that we

429
00:27:28,798 --> 00:27:32,262
are seeing here, it's one that you have to use in hugging

430
00:27:32,278 --> 00:27:35,934
face. So the website, you just search for deeper paraphrase,

431
00:27:35,974 --> 00:27:39,406
hugging face, something like that. And you'll arrive to a hugging face where you can

432
00:27:39,430 --> 00:27:42,846
actually go and put

433
00:27:42,870 --> 00:27:45,234
your text, get the written thing.

434
00:27:47,214 --> 00:27:50,870
T five is another paraphraser that's hosted on huggingface.

435
00:27:50,902 --> 00:27:53,990
So you just go there, you write your text and you get the

436
00:27:54,022 --> 00:27:57,790
paraphrased version. And you can also

437
00:27:57,862 --> 00:28:01,310
do a smart thing, I'd say, which is you take a text,

438
00:28:01,342 --> 00:28:04,918
for example, like a text in English, you translate it

439
00:28:04,926 --> 00:28:08,590
to French, and then you take the french translation and you translate back

440
00:28:08,622 --> 00:28:11,894
to English. And you should get some differences in the

441
00:28:11,974 --> 00:28:16,114
way that it's been translated, especially if the translator is not

442
00:28:16,574 --> 00:28:21,118
perfect, then you should just like,

443
00:28:21,206 --> 00:28:24,486
yeah, have changes in phrases, etcetera. So that's a way

444
00:28:24,510 --> 00:28:27,404
to write. But if you're going to use DPel,

445
00:28:27,944 --> 00:28:31,604
there's a great tool and that's the one I'm going to show you right now,

446
00:28:32,184 --> 00:28:34,684
which is my favorite thing to write.

447
00:28:35,744 --> 00:28:39,616
Deeper. So you see like you have the translator

448
00:28:39,800 --> 00:28:43,296
or DeepL write, which is a new thing that came out

449
00:28:43,360 --> 00:28:46,404
pretty recently. And what's the great thing about this?

450
00:28:46,744 --> 00:28:51,400
The great thing is that it allows you to just select

451
00:28:51,472 --> 00:28:55,866
what you want us, your rewritten version

452
00:28:55,970 --> 00:28:59,202
interactively. So I can say, I don't like composition,

453
00:28:59,298 --> 00:29:02,922
I prefer resistance. Or instead of this,

454
00:29:02,978 --> 00:29:06,654
you should write this thing. You see, like you can

455
00:29:07,034 --> 00:29:10,666
interactively change the text. And that's the best

456
00:29:10,730 --> 00:29:14,170
way to really like do rewriting well.

457
00:29:14,282 --> 00:29:17,654
So you want to rewrite. That's my

458
00:29:18,014 --> 00:29:21,534
favorite tool. Really go for any of them.

459
00:29:21,574 --> 00:29:25,214
But this one really, I'd say it's top

460
00:29:25,254 --> 00:29:29,554
notch. So deeper write. Really great tool.

461
00:29:30,614 --> 00:29:34,342
Having said that, I'd also love to give you like some manual tricks

462
00:29:34,358 --> 00:29:37,750
that you can try to apply to. Just, I don't know,

463
00:29:37,822 --> 00:29:41,502
avoid being detected. And one of

464
00:29:41,518 --> 00:29:45,010
them is that you should give the system a

465
00:29:45,042 --> 00:29:49,114
lot of information about who you are. So you should tell the system. You should

466
00:29:49,154 --> 00:29:51,854
write as if you are a high school student. That is,

467
00:29:52,354 --> 00:29:56,354
I don't know, like writing about the

468
00:29:56,394 --> 00:30:00,602
french revolution and you're

469
00:30:00,698 --> 00:30:04,134
like a student from the US.

470
00:30:04,514 --> 00:30:08,330
Whatever, you know, like you give lots of information about

471
00:30:08,362 --> 00:30:12,672
like that person, you, and that way

472
00:30:12,848 --> 00:30:16,920
you will allow the system to write a bit more like you.

473
00:30:17,112 --> 00:30:20,684
Also, it's nice to use the active voice.

474
00:30:20,984 --> 00:30:25,352
You should try to ask for that because like a lot of the examples

475
00:30:25,408 --> 00:30:28,968
that are given to these systems to train the systems are in passive

476
00:30:29,016 --> 00:30:31,204
voice. So that means, for example,

477
00:30:31,864 --> 00:30:35,522
500 patients were used for this study.

478
00:30:35,648 --> 00:30:39,294
That's passive voice. That, like in a scientific study, right? Like they use

479
00:30:39,334 --> 00:30:43,758
lots of passive voice. If you ask for active voice,

480
00:30:43,926 --> 00:30:47,486
not only does your text sound like, well, more personal,

481
00:30:47,590 --> 00:30:50,670
but also it changes the distribution of,

482
00:30:50,822 --> 00:30:53,862
like the tokens, like the probabilities of

483
00:30:53,878 --> 00:30:57,414
the tokens, it makes the copyware problem a little

484
00:30:57,454 --> 00:31:00,782
bit worse. So if you ask for the

485
00:31:00,838 --> 00:31:04,222
active voice, that's a good idea to ask for.

486
00:31:04,358 --> 00:31:08,390
It's not going to change a lot. But like, I would try

487
00:31:08,422 --> 00:31:12,318
to ask for this also using very specific data.

488
00:31:12,406 --> 00:31:15,702
So if we go back to the French Revolution example,

489
00:31:15,878 --> 00:31:19,134
just give like lots of data about like what you are

490
00:31:19,294 --> 00:31:22,622
asking about. It started this year,

491
00:31:22,718 --> 00:31:25,990
it lasted for this many months,

492
00:31:26,062 --> 00:31:29,798
and I don't know, like this many people died,

493
00:31:29,886 --> 00:31:33,142
etc. Etcetera. As much specific data as

494
00:31:33,158 --> 00:31:36,566
you come that will help the system to write a

495
00:31:36,590 --> 00:31:40,234
text. That is much more surprising.

496
00:31:40,974 --> 00:31:44,702
Also avoid quotes. I'm not saying to avoid quoting

497
00:31:44,798 --> 00:31:48,158
other people. What I'm trying to say is that you should try to

498
00:31:48,326 --> 00:31:52,798
avoid quotes like George

499
00:31:52,846 --> 00:31:56,898
Washington said, whatever those

500
00:31:56,946 --> 00:32:00,974
phrases that, like everyone says, you should try to avoid them,

501
00:32:01,514 --> 00:32:04,930
because when the system sees one of those phrases,

502
00:32:05,082 --> 00:32:08,570
it will see that all of the words are green

503
00:32:08,682 --> 00:32:11,694
because it's being seen a thousand times already.

504
00:32:11,994 --> 00:32:15,522
So it will think that it's been written by AI when

505
00:32:15,578 --> 00:32:19,274
actually it's not. It's just like a very common phrase, but it will think

506
00:32:19,314 --> 00:32:22,594
that it was written by AI because it's a thing that doesn't surprise the system

507
00:32:22,634 --> 00:32:25,784
at all. So if you want

508
00:32:25,824 --> 00:32:29,808
to not get in trouble, I would avoid

509
00:32:29,856 --> 00:32:33,800
using quotes like that. Quotes and also legal texts

510
00:32:33,832 --> 00:32:37,800
and texts that appear in like, lots of places. Just try

511
00:32:37,832 --> 00:32:41,504
to avoid them. Another great thing to do is to just

512
00:32:41,584 --> 00:32:45,608
outline the structure that you want. First, I want an introduction about

513
00:32:45,736 --> 00:32:49,312
the effects of the French Revolution. Then I want a description of what happened in

514
00:32:49,328 --> 00:32:51,916
the first three months. Whatever, you know, like,

515
00:32:52,040 --> 00:32:55,024
just tell the system what you really want.

516
00:32:55,484 --> 00:32:59,820
Don't let the system choose for you. If you

517
00:32:59,972 --> 00:33:03,264
tell the system everything that you want and are very specific,

518
00:33:04,524 --> 00:33:07,704
for example, by writing the beginning of the answer,

519
00:33:08,284 --> 00:33:11,628
you will get an answer that is much more surprising,

520
00:33:11,796 --> 00:33:15,224
hence much more difficult to detect.

521
00:33:17,044 --> 00:33:21,360
Also, if you write the beginning of the answer, you are allowing the system to

522
00:33:21,552 --> 00:33:24,564
see how you write. So, for example, I said,

523
00:33:25,424 --> 00:33:28,808
I use lots of likes when I speak,

524
00:33:28,856 --> 00:33:32,696
or I try to not do it when I write. But everyone has

525
00:33:32,720 --> 00:33:36,168
a certain style when they write. So if you allow the system

526
00:33:36,216 --> 00:33:39,680
to see how you write, it's more likely that it will

527
00:33:39,752 --> 00:33:43,152
pick up that way of writing and that it will just keep writing

528
00:33:43,208 --> 00:33:47,398
like that. So that's a great way to take advantage

529
00:33:47,446 --> 00:33:51,038
of those systems. Also, maybe consider using

530
00:33:51,086 --> 00:33:54,846
other llms. Like, not everything is just chat

531
00:33:54,870 --> 00:33:57,486
GPT. Like there's cloud, there's bar,

532
00:33:57,590 --> 00:34:01,038
there's lots of them. There's also like hugging

533
00:34:01,086 --> 00:34:04,902
face chat. You can just google for that. There's a lot of

534
00:34:04,918 --> 00:34:08,622
llms, so just try to experiment with other things.

535
00:34:08,678 --> 00:34:12,238
Also, if you don't use English, it's better because the detectors

536
00:34:12,286 --> 00:34:15,559
just, well, lose a lot of accuracy when you

537
00:34:15,631 --> 00:34:19,207
don't use English. You could also

538
00:34:19,255 --> 00:34:22,543
try changing some of the words for other

539
00:34:22,583 --> 00:34:26,823
things. So, for example, like, if you take a word and you switch

540
00:34:26,863 --> 00:34:31,087
it into an emoji, it could potentially

541
00:34:31,255 --> 00:34:35,415
change the distribution of the probabilities in the text and confuse

542
00:34:35,439 --> 00:34:39,039
the system. Doing these types of changes

543
00:34:39,071 --> 00:34:42,384
that you see in the table could potentially

544
00:34:43,324 --> 00:34:47,384
change the detection of the detected text.

545
00:34:48,244 --> 00:34:52,284
And if you want

546
00:34:52,324 --> 00:34:56,020
to rewrite your text, I'd say that

547
00:34:56,052 --> 00:34:59,756
you should probably focus on the start of the

548
00:34:59,780 --> 00:35:03,268
text, because it's like, it's just a simple reason,

549
00:35:03,316 --> 00:35:06,586
it's because it's computationally expensive to

550
00:35:06,610 --> 00:35:10,002
analyze the whole text. So if you have like 500 pages

551
00:35:10,178 --> 00:35:13,014
and you try to run that through,

552
00:35:13,314 --> 00:35:16,498
like, I don't know, detect GPT or some of those tools,

553
00:35:16,666 --> 00:35:20,178
it's very expensive to do. So. Usually the systems that detect

554
00:35:20,226 --> 00:35:23,346
generated text just scan the first two

555
00:35:23,370 --> 00:35:26,986
paragraphs or the first paragraph, and they assume that if

556
00:35:27,010 --> 00:35:30,914
the first paragraph looks generated, then the rest of the text has been generated.

557
00:35:31,034 --> 00:35:34,518
Otherwise it's human. So if you were right,

558
00:35:34,646 --> 00:35:37,950
I'd say that you should probably try to focus on the first

559
00:35:38,062 --> 00:35:41,774
part of the text. It's not necessarily the case

560
00:35:41,894 --> 00:35:45,438
all the time, but many systems take like

561
00:35:45,486 --> 00:35:49,158
this little shortcut. So I'd suggest that you focus

562
00:35:49,206 --> 00:35:52,782
on the start of the text. And the last

563
00:35:52,958 --> 00:35:57,614
tip is that just try to see if it looks generated

564
00:35:57,654 --> 00:36:00,774
or not. So like, you just go to a website like this,

565
00:36:01,914 --> 00:36:04,734
you check if it looks like AI,

566
00:36:05,154 --> 00:36:07,934
it looks like AI. Okay. I have to keep writing.

567
00:36:09,394 --> 00:36:13,226
And those are the main things that

568
00:36:13,290 --> 00:36:17,394
you could do to, well, just use AI without

569
00:36:17,474 --> 00:36:18,854
getting caught, you know,

570
00:36:21,154 --> 00:36:24,890
having said that, I'd love to have a final note on the

571
00:36:24,922 --> 00:36:28,282
fact that it's gonna get harder to attack these texts over

572
00:36:28,338 --> 00:36:32,410
time. And what I mean by that is that as

573
00:36:32,442 --> 00:36:35,734
these models improve over time, the text will look,

574
00:36:36,314 --> 00:36:39,730
well, more human and more human, like, all the time.

575
00:36:39,842 --> 00:36:42,094
So it's going to be much harder to attack.

576
00:36:43,514 --> 00:36:46,898
Bottom line, you might be able to attack

577
00:36:46,946 --> 00:36:50,454
your text, especially if you apply watermarks, et cetera, et cetera.

578
00:36:50,754 --> 00:36:54,334
But this thing of detecting a generated

579
00:36:54,414 --> 00:36:57,622
text is really hard and it's going to be a problem

580
00:36:57,678 --> 00:37:01,710
in the future. So that's where the

581
00:37:01,742 --> 00:37:05,994
talk ends, but with a lot of open questions

582
00:37:06,654 --> 00:37:10,254
regarding ethics, like what should we do about this,

583
00:37:10,334 --> 00:37:14,054
etcetera, etcetera. I don't have time to go into that. It's a really interesting

584
00:37:14,094 --> 00:37:16,674
topic, just too broad to cover.

585
00:37:17,474 --> 00:37:21,290
But I'd love to leave you with something which is

586
00:37:21,322 --> 00:37:25,170
a QR code to rate the session. So if you like the

587
00:37:25,202 --> 00:37:28,762
session, I really love it. If you could give me some positive

588
00:37:28,818 --> 00:37:32,482
feedback, I really appreciate it. And if

589
00:37:32,498 --> 00:37:36,094
you have things that you think that I could improve,

590
00:37:36,714 --> 00:37:40,578
that's also very helpful because I always look at all the comments here

591
00:37:40,706 --> 00:37:44,912
and I try to take them into account for improving

592
00:37:45,058 --> 00:37:48,580
other sessions. So I really be

593
00:37:48,652 --> 00:37:51,852
super thankful. If you could just take like 15 seconds. It's super,

594
00:37:51,908 --> 00:37:55,984
super short. Like really, it's two questions and

595
00:37:56,404 --> 00:37:59,508
just give me like, and well,

596
00:37:59,636 --> 00:38:01,744
what you thought about the session.

597
00:38:02,364 --> 00:38:05,540
Also, I should mention there's a surprise when

598
00:38:05,572 --> 00:38:08,796
you submit the form. So, like, when you click the submit button,

599
00:38:08,900 --> 00:38:12,728
there's, well, a little surprise there. So that's,

600
00:38:12,856 --> 00:38:16,416
well, just to encourage you to fill in

601
00:38:16,440 --> 00:38:19,644
the form. And having said that,

602
00:38:19,944 --> 00:38:23,824
I would love to give you some pointers for, well, some things

603
00:38:23,864 --> 00:38:27,184
you might want to read about if you're interested or

604
00:38:27,224 --> 00:38:31,324
just message me, I'm super happy to speak about these things. All the time.

605
00:38:31,864 --> 00:38:35,552
But having said that, I'm gonna

606
00:38:35,688 --> 00:38:39,444
say goodbye for now, and I hope to see you at another session.

607
00:38:39,484 --> 00:38:43,756
And I really, really wish you a super great conference

608
00:38:43,940 --> 00:38:47,236
full of fun stuff. So goodbye

609
00:38:47,380 --> 00:38:48,564
and see you soon.

