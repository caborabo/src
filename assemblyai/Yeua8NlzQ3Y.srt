1
00:00:20,920 --> 00:00:24,606
Hi, Tim Spann here. My talk is adding

2
00:00:24,750 --> 00:00:28,034
generative AI to real time streaming pipelines,

3
00:00:28,534 --> 00:00:32,398
and we're here for the large language model

4
00:00:32,446 --> 00:00:35,742
conference at Comp 42, which is always a nice one,

5
00:00:35,838 --> 00:00:39,534
great place to be. I'm going to discuss a couple of

6
00:00:39,574 --> 00:00:43,534
different open source technologies that work together to enable

7
00:00:43,614 --> 00:00:46,934
you to build real time pipelines

8
00:00:47,054 --> 00:00:50,758
using large language models. So we'll touch on Kafka,

9
00:00:50,806 --> 00:00:53,722
Nifi, Flink, Python, Iceberg,

10
00:00:53,838 --> 00:00:57,294
and I'll show you a little bit of each one in the demos.

11
00:00:57,674 --> 00:01:00,694
I've been working with data machine learning,

12
00:01:01,154 --> 00:01:04,970
streaming IoT, some other things for a number of years,

13
00:01:05,122 --> 00:01:09,210
and you could contact me at any of these places,

14
00:01:09,362 --> 00:01:13,330
whether Twitter or whatever it's called, some different blogs,

15
00:01:13,362 --> 00:01:17,214
or in person at my meetups and at different conferences around the world.

16
00:01:17,914 --> 00:01:22,360
I do a weekly newsletter, cover streaming ML,

17
00:01:22,512 --> 00:01:26,912
a lot of LLM, open source, Python, Java, all kinds of fun stuff,

18
00:01:27,048 --> 00:01:30,464
as I mentioned, do a bunch of different meetups. They are not

19
00:01:30,504 --> 00:01:34,664
just in the east coast of the US, they are available

20
00:01:34,784 --> 00:01:38,384
virtually live, and I also put them on

21
00:01:38,424 --> 00:01:41,280
YouTube, and if you need them somewhere else, let me know.

22
00:01:41,352 --> 00:01:45,208
We publish all the slides, all the code

23
00:01:45,256 --> 00:01:48,926
and GitHub. Everything you need is out there. Let's get

24
00:01:48,950 --> 00:01:52,222
into the talk. Llm, if you didn't know,

25
00:01:52,398 --> 00:01:56,094
is rapidly evolving. While you're typing down

26
00:01:56,134 --> 00:01:59,094
the things that you use, it's changed, right?

27
00:01:59,214 --> 00:02:02,518
So trying to keep up to dates a lot. There's a

28
00:02:02,526 --> 00:02:06,854
lot of different models, and I'll show you it doesn't really matter how,

29
00:02:06,974 --> 00:02:10,550
you know, how we integrate, because we could do it all

30
00:02:10,582 --> 00:02:13,782
through a couple different ways with open source and the models

31
00:02:13,838 --> 00:02:17,264
is some closed ones, we'll integrate with some open ones.

32
00:02:17,604 --> 00:02:21,244
Which one makes the most sense for you? Depends on your use

33
00:02:21,284 --> 00:02:24,144
case, but we'll integrate with GPT 35,

34
00:02:24,484 --> 00:02:27,492
we'll integrate with Olama running some models.

35
00:02:27,548 --> 00:02:31,148
We're running with Mistral and mixtural, working with

36
00:02:31,276 --> 00:02:34,396
pine cone today and chroma. Touch on those

37
00:02:34,420 --> 00:02:37,564
a little bit. There's a bunch of other ones out there, all of them pretty

38
00:02:37,604 --> 00:02:40,984
good. We do a very little bit with lang chain.

39
00:02:42,284 --> 00:02:46,188
It is used by some of my processors

40
00:02:46,236 --> 00:02:49,988
that are written in Python, which are integrated through Nifi,

41
00:02:50,156 --> 00:02:54,092
and we're using a bunch of libraries like transformers

42
00:02:54,228 --> 00:02:57,984
and also hugging face hosted models,

43
00:02:58,364 --> 00:03:02,588
as well as models hosted by Watson X and some hosted by Cloudera.

44
00:03:02,716 --> 00:03:06,224
There's a lot of different ways to interact with models. We'll also do

45
00:03:06,604 --> 00:03:10,152
Olama, which is running on my laptop along

46
00:03:10,248 --> 00:03:13,880
with my whole open source stack, so I wish

47
00:03:13,912 --> 00:03:17,312
I had more ram and maybe a GPU in there, but you

48
00:03:17,328 --> 00:03:20,656
do what you can. Lots of different use cases.

49
00:03:20,840 --> 00:03:24,520
If you're at this conference, you probably know them. We're not going to

50
00:03:24,552 --> 00:03:28,152
touch on all of them. They take forever. But the same ideas,

51
00:03:28,208 --> 00:03:30,884
though, get your prompt ready,

52
00:03:31,344 --> 00:03:34,480
send it to the model, get your results back and do something with them.

53
00:03:34,592 --> 00:03:36,664
That enrichment, transformation,

54
00:03:36,784 --> 00:03:40,232
processing, all that you need to do around prompt

55
00:03:40,288 --> 00:03:43,736
engineering really needs tools. So you're not handwriting,

56
00:03:43,800 --> 00:03:47,360
everything. That's why we like to use something like Apache, Nifi,

57
00:03:47,392 --> 00:03:51,016
which I'll show you can help you with all of this, whether it's

58
00:03:51,080 --> 00:03:54,488
getting stuff into your vector store, helping,

59
00:03:54,536 --> 00:03:58,320
cleaning up, calling multiple models, you know, maybe code assistant,

60
00:03:58,392 --> 00:04:02,088
step one. And then you need to summarize documentation

61
00:04:02,216 --> 00:04:05,892
and translate it into another language. You know, maybe you

62
00:04:05,908 --> 00:04:08,824
need to build some content, push it somewhere.

63
00:04:09,324 --> 00:04:12,904
NiFi really helps enable a lot of those use cases.

64
00:04:13,964 --> 00:04:17,412
You know, this is probably more important in this decision. You could

65
00:04:17,468 --> 00:04:21,364
change, you know, maybe you start off with a closed model because

66
00:04:21,404 --> 00:04:24,852
they are probably the best at this point. They do a

67
00:04:24,868 --> 00:04:27,876
lot of good stuff. But then you're worried, can I get the data there safely?

68
00:04:27,940 --> 00:04:31,426
Maybe I can, maybe I can't. Is there data? I don't want

69
00:04:31,620 --> 00:04:35,742
to be used in their models. Open source,

70
00:04:35,798 --> 00:04:39,550
because we love open source. We love these open models. That works

71
00:04:39,582 --> 00:04:43,374
really well with our ideas around open source

72
00:04:43,414 --> 00:04:46,726
tools for streaming. So it makes sense. But if that's

73
00:04:46,870 --> 00:04:50,878
not what makes sense for use case, you know, make that decision. What's nice is

74
00:04:51,006 --> 00:04:54,446
with Nifi I can run and I'll show you an example where

75
00:04:54,470 --> 00:04:58,174
I have like four or five different models in

76
00:04:58,254 --> 00:05:02,030
different locations that I could decide or not decide when to

77
00:05:02,062 --> 00:05:05,854
run. Maybe when one is cheaper, maybe when one is

78
00:05:05,894 --> 00:05:09,070
better for certain use cases, you know,

79
00:05:09,142 --> 00:05:13,166
you make that decision if you're going to do this. In the enterprise

80
00:05:13,350 --> 00:05:17,350
cloud, Arrow works with AWS and IBM

81
00:05:17,502 --> 00:05:21,638
and Pinecone and Nvidia and hugging face and ray to make sure you can do

82
00:05:21,766 --> 00:05:25,398
everything you need to do to work with the foundation models,

83
00:05:25,486 --> 00:05:30,134
have the proper search and vectorization, have as much GPU

84
00:05:30,254 --> 00:05:34,350
powered performance, you know, all the right tooling,

85
00:05:34,462 --> 00:05:37,654
and run all the different compute options that you

86
00:05:37,694 --> 00:05:40,874
need. And again, the amount of

87
00:05:41,254 --> 00:05:45,070
models and projects and

88
00:05:45,142 --> 00:05:48,734
software, and software as a service and open source tools

89
00:05:48,854 --> 00:05:53,294
that come available is massive

90
00:05:53,334 --> 00:05:56,636
and it's changing constantly. New versions of things. You're like,

91
00:05:56,660 --> 00:05:59,972
oh, this is the coolest new model. It's like, ooh, Google Gemma

92
00:05:59,988 --> 00:06:04,068
is the new one. Oh, no mixture. Now this, it's, it's fun

93
00:06:04,116 --> 00:06:07,236
because there's always something new going on, and probably during

94
00:06:07,300 --> 00:06:09,860
this conference there'll be two or three new things.

95
00:06:10,012 --> 00:06:13,444
So I'm keeping my eyes open, I want to see what comes out.

96
00:06:13,604 --> 00:06:17,204
But the middle orange bar is where I'm mostly working,

97
00:06:17,364 --> 00:06:20,692
is getting that real time ingest, enrichment,

98
00:06:20,788 --> 00:06:24,900
wrangling, transformation, all of that. Get that data stored,

99
00:06:25,052 --> 00:06:28,708
get the data to the model, whether it's for training, whether it's

100
00:06:28,756 --> 00:06:32,316
for context, for prompts, whether that's calling

101
00:06:32,340 --> 00:06:35,724
the model, get results back, get your classification all

102
00:06:35,764 --> 00:06:39,076
important, whether that's in private cloud, public cloud,

103
00:06:39,140 --> 00:06:42,780
hybrid cloud. So maybe you need to run on Amazon Azure,

104
00:06:42,852 --> 00:06:46,564
Google and your own red hat

105
00:06:46,724 --> 00:06:50,418
Kubernetes. You could do that. Doesn't matter if it's

106
00:06:50,466 --> 00:06:54,334
Google specific hardware. AMD, really good chips,

107
00:06:54,794 --> 00:06:57,418
Nvidia, IBM, Intel, Dell,

108
00:06:57,586 --> 00:07:00,866
everybody's got good stuff that helps you out there.

109
00:07:01,010 --> 00:07:04,666
You just got to know that you're using the right framework

110
00:07:04,850 --> 00:07:08,226
and libraries and versions of Python and

111
00:07:08,250 --> 00:07:11,970
Java that are tweaked for those things. And we definitely have

112
00:07:12,002 --> 00:07:15,554
people that can help there. Get all your models out there and

113
00:07:15,594 --> 00:07:18,442
running common use case.

114
00:07:18,578 --> 00:07:22,914
And this is where NiFi really shines for this price process is

115
00:07:22,994 --> 00:07:26,858
interacting with people and data

116
00:07:27,026 --> 00:07:30,414
and for, you know, automated tools,

117
00:07:30,714 --> 00:07:34,226
they may not know the difference. Now you know the difference if you're doing

118
00:07:34,290 --> 00:07:37,962
live QA and I show you that through slack because

119
00:07:38,018 --> 00:07:42,058
you're typing it. But that data could be done by another bot. That data

120
00:07:42,106 --> 00:07:45,146
can be coming from a database, it can be coming from rest,

121
00:07:45,210 --> 00:07:49,076
endpoints, documents, social media transactions,

122
00:07:49,140 --> 00:07:53,204
public data feeds, s three and other files somewhere logs,

123
00:07:53,244 --> 00:07:55,984
ATM's, other live things,

124
00:07:56,404 --> 00:08:00,020
weather reports, wherever that is. We're getting that data in,

125
00:08:00,132 --> 00:08:03,780
sending data out when it needs to go out, you know,

126
00:08:03,892 --> 00:08:07,452
cleaning it up, get all the enrichments, do any alerts that need to happen

127
00:08:07,508 --> 00:08:10,956
right away. Get things vectorized,

128
00:08:11,060 --> 00:08:15,084
chunked, parsed and into whatever database

129
00:08:15,124 --> 00:08:18,932
or data stores it needs to go to get it to

130
00:08:18,948 --> 00:08:22,036
the right models, wherever they may be, whether that's in clutter,

131
00:08:22,100 --> 00:08:25,716
machine learning, Watson, 100 other places.

132
00:08:25,820 --> 00:08:29,372
Get that into Kafka. So we could distribute it to any information

133
00:08:29,468 --> 00:08:32,788
that needs to be shared and shared instantly with as many

134
00:08:32,836 --> 00:08:36,660
people in as many places as possible. We get it into Kafka

135
00:08:36,772 --> 00:08:40,204
if that needs to spread, if that can't be accessed, because maybe

136
00:08:40,244 --> 00:08:43,660
it's in a very secure internal Kafka,

137
00:08:43,692 --> 00:08:47,020
we could replicate that one to ones in the public cloud

138
00:08:47,172 --> 00:08:51,012
so it could be shared with other applications systems. Great way

139
00:08:51,028 --> 00:08:54,916
to distribute your data quickly to whoever needs it without

140
00:08:54,980 --> 00:08:58,828
duplication, without tight coupling. It's really nice

141
00:08:58,876 --> 00:09:02,676
feature. Get it stored, get it enriched, and we'll show you

142
00:09:02,700 --> 00:09:05,732
that in examples. And a common way to

143
00:09:05,748 --> 00:09:09,668
do that is CDC. And we can pull that CDC instantly,

144
00:09:09,756 --> 00:09:13,636
get it working like we mentioned before, get all those llms

145
00:09:13,700 --> 00:09:17,540
together. A common thing

146
00:09:17,572 --> 00:09:20,784
that we do as part of these flows is ingest.

147
00:09:21,204 --> 00:09:24,156
We're ingesting and that could be, you know,

148
00:09:24,300 --> 00:09:27,164
constantly a stream that's getting pushed, we're pulling.

149
00:09:27,204 --> 00:09:31,224
It could be Cron's, it could be

150
00:09:32,084 --> 00:09:34,652
one time grab documents, messages,

151
00:09:34,748 --> 00:09:38,058
events, files, whatever it is, it doesn't matter.

152
00:09:38,106 --> 00:09:41,762
Nifi supports hundreds of different inputs and can convert

153
00:09:41,818 --> 00:09:45,890
them on the fly. We can parse documents very

154
00:09:45,962 --> 00:09:49,490
easily. Thank you. Unstructured IO, chunk them up into the

155
00:09:49,522 --> 00:09:53,410
right size pieces so that it'll be optimal for your different

156
00:09:53,602 --> 00:09:57,378
vector store or if you're pushing it somewhere else. I mean,

157
00:09:57,546 --> 00:10:00,434
the nice thing is now if I could send it to as many places as

158
00:10:00,474 --> 00:10:04,574
possible, get that into the stream, whatever we need to do there,

159
00:10:04,894 --> 00:10:08,982
the data pipelines, getting that external

160
00:10:09,038 --> 00:10:12,630
context when you need it. So I type a question,

161
00:10:12,782 --> 00:10:17,038
what is Apache nifi? Grab some external

162
00:10:17,086 --> 00:10:20,662
context so I could pull all of my recent articles from

163
00:10:20,718 --> 00:10:23,678
medium, get them cleaned up, enriched transform,

164
00:10:23,766 --> 00:10:27,150
parsed, chunked, vectorized and available so

165
00:10:27,182 --> 00:10:30,856
that when I ask a question, if that's

166
00:10:31,356 --> 00:10:35,282
already you there, I pull that, add that as a context with my prompt,

167
00:10:35,378 --> 00:10:39,002
clean that prompt up, get it in the format that's

168
00:10:39,058 --> 00:10:42,634
needed by whoever's model I'm calling, you know, a llama.

169
00:10:42,714 --> 00:10:46,882
Slightly different from hugging, face rest versus Watson versus Cloudera.

170
00:10:46,978 --> 00:10:50,014
Get it in that right format, probably Jason.

171
00:10:50,514 --> 00:10:53,874
And make sure it fits, you know, make sure it's not too big,

172
00:10:54,034 --> 00:10:58,174
get that to them, get, get those results back, maybe start caching

173
00:10:58,214 --> 00:11:02,086
at different layers, maybe that goes into a database, maybe that goes into

174
00:11:02,150 --> 00:11:06,102
ram, lots of options there. And do

175
00:11:06,118 --> 00:11:10,302
the round tripping for you so you don't have to write an application for

176
00:11:10,438 --> 00:11:14,270
everything. You know, someone type something in discord or

177
00:11:14,302 --> 00:11:17,998
arrest or comes from Kafka, you can pull it from a database,

178
00:11:18,046 --> 00:11:21,358
pull it from Slack and send the messages out. Whether it's going to

179
00:11:21,406 --> 00:11:24,906
email slack, a database, wherever you

180
00:11:24,930 --> 00:11:28,778
need it to go, we'll send it there pretty easy and

181
00:11:28,826 --> 00:11:32,298
we could deal with all your types of data, even if it's

182
00:11:32,346 --> 00:11:35,298
zipped up, tarred up, if it's images,

183
00:11:35,466 --> 00:11:38,650
videos, documents obviously is a big one

184
00:11:38,682 --> 00:11:42,666
for most of the use cases for large language models,

185
00:11:42,730 --> 00:11:46,266
so, but it doesn't matter. Thanks to unstructured IO and

186
00:11:46,290 --> 00:11:50,526
some other processors in IFI, I could deal with HTML and Markdown and

187
00:11:50,550 --> 00:11:53,702
RSS and PDF, word docs and

188
00:11:53,718 --> 00:11:57,438
Google Docs and RTF and regular text. And I added

189
00:11:57,486 --> 00:12:01,154
one to do VTT. If you know about that format,

190
00:12:01,454 --> 00:12:04,514
sound any kind of the feeds from social

191
00:12:04,974 --> 00:12:08,694
and XML too, and we can figure

192
00:12:08,734 --> 00:12:11,814
out what type it is, chunk it up, store it,

193
00:12:11,934 --> 00:12:15,364
parse it, do all those things you need with that unstructured

194
00:12:15,404 --> 00:12:18,836
and semi structured data interface with whoever you

195
00:12:18,860 --> 00:12:23,300
need in the cloud or on premise or in your private

196
00:12:23,372 --> 00:12:26,852
kubernetes, all the major ones. If it's not listed,

197
00:12:26,988 --> 00:12:30,948
it means we haven't tried it yet. Definitely reach out. I'm always looking

198
00:12:30,996 --> 00:12:33,944
to find new things to try and integrate.

199
00:12:34,844 --> 00:12:38,476
Fun stuff out there. Now if I just got into version two,

200
00:12:38,620 --> 00:12:42,136
this is the one you want to use. This will be official

201
00:12:42,240 --> 00:12:45,984
production release very shortly, possibly by the time

202
00:12:46,024 --> 00:12:49,776
you're seeing this. It's got hardcore python integration.

203
00:12:49,840 --> 00:12:53,824
So I can run really cool stuff in there. Really cool way to deal with

204
00:12:53,904 --> 00:12:57,528
parameters in it so you can do a lot of interesting

205
00:12:57,576 --> 00:13:01,024
DevOps stuff using the latest JDK. If you're not a Java

206
00:13:01,064 --> 00:13:04,712
person, that one has a lot of enhancements that makes

207
00:13:04,768 --> 00:13:09,152
Java incredibly fast. If you saw that 1 billion row challenge out there,

208
00:13:09,328 --> 00:13:12,528
really cool stuff there. And just to show you how fast

209
00:13:12,576 --> 00:13:16,080
it is, I also recently found that we

210
00:13:16,112 --> 00:13:19,672
can do real time integration with models

211
00:13:19,808 --> 00:13:23,912
while we're running real time SQL flink. SQL is a real

212
00:13:24,008 --> 00:13:28,008
time SQL on top of fast

213
00:13:28,136 --> 00:13:31,648
data such as Kafka, such as Pulsar,

214
00:13:31,736 --> 00:13:35,000
such as real time streams. And we can also grab data from

215
00:13:35,032 --> 00:13:37,452
things like Kudu and Iceberg,

216
00:13:37,608 --> 00:13:40,796
and JDBC stores like postgres,

217
00:13:40,860 --> 00:13:44,060
Oracle in real time or through CDC.

218
00:13:44,212 --> 00:13:47,424
But what this means is by writing a simple function,

219
00:13:47,964 --> 00:13:51,020
I can call a model as part of a

220
00:13:51,052 --> 00:13:54,644
query, and I'll show you that it's really cool. And I could

221
00:13:54,684 --> 00:13:58,420
do that whether I'm doing that directly to something like cloud

222
00:13:58,452 --> 00:14:02,748
error, machine learning, or if it's something that takes a little more enrichment,

223
00:14:02,876 --> 00:14:06,278
like that process we were mentioning. So I'm going to have Nifi

224
00:14:06,326 --> 00:14:10,086
host a rest endpoint for me so I can have this tool,

225
00:14:10,150 --> 00:14:13,534
call that, and that'll do all the cleanup. Make sure your prompt

226
00:14:13,574 --> 00:14:17,278
is nice and send that over to say hugging face

227
00:14:17,326 --> 00:14:20,470
and get your results and send them back to you

228
00:14:20,582 --> 00:14:22,994
so you could use them in your live system.

229
00:14:23,854 --> 00:14:27,630
Got a link to the article here and an

230
00:14:27,662 --> 00:14:32,294
example of the SQL that we have there. Pretty straightforward.

231
00:14:33,514 --> 00:14:36,658
Like I mentioned, I'm working with the Gemma model so you

232
00:14:36,666 --> 00:14:39,946
can access that. Take a look at that example

233
00:14:40,090 --> 00:14:43,962
again. The difference for using one model versus

234
00:14:44,018 --> 00:14:47,786
another. Not really that much for me. Now,

235
00:14:47,850 --> 00:14:50,538
I mentioned being able to use Python.

236
00:14:50,706 --> 00:14:54,338
We'll go through that pretty quick. If you saw the comp 42 python,

237
00:14:54,466 --> 00:14:58,608
I explained that in detail, how to build your own python processor.

238
00:14:58,786 --> 00:15:02,212
This is that VTT I told you so I can parse that

239
00:15:02,348 --> 00:15:06,268
so we can grab those web video text, use that

240
00:15:06,316 --> 00:15:10,556
to either put in your vector store, enrich prompts,

241
00:15:10,580 --> 00:15:14,260
or other stuff. We can call Watson right

242
00:15:14,292 --> 00:15:18,204
from there, create our own fake data, grab wikipedia feeds

243
00:15:18,244 --> 00:15:21,876
and get them in a clean format. Very nice. Saves a couple steps

244
00:15:21,900 --> 00:15:26,106
there. This one is important for me because I found sometimes

245
00:15:26,130 --> 00:15:30,066
I want to pre process a prompt, and the reason I might do

246
00:15:30,090 --> 00:15:33,186
that is someone might ask a question that I don't need to send

247
00:15:33,210 --> 00:15:36,346
to a model. Save me some money, save me some time.

248
00:15:36,530 --> 00:15:39,810
If you don't need to use a large language model,

249
00:15:39,922 --> 00:15:43,154
maybe use regular ML or regular small

250
00:15:43,234 --> 00:15:46,682
model. Or in some cases just do a rest call.

251
00:15:46,818 --> 00:15:50,530
Like if someone goes what is the current stock price for Amazon?

252
00:15:50,642 --> 00:15:53,918
Don't send that to chat GPT. So I parse

253
00:15:53,966 --> 00:15:57,310
out the company names with some cool libraries out

254
00:15:57,342 --> 00:16:00,886
there. And then I could do another thing where I can look

255
00:16:00,910 --> 00:16:04,286
up the stock symbol, get the current stock price and send that back.

256
00:16:04,390 --> 00:16:07,878
That is faster and cheaper than calling a language

257
00:16:07,926 --> 00:16:11,262
model, even on premise. And I have that in some

258
00:16:11,278 --> 00:16:15,382
of the articles. It's very easy. Now I've also added a new

259
00:16:15,478 --> 00:16:18,878
library using salesforce blip image

260
00:16:18,926 --> 00:16:22,492
captioning to be able to generate captions for images.

261
00:16:22,588 --> 00:16:25,980
Because remember, I could have flows full of images and I use

262
00:16:26,012 --> 00:16:29,252
this as part of parsing out things when

263
00:16:29,268 --> 00:16:32,868
I do meetups. And I think it's generally

264
00:16:32,956 --> 00:16:36,732
useful thing to have and I could change the model if there's a better

265
00:16:36,788 --> 00:16:40,580
model for that. I also added one for Resnet 50 just

266
00:16:40,612 --> 00:16:42,104
to do classifications.

267
00:16:44,284 --> 00:16:48,390
Pretty useful for flows. Again, as part of that multi step process,

268
00:16:48,572 --> 00:16:52,018
I might want to caption an image. I might want to make sure it's

269
00:16:52,066 --> 00:16:56,134
not problematic image. So I'm using this one to make sure

270
00:16:56,794 --> 00:17:00,506
we don't even deal with those, we just delete them. Maybe I need

271
00:17:00,530 --> 00:17:04,338
to see people's face in the image again. Getting more

272
00:17:04,386 --> 00:17:07,978
data could be helpful before you send things downstream to other

273
00:17:08,026 --> 00:17:12,082
models. And again, there's Python processors to

274
00:17:12,098 --> 00:17:15,854
do chunking, parsing, calling chat GPT,

275
00:17:16,314 --> 00:17:19,534
working with vector stores, just to give you an example there.

276
00:17:19,834 --> 00:17:22,970
Let's get into some demos. I just have

277
00:17:23,002 --> 00:17:26,914
some links to some other articles here, but let's

278
00:17:26,954 --> 00:17:30,282
get into the demo. You don't need

279
00:17:30,298 --> 00:17:33,810
to see any more slides you will get these slides and they are

280
00:17:33,842 --> 00:17:38,010
fully available for everything you need there. So no problems.

281
00:17:38,122 --> 00:17:41,586
Very easy. So first

282
00:17:41,650 --> 00:17:45,620
thing we do is this is Nifi. It is running for us.

283
00:17:45,802 --> 00:17:49,440
And what we can do here is, you know,

284
00:17:49,472 --> 00:17:53,488
if we wanted to build new stuff, we can add any of these processors.

285
00:17:53,616 --> 00:17:57,392
If you want to see the cool new Python ones, just type in Python.

286
00:17:57,528 --> 00:18:00,824
There's a bunch of them, as you saw in the list there

287
00:18:00,864 --> 00:18:04,056
before, like extracting company name, just to give you an

288
00:18:04,080 --> 00:18:07,624
example. So what we can do is, well,

289
00:18:07,704 --> 00:18:11,560
I want to get a meetup chat going.

290
00:18:11,712 --> 00:18:16,024
So I have a processor here that just listens for events

291
00:18:16,064 --> 00:18:19,640
as they come from slack. And there's a couple tokens you got to

292
00:18:19,672 --> 00:18:22,536
set. I have an article shows you how to do that. It's very easy to

293
00:18:22,560 --> 00:18:26,712
set up those. So here is my slack

294
00:18:26,808 --> 00:18:30,336
group. If you're interested, you could join it and you could use

295
00:18:30,360 --> 00:18:33,204
it to ask application questions.

296
00:18:34,664 --> 00:18:38,084
Let's see if it knows this one. I don't think it does. When is the

297
00:18:39,124 --> 00:18:41,744
comp 42 large?

298
00:18:42,124 --> 00:18:45,144
Should I do LM lm conference?

299
00:18:48,964 --> 00:18:52,276
So what happens next is it comes in here.

300
00:18:52,420 --> 00:18:56,508
As you can see now it's running because it just got messages. And just

301
00:18:56,556 --> 00:18:59,916
for fun, I store them in a database. Oh, unless someone

302
00:18:59,980 --> 00:19:03,820
stopped this. The nice thing with Nifi is you could start and stop

303
00:19:03,892 --> 00:19:07,248
things either manually as you see through this,

304
00:19:07,416 --> 00:19:11,104
or through a DevOps process. That could be rest or command line.

305
00:19:11,144 --> 00:19:14,504
You get the idea. But let's make sure that data went through.

306
00:19:14,664 --> 00:19:18,048
This is data provenance. This is really good

307
00:19:18,096 --> 00:19:21,712
because I don't know what that black box LlM is doing, but I

308
00:19:21,728 --> 00:19:25,160
could tell you exactly what I did to prepare that prompt

309
00:19:25,352 --> 00:19:28,800
when I sent it, how big it was, and what I got back.

310
00:19:28,952 --> 00:19:32,334
So you don't have the provenance inside that

311
00:19:32,374 --> 00:19:35,926
model, but I know the providence getting there and coming back.

312
00:19:36,110 --> 00:19:40,114
So that could be very helpful. So this is my

313
00:19:40,574 --> 00:19:43,846
message here. It's me because I

314
00:19:43,910 --> 00:19:47,714
typed the message in. There's my input, there's an id

315
00:19:48,214 --> 00:19:50,794
timestamps where I am in the world.

316
00:19:51,254 --> 00:19:54,598
Helpful stuff. So that message went in, I saved it in a

317
00:19:54,646 --> 00:19:58,980
postgres database. I also pushed it into Kafka for

318
00:19:59,012 --> 00:20:02,340
slack messages. And you'll see why when we get

319
00:20:02,372 --> 00:20:06,544
to that next step. So we've got that data again.

320
00:20:07,284 --> 00:20:10,716
We're using two parts of the process already, and then I'm going to

321
00:20:10,740 --> 00:20:14,836
do the prompt cleaning

322
00:20:14,860 --> 00:20:19,060
and engineering. So I'm just going to pull out the fields

323
00:20:19,252 --> 00:20:22,892
from that slack message that I want. If this is coming from something

324
00:20:22,948 --> 00:20:26,000
else, you might have a different extraction,

325
00:20:26,072 --> 00:20:30,160
but pretty easy. I make sure that it's not myself.

326
00:20:30,312 --> 00:20:33,808
You know, I don't want to reply to the reply

327
00:20:33,856 --> 00:20:37,616
we send to it. Filter this out. I forgot to

328
00:20:37,640 --> 00:20:41,952
mention this model. There's a model from hugging face

329
00:20:42,128 --> 00:20:45,136
for not safe for work text,

330
00:20:45,320 --> 00:20:48,444
so I want to parse them out. I don't want those.

331
00:20:49,024 --> 00:20:52,966
Oh, someone added another step here. Yeah, I was running a demo. I wanted

332
00:20:52,990 --> 00:20:56,270
to look at those. We were just going to delete that, which is the

333
00:20:56,302 --> 00:21:00,014
awesome thing that we started up. We have these configurable queues that run

334
00:21:00,054 --> 00:21:04,150
even on a big cluster and then make sure things don't go

335
00:21:04,182 --> 00:21:07,118
somewhere. Oh, one of the questions was not safe,

336
00:21:07,246 --> 00:21:10,230
so that didn't make it through the system, which is fine.

337
00:21:10,382 --> 00:21:14,366
And here we're going to query pinecone with that input,

338
00:21:14,430 --> 00:21:17,818
see what we get back, extract the results

339
00:21:17,906 --> 00:21:21,730
there, build up our new prompt, and then figure

340
00:21:21,762 --> 00:21:24,850
out who we want to send it to. Do we want chat GBT to get

341
00:21:24,882 --> 00:21:28,386
it? Do we want someone else to get it?

342
00:21:28,410 --> 00:21:32,354
I've been just doing mixture most recently, but I also have mistral

343
00:21:32,474 --> 00:21:36,218
running on Olama. And down here I've got Google Gemma

344
00:21:36,266 --> 00:21:40,610
running again, pushing the cleaned

345
00:21:40,642 --> 00:21:44,624
up prompts to Kafka as well. So we come into,

346
00:21:46,524 --> 00:21:50,764
into here and we format the prompts exactly

347
00:21:50,804 --> 00:21:54,972
with some instructions added. You kind of need that for mixed roll eight

348
00:21:55,028 --> 00:21:59,292
seven B instruction. I mean, every model is a little bit

349
00:21:59,308 --> 00:22:02,716
of different tweaking. So then we call it, and if

350
00:22:02,740 --> 00:22:06,524
we look at the results here, we could see what we got back from

351
00:22:06,564 --> 00:22:09,622
there. And then I'm going to clean it up, add a

352
00:22:09,638 --> 00:22:13,470
couple fields and push that into a different

353
00:22:13,542 --> 00:22:16,910
topic and also push that out to slack. While I

354
00:22:16,942 --> 00:22:20,966
say this, we got our threaded result back and

355
00:22:21,150 --> 00:22:24,702
see if it's any good. Comp 42, April 11 oh, and it's got

356
00:22:24,718 --> 00:22:28,114
the link. Oh, look at that. Okay,

357
00:22:28,534 --> 00:22:32,486
I must have had that. Remember we looked into Pinecone.

358
00:22:32,590 --> 00:22:36,184
Well, that's where I put my medium. And in my medium I've

359
00:22:36,224 --> 00:22:39,416
linked to the conference because I'm speaking

360
00:22:39,480 --> 00:22:43,240
here. So I like that. Pretty good,

361
00:22:43,432 --> 00:22:46,720
pretty good thing. So we got the data and we got

362
00:22:46,752 --> 00:22:49,976
it back. And like I said, we could have sent it there, could have

363
00:22:50,000 --> 00:22:53,480
sent it to Watson, Google, Jim. Lots of

364
00:22:53,512 --> 00:22:57,444
different options here to get things back to slack. I have

365
00:22:57,824 --> 00:23:01,074
just a little cleanup and I just,

366
00:23:01,224 --> 00:23:04,574
you know, add that generated text and I push

367
00:23:04,614 --> 00:23:08,638
it to the right channel. This gives me the right timestamp.

368
00:23:08,686 --> 00:23:12,262
So I'm in that thread. You don't have to thread the results back.

369
00:23:12,358 --> 00:23:15,630
I put a lot of extra metadata in here just

370
00:23:15,662 --> 00:23:19,006
for myself. I don't need to put this here. As you can see,

371
00:23:19,030 --> 00:23:23,006
it tells me what model, you know, how long the compute took,

372
00:23:23,190 --> 00:23:27,242
all those sort of things. So if you see that this is just metadata

373
00:23:27,398 --> 00:23:30,362
for me or just to be interesting. Oh, it was mixtural.

374
00:23:30,538 --> 00:23:34,170
It took 100 tokens, you know, that kind of stuff.

375
00:23:34,282 --> 00:23:37,706
We could filter that down if we don't. So remember I said I pushed

376
00:23:37,730 --> 00:23:41,002
stuff to Kafka. Let's get there. These are these

377
00:23:41,058 --> 00:23:44,802
Kafka messages I pushed out there.

378
00:23:44,978 --> 00:23:49,034
Well, I can do real time queries on them with flink

379
00:23:49,114 --> 00:23:52,714
against those Kafka messages. And if you look here

380
00:23:52,874 --> 00:23:57,618
as part of that, I'm passing in that message text and

381
00:23:57,706 --> 00:24:01,986
sending it against a function which

382
00:24:02,010 --> 00:24:06,494
is this function here which calls into Nifi

383
00:24:06,834 --> 00:24:10,554
to get the results back. And then we get there's

384
00:24:10,594 --> 00:24:14,074
that Kafka we saw. So it is running and

385
00:24:14,114 --> 00:24:17,970
getting the generated text here which

386
00:24:18,002 --> 00:24:21,578
are results for that. And you can see the flink

387
00:24:21,746 --> 00:24:25,246
job is running. If I had a big cluster it could

388
00:24:25,270 --> 00:24:28,406
be running there. Not much going on here,

389
00:24:28,430 --> 00:24:32,182
but I have a materialized view. And what's nice about

390
00:24:32,238 --> 00:24:35,630
that is it produces all this data as a JSON

391
00:24:35,702 --> 00:24:38,782
rest endpoint so I can ingest it somewhere.

392
00:24:38,958 --> 00:24:43,102
Now like I mentioned here, that USB is calling

393
00:24:43,158 --> 00:24:46,734
out, that is calling out to

394
00:24:46,774 --> 00:24:50,434
another process here. And this is

395
00:24:50,554 --> 00:24:54,522
how Nifi acts as a whole website. So it receives

396
00:24:54,578 --> 00:24:58,146
a message. And as you see here, it can be get, post, put, whatever you

397
00:24:58,170 --> 00:25:01,610
want. I can limit that. And then I parse what

398
00:25:01,642 --> 00:25:05,626
they send in here. I'm going to route it because I want to

399
00:25:05,650 --> 00:25:09,210
send to LLM and I have Google Gemma reply to it.

400
00:25:09,322 --> 00:25:12,894
So Google Gemma, clean it up, make my prompt,

401
00:25:13,274 --> 00:25:17,130
call Gemma, get the results back again,

402
00:25:17,202 --> 00:25:21,378
push it into Kafka so I can, you know, have a copy for someone else.

403
00:25:21,546 --> 00:25:25,034
And then we return it

404
00:25:25,074 --> 00:25:29,162
to that rest endpoint which is the response

405
00:25:29,218 --> 00:25:33,002
here. We send that response back to

406
00:25:33,058 --> 00:25:36,962
flink and it shows up here. And as you see, that ran pretty

407
00:25:37,018 --> 00:25:39,814
quickly even with all that round turn.

408
00:25:40,474 --> 00:25:43,986
That's really it. When I want to show you. Thank you

409
00:25:44,050 --> 00:25:47,210
for attending this talk. I'm going to be

410
00:25:47,242 --> 00:25:50,746
speaking at some other events very shortly. If you're

411
00:25:50,770 --> 00:25:54,242
in the air, if you're in New York or Seattle or Boston,

412
00:25:54,418 --> 00:25:57,626
say hi. It's been really good showing you what we could do today.

413
00:25:57,690 --> 00:25:57,994
Thank you.

