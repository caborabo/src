1
00:00:27,120 --> 00:00:30,394
My name is Pratik. Thank you. And I'm a principal

2
00:00:30,434 --> 00:00:34,162
software engineer in one of the leading human

3
00:00:34,218 --> 00:00:36,614
capital management companies in the US.

4
00:00:37,834 --> 00:00:41,546
I'm also an active researcher in the field of

5
00:00:41,690 --> 00:00:45,090
artificial intelligence DevOps machine learning and

6
00:00:45,122 --> 00:00:49,338
security. It's great to be here at Conf 42

7
00:00:49,426 --> 00:00:52,818
machine learning conference and to discuss the important

8
00:00:52,906 --> 00:00:56,714
topic of large language model and the

9
00:00:56,754 --> 00:01:00,266
security risk that there is. So let's dive

10
00:01:00,290 --> 00:01:03,762
into my presentation. So today my topic

11
00:01:03,818 --> 00:01:07,554
is the privacy predicament with the

12
00:01:07,594 --> 00:01:10,506
transformative potential of large language models.

13
00:01:10,690 --> 00:01:14,546
So we are living in exciting times. With this advancement in

14
00:01:14,570 --> 00:01:18,330
AI, we find ourselves at a remarkable technological

15
00:01:18,402 --> 00:01:21,922
crossroad which is propelled by this large language models.

16
00:01:22,098 --> 00:01:25,962
This artificial intelligence marvels can engage in dialogue just

17
00:01:26,018 --> 00:01:29,906
like human, generate creative content, and even

18
00:01:30,050 --> 00:01:33,378
write a quote without any issues or error.

19
00:01:33,506 --> 00:01:37,546
With this, immense capabilities are overshadowed by trouble

20
00:01:37,690 --> 00:01:40,802
paradox, which is shielding this data. This model,

21
00:01:40,858 --> 00:01:44,762
which possesses a severe risk to the individual privacy

22
00:01:44,818 --> 00:01:48,386
and the human rights. As useful these models

23
00:01:48,450 --> 00:01:51,894
are, their OPEC training processes raises a significant

24
00:01:52,984 --> 00:01:56,744
privacy risk. And this is a privacy paradox that

25
00:01:56,784 --> 00:02:00,056
we need to grapple with. And one should ask like how we

26
00:02:00,080 --> 00:02:04,416
can take a full advantage of this large language models while

27
00:02:04,560 --> 00:02:08,104
protecting the people's privacy. Let's talk

28
00:02:08,144 --> 00:02:11,712
about the data privacy paradox. The training large

29
00:02:11,768 --> 00:02:15,552
language model involves ingesting a mind boggling

30
00:02:15,608 --> 00:02:19,854
amount of data, and which are randomly scrapped from the

31
00:02:19,894 --> 00:02:23,198
open source or Internet, like from the website, book,

32
00:02:23,326 --> 00:02:27,094
personal communication, you just name it. This unfiltered

33
00:02:27,134 --> 00:02:30,534
data absorption allows the model to inevitably

34
00:02:30,614 --> 00:02:34,734
memorize and spit out verbatim sensitive personal

35
00:02:34,814 --> 00:02:38,910
information, like credit card numbers, private messages, copyright materials

36
00:02:38,942 --> 00:02:42,430
and other defamatory content which

37
00:02:42,462 --> 00:02:46,326
is present in this training datasets. But the

38
00:02:46,350 --> 00:02:49,734
problem just don't stop here at this

39
00:02:49,774 --> 00:02:53,446
data leaks. These models are also absorbing and spreading

40
00:02:53,510 --> 00:02:55,782
the societal biases,

41
00:02:55,878 --> 00:02:59,550
stereotype and misinformation, which present in the

42
00:02:59,582 --> 00:03:03,566
massive amount of online data that they trained on. This private

43
00:03:03,670 --> 00:03:07,134
risk go beyond this individual information, and which

44
00:03:07,174 --> 00:03:10,550
includes discrimination and

45
00:03:10,702 --> 00:03:14,918
allowing misinformation to spread like wildfire across the Internet.

46
00:03:15,086 --> 00:03:18,514
So here is the profound paradox that we face.

47
00:03:19,094 --> 00:03:22,958
The discriminate data driving this model's extraordinary

48
00:03:23,006 --> 00:03:26,790
capabilities is also the very source of

49
00:03:26,942 --> 00:03:30,510
geopolitic, the privacy and the human rights of

50
00:03:30,542 --> 00:03:34,734
the societal human being striking a delicate

51
00:03:34,774 --> 00:03:39,294
balance. So let's find a balance, how we can maintain the

52
00:03:39,334 --> 00:03:42,570
capability of the LLM models and we

53
00:03:42,602 --> 00:03:46,138
can make it secure at the same time. So resolving

54
00:03:46,186 --> 00:03:50,354
this paradox requires like walking a tight rope.

55
00:03:50,434 --> 00:03:54,346
Excessively constraining this training data could undermine

56
00:03:54,490 --> 00:03:57,774
the models broad knowledge and hamper the performance.

57
00:03:58,514 --> 00:04:02,946
Yet unchecked this data ingestion possess unacceptable

58
00:04:03,010 --> 00:04:06,154
privacy risk. Technical approaches like data filtering,

59
00:04:06,274 --> 00:04:09,760
differential privacy and synthetic data can help

60
00:04:09,792 --> 00:04:13,664
to mitigate issues. But implementing this massive scale of

61
00:04:13,704 --> 00:04:16,968
the modern language models is computationally

62
00:04:17,016 --> 00:04:20,400
and logically it's very challenging. We may also need

63
00:04:20,432 --> 00:04:24,440
to accept that some calculated privacy trade offs are

64
00:04:24,472 --> 00:04:28,000
unavoidable, at least with this current method. The key

65
00:04:28,032 --> 00:04:31,416
will be developing a governance framework

66
00:04:31,560 --> 00:04:35,624
to assess and manage the risk appropriately

67
00:04:35,664 --> 00:04:39,616
for the different use cases, and this will be a collaborative

68
00:04:39,680 --> 00:04:43,928
effort. So let's talk about the transparency

69
00:04:44,016 --> 00:04:48,160
and the accountability challenges. Even when the privacy

70
00:04:48,232 --> 00:04:51,872
risk are from this training, data are reduced. The large

71
00:04:51,928 --> 00:04:55,200
language model present another major challenge.

72
00:04:55,352 --> 00:04:59,088
That these are the opaque black boxes, and their

73
00:04:59,216 --> 00:05:02,872
complexity makes it impossible to audit what is specific

74
00:05:02,928 --> 00:05:07,196
data influenced given output just to understand

75
00:05:07,300 --> 00:05:11,092
this machine reasoning process behind it. The lack of

76
00:05:11,148 --> 00:05:14,748
transparency fundamentally undermines the ability to

77
00:05:14,796 --> 00:05:18,428
ensure the safe, unbiased operation of the system

78
00:05:18,596 --> 00:05:22,052
and to hold them accountable when things

79
00:05:22,148 --> 00:05:25,900
go south. So techniques like water modeling,

80
00:05:25,972 --> 00:05:29,324
constant decoding and robust monitoring, this all could provide

81
00:05:29,404 --> 00:05:33,346
more visibility into this behavior. But ultimately,

82
00:05:33,450 --> 00:05:37,906
we must find ways to build transparency and auditability

83
00:05:38,090 --> 00:05:41,898
into the system from the ground up to secure

84
00:05:41,986 --> 00:05:45,826
privacy minded development practices. Let's talk about

85
00:05:45,890 --> 00:05:50,322
this devsecops for the responsibility development devsecops

86
00:05:50,378 --> 00:05:53,986
term is just like a dev operations with

87
00:05:54,050 --> 00:05:57,882
security. So when we are tackling this privacy

88
00:05:57,938 --> 00:06:02,286
paradox surrounding this language models, which demands

89
00:06:02,310 --> 00:06:05,646
a multifaceted, holistic solution with the

90
00:06:05,670 --> 00:06:09,726
principal governance framework on the tech side like

91
00:06:09,790 --> 00:06:13,494
this, continue research into privacy preserving

92
00:06:13,534 --> 00:06:17,006
training methods. The new secure machine learning

93
00:06:17,070 --> 00:06:20,374
techniques will be crucial. Perhaps more particularly,

94
00:06:20,414 --> 00:06:24,302
we must embrace like this

95
00:06:24,358 --> 00:06:27,514
devsecop practices that integrate the security,

96
00:06:27,934 --> 00:06:31,454
privacy and ethical AI principle into each and every

97
00:06:31,494 --> 00:06:35,020
phase of the software development cycle and building this LLM

98
00:06:35,052 --> 00:06:38,492
models. And this can be achieved through cross

99
00:06:38,548 --> 00:06:42,532
functional collaboration in parallel. Comprehensive AI

100
00:06:42,588 --> 00:06:46,500
governance framework align the technological development

101
00:06:46,612 --> 00:06:51,036
with the human values must be cooperatively developed by all stakeholders

102
00:06:51,100 --> 00:06:54,692
like firms, policymakers and

103
00:06:54,868 --> 00:06:58,724
the representative from the impacted communities. Only by

104
00:06:58,804 --> 00:07:02,806
combining this edge cutting solution with the rigorous

105
00:07:02,870 --> 00:07:06,126
governance, we can truly unleash the

106
00:07:06,230 --> 00:07:09,238
large language model's immense potential,

107
00:07:09,406 --> 00:07:12,886
while we are holding the privacy and

108
00:07:12,910 --> 00:07:15,754
this non discrimination and human right.

109
00:07:16,454 --> 00:07:20,062
Now, let's discuss about this governance framework

110
00:07:20,118 --> 00:07:23,462
and ethical principles as we stand.

111
00:07:23,558 --> 00:07:27,498
We understand the capabilities of large language models are

112
00:07:27,586 --> 00:07:31,834
incredible, we have all seen it. But we must maintain, we must

113
00:07:31,914 --> 00:07:35,474
remain constantly committed to confront that

114
00:07:35,554 --> 00:07:39,034
the privacy predicament that they present.

115
00:07:39,154 --> 00:07:43,154
Resolving this paradox is an urgent imperative that

116
00:07:43,194 --> 00:07:46,906
will shape the responsible development of the AI and

117
00:07:47,010 --> 00:07:51,010
for the generations to come through continuous innovation, holistic secure

118
00:07:51,042 --> 00:07:54,400
practices and ethical grounded governance

119
00:07:54,562 --> 00:07:58,860
we can place a trail alignment of transformative

120
00:07:58,932 --> 00:08:02,744
AI with the core human values. This makes no mistake,

121
00:08:03,124 --> 00:08:06,892
like path ahead will be immensely

122
00:08:06,948 --> 00:08:10,724
challenging, but our collective principles and

123
00:08:10,764 --> 00:08:14,644
commitment to prioritizing this humanity webping must

124
00:08:14,684 --> 00:08:18,540
light the way forward. And with the dedication and collaboration

125
00:08:18,612 --> 00:08:22,072
across the sectors, we can unlock the language

126
00:08:22,208 --> 00:08:26,648
large language model potential while maintaining

127
00:08:26,816 --> 00:08:30,304
the security at the same time in this digital

128
00:08:30,384 --> 00:08:35,136
world. In the conclusion, I would say together let's

129
00:08:35,200 --> 00:08:36,624
build a safer digital world.

