1
00:00:27,920 --> 00:00:31,186
Before we go ahead, I would like to talk

2
00:00:31,210 --> 00:00:34,898
about myself. I'm a software engineer with over five years

3
00:00:34,986 --> 00:00:38,314
experience developing digital banking solutions

4
00:00:38,394 --> 00:00:41,794
for financial institutions in West Africa, that is Lagos,

5
00:00:41,834 --> 00:00:44,534
Nigeria and Freetown, Sierra Leone.

6
00:00:45,034 --> 00:00:48,602
Personally, I founded two companies in Nigeria. One is

7
00:00:48,618 --> 00:00:52,266
a waste management company called Dustbin Boy, which basically

8
00:00:52,370 --> 00:00:56,124
leverages technology to deliver waste management

9
00:00:56,204 --> 00:01:00,332
services in Lagos, Nigeria. And the second is a music company

10
00:01:00,428 --> 00:01:04,020
that is a record label. Well, because one of my hobbies is

11
00:01:04,052 --> 00:01:07,420
music. So now let's start

12
00:01:07,532 --> 00:01:11,604
by talking about what model observability is. Model observability

13
00:01:11,684 --> 00:01:14,924
is the practice of validating and monitoring MM model performance

14
00:01:15,004 --> 00:01:18,284
and behavior. It involves measuring critical metrics,

15
00:01:18,324 --> 00:01:22,212
indicators, and processes to ensure model work as expected in the production

16
00:01:22,268 --> 00:01:25,758
environment. So simply boot model observability is the process

17
00:01:25,806 --> 00:01:29,166
of validating, evaluating, measuring, monitoring,

18
00:01:29,230 --> 00:01:32,742
and ensuring our model performs the way we expect

19
00:01:32,798 --> 00:01:35,674
it to perform in production environments.

20
00:01:36,174 --> 00:01:40,166
So before we move forward, let's talk about how model observability is

21
00:01:40,190 --> 00:01:44,270
different from model monitoring. While model observability

22
00:01:44,382 --> 00:01:48,470
provides real time insights, model monitoring collects

23
00:01:48,502 --> 00:01:51,940
and analyzes metrics over time. Model monitoring

24
00:01:51,972 --> 00:01:55,436
detects anomalies and trends, while model observability

25
00:01:55,540 --> 00:01:59,940
diagnoses issues within processes. Model observability also

26
00:02:00,012 --> 00:02:03,860
reviews underlying system dependencies and understands

27
00:02:03,892 --> 00:02:07,692
why anomalies occur, while model monitoring ensures model

28
00:02:07,748 --> 00:02:11,444
operates within thresholds and generally focuses on

29
00:02:11,564 --> 00:02:15,836
system health. So why is model observability

30
00:02:15,940 --> 00:02:19,802
important? Why should we practice modeling observability?

31
00:02:19,978 --> 00:02:24,250
Easily on top of this list is transparency, because oftentimes

32
00:02:24,362 --> 00:02:28,538
AI functions as a black box that lacks transparency in its processes.

33
00:02:28,666 --> 00:02:31,794
So model observability is a way to gain transparent

34
00:02:31,834 --> 00:02:35,618
and to shed light into some of these processes. Model observability also

35
00:02:35,666 --> 00:02:39,578
helps in error detection because users may not notice when large

36
00:02:39,626 --> 00:02:43,254
language models like TPT four make mistakes.

37
00:02:43,794 --> 00:02:47,498
By also detecting these mistakes and providing transparencies and

38
00:02:47,546 --> 00:02:51,330
identifying some errors, the credibility of our model

39
00:02:51,442 --> 00:02:55,410
is increased. And by understanding

40
00:02:55,442 --> 00:03:00,018
and getting insights on why errors are occurring, we gain an understanding

41
00:03:00,186 --> 00:03:03,574
of our model and of course, all of this,

42
00:03:04,354 --> 00:03:07,746
which is maintaining visibility and understanding our model.

43
00:03:07,810 --> 00:03:11,962
And, you know, coping with the credibility helps users to

44
00:03:12,138 --> 00:03:15,234
sustain, and it's the trust and helps us to gain

45
00:03:15,274 --> 00:03:17,604
more trust in the AI system.

46
00:03:19,744 --> 00:03:23,712
So now let's talk about why model observability is important

47
00:03:23,848 --> 00:03:27,384
with a practical use case and a practical example

48
00:03:27,544 --> 00:03:31,032
using Google chatbot bad right after its launch,

49
00:03:31,168 --> 00:03:34,248
Bird claimed in a promotional campaign that the James Webb

50
00:03:34,336 --> 00:03:37,432
Space Telescope took the first ever image of an

51
00:03:37,448 --> 00:03:41,168
exoplanet. This was not true, and the consequences of this was

52
00:03:41,216 --> 00:03:45,220
well, customers raised a lot of doubt on the model's

53
00:03:45,252 --> 00:03:48,420
efficiency and well, Google reportedly

54
00:03:48,532 --> 00:03:52,984
lost $100 billion in market value because of this blunder.

55
00:03:54,644 --> 00:03:58,492
So now let's, now that we understand the practical use case of why,

56
00:03:58,588 --> 00:04:02,020
you know, what can happen without model observability, let's, let's talk

57
00:04:02,052 --> 00:04:05,224
about how model observability helps and the benefits

58
00:04:05,644 --> 00:04:08,796
that we get from model observability. Model observability

59
00:04:08,900 --> 00:04:12,604
enables engineer to perform root cause analysis, identify the reasons behind

60
00:04:12,684 --> 00:04:16,042
specific issues. That means it doesn't just generalize the errors and it

61
00:04:16,058 --> 00:04:19,706
doesn't just give us a basic overview of the errors. It helps engineers to go

62
00:04:19,730 --> 00:04:23,714
down into the root cause and helps them understand the specifics behind

63
00:04:23,834 --> 00:04:28,034
any specific issue. So the benefits of this obviously is continuous performance

64
00:04:28,114 --> 00:04:31,794
improvements. It ensures expected behavior in production and

65
00:04:31,834 --> 00:04:34,314
streamlined machine language workflow, of course,

66
00:04:34,354 --> 00:04:38,454
scalability, and it reduces time to resolution.

67
00:04:39,194 --> 00:04:43,090
So what are the components of key components of

68
00:04:43,122 --> 00:04:47,072
machine language observability? We have the event login,

69
00:04:47,248 --> 00:04:51,488
the tracing, model profiling, bias detection, and anomaly

70
00:04:51,536 --> 00:04:55,744
identification. So the Venn login deals with the detailed

71
00:04:55,784 --> 00:04:59,544
logs of model activities. The tracing involves tracking

72
00:04:59,584 --> 00:05:03,952
data through stages. The model profiling involves performance analysis,

73
00:05:04,088 --> 00:05:07,640
while bias detection involves identifying and mitigating biases,

74
00:05:07,752 --> 00:05:11,528
and anomaly identification involves detecting unusual patterns.

75
00:05:11,616 --> 00:05:15,640
So here we have it in the diagram. It shows from the discovery to

76
00:05:15,712 --> 00:05:18,524
analysis to diagnosis and to resolve.

77
00:05:20,894 --> 00:05:24,354
So what are the key challenges that face our model observability?

78
00:05:24,734 --> 00:05:28,194
Here in this presentation, I will talk about data drift,

79
00:05:28,614 --> 00:05:32,046
performance degradation and data quality. So for data

80
00:05:32,110 --> 00:05:35,838
drift, this occurs when the statistical properties of the training data change over

81
00:05:35,886 --> 00:05:39,046
time. It can include covariate shift, which is changes

82
00:05:39,070 --> 00:05:42,118
in inputs, future distributions, and model drift,

83
00:05:42,206 --> 00:05:46,300
which is the changes in the relationship between inputs and target variables.

84
00:05:46,462 --> 00:05:49,920
Causes of these drifts include changes in customer behavior,

85
00:05:50,032 --> 00:05:53,568
shifts in the external environment, demographic changes,

86
00:05:53,616 --> 00:05:56,404
and product updates and upgrades.

87
00:05:57,264 --> 00:06:00,800
Another key challenge, like we said, is the performance degradation,

88
00:06:00,952 --> 00:06:04,624
which is basically over time, as machine learning applications gain more

89
00:06:04,664 --> 00:06:07,952
users, their model performance can decline

90
00:06:08,048 --> 00:06:11,416
due to model overfitting, presence of outliers,

91
00:06:11,520 --> 00:06:15,014
adversarial attacks, and changing data patterns.

92
00:06:16,074 --> 00:06:19,434
Lastly, another key challenge is the quality of the data.

93
00:06:19,594 --> 00:06:22,994
Maintaining consistent data quality in production is challenging

94
00:06:23,034 --> 00:06:26,370
due to reliance on various input factors such as data collection,

95
00:06:26,402 --> 00:06:29,826
method pipelines, storage platforms, and preposition

96
00:06:29,890 --> 00:06:33,754
techniques. Some of the possible issues we can encounter here

97
00:06:33,834 --> 00:06:36,546
are the missing data labeling errors,

98
00:06:36,690 --> 00:06:39,978
disparate data sources, privacy constraints,

99
00:06:40,066 --> 00:06:43,694
inconsistent formatting, and lack of representativeness.

100
00:06:45,204 --> 00:06:49,504
So now let's move to model observability challenges in large language modules.

101
00:06:49,924 --> 00:06:53,340
Large language modules, otherwise known as LLMs,

102
00:06:53,412 --> 00:06:56,820
face some unique issues. While we have hallucinations, which is,

103
00:06:56,852 --> 00:07:00,348
you know, degenerates, nonsensical or inaccurate

104
00:07:00,396 --> 00:07:04,204
responses, we also have no single ground truth, which is when multiple

105
00:07:04,244 --> 00:07:07,796
possible answers are generated for the same questions, which make

106
00:07:07,820 --> 00:07:11,082
evaluation difficult. The response quality responses may

107
00:07:11,098 --> 00:07:14,642
actually be correct, but relevant or poorly tuned. And we

108
00:07:14,658 --> 00:07:18,770
have instances of jbreaks where, you know, some prompts can bypass security,

109
00:07:18,962 --> 00:07:22,450
leading to harmful outputs. And, you know, the cost

110
00:07:22,482 --> 00:07:25,882
of retraining, because this is because ensuring up

111
00:07:25,898 --> 00:07:29,414
to date responses over time requires expensive retraining.

112
00:07:30,074 --> 00:07:33,214
And these are the issues faced by, you know,

113
00:07:34,314 --> 00:07:38,288
so now that I've spoken about the challenges that model observability

114
00:07:38,376 --> 00:07:41,912
phases in large language models, let's talk about some

115
00:07:41,928 --> 00:07:45,824
of the evaluation techniques for large language models. A tailored model

116
00:07:45,864 --> 00:07:50,016
observability strategy can help address challenges and improve evaluation.

117
00:07:50,160 --> 00:07:53,784
Some of the common techniques that we use include user feedback,

118
00:07:53,904 --> 00:07:56,924
embedding, visualization, prompt re engineering,

119
00:07:57,224 --> 00:08:00,864
retrieval systems, and fine tuning. With user feedback, we collect

120
00:08:00,904 --> 00:08:03,788
and access reports, unbiased and misinformation.

121
00:08:03,976 --> 00:08:07,788
With embedding visualization, we compare response and prompt embeddings

122
00:08:07,836 --> 00:08:10,972
for relevance. With prompt engineering, we test various

123
00:08:11,028 --> 00:08:14,196
prompts to enhance performance and detect issues with

124
00:08:14,220 --> 00:08:18,172
the retrieval systems. We ensure our LLMs fetch correct information

125
00:08:18,268 --> 00:08:21,940
from relevant sources, and with fine tuning, we adjust the model with

126
00:08:21,972 --> 00:08:25,424
domain specific data instead of full retraining.

127
00:08:26,844 --> 00:08:30,522
So now let's go to challenges in computer vision. Here we have

128
00:08:30,538 --> 00:08:33,866
the image drift, which is, you know, changes in image properties over time,

129
00:08:33,930 --> 00:08:37,706
like lightning and background. We have occlusion, which, as seen in

130
00:08:37,730 --> 00:08:40,810
this diagram, objects blocking the primary objects,

131
00:08:40,842 --> 00:08:44,730
leading to misclassification, lack of annotated samples, which is

132
00:08:44,762 --> 00:08:48,466
difficult in finding labeled images for training. And we have,

133
00:08:48,490 --> 00:08:52,322
you know, of course, sensitive use cases where the cost of making errors

134
00:08:52,418 --> 00:08:56,282
and making mistakes is disastrous, like in medical diagnosis

135
00:08:56,338 --> 00:08:59,698
and self driving cars. So some

136
00:08:59,746 --> 00:09:04,378
components which are some ways to address challenges in computer vision.

137
00:09:04,546 --> 00:09:08,242
Well, on top of the list is the morning turning metrics, which means we

138
00:09:08,258 --> 00:09:11,494
should measure image quality and model performance.

139
00:09:11,954 --> 00:09:15,578
We should also use specialized workforce, which means we should

140
00:09:15,626 --> 00:09:18,674
involve domain experts in the labeling process and

141
00:09:18,714 --> 00:09:22,282
the quality of our edge devices. We should also monitor the

142
00:09:22,298 --> 00:09:26,062
most devices like camera and sensors in real time, the label quality.

143
00:09:26,118 --> 00:09:30,542
Also we should ensure high quality labeling with automation and regular

144
00:09:30,598 --> 00:09:34,302
reviews. And lastly domain adaptation. We should indicate

145
00:09:34,358 --> 00:09:38,154
when to fight two models based on data divergence.

146
00:09:40,094 --> 00:09:44,086
So some monitoring techniques that we use in machine language observability,

147
00:09:44,270 --> 00:09:47,902
we have the standard ML metrics like recall, precision, f one s

148
00:09:47,918 --> 00:09:51,600
called MAE. We have the Lad language model metrics

149
00:09:51,632 --> 00:09:55,240
like the blue material SIDAR for automated scoring.

150
00:09:55,352 --> 00:09:59,544
We also use human feedback, custom metrics and ARLHF

151
00:09:59,584 --> 00:10:02,744
for human based assessment. And we also have the computer

152
00:10:02,784 --> 00:10:06,416
vision metrics like the mean average position intersection over

153
00:10:06,480 --> 00:10:09,744
union, panoptic quality for tasks like object detection,

154
00:10:09,824 --> 00:10:13,464
classification and segmentation. So let's talk about

155
00:10:13,504 --> 00:10:16,784
explainability techniques in standard ML systems.

156
00:10:16,904 --> 00:10:20,296
Explainability is the capability of observability tools to

157
00:10:20,320 --> 00:10:24,016
provide clear, understandable insight into system behavior and performance,

158
00:10:24,120 --> 00:10:27,164
enabling stakeholders to easily interpret and act on the data.

159
00:10:27,744 --> 00:10:31,264
There are two techniques one can use to interpret the models decision

160
00:10:31,384 --> 00:10:34,832
making process. Here we have the sharp and the line. The sharp, which is

161
00:10:34,848 --> 00:10:38,536
the sharply addictive explanations, computes the sharply value of each vector

162
00:10:38,640 --> 00:10:42,712
of each feature, indicating future importance for global and local explainability,

163
00:10:42,888 --> 00:10:46,432
while the lime the local interpretable model, agnostic explanations

164
00:10:46,568 --> 00:10:50,568
per tubes and p data to generate fake predictions. It then trains the simpler

165
00:10:50,616 --> 00:10:54,472
model of the generated values to measure future importance. Here explainability,

166
00:10:54,568 --> 00:10:58,848
simply put, is the capacity of our observability to generate

167
00:10:58,936 --> 00:11:03,024
insights which can enable us, which we can easily interpret

168
00:11:03,144 --> 00:11:06,496
and act on to enable us make decisions on the data

169
00:11:06,640 --> 00:11:10,468
under our model. So now let's talk about

170
00:11:10,556 --> 00:11:14,092
explainability techniques in large language models. Here we have

171
00:11:14,108 --> 00:11:17,748
the attention based techniques where we visualize which word

172
00:11:17,796 --> 00:11:20,940
the model considers most important in an input sequence.

173
00:11:21,052 --> 00:11:24,300
It is useful in models like charge, DBT bed and t five that use

174
00:11:24,332 --> 00:11:28,228
transformer architecture. We also have the saliency based techniques which

175
00:11:28,356 --> 00:11:31,580
computes gradients with respect to input features to measure

176
00:11:31,652 --> 00:11:35,232
their importance. Masking features and analyzing output relations

177
00:11:35,328 --> 00:11:38,896
can reveal crucial features. So now let's talk

178
00:11:38,920 --> 00:11:42,112
about the explainability techniques in computer vision. Here we

179
00:11:42,128 --> 00:11:45,424
have the integrated gradient, Xari and Gradcam.

180
00:11:45,544 --> 00:11:48,472
I will show you the difference between the three on the next slide. For the

181
00:11:48,488 --> 00:11:52,040
gradcam, it generates a heatmap for CNN models highlighting

182
00:11:52,072 --> 00:11:55,232
important regions by overlaying the heatmap on the original image.

183
00:11:55,368 --> 00:11:59,096
For the integrated gradient, it builds a baseline image and adds features

184
00:11:59,160 --> 00:12:02,652
gradually computing gradients to identify important features for object

185
00:12:02,708 --> 00:12:06,724
prediction, while for the Xari enhances the integrated gradient

186
00:12:06,804 --> 00:12:10,540
by highlighting pixel regions instead of single pixels,

187
00:12:10,652 --> 00:12:14,412
segmenting similar image parties and computing saliency for

188
00:12:14,508 --> 00:12:18,292
each region. So as you can see here, the integrated

189
00:12:18,348 --> 00:12:22,404
gradient, the Xari and the grad camp. So the integrated gradient

190
00:12:22,524 --> 00:12:26,052
basically generates a baseline image and the XCRI,

191
00:12:26,148 --> 00:12:29,640
which is an extension of it. And as you can see, the grad camp generates

192
00:12:29,672 --> 00:12:33,600
an its map which for the for its own.

193
00:12:33,792 --> 00:12:37,520
So let's give a quick

194
00:12:37,552 --> 00:12:41,664
summary of everything we've discussed over before we end

195
00:12:41,704 --> 00:12:45,224
it. We started by talking about model

196
00:12:45,264 --> 00:12:48,024
observability, which is the validation and, you know,

197
00:12:48,144 --> 00:12:51,768
measuring and ensuring the performance of our models.

198
00:12:51,896 --> 00:12:55,268
I will talk about the observability to different from model

199
00:12:55,316 --> 00:12:58,460
monitoring. And then we talked about why observability is important.

200
00:12:58,572 --> 00:13:03,332
And we used Google's use case of Google's

201
00:13:03,348 --> 00:13:06,804
chatbot bad and the wrong information it gave

202
00:13:06,844 --> 00:13:10,524
and the effect on Google. Then we talked about the components of model

203
00:13:10,564 --> 00:13:13,740
observability, which things like which involve things like

204
00:13:13,772 --> 00:13:17,460
events, login bias detection and mobile model profiling.

205
00:13:17,612 --> 00:13:21,012
And then we talked about the key challenges in model

206
00:13:21,068 --> 00:13:24,900
observability in machine language, where we have the

207
00:13:24,932 --> 00:13:28,620
data drift, performance degradation and data quality.

208
00:13:28,772 --> 00:13:33,224
We also talked about the key challenges in light language models like hallucinations,

209
00:13:33,564 --> 00:13:36,940
jbreaks, and we talked about the challenges in

210
00:13:36,972 --> 00:13:40,324
computer vision, which is the occlusion, image drift.

211
00:13:40,444 --> 00:13:44,988
And then we talked about some measuring techniques. And we

212
00:13:45,076 --> 00:13:49,182
finally talked about the explainability and model

213
00:13:49,238 --> 00:13:53,382
observability, which is the degree to which our model

214
00:13:53,518 --> 00:13:57,390
can be explained and to which insights can be gotten

215
00:13:57,502 --> 00:14:01,102
from the model. So finally, let's talk about

216
00:14:01,278 --> 00:14:03,914
future trends in model observability.

217
00:14:04,534 --> 00:14:08,502
Here we have the user friendly XAI, which is developing techniques

218
00:14:08,518 --> 00:14:12,254
to generate simple, understandable explanations. We also have the AI

219
00:14:12,294 --> 00:14:15,720
model fairness, which is using axi to visualize

220
00:14:15,792 --> 00:14:19,784
learned features and detect bias. We also have the

221
00:14:19,864 --> 00:14:23,424
human centric explainability, which is combining insights from

222
00:14:23,464 --> 00:14:27,040
psychology and philosophy for better explainability methods.

223
00:14:27,152 --> 00:14:31,000
And we have the casual AI, which is highlighting why a model uses

224
00:14:31,032 --> 00:14:35,324
particular features for predictions, adding value to explanations and

225
00:14:35,624 --> 00:14:37,364
increasing robustness.

226
00:14:38,664 --> 00:14:43,064
So. So this brings us to the end of the presentation.

227
00:14:43,844 --> 00:14:47,740
Thank you so much again for having me, and bye for

228
00:14:47,772 --> 00:14:49,314
nowhere.

