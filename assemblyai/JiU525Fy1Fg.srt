1
00:00:27,440 --> 00:00:31,356
Hi everyone. I'm Gayathri Shivraj and I'm honored to be a speaker at the

2
00:00:31,380 --> 00:00:34,908
Con 42. I'm a senior program manager at Amazon.

3
00:00:34,956 --> 00:00:38,804
In the fulfillment services, I primarily focus on

4
00:00:38,844 --> 00:00:42,188
program and product excellence to provide best in class seller

5
00:00:42,236 --> 00:00:45,824
experience by optimizing the storage and fulfillment capabilities.

6
00:00:46,804 --> 00:00:50,260
Large language models are a big part of the products we build as we

7
00:00:50,292 --> 00:00:53,764
deal with large datasets of seller communication across

8
00:00:53,844 --> 00:00:55,784
different modalities worldwide.

9
00:00:59,894 --> 00:01:03,446
Before we dive into the details, let's take a quick look at the agenda

10
00:01:03,470 --> 00:01:07,006
for today's presentation. We have a lot of ground to cover,

11
00:01:07,150 --> 00:01:10,982
and I want to ensure we have a structured approach to understanding how

12
00:01:11,118 --> 00:01:15,274
large language models can be leveraged for advanced AI applications.

13
00:01:16,134 --> 00:01:19,366
We will start with an introduction to large language models,

14
00:01:19,510 --> 00:01:22,790
or LLMs. This section will provide a foundational

15
00:01:22,862 --> 00:01:27,240
understanding of what LLMs are, their significance in the field of AI,

16
00:01:27,392 --> 00:01:30,764
and why they have become so prominent in the recent years.

17
00:01:31,424 --> 00:01:35,152
Next, we will dwell into the architecture of LLMs.

18
00:01:35,328 --> 00:01:39,912
We will explore how these models are built, the underlying technologies

19
00:01:40,048 --> 00:01:44,400
that power them, and the key components that make them effective at processing

20
00:01:44,432 --> 00:01:46,364
and generating human like text.

21
00:01:47,544 --> 00:01:50,972
Next, we will talk about methods for leveraging LLMs.

22
00:01:51,128 --> 00:01:54,772
In this section, I will discuss how to use LLMs

23
00:01:54,828 --> 00:01:59,204
effectively by leveraging APIs and interactive playgrounds.

24
00:01:59,364 --> 00:02:03,700
I'll explain how to deploy these models for production use cases,

25
00:02:03,852 --> 00:02:06,384
ensuring scalability and reliability.

26
00:02:06,924 --> 00:02:10,300
Additionally, we will cover how to customize LLMs to

27
00:02:10,332 --> 00:02:14,172
meet specific needs and how to deploy these customized versions,

28
00:02:14,308 --> 00:02:17,436
and how to create and use effective prompts to get

29
00:02:17,460 --> 00:02:19,384
the best results from LLMs.

30
00:02:20,704 --> 00:02:23,888
Next limitations of using LLMs while LLMs

31
00:02:23,936 --> 00:02:27,964
are powerful, they come with their own set of constraints and challenges.

32
00:02:28,264 --> 00:02:32,760
We will cover the limitations, potential pitfalls, and ethical considerations

33
00:02:32,872 --> 00:02:35,924
when deploying these models in real world scenarios.

34
00:02:36,344 --> 00:02:39,392
And finally, with the real world success stories,

35
00:02:39,568 --> 00:02:43,200
we will look at some of the real world success stories. I will

36
00:02:43,232 --> 00:02:46,896
share case studies and examples of how organizations,

37
00:02:46,960 --> 00:02:50,556
including Amazon, have successfully implemented LLMs

38
00:02:50,700 --> 00:02:55,132
to solve complex problems, improve efficiency,

39
00:02:55,268 --> 00:02:57,624
and enhance customer experiences.

40
00:03:00,764 --> 00:03:04,052
Let's take a closer look at what LLMs are, their key

41
00:03:04,108 --> 00:03:08,504
components, capabilities, and applications across various industries.

42
00:03:08,964 --> 00:03:12,588
What are LLMs? Large language models are advanced

43
00:03:12,636 --> 00:03:15,924
AI models trained on extensive datasets to understand

44
00:03:16,044 --> 00:03:19,714
and generate human like language. These models are

45
00:03:19,754 --> 00:03:23,098
designed to perform a wide range of language related tasks,

46
00:03:23,186 --> 00:03:26,562
making them incredibly versatile and powerful tools in the

47
00:03:26,578 --> 00:03:29,930
field of AI. The key components of

48
00:03:29,962 --> 00:03:33,442
LLMs transformer architecture at

49
00:03:33,458 --> 00:03:36,574
the heart of LLMs is the transformer architecture.

50
00:03:36,994 --> 00:03:41,106
This architecture allows the model to handle long reach dependencies

51
00:03:41,290 --> 00:03:44,402
in text, making it possible to generate coherent

52
00:03:44,458 --> 00:03:47,294
and contextual relevant responses.

53
00:03:48,014 --> 00:03:51,134
Pre trained parameters LLMs come with

54
00:03:51,174 --> 00:03:55,310
millions, billions, and sometimes trillions of pre trained parameters.

55
00:03:55,462 --> 00:03:58,878
These parameters are learned from vast amounts of text data,

56
00:03:58,966 --> 00:04:02,174
enabling the model to understand language nuances

57
00:04:02,254 --> 00:04:05,942
and context. Finally, fine tuning

58
00:04:06,078 --> 00:04:09,726
after pre training, LLMs can be fine tuned on specific

59
00:04:09,790 --> 00:04:13,176
data sets to adapt to particular tasks or domains.

60
00:04:13,350 --> 00:04:16,876
This fine tuning process tailors the model's capabilities to

61
00:04:16,900 --> 00:04:19,464
meet specific needs more effectively.

62
00:04:21,404 --> 00:04:25,904
Capabilities of LLMs content generation and comprehensions

63
00:04:26,604 --> 00:04:30,356
LLMs excel at text generation, allowing them to create human

64
00:04:30,420 --> 00:04:34,444
like text based on given prompts. They can also perform question

65
00:04:34,524 --> 00:04:38,784
answering, providing relevant and accurate responses to user queries.

66
00:04:39,504 --> 00:04:42,528
Language processing these models are capable

67
00:04:42,576 --> 00:04:45,144
of language translation and summarization,

68
00:04:45,264 --> 00:04:48,288
breaking down language barriers, and condensing information

69
00:04:48,416 --> 00:04:52,384
into more digestible formats. Analysis and

70
00:04:52,424 --> 00:04:56,072
recognition LLMs can analyze sentiments,

71
00:04:56,208 --> 00:04:59,440
classify text, and recognize named entities,

72
00:04:59,632 --> 00:05:04,728
making them useful for tasks such as sentiment analysis, text classification,

73
00:05:04,896 --> 00:05:08,900
and named entity recognition applications

74
00:05:08,972 --> 00:05:12,668
across industries. Software development in software

75
00:05:12,716 --> 00:05:15,780
development, LLMs facilitate code summarization,

76
00:05:15,892 --> 00:05:20,024
natural code search, and automated documentation generation.

77
00:05:20,324 --> 00:05:24,268
These capabilities enhance developer productivity and

78
00:05:24,316 --> 00:05:28,532
improve code understanding. Learning LLMs

79
00:05:28,588 --> 00:05:32,292
can serve as education tools for learning programming languages.

80
00:05:32,428 --> 00:05:35,828
They provide personalized feedback and tutoring to

81
00:05:35,876 --> 00:05:39,418
aspiring developers and support the creation of attractive

82
00:05:39,466 --> 00:05:43,054
coding exercises and adaptive learning platforms.

83
00:05:51,874 --> 00:05:55,370
Thank you, Gayathri hello, I'm Satya

84
00:05:55,482 --> 00:05:59,570
and thank you for the opportunity to speak at Conf 42.

85
00:05:59,682 --> 00:06:03,214
I am a senior engineer at Amazon in the brand protection organization.

86
00:06:03,834 --> 00:06:08,026
In my role, my team and I build systems to protect the integrity of

87
00:06:08,210 --> 00:06:12,094
our website by monitoring and preventing infringements and counterfeits.

88
00:06:14,234 --> 00:06:17,706
We focus on preventing the misuse of IP of brands,

89
00:06:17,770 --> 00:06:21,786
ensuring that our customers can shop with confidence in

90
00:06:21,810 --> 00:06:25,818
building these systems. We leverage multiple LLMs and multimodal

91
00:06:25,946 --> 00:06:27,934
LLMs to accomplish this goal.

92
00:06:30,194 --> 00:06:33,514
Let's quickly delve into the architecture of transformers.

93
00:06:43,534 --> 00:06:47,302
The transformers architecture represents a significant breakthrough

94
00:06:47,438 --> 00:06:50,854
in the field of natural language processing and serves as a backbone

95
00:06:50,894 --> 00:06:53,514
for many state of the art LLMs.

96
00:06:53,814 --> 00:06:58,698
The transformer architecture is a game changer in the field of LLMs

97
00:06:58,826 --> 00:07:02,298
because it overcomes the limitations of previous architectures like

98
00:07:02,386 --> 00:07:06,498
RNN's and LSTM networks. The problem

99
00:07:06,546 --> 00:07:10,610
with RNN's is they tend to forget important information from

100
00:07:10,682 --> 00:07:14,058
earlier in a sequence because they process words

101
00:07:14,106 --> 00:07:18,458
one by one, making them very slow and less accurate for very

102
00:07:18,506 --> 00:07:21,814
long ticks. Lstms improve memory retention,

103
00:07:22,154 --> 00:07:26,338
but they are still slow since they also handle words

104
00:07:26,386 --> 00:07:30,250
sequentially. Some of the key components

105
00:07:30,282 --> 00:07:34,122
of transformers architecture are self attention mechanism,

106
00:07:34,298 --> 00:07:37,450
positional encodings, feed forward neural networks,

107
00:07:37,522 --> 00:07:41,106
encoder decoder, multi head attention layer normalization,

108
00:07:41,170 --> 00:07:44,410
and residual connections. The fundamental parts are

109
00:07:44,442 --> 00:07:47,746
basically the encoder and the decoder. And it

110
00:07:47,770 --> 00:07:51,574
all started when a paper was released in 2017

111
00:07:51,874 --> 00:07:55,258
which had the title attention is all you need, and that's from

112
00:07:55,386 --> 00:08:00,170
Washirani and others. And going

113
00:08:00,242 --> 00:08:01,894
into the architecture of LLMs,

114
00:08:03,114 --> 00:08:06,714
the self attention mechanism think of the self

115
00:08:06,754 --> 00:08:10,362
attention mechanism as a way for the model to look at all the

116
00:08:10,378 --> 00:08:13,454
words in a sentence and decide which ones are most important.

117
00:08:14,594 --> 00:08:17,898
And for example, in the sentence

118
00:08:17,946 --> 00:08:22,250
the cat sat on the mat. The word cat may pay more

119
00:08:22,282 --> 00:08:25,722
attention to words sat and mat because

120
00:08:25,858 --> 00:08:27,214
they are closely related.

121
00:08:28,314 --> 00:08:32,618
Now, the second component of transformers

122
00:08:32,666 --> 00:08:36,634
architecture, the positional encoding, is just because

123
00:08:36,674 --> 00:08:39,906
the transformers don't naturally understand the words in which

124
00:08:39,930 --> 00:08:43,642
they are ordered. Positional encoding helps the model know

125
00:08:43,658 --> 00:08:47,232
the position of each word in the sentence. Then comes the

126
00:08:47,248 --> 00:08:51,296
feed forward neural networks the

127
00:08:51,360 --> 00:08:54,640
feed forward neural networks after applying the self

128
00:08:54,672 --> 00:08:58,376
attention and positional encoding, the process tokens are

129
00:08:58,480 --> 00:09:01,776
passed through the free forward neural networks within each layer

130
00:09:01,800 --> 00:09:05,512
of the transformer. These feed forward neural networks

131
00:09:05,688 --> 00:09:09,040
consist of multiple fully connected layers with nonlinear

132
00:09:09,072 --> 00:09:11,402
activation functions, for example,

133
00:09:11,568 --> 00:09:14,862
relu, and they enable the model to

134
00:09:14,878 --> 00:09:18,190
learn complex patterns and representations from the input

135
00:09:18,222 --> 00:09:22,494
data. Then the

136
00:09:22,534 --> 00:09:26,390
next part is the encoder decoder. The encoder component

137
00:09:26,462 --> 00:09:30,566
is basically responsible for translating the input sequence.

138
00:09:30,670 --> 00:09:34,310
It processes the input sequence while the decoder generates the output

139
00:09:34,342 --> 00:09:35,074
sequence.

140
00:09:38,514 --> 00:09:41,402
The next one is the multi head.

141
00:09:41,498 --> 00:09:45,730
Attention. To enhance the learning capabilities

142
00:09:45,802 --> 00:09:48,694
of the LLM and capture different types of information,

143
00:09:48,994 --> 00:09:52,414
transformers typically employ multi head attention mechanisms.

144
00:09:52,954 --> 00:09:58,474
This feature allows the model to focus on certain parts of the input

145
00:09:58,514 --> 00:10:02,106
sentence, simultaneously enhancing its understanding

146
00:10:02,130 --> 00:10:05,170
of the text. For example, in a translation task,

147
00:10:05,322 --> 00:10:08,698
one part of the model might focus on nouns, while the

148
00:10:08,706 --> 00:10:12,894
other part focuses on verbs. These points of focus

149
00:10:13,794 --> 00:10:16,654
can be referred as heads.

150
00:10:18,234 --> 00:10:22,134
Then the last part of it is the layer normalization and residual connections

151
00:10:22,594 --> 00:10:26,202
to stabilize the training process and ensure that

152
00:10:26,218 --> 00:10:29,750
the model learns efficiently by

153
00:10:29,782 --> 00:10:32,582
allowing information to flow smoothly between the layers.

154
00:10:32,718 --> 00:10:35,974
This layer normalization technique is used to normalize the

155
00:10:36,094 --> 00:10:36,954
weights.

156
00:10:45,094 --> 00:10:48,606
Pre trained parameters are the numerical values associated

157
00:10:48,670 --> 00:10:52,270
with the connections between neurons in the neural network architecture

158
00:10:52,302 --> 00:10:56,510
of an LLM. These parameters represent the learned knowledge and

159
00:10:56,542 --> 00:11:00,202
patterns extracted from the training data during the pre training

160
00:11:00,258 --> 00:11:03,826
phase. As the model processes the input text,

161
00:11:03,970 --> 00:11:07,642
it adjusts its parameters, that is, weights and biases,

162
00:11:07,818 --> 00:11:11,914
to minimize a predefined loss function, such as cross entropy

163
00:11:11,954 --> 00:11:14,494
loss, applied to the pre training objective,

164
00:11:15,474 --> 00:11:18,970
the components of pre trained parameters are

165
00:11:19,122 --> 00:11:22,330
word embeddings, parameters that represent the

166
00:11:22,362 --> 00:11:25,860
initial numerical representations of word words or subwords

167
00:11:25,892 --> 00:11:29,564
in the vocabulary. Word embeddings capture semantic

168
00:11:29,604 --> 00:11:33,196
similarities between words based on their contextual usage in the

169
00:11:33,220 --> 00:11:37,004
training data transformer layers parameters

170
00:11:37,044 --> 00:11:41,424
associated with the multiple layers of the transformer architecture used in LLMs.

171
00:11:41,764 --> 00:11:45,492
These layers include self attention mechanisms and feed forward

172
00:11:45,588 --> 00:11:49,564
neural networks. Output layer parameters

173
00:11:49,684 --> 00:11:53,588
parameters of the output layer, which map the final hidden

174
00:11:53,636 --> 00:11:57,812
states of the model to predictions for specific tasks

175
00:11:57,948 --> 00:12:00,864
such as classification and task generation.

176
00:12:03,684 --> 00:12:07,252
Fine tuning is the process of adjusting the parameters

177
00:12:07,308 --> 00:12:10,788
of a pre trained large language model to a specific task or

178
00:12:10,836 --> 00:12:14,636
domain. Although pre trained language models possess vast

179
00:12:14,700 --> 00:12:18,704
language knowledge, they lack specialization in specific areas.

180
00:12:19,204 --> 00:12:22,420
Fine tuning addresses this limitation by allowing the

181
00:12:22,452 --> 00:12:26,180
model to learn from domain specific data to be more accurate

182
00:12:26,292 --> 00:12:30,002
and for targeted applications. Some of

183
00:12:30,018 --> 00:12:34,734
the commonly used fine tuning techniques are hyperparameterization.

184
00:12:35,434 --> 00:12:39,194
It is a simple approach that involves manually adjusting the model

185
00:12:39,234 --> 00:12:42,938
hyperparameters such as the learning rate, batch size,

186
00:12:43,066 --> 00:12:47,014
and the number of epochs until you achieve the desired performance.

187
00:12:48,154 --> 00:12:51,474
One or few shot learning enables a model to

188
00:12:51,514 --> 00:12:55,176
adapt to a new task with little task specific data.

189
00:12:55,360 --> 00:12:59,400
In this technique, the model is given one or few examples

190
00:12:59,552 --> 00:13:02,764
during inference time to learn a new task.

191
00:13:03,104 --> 00:13:06,984
The idea behind this approach is to guide the model's predictions by

192
00:13:07,024 --> 00:13:10,444
providing context and examples directly in the prompt.

193
00:13:10,744 --> 00:13:14,296
This approach is beneficial when the task specific label

194
00:13:14,360 --> 00:13:16,724
data is scarce or expensive.

195
00:13:17,944 --> 00:13:21,700
Domain adaptation Domain adaptation is particularly

196
00:13:21,732 --> 00:13:25,484
valuable when you want to optimize the model's performance for a single

197
00:13:25,564 --> 00:13:28,812
well defined task, ensuring that the model excels

198
00:13:28,868 --> 00:13:33,384
in generating task specific content with precision and accuracy.

199
00:13:35,404 --> 00:13:38,772
Now look at how we can leverage large language models in

200
00:13:38,788 --> 00:13:40,464
our day to day life.

201
00:13:41,964 --> 00:13:46,794
So for developers you have multiple

202
00:13:46,834 --> 00:13:50,210
ways to leverage LLMs starting from directly using their

203
00:13:50,242 --> 00:13:53,898
APIs. Are playgrounds with standalone

204
00:13:53,946 --> 00:13:57,218
models to developing your own customized

205
00:13:57,266 --> 00:14:01,214
models for basically

206
00:14:02,234 --> 00:14:05,954
for your own domain or use case and deploying it in your own custom

207
00:14:06,114 --> 00:14:08,574
environments or hosts.

208
00:14:15,214 --> 00:14:19,142
The first easiest way to interact with LLMs

209
00:14:19,198 --> 00:14:23,294
is by using playgrounds or direct

210
00:14:23,334 --> 00:14:27,070
API integration. Here is one such example where you can use

211
00:14:27,142 --> 00:14:31,190
AWS bedrock to load the anthropoclad

212
00:14:31,222 --> 00:14:35,274
v two model and then invoke the model with a specific prompt.

213
00:14:35,734 --> 00:14:39,410
There are other uis are playgrounds

214
00:14:39,482 --> 00:14:42,970
that don't need any coding and you would be able to interact

215
00:14:43,002 --> 00:14:46,426
with those everyone knows about chat, GPT and

216
00:14:46,570 --> 00:14:49,854
AWS. Bedrock also has playground where you can basically

217
00:14:50,154 --> 00:14:53,426
give your prompts and get responses. Some of the bedrock some

218
00:14:53,450 --> 00:14:56,842
of the models that are supported by bedrock are listed here.

219
00:14:56,978 --> 00:15:00,254
You have Jurassic Titan command,

220
00:15:01,114 --> 00:15:04,622
Lama Mistral for text generation. Then for

221
00:15:04,638 --> 00:15:07,990
the image generation there is titan image generator and stability

222
00:15:08,062 --> 00:15:12,278
diffusion from stability AI. There are multimodal

223
00:15:12,326 --> 00:15:16,194
models as well on bedrock like

224
00:15:17,334 --> 00:15:21,274
cloud three, Haiku and cloud three sonnet.

225
00:15:21,654 --> 00:15:24,942
These two are the popular ones and there

226
00:15:24,958 --> 00:15:28,754
is another one from anthropic which is cloud three

227
00:15:29,434 --> 00:15:32,618
opus. And apart from this for the

228
00:15:32,706 --> 00:15:36,658
similarity similar similarity search based use cases you have a

229
00:15:36,666 --> 00:15:40,642
couple of embeddings model embedding based models on bedrock

230
00:15:40,658 --> 00:15:41,374
as well.

231
00:15:46,754 --> 00:15:50,154
This is an example of a screen that is taken

232
00:15:50,194 --> 00:15:54,544
from AWS bedrock. Here is a playground

233
00:15:54,674 --> 00:15:58,276
on cloud based on cloud this

234
00:15:58,300 --> 00:16:02,044
is one of the prompts that you can use and tune some of the parameters

235
00:16:02,084 --> 00:16:05,556
here to get your inference response.

236
00:16:05,740 --> 00:16:09,076
Here is a question and here is the answer. You can format the question such

237
00:16:09,100 --> 00:16:12,460
a way that your answer is well

238
00:16:12,492 --> 00:16:16,420
structured. We'll get into this in the prompt engineering section that

239
00:16:16,452 --> 00:16:20,064
will later be explained by Gayathri in the

240
00:16:20,444 --> 00:16:24,232
upcoming slides. One of the

241
00:16:24,248 --> 00:16:27,384
easier ways that I have discussed is using playgrounds

242
00:16:27,464 --> 00:16:30,404
or APIs. You can use hugging face for it.

243
00:16:31,544 --> 00:16:35,120
Let me show a quick demo of how you can use

244
00:16:35,152 --> 00:16:38,624
hugging face. Here is the model

245
00:16:38,664 --> 00:16:42,568
hub for hugging face. You can see that there are bunch of models

246
00:16:42,736 --> 00:16:46,232
listed on hugging face. Let's search for

247
00:16:46,328 --> 00:16:47,164
Mistral.

248
00:16:50,204 --> 00:16:52,864
Let's go with the Mistral seven b instruct model.

249
00:16:53,604 --> 00:16:57,944
Here is an example of the playground that they have. This is a serverless one.

250
00:16:58,404 --> 00:17:02,304
It's free. It can be used for experimentation.

251
00:17:03,244 --> 00:17:05,264
It is hugging face.

252
00:17:06,924 --> 00:17:11,716
It gives you an opportunity to test out a few models before

253
00:17:11,780 --> 00:17:13,744
using it for production use cases.

254
00:17:15,433 --> 00:17:18,945
Even though this is free, they throttle you based on

255
00:17:18,969 --> 00:17:21,973
the API key that you provide.

256
00:17:23,833 --> 00:17:27,493
It cannot be used for production use cases because you will not be able to

257
00:17:28,393 --> 00:17:31,533
get the guarantee on availability.

258
00:17:33,673 --> 00:17:36,921
They give you options around deploying the model as

259
00:17:36,937 --> 00:17:40,434
a dedicated endpoint. Here's one such option where you can deploy

260
00:17:40,474 --> 00:17:43,818
the model. If it is a standalone model and

261
00:17:43,866 --> 00:17:47,402
you wanted a standalone version of the model, then you can deploy it on

262
00:17:47,418 --> 00:17:51,854
your one of the cloud service providers.

263
00:17:53,154 --> 00:17:57,054
Here are the costs and you can choose one of the instance types and

264
00:17:57,514 --> 00:18:00,974
this is a very seamless integration with the endpoint.

265
00:18:02,314 --> 00:18:06,044
You'll be charged based on the usage per hours.

266
00:18:06,234 --> 00:18:09,880
There is another way where you could. You could take

267
00:18:09,912 --> 00:18:13,552
control of the you could take control of the

268
00:18:13,568 --> 00:18:17,616
host as well. If you want to deploy it on your AWS sagemaker

269
00:18:17,640 --> 00:18:21,324
accounts for your service or for your application, you can do that as well.

270
00:18:23,064 --> 00:18:26,584
They they provide you the code on how to deploy

271
00:18:26,624 --> 00:18:28,964
it. This is one such example.

272
00:18:31,504 --> 00:18:35,056
They also give options to deploy it in

273
00:18:35,120 --> 00:18:38,288
Azure and Google Cloud. They have.

274
00:18:38,376 --> 00:18:42,144
They also give options to train the model

275
00:18:42,224 --> 00:18:43,564
or fine tune the model.

276
00:18:49,664 --> 00:18:53,272
Here is a serverless inference for prototyping. This is

277
00:18:53,288 --> 00:18:56,776
an image segmentation kind of use case and this

278
00:18:56,800 --> 00:19:01,600
is a very simple code that you can use to call

279
00:19:01,632 --> 00:19:05,084
any model host run

280
00:19:05,704 --> 00:19:06,724
hanging face.

281
00:19:10,144 --> 00:19:13,936
This is the code for deploying the hugging face model directly on to

282
00:19:13,960 --> 00:19:18,664
your AWS account on the sagemaker or

283
00:19:18,704 --> 00:19:22,408
you can choose to deploy it. This is the same code that

284
00:19:22,496 --> 00:19:24,364
you get once you click this button.

285
00:19:26,764 --> 00:19:30,624
This is a this is what we have seen in the demo

286
00:19:31,524 --> 00:19:35,564
and you have other option of deploying using AWS SageMaker

287
00:19:35,604 --> 00:19:39,516
Studio to find the foundational models from

288
00:19:39,540 --> 00:19:42,740
hugging face or from the other repositories and then

289
00:19:42,772 --> 00:19:46,380
deploy it on the sagemaker instance or train it or evaluate

290
00:19:46,492 --> 00:19:49,676
compare it with other models. They have good tools

291
00:19:49,700 --> 00:19:50,544
to do that.

292
00:19:53,164 --> 00:19:56,540
You can also deploy the customized version according to your

293
00:19:56,692 --> 00:19:59,644
domain or fine tune version according to your domain.

294
00:19:59,764 --> 00:20:03,492
AWS bedrock offers you easier

295
00:20:03,588 --> 00:20:07,436
ways to fine tune it based

296
00:20:07,460 --> 00:20:09,624
on the foundation models that you choose.

297
00:20:11,324 --> 00:20:14,784
They also allow you to custom import a model,

298
00:20:15,164 --> 00:20:18,868
but that is supported just for Mistral, Flanti, five and Lama.

299
00:20:18,916 --> 00:20:19,784
As of now,

300
00:20:22,484 --> 00:20:25,340
you can also write your own custom inference code,

301
00:20:25,452 --> 00:20:28,828
write your own get your own model artifacts

302
00:20:28,876 --> 00:20:32,060
and deploy it using on a gpu

303
00:20:32,092 --> 00:20:36,144
or on a CPU or AWS inferential chips

304
00:20:36,924 --> 00:20:40,264
along with the custom images that AWS provides,

305
00:20:42,324 --> 00:20:44,584
and then host it yourself.

306
00:20:46,624 --> 00:20:50,392
And this is a sample inference code that you can use to

307
00:20:50,448 --> 00:20:52,004
deploy your own custom model.

308
00:20:54,384 --> 00:20:58,204
Now let's talk about some of the limitations of standalone LLMs.

309
00:20:58,624 --> 00:21:02,512
First, LLMs can sometimes produce content that is inaccurate

310
00:21:02,568 --> 00:21:05,856
or completely fabricated, known as hallucinations.

311
00:21:06,040 --> 00:21:10,200
This can be problematic, especially in applications requiring precise

312
00:21:10,272 --> 00:21:14,354
and reliable information. Secondly, LLMs struggle

313
00:21:14,394 --> 00:21:18,026
with providing up to date information because they are trained on data

314
00:21:18,090 --> 00:21:20,814
available up to a certain cutoff point.

315
00:21:21,274 --> 00:21:24,546
Any developments or changes that occur after this point

316
00:21:24,650 --> 00:21:27,254
won't be reflected in their responses.

317
00:21:28,274 --> 00:21:31,426
Another challenge is that general purpose LLMs often

318
00:21:31,490 --> 00:21:35,654
have difficulty handling domain specific queries effectively.

319
00:21:36,154 --> 00:21:39,224
They might not have the specialized knowledge needed for

320
00:21:39,264 --> 00:21:43,164
specific industries or fields without further customization.

321
00:21:44,224 --> 00:21:47,564
Limited contextual understanding is also a concern.

322
00:21:48,104 --> 00:21:51,736
LLMs may not always grasp the full context of complex

323
00:21:51,800 --> 00:21:55,472
queries or conversations, leading to responses that are

324
00:21:55,528 --> 00:21:59,184
off target or incomplete. Ethical and

325
00:21:59,224 --> 00:22:02,608
bias issues are significant as well. These models

326
00:22:02,656 --> 00:22:06,580
can sometimes produce biased or ethically questionable

327
00:22:06,652 --> 00:22:10,504
outputs reflecting biases present in the training data.

328
00:22:11,404 --> 00:22:15,284
Fine tuning large LLMs to improve their performance

329
00:22:15,444 --> 00:22:19,700
for specific tasks requires substantial computational resources,

330
00:22:19,772 --> 00:22:22,744
which can be costly and time consuming.

331
00:22:23,204 --> 00:22:27,220
Lastly, handling of potentially sensitive data underscores

332
00:22:27,252 --> 00:22:29,864
the importance of stringent data governance.

333
00:22:34,864 --> 00:22:38,280
For the limitations that were discussed in the previous slide, we can have

334
00:22:38,312 --> 00:22:42,120
a system called rag to reduce the problems

335
00:22:42,152 --> 00:22:45,960
caused by the hallucinations. What is Rac rag

336
00:22:45,992 --> 00:22:49,224
is basically a advanced AI approach that combines the strength

337
00:22:49,264 --> 00:22:51,964
of retrieval systems with generative models.

338
00:22:52,544 --> 00:22:56,354
It aims to enhance the capabilities of LLMs by grounding

339
00:22:56,544 --> 00:23:00,534
generated responses in factual information retrieved from

340
00:23:00,574 --> 00:23:03,394
knowledge basis. How does a rack work?

341
00:23:04,614 --> 00:23:07,846
It has two components, retrieval component and generative component.

342
00:23:07,910 --> 00:23:11,254
The retrieval system features relevant documents or

343
00:23:11,334 --> 00:23:15,222
pieces of information from a predefined knowledge base based on

344
00:23:15,238 --> 00:23:18,910
the user's query. Techniques such as

345
00:23:19,022 --> 00:23:23,100
keyword matching, semantic search, or vector based retrieval ensures

346
00:23:23,172 --> 00:23:26,876
accurate and contextually relevant information is being retrieved.

347
00:23:27,060 --> 00:23:30,784
The second component is a generative component.

348
00:23:32,004 --> 00:23:35,668
The generative model typically in LLM uses these retrieved

349
00:23:35,716 --> 00:23:39,244
information from the retrieval component to generate

350
00:23:39,284 --> 00:23:42,104
coherent and contextually enriched responses.

351
00:23:42,804 --> 00:23:46,252
This integration allows the LLM to provide answers that are

352
00:23:46,268 --> 00:23:49,948
not fluently, that are not only fluent, but all but also

353
00:23:50,076 --> 00:23:51,384
contextually relevant.

354
00:23:54,004 --> 00:23:56,944
What are the coming to the benefits of Frag?

355
00:23:57,564 --> 00:24:00,184
The first one is the improved accuracy.

356
00:24:01,684 --> 00:24:04,824
By incorporating retriever retrieved factual information,

357
00:24:05,404 --> 00:24:08,804
Rag significantly reduces the likelihood of generating incorrect

358
00:24:08,844 --> 00:24:12,500
responses. And by providing

359
00:24:12,572 --> 00:24:16,240
more context from the information that is retrieved by the retrieval component,

360
00:24:16,372 --> 00:24:20,072
you can have more contextual relevance on

361
00:24:20,088 --> 00:24:21,444
the responses that you get.

362
00:24:24,424 --> 00:24:27,520
Like the limitation that was discussed earlier, the knowledge cut

363
00:24:27,552 --> 00:24:31,256
off. You can use the rag. You can populate

364
00:24:31,280 --> 00:24:34,792
the rag with up to date information to get the up to

365
00:24:34,808 --> 00:24:38,376
date knowledge and ask queries based on that up to

366
00:24:38,400 --> 00:24:39,244
date knowledge.

367
00:24:48,364 --> 00:24:51,236
And how do you implement a Rag?

368
00:24:51,380 --> 00:24:54,748
Typically it contains four steps. First one is selecting a

369
00:24:54,756 --> 00:24:57,924
knowledge database. This is a company's internal

370
00:24:57,964 --> 00:25:01,748
database. You can have it as a vector database. You can have it as a

371
00:25:01,876 --> 00:25:06,324
keyword store or anything where you can comprehensively

372
00:25:06,484 --> 00:25:10,700
put all the documents that are relevant to your company or

373
00:25:10,772 --> 00:25:13,932
for the domain. Then next step is the data preparation.

374
00:25:14,068 --> 00:25:17,836
You clean up the data, have the data structured,

375
00:25:18,020 --> 00:25:21,396
choose a good storage solution where you can

376
00:25:21,420 --> 00:25:24,144
efficiently retrieve the data on demand.

377
00:25:24,844 --> 00:25:29,220
So techniques such as vector based search are

378
00:25:29,332 --> 00:25:32,772
techniques to store the knowledge as embeddings

379
00:25:32,828 --> 00:25:36,244
would help a lot in this particular step.

380
00:25:36,544 --> 00:25:39,896
There are custom solutions available in the market like

381
00:25:39,920 --> 00:25:44,480
AWS open search and AWS document

382
00:25:44,512 --> 00:25:47,856
DB to store these documents. There is another

383
00:25:47,960 --> 00:25:51,608
system called Pinecone which is a popular

384
00:25:51,736 --> 00:25:52,924
vector database.

385
00:25:54,824 --> 00:25:58,280
You can index all the documents that are relevant to your company's

386
00:25:58,312 --> 00:26:01,792
knowledge into that vector database as embeddings using one

387
00:26:01,808 --> 00:26:06,000
of the FIAS storage techniques

388
00:26:06,192 --> 00:26:09,320
using the fyess engine and then use KNN to basically retrieve those

389
00:26:09,352 --> 00:26:13,224
documents. Then third part is the retrieval develop

390
00:26:13,264 --> 00:26:16,672
the retrieval system. This system, usually we aim

391
00:26:16,768 --> 00:26:20,176
to be very fast, so we try to

392
00:26:20,320 --> 00:26:24,016
do a KNN search on the database.

393
00:26:24,200 --> 00:26:27,964
Typically a KNN search or you can probably do a semantic search

394
00:26:28,344 --> 00:26:31,872
or a keyword based search. The retrieval system has to be

395
00:26:31,888 --> 00:26:35,474
fast to give the documents more relevant documents

396
00:26:36,014 --> 00:26:39,834
so that you can plug it into the LLM as part of its context.

397
00:26:40,214 --> 00:26:44,166
The fourth is the combining the retrieval responses and

398
00:26:44,350 --> 00:26:48,606
adding it as a input query to the LLM before forming

399
00:26:48,630 --> 00:26:56,390
your question. So apart

400
00:26:56,422 --> 00:27:02,270
from the knowledge limitations that are also typically cost concerns and around

401
00:27:02,302 --> 00:27:06,054
the LLMs, typically these LLMs are resource

402
00:27:06,094 --> 00:27:09,118
intensive. They require high computational requirements.

403
00:27:09,246 --> 00:27:13,262
For example, the Lama three 8 billion parameter model

404
00:27:13,318 --> 00:27:16,502
and 70 billion parameter model. You would need

405
00:27:16,518 --> 00:27:22,846
a minimum of g 512 x large for the 8 billion parameter model and p

406
00:27:22,870 --> 00:27:26,078
large for the 70 billion parameter model. And you're looking at

407
00:27:26,086 --> 00:27:29,544
a cost of $7 per hour for the twelve x largest

408
00:27:29,664 --> 00:27:33,096
and $37 for the 24 x largest.

409
00:27:33,240 --> 00:27:37,016
And they have four to eight Nvidia

410
00:27:37,040 --> 00:27:40,004
GPU's of different configurations.

411
00:27:40,504 --> 00:27:43,744
And second, you are looking at a high cost and

412
00:27:43,784 --> 00:27:47,284
the maintaining maintenance of this knowledge basis.

413
00:27:48,504 --> 00:27:51,840
Typically if your knowledge base is huge like we

414
00:27:51,872 --> 00:27:55,660
have it on my team, we have a

415
00:27:55,852 --> 00:27:59,724
huge knowledge base of infringements of around 20 billion documents.

416
00:27:59,804 --> 00:28:03,412
Sorry, my bad, 2 billion documents that costs

417
00:28:03,428 --> 00:28:07,468
us around million dollars a year. So unless

418
00:28:07,516 --> 00:28:11,780
you choose some optimized ways of storing these documents, like IVF

419
00:28:11,812 --> 00:28:15,784
flat format or IVF product

420
00:28:16,724 --> 00:28:20,076
quantization techniques applied

421
00:28:20,100 --> 00:28:23,812
to the document are choosing it as indexing strategies.

422
00:28:23,988 --> 00:28:27,664
While indexing will help a lot in reducing the cost,

423
00:28:28,324 --> 00:28:31,604
the third is the operational maintenance costs. The maintenance

424
00:28:31,644 --> 00:28:34,908
of LLMs is a significant factor because you have to scale up the

425
00:28:34,916 --> 00:28:38,028
LLM according to your needs. You have to basically

426
00:28:38,156 --> 00:28:41,868
fine tune it. It also, the fine tuning process

427
00:28:41,916 --> 00:28:45,212
is also kind of slightly expensive because you would need

428
00:28:45,228 --> 00:28:49,084
to procure more hosts for fine tuning and then typically

429
00:28:49,164 --> 00:28:52,664
you run into availability issues.

430
00:28:55,364 --> 00:28:59,224
Some of the cost reduction strategies that we can look at is basically

431
00:28:59,524 --> 00:29:02,884
using if your system, if your

432
00:29:02,964 --> 00:29:06,144
use cases do not warrant for a deployment of

433
00:29:07,844 --> 00:29:11,532
a fine tuned model, then you can use pre trained models and

434
00:29:11,588 --> 00:29:15,034
you can interact with them with APIs and other

435
00:29:15,074 --> 00:29:18,402
offerings by the cloud service providers. Typically they charge you

436
00:29:18,418 --> 00:29:22,930
by the request so you don't have to bear the upfront cost of hosting

437
00:29:22,962 --> 00:29:25,214
it and keeping it alive.

438
00:29:27,034 --> 00:29:30,506
Then you can also leverage foundation models of

439
00:29:30,610 --> 00:29:34,042
model offerings by cloud service provider like AWS, bedrock and

440
00:29:34,058 --> 00:29:37,690
Sagemaker. They have a good set of popular models where

441
00:29:37,722 --> 00:29:43,498
you can directly use it without having to host it yourself. Then you

442
00:29:43,506 --> 00:29:47,694
can optimize a large model into a smaller model

443
00:29:47,994 --> 00:29:51,650
by model distillation, transfer the knowledge of the larger model to a smaller

444
00:29:51,682 --> 00:29:55,014
model, distill that knowledge and then have a smaller model.

445
00:29:55,354 --> 00:29:58,570
Run your request, process your requests

446
00:29:58,642 --> 00:30:03,418
and you can also do quantization by changing the precision

447
00:30:03,466 --> 00:30:06,810
for the model from FP 32

448
00:30:06,842 --> 00:30:10,066
to FP 16, which will bring down the memory.

449
00:30:10,250 --> 00:30:13,642
And you can also prune the model to remove the unnecessary weights

450
00:30:13,658 --> 00:30:16,818
or layers and probably reduce the size of

451
00:30:16,826 --> 00:30:20,178
the model significantly. Then for efficient

452
00:30:20,226 --> 00:30:24,010
resource utilization, you can choose to configure

453
00:30:24,042 --> 00:30:27,370
auto scaling, automated scale out

454
00:30:27,402 --> 00:30:31,210
and scale in based on your traffic patterns. And then you can

455
00:30:31,242 --> 00:30:34,690
batch more and then go with an asynchronous invocation

456
00:30:34,762 --> 00:30:38,356
where you don't need the response immediately. You can reserve some of the instances

457
00:30:38,420 --> 00:30:42,196
on Sagemaker and other cloud service providers so that you can procure

458
00:30:42,220 --> 00:30:45,628
the host at a cheaper cost. You can cache your responses. These are some of

459
00:30:45,636 --> 00:30:49,740
the strategies that you can employ then for the data management for

460
00:30:49,772 --> 00:30:53,764
hosting knowledge databases are indexing solutions. You have

461
00:30:53,804 --> 00:30:56,924
IVF flat, IVF PQ. You can prefer these

462
00:30:56,964 --> 00:31:00,316
techniques indexing techniques instead of

463
00:31:00,420 --> 00:31:04,562
storing the documents in HNSW format to

464
00:31:04,578 --> 00:31:07,734
reduce the memory and thereby reducing your costs,

465
00:31:08,274 --> 00:31:14,426
you can also use model cascading. You can deploy the smaller

466
00:31:14,450 --> 00:31:19,426
versions of the model or low precision models at cheaper cost as

467
00:31:19,450 --> 00:31:23,306
a filter. And then for those requests that come

468
00:31:23,330 --> 00:31:26,418
out of these smaller models, you can probably use a complex

469
00:31:26,466 --> 00:31:29,674
model to look at some of the complex patterns.

470
00:31:29,794 --> 00:31:33,170
So just like a filtering technique, you can do the model

471
00:31:33,202 --> 00:31:34,454
cascading as well.

472
00:31:37,794 --> 00:31:41,090
Prompt engineering is about crafting inputs that guide the

473
00:31:41,122 --> 00:31:44,714
model towards the desired output. A good example

474
00:31:44,754 --> 00:31:48,674
of an effective prompt should contain contextual information

475
00:31:48,794 --> 00:31:52,674
about the task. Reference text for the task clear

476
00:31:52,754 --> 00:31:56,266
and complete instruction clear instruction at

477
00:31:56,290 --> 00:32:00,112
the end of the prompt and as an option, you can specify

478
00:32:00,168 --> 00:32:05,536
the format of the output for

479
00:32:05,560 --> 00:32:09,384
a task like text classification. Here is a good example by

480
00:32:09,424 --> 00:32:13,800
anthropic cloud, where you have the description of the task,

481
00:32:13,992 --> 00:32:18,164
reference text for the task, and the classification labels.

482
00:32:21,944 --> 00:32:25,444
Another example of question answer based prompt

483
00:32:26,074 --> 00:32:30,130
you need to provide the instruction reference based text

484
00:32:30,282 --> 00:32:39,162
and at the end you have a clear and concise question for

485
00:32:39,178 --> 00:32:42,370
the text summarization task, you have the text for the

486
00:32:42,402 --> 00:32:46,258
reference text and a clear instruction to summarize

487
00:32:46,426 --> 00:32:48,254
it in the format you choose.

488
00:32:53,294 --> 00:32:57,190
For code generation, a clear instruction on what you want,

489
00:32:57,342 --> 00:33:01,514
and the specific programming language that you need the code to be in.

490
00:33:05,934 --> 00:33:09,326
Large language models offers a myriad of applications for

491
00:33:09,350 --> 00:33:13,462
both software engineers and tech professionals. Let's explore

492
00:33:13,518 --> 00:33:17,790
some of these practical uses. As a software engineer,

493
00:33:17,942 --> 00:33:21,894
automated code generation can significantly speed up development by

494
00:33:21,934 --> 00:33:25,274
handling repetitive tasks and providing code suggestions.

495
00:33:25,574 --> 00:33:29,446
For instance, GitHub copilot can generate code snippets

496
00:33:29,510 --> 00:33:32,590
based on comments. LLMs assist

497
00:33:32,662 --> 00:33:36,214
in code review and debugging by identifying potential bugs

498
00:33:36,254 --> 00:33:40,514
and suggesting fix. Similar to tools like deep code and codeguru,

499
00:33:41,654 --> 00:33:45,006
generating documentation becomes easier with LLMs,

500
00:33:45,110 --> 00:33:48,422
which can create detailed doc strings, readme files,

501
00:33:48,478 --> 00:33:51,804
and API documentation from the code base.

502
00:33:52,624 --> 00:33:56,472
Natural language interfaces allow for more intuitive software

503
00:33:56,528 --> 00:34:00,248
interactions, enabling users to perform tasks using chatbots

504
00:34:00,296 --> 00:34:03,896
or voice assistants. As a tech professional,

505
00:34:04,000 --> 00:34:08,224
technical support is enhanced with AI driven chatbots that

506
00:34:08,264 --> 00:34:11,744
provide first level support, reducing the burden on human teams

507
00:34:11,784 --> 00:34:13,684
and improving response times.

508
00:34:14,424 --> 00:34:18,579
LLMs can analyze data, generate reports, and extract insights

509
00:34:18,611 --> 00:34:22,723
from textual data, aiding in decision making and strategy

510
00:34:22,763 --> 00:34:26,819
formulation. Content creation for marketing documentation

511
00:34:26,891 --> 00:34:30,043
and internal communications can be automated,

512
00:34:30,123 --> 00:34:33,223
streamlining workflows and ensuring consistency.

513
00:34:34,243 --> 00:34:38,643
Training programs powered by LLMs offer personalized learning experiences,

514
00:34:38,723 --> 00:34:42,663
making knowledge sharing more efficient and interactive.

515
00:34:44,174 --> 00:34:47,790
In conclusion, by leveraging LLMs, both software engineers

516
00:34:47,862 --> 00:34:50,518
and tech professionals can enhance productivity,

517
00:34:50,646 --> 00:34:54,314
improve efficiency and innovate in their respective fields.

518
00:34:58,494 --> 00:35:03,110
Coming to how we do it in our brand protection organization

519
00:35:03,302 --> 00:35:06,654
how we leverage LLM we leverage LLM

520
00:35:06,694 --> 00:35:09,754
for trademark and copyright wireless detections.

521
00:35:11,694 --> 00:35:15,622
We analyze the brand names, logos and other intellectual properties

522
00:35:15,678 --> 00:35:19,542
on the product listings and we

523
00:35:19,558 --> 00:35:22,902
try to identify the brands to whom the trademarks belongs

524
00:35:22,918 --> 00:35:26,670
to. We have a corpus of trademarks

525
00:35:26,702 --> 00:35:30,510
and copyrights belonging to the brands for it for,

526
00:35:30,542 --> 00:35:33,326
I believe, trademarks for around 1 million,

527
00:35:33,430 --> 00:35:36,844
sorry, around 100k brands and copyrights

528
00:35:36,884 --> 00:35:40,224
and logos for another 50,000 brands.

529
00:35:41,524 --> 00:35:46,180
For the counterfeit detection we do use LLMs to notice

530
00:35:46,372 --> 00:35:50,460
to recognize subtle differences between genuine and fake product listings

531
00:35:50,652 --> 00:35:54,460
and the lms are also very helpful in

532
00:35:54,532 --> 00:35:57,956
detecting obfuscations like people who use n

533
00:35:57,980 --> 00:36:01,580
one ke instead of Nike and for analyzing

534
00:36:01,612 --> 00:36:05,774
behavioral and analytics of the seller behavior and

535
00:36:09,674 --> 00:36:12,854
some of the real world examples that we have on our site,

536
00:36:14,074 --> 00:36:17,594
the last one being ours. The first three are public now.

537
00:36:17,674 --> 00:36:20,786
I guess everybody

538
00:36:20,850 --> 00:36:24,226
now can see review highlights on the product listings page

539
00:36:24,250 --> 00:36:28,154
of Amazon you see a summary of what

540
00:36:28,194 --> 00:36:33,070
customers say. Then there is a this

541
00:36:33,102 --> 00:36:37,174
is early access for the offered offer to the sellers when

542
00:36:37,214 --> 00:36:38,994
they create listings on Amazon.

543
00:36:40,694 --> 00:36:44,494
The LLMs can generate content based on very

544
00:36:44,534 --> 00:36:47,806
small description of the product that you are selling.

545
00:36:47,910 --> 00:36:51,434
It can fill the gaps or it can fill more details about the product.

546
00:36:51,734 --> 00:36:55,762
Then Amazon pharmacy started

547
00:36:55,818 --> 00:36:59,970
using this LLMs recently to answer questions more

548
00:37:00,002 --> 00:37:03,594
quickly because the LLMs can now look at

549
00:37:03,634 --> 00:37:06,594
the whole corpus of internal wikis and provide more info,

550
00:37:06,714 --> 00:37:10,334
more information on the drugs, and much more.

551
00:37:11,674 --> 00:37:15,674
Quickly then in our space, we reduce the human

552
00:37:15,714 --> 00:37:19,378
audits for detecting infringements by 80% for famous brands like Apple,

553
00:37:19,426 --> 00:37:20,214
et cetera.

554
00:37:22,554 --> 00:37:25,214
For hard to find copyright violations,

555
00:37:26,354 --> 00:37:31,266
we run the LLM for around 1

556
00:37:31,290 --> 00:37:32,574
million products a day.

557
00:37:35,514 --> 00:37:39,082
And the final output coming out of the LLMs

558
00:37:39,178 --> 00:37:43,534
that is flagged for deeper look

559
00:37:43,954 --> 00:37:48,066
is around 20% of those 1

560
00:37:48,090 --> 00:37:50,054
million. So around two hundred k.

561
00:37:53,154 --> 00:37:55,594
And finally, thank you for this opportunity.

