1
00:00:27,654 --> 00:00:31,714
Hi, welcome to conference 42. So here's my session today

2
00:00:32,094 --> 00:00:35,766
for managing service reliability by managing

3
00:00:35,830 --> 00:00:38,886
risks. Risks are very, very vital for any SRE.

4
00:00:38,950 --> 00:00:41,674
So just broadly cover upon this,

5
00:00:43,534 --> 00:00:46,434
going to the next slide here.

6
00:00:46,854 --> 00:00:49,834
So first of all, let's quickly understand,

7
00:00:50,774 --> 00:00:54,674
are your SLR realistic. So if you see

8
00:00:54,714 --> 00:00:58,370
here there's an application which has multiple services and

9
00:00:58,402 --> 00:01:01,986
then there are again sub services within that,

10
00:01:02,050 --> 00:01:05,202
but are really sure the SLO,

11
00:01:05,258 --> 00:01:09,170
the slis defined can be met, what are the risks

12
00:01:09,362 --> 00:01:12,894
associated with this? What would happen if one of the services

13
00:01:13,194 --> 00:01:16,778
get impacted? Or what would happen if the underlying

14
00:01:16,826 --> 00:01:20,896
cloud infrastructure or the underlying infrastructure gets unavailable

15
00:01:20,960 --> 00:01:24,880
for a moment? Do we have an auto healing mechanism

16
00:01:24,952 --> 00:01:29,296
in place? There are so many risks which anyone

17
00:01:29,360 --> 00:01:32,944
is unaware, and thereby there need to be

18
00:01:33,024 --> 00:01:36,256
a very stringent mechanism of putting up

19
00:01:36,320 --> 00:01:39,832
risks within any scenario or any

20
00:01:39,888 --> 00:01:43,912
service. So that's what we define as risk analysis.

21
00:01:44,088 --> 00:01:48,536
It helps you to provide, prioritize and communicate.

22
00:01:48,720 --> 00:01:52,524
So services obviously can be made more

23
00:01:54,304 --> 00:01:57,824
reliable if you know the risks. For example, the example,

24
00:01:57,864 --> 00:02:01,560
the previous diagram, we looked into it like there are so

25
00:02:01,592 --> 00:02:04,684
many different applications, services,

26
00:02:05,704 --> 00:02:09,136
there could be a challenge, or there could be an impact on dependency,

27
00:02:09,200 --> 00:02:12,282
there could be an impact on capacity, there could be

28
00:02:12,298 --> 00:02:16,294
an impact within, on the operations or on the release cycles.

29
00:02:16,594 --> 00:02:20,114
So these are underlying risks and what

30
00:02:20,234 --> 00:02:24,242
they enable us is to understand what

31
00:02:24,298 --> 00:02:28,026
could it take to recover. So meantime, to detect becomes

32
00:02:28,090 --> 00:02:31,746
one of the vital criteria and it

33
00:02:31,850 --> 00:02:35,714
is part of the risk. So when you create a risk analysis, you look

34
00:02:35,754 --> 00:02:36,894
at your different,

35
00:02:38,984 --> 00:02:42,968
different reliability matrices like MTTD,

36
00:02:43,016 --> 00:02:46,656
which is your main time to detect whether it is mean time to

37
00:02:46,680 --> 00:02:50,224
repair, how many percentage of users get impacted,

38
00:02:50,304 --> 00:02:54,296
and then what's the probability of occurrence. So that's

39
00:02:54,320 --> 00:02:57,776
the value of the risk analysis. So how do you

40
00:02:57,800 --> 00:03:01,216
define the risk? So the way we have

41
00:03:01,240 --> 00:03:05,182
thought, or I've seen most of our customers builders

42
00:03:05,238 --> 00:03:09,406
use a risk catalog. So what do you mean by a risk catalog?

43
00:03:09,550 --> 00:03:13,494
So risk catalog basically is a structured way where you prioritize your

44
00:03:13,534 --> 00:03:17,246
risks or you capture all your risks and then you

45
00:03:17,270 --> 00:03:20,174
can definitely look at prioritizing them,

46
00:03:20,254 --> 00:03:24,214
identifying different counters, identifying and brain storming within

47
00:03:24,254 --> 00:03:27,594
the team, what would happen,

48
00:03:27,894 --> 00:03:32,426
looking at the past data. So see what are the various matrices

49
00:03:32,490 --> 00:03:36,354
for those risks. So it is very essential,

50
00:03:36,434 --> 00:03:40,394
a very essential part of your reliability

51
00:03:40,474 --> 00:03:43,682
management to first define your risks.

52
00:03:43,738 --> 00:03:47,650
Create a catalog which could be brainstormed with

53
00:03:47,682 --> 00:03:50,282
multiple team members, multiple teams,

54
00:03:50,338 --> 00:03:53,610
developers, as well as system engineers,

55
00:03:53,722 --> 00:03:57,464
cloud architects. It should include

56
00:03:57,764 --> 00:04:01,068
this catalog across infrastructure and

57
00:04:01,116 --> 00:04:05,700
software. It should also define your key Slis,

58
00:04:05,892 --> 00:04:09,844
your MTTD, MTTR and one

59
00:04:09,884 --> 00:04:13,884
prominent way we have seen or I've seen customers or our teams

60
00:04:13,964 --> 00:04:17,452
do it is to map the user journey. So looking

61
00:04:17,508 --> 00:04:20,988
at from the areas or the points of

62
00:04:21,036 --> 00:04:24,900
interface a user does and tracing it

63
00:04:24,932 --> 00:04:28,980
back straight to the different, different paths

64
00:04:29,012 --> 00:04:32,916
the user takes. So imagining if it's a retail

65
00:04:32,980 --> 00:04:37,068
commerce. So users might be interacting on a

66
00:04:37,236 --> 00:04:40,844
particular, let's say catalog

67
00:04:40,884 --> 00:04:43,988
service which where you could see all different products

68
00:04:44,076 --> 00:04:47,308
and different catalogs. And then the user journey

69
00:04:47,356 --> 00:04:50,362
might be to select a product, add it to a cartridge,

70
00:04:50,448 --> 00:04:54,086
and then finally could be multiple products and then could

71
00:04:54,110 --> 00:04:57,718
be finally checking out and possibly shipping. So you look at the

72
00:04:57,766 --> 00:05:02,006
user journey, looking at what are the different services

73
00:05:02,110 --> 00:05:06,078
where the user gets impacted or user touches upon.

74
00:05:06,246 --> 00:05:09,966
And obviously one of the very key point is to look at

75
00:05:09,990 --> 00:05:13,222
the past incidents. Past incidents give you a

76
00:05:13,238 --> 00:05:17,102
lot of data to define and include it in your

77
00:05:17,278 --> 00:05:21,404
risk catalog. So I'll quickly show you one of the sample risk catalog.

78
00:05:21,444 --> 00:05:25,012
This is from Google SRE.

79
00:05:25,108 --> 00:05:28,916
So here if you could see what does the

80
00:05:28,940 --> 00:05:33,364
risk catalog looks like. For example, starts with a

81
00:05:33,404 --> 00:05:37,664
configuration mishap which reduces capacity.

82
00:05:38,044 --> 00:05:41,580
And then we have MTD typically meantime

83
00:05:41,612 --> 00:05:45,412
to detect. So you could look at systems getting detected this

84
00:05:45,468 --> 00:05:49,392
within 30 minutes, what could be the possible time to repair.

85
00:05:49,448 --> 00:05:53,448
Again, this comes from discussions,

86
00:05:53,536 --> 00:05:56,968
brainstorming, connecting production

87
00:05:57,016 --> 00:06:00,264
incidents, collecting data. So you refine this data

88
00:06:00,424 --> 00:06:03,536
or refine this matrix as you

89
00:06:03,680 --> 00:06:07,336
get more and more information. And then you look at what is the

90
00:06:07,360 --> 00:06:10,920
likely impact of this. Again, this 20% number

91
00:06:10,992 --> 00:06:14,862
comes from experience, looking at teams and

92
00:06:14,878 --> 00:06:18,594
then the likely impact, this is your own release cycles. You could

93
00:06:19,014 --> 00:06:22,230
look at and depict this number. And then you finally look

94
00:06:22,262 --> 00:06:26,214
at what is your incidence per year and how much is your

95
00:06:26,254 --> 00:06:30,594
bad minutes per year. Similarly, if you look at new release,

96
00:06:30,894 --> 00:06:34,238
again when you are ready with the release and

97
00:06:34,406 --> 00:06:37,782
roll back if it is required,

98
00:06:37,918 --> 00:06:42,638
so what's the likelihood. And then we have various parameters

99
00:06:42,686 --> 00:06:46,790
or reliability matrices and

100
00:06:46,822 --> 00:06:52,454
similarly so, and so forth. You could see others like unique

101
00:06:52,494 --> 00:06:56,574
breakdown or unnoticed growth. And possibly

102
00:06:56,614 --> 00:07:00,398
there is a outage within the cloud as well. This is a great

103
00:07:00,446 --> 00:07:04,134
dependency and there could be another scenarios. I mean these

104
00:07:04,174 --> 00:07:08,750
are all likely possible, like operators

105
00:07:08,822 --> 00:07:12,550
are slow or, and so basically what

106
00:07:12,702 --> 00:07:16,838
it gives you is a fair amount of elements.

107
00:07:16,926 --> 00:07:20,550
And this list can be endless. Like you could add many risks

108
00:07:20,702 --> 00:07:23,654
and good thing is that you can refine,

109
00:07:23,734 --> 00:07:27,158
you can prioritize, you can maybe reduce

110
00:07:27,286 --> 00:07:31,526
or include and continuously keep on enhancing or

111
00:07:31,670 --> 00:07:35,654
revising this basis, your own observations

112
00:07:35,774 --> 00:07:39,580
where your performance, this is your own experience and this

113
00:07:39,612 --> 00:07:43,108
is the data points. So a very high level view

114
00:07:43,156 --> 00:07:47,604
of typical risk catalog and

115
00:07:47,724 --> 00:07:51,244
what it helps, ultimately, once you create your risk

116
00:07:51,284 --> 00:07:55,264
catalog, is to rate your risks. Now, as I said,

117
00:07:55,604 --> 00:07:58,772
we saw the earlier one, like bad minutes per year.

118
00:07:58,948 --> 00:08:03,540
But the way to rate, again, this is so

119
00:08:03,572 --> 00:08:06,796
looking at this user journey, again, the user looks to be happy when it

120
00:08:06,820 --> 00:08:09,952
is in blue, but then it all

121
00:08:10,008 --> 00:08:12,928
goes into a challenge when it is in orange.

122
00:08:12,976 --> 00:08:16,704
So there are various parameters, the time to detect, which is your

123
00:08:16,864 --> 00:08:20,560
MTT, which is very, very critical, then time to repair

124
00:08:20,632 --> 00:08:24,560
again is very critical, and then the time between failures.

125
00:08:24,672 --> 00:08:27,864
So as I said, you first define your risks,

126
00:08:28,024 --> 00:08:31,656
prioritize those risks based on this data,

127
00:08:31,840 --> 00:08:35,056
and then you collaborate between the teams. Start with

128
00:08:35,080 --> 00:08:38,574
the estimates, you collect more data and you enter

129
00:08:38,614 --> 00:08:41,954
it. So very, very essential, like time to detect. So we've seen

130
00:08:42,374 --> 00:08:45,174
today particularly observability,

131
00:08:45,334 --> 00:08:49,246
then some more time, like log ingestion,

132
00:08:49,350 --> 00:08:53,354
log management, all those stuff really helps in

133
00:08:53,774 --> 00:08:57,294
reducing your time to detect. And then we have

134
00:08:57,414 --> 00:09:00,478
automation, particularly looking into the time

135
00:09:00,526 --> 00:09:04,292
to repair. So again, the matter of fact is,

136
00:09:04,428 --> 00:09:08,636
as you rate your risk, you can also then start planning

137
00:09:08,700 --> 00:09:12,420
and looking at how do I mitigate this risk? Is this

138
00:09:12,452 --> 00:09:15,788
a risk okay, for me to carry? Is this a risk which

139
00:09:15,836 --> 00:09:19,316
definitely needs to address, which there

140
00:09:19,380 --> 00:09:22,184
needs to be some mechanism to address.

141
00:09:22,724 --> 00:09:26,036
So these are very vital elements. Automation really

142
00:09:26,100 --> 00:09:29,146
helps. And again, time between failures. How do you

143
00:09:29,170 --> 00:09:33,666
make your systems and your releases more stable,

144
00:09:33,770 --> 00:09:37,290
so you can look at maybe embedding some

145
00:09:37,442 --> 00:09:41,114
different, different options, so that you are able to

146
00:09:41,154 --> 00:09:44,586
get this more in a more.

147
00:09:44,690 --> 00:09:48,450
I mean, you are reducing the time between the failures. So far

148
00:09:48,482 --> 00:09:52,002
I've just broadly covered the risk catalogic risk analysis

149
00:09:52,098 --> 00:09:55,262
and why risk is very critical. And then

150
00:09:55,358 --> 00:09:58,702
how do you rate your risk now I could quickly cover

151
00:09:58,758 --> 00:10:02,374
upon accepting risk, and this is a very vital part of your

152
00:10:02,534 --> 00:10:06,118
SRE work, which risk should be prioritized,

153
00:10:06,166 --> 00:10:09,486
where we should focus and put an engineering effort,

154
00:10:09,670 --> 00:10:13,374
or whether we should bring in a larger

155
00:10:13,414 --> 00:10:17,078
team or maybe product management team to just check. This is an

156
00:10:17,126 --> 00:10:20,440
extra engineering effort which is required. Otherwise there

157
00:10:20,472 --> 00:10:24,824
could be an impact on error budget. So here, if you could see there

158
00:10:24,864 --> 00:10:27,724
are certain parameters, for example,

159
00:10:28,104 --> 00:10:32,288
operator accidentally deletes. This is one of the risk and

160
00:10:32,456 --> 00:10:35,896
this creates 129. Is it acceptable? Possibly yes,

161
00:10:35,960 --> 00:10:40,256
because it falls within my error budget. I could

162
00:10:40,440 --> 00:10:44,120
use this. So the ones in blue are possibly the ones

163
00:10:44,152 --> 00:10:47,856
which possibly I could accepted. So just marked it?

164
00:10:47,880 --> 00:10:51,672
Yes. And then the ones in the ones in

165
00:10:51,688 --> 00:10:55,072
the pink or the, or the

166
00:10:55,088 --> 00:10:58,344
peach color shows address which cannot be accepted.

167
00:10:58,504 --> 00:11:02,404
There could be a possibility the first three will not get even accepted.

168
00:11:03,304 --> 00:11:06,968
Basically it definitely might impact

169
00:11:07,016 --> 00:11:10,764
the other budget therefore would have a major impact.

170
00:11:11,104 --> 00:11:14,764
So it is going to require, if required,

171
00:11:14,804 --> 00:11:18,476
I mean, I need to spend some time to look at or put some engineering

172
00:11:18,540 --> 00:11:22,524
effort in order to address or mitigate this

173
00:11:22,564 --> 00:11:26,380
kind of risk. The ones within yellow should not be

174
00:11:26,412 --> 00:11:29,828
accepted. There could be a major issue,

175
00:11:29,876 --> 00:11:33,204
or this could be a major consumer of mirror

176
00:11:33,244 --> 00:11:35,864
budget. So there, there has to be a mechanism.

177
00:11:36,564 --> 00:11:40,596
So this amber color defines that. And it

178
00:11:40,620 --> 00:11:43,264
is something which may need an address,

179
00:11:45,204 --> 00:11:48,836
either maybe as a second priority or a third party,

180
00:11:48,900 --> 00:11:52,388
but it requires addressal. And then the ones which

181
00:11:52,436 --> 00:11:55,588
are in green possibly could be ones which I could accept.

182
00:11:55,676 --> 00:11:58,916
And even though these occur,

183
00:11:58,980 --> 00:12:02,540
they may not significantly consume my error budget. So the

184
00:12:02,572 --> 00:12:06,384
risk mitigation, the risk cataloging is a very vital element.

185
00:12:06,694 --> 00:12:10,558
And we've seen, I've worked with many customers

186
00:12:10,606 --> 00:12:14,110
in helping them build such risk catalogs,

187
00:12:14,142 --> 00:12:18,166
then putting a risk mechanism

188
00:12:18,230 --> 00:12:21,550
or risk analysis to then prioritize this risk.

189
00:12:21,702 --> 00:12:25,454
So one very key element which I also observed is the role

190
00:12:25,494 --> 00:12:28,894
of chaos. So here I've seen

191
00:12:29,014 --> 00:12:33,516
chaos engineering could be a very vital role, because when

192
00:12:33,540 --> 00:12:36,100
you look at estimating the risks for different,

193
00:12:36,172 --> 00:12:40,036
different scenarios or different,

194
00:12:40,100 --> 00:12:44,684
different areas, where this could be useful

195
00:12:44,764 --> 00:12:47,796
is by chaos, you can attempt or

196
00:12:47,900 --> 00:12:52,004
identify certain blind spots. You can attempt some failures,

197
00:12:52,084 --> 00:12:57,172
inject some failures, inject some faults,

198
00:12:57,228 --> 00:13:00,786
and then really understand this risk.

199
00:13:00,930 --> 00:13:04,106
Is it really, there are your observability,

200
00:13:04,210 --> 00:13:07,562
or what is your MTTD? What is your MTBF?

201
00:13:07,618 --> 00:13:11,658
And what is your MTTR? Whether your catalogs go

202
00:13:11,706 --> 00:13:15,554
and one very vital component can also build different

203
00:13:15,714 --> 00:13:19,002
risks or certain unnoticed risk or blind spots,

204
00:13:19,058 --> 00:13:22,114
which you never realized this could even happen.

205
00:13:22,194 --> 00:13:25,792
So definitely chaos engineering comes, and it is

206
00:13:25,808 --> 00:13:29,924
a very vital portion of it. So what can help is

207
00:13:30,624 --> 00:13:34,248
identifying and putting

208
00:13:34,296 --> 00:13:37,680
certain fault areas and then using this as part

209
00:13:37,712 --> 00:13:41,144
of your risk analysis. So a lot of work we could

210
00:13:41,264 --> 00:13:44,520
build, or as an SRE can use the chaos

211
00:13:44,592 --> 00:13:48,204
and many such principles to be able to

212
00:13:50,184 --> 00:13:53,486
refine and build risk catalog, analyze the risk,

213
00:13:53,550 --> 00:13:56,934
and as well as prioritize the risks and then define your

214
00:13:56,974 --> 00:13:59,926
engineering efforts. With this,

215
00:13:59,950 --> 00:14:03,310
I come to an end. It's a fascinating discussion. Hope you

216
00:14:03,342 --> 00:14:07,366
enjoyed my talk and any questions

217
00:14:07,390 --> 00:14:11,878
or any queries, I'm always available. If there is anything

218
00:14:11,926 --> 00:14:15,606
which is needed or any contact needed, please feel free to

219
00:14:15,630 --> 00:14:18,854
reach out. These are my coordinates. Thank you. Thank you very much.

