1
00:00:27,160 --> 00:00:31,348
Hey, everyone, this is Hari Shah. I am a solutions architect

2
00:00:31,436 --> 00:00:35,624
at AWS. Today we are going to talk about chaos engineering.

3
00:00:36,404 --> 00:00:40,372
My goal for this session is to give you a high level introduction

4
00:00:40,548 --> 00:00:44,184
to chaos engineering and talk a little bit about the best practices.

5
00:00:44,564 --> 00:00:48,140
So if you are new to chaos engineering and are curious to learn

6
00:00:48,172 --> 00:00:51,384
about what it is, this is a session for you.

7
00:00:53,044 --> 00:00:57,464
Here is our agenda. I'm going to start by talking about

8
00:00:57,584 --> 00:01:01,792
why we need chaos engineering. Then we'll discuss

9
00:01:01,848 --> 00:01:05,744
a little bit about what it is. Define chaos engineering,

10
00:01:05,904 --> 00:01:09,392
and then wrap it up with how you can implement chaos

11
00:01:09,448 --> 00:01:13,204
practices, chaos engineering practices in your organization.

12
00:01:14,104 --> 00:01:17,608
Let's dive in. So let's start with the why.

13
00:01:17,776 --> 00:01:20,900
Why do we need chaos engineering? Okay,

14
00:01:20,932 --> 00:01:23,304
so this is from December 2022,

15
00:01:24,124 --> 00:01:27,476
when Southwest Airlines, one of the major airlines

16
00:01:27,500 --> 00:01:30,624
in the US, had a huge meltdown.

17
00:01:31,164 --> 00:01:34,492
Between December 21 and December 30, they had

18
00:01:34,508 --> 00:01:37,780
to cancel around 15,000 flights. Remember, this is

19
00:01:37,812 --> 00:01:40,988
during the peak travel period. Some of the days they

20
00:01:41,036 --> 00:01:44,820
had to cancel around 60% of the daily flights

21
00:01:44,852 --> 00:01:48,924
they scheduled. Other than the monetary

22
00:01:49,384 --> 00:01:53,448
impact, which Southwest Airlines reported as around 1.2

23
00:01:53,536 --> 00:01:56,880
billion, this was a major pr

24
00:01:56,992 --> 00:01:59,724
disaster for the airline.

25
00:02:02,264 --> 00:02:06,880
What triggered this whole situation was

26
00:02:06,912 --> 00:02:10,616
a massive windstorm across multiple cities

27
00:02:10,760 --> 00:02:15,042
in the US. So that caused the airline to cancel or delay

28
00:02:15,098 --> 00:02:17,574
many of the flights during that period.

29
00:02:18,274 --> 00:02:22,450
But what made this situation

30
00:02:22,642 --> 00:02:26,418
to a major disaster for Southwest was the backend

31
00:02:26,506 --> 00:02:31,122
crew scheduling system. The scheduling system couldn't

32
00:02:31,178 --> 00:02:34,826
handle the amount of requests for scheduling changes and

33
00:02:34,850 --> 00:02:38,750
it just went down. But airlines are not

34
00:02:38,782 --> 00:02:41,634
the only ones who have outages.

35
00:02:42,494 --> 00:02:46,454
This is a snapshot of a service interruption

36
00:02:46,494 --> 00:02:49,514
that meta had back in 2021.

37
00:02:50,014 --> 00:02:52,914
So in October 2021,

38
00:02:54,374 --> 00:02:57,822
Facebook, Instagram, WhatsApp, all were

39
00:02:57,878 --> 00:03:01,094
down for more than 5 hours. I personally

40
00:03:01,134 --> 00:03:04,110
think this was a good thing for humanity.

41
00:03:04,182 --> 00:03:07,842
We all got a chance to step out of social media, talk to

42
00:03:07,858 --> 00:03:10,054
each other, get some fresh air.

43
00:03:10,594 --> 00:03:14,730
But for meta, this translated to millions

44
00:03:14,762 --> 00:03:18,810
of lost revenue from ads.

45
00:03:19,002 --> 00:03:22,210
If you look at the root cause analysis for

46
00:03:22,242 --> 00:03:24,134
this incident from meta,

47
00:03:25,234 --> 00:03:28,306
you can see that this was caused by human error.

48
00:03:28,490 --> 00:03:31,734
One of the engineers who was performing a routine maintenance,

49
00:03:32,064 --> 00:03:35,864
unintentionally disconnected metadata

50
00:03:35,944 --> 00:03:39,632
centers from Internet. So I have one more

51
00:03:39,728 --> 00:03:43,320
example, and this one is closer to home for

52
00:03:43,352 --> 00:03:46,576
me. So AWS, Amazon Web Services,

53
00:03:46,760 --> 00:03:50,744
had a service interruption for one of its

54
00:03:50,784 --> 00:03:54,312
services called Amazon S three back in February

55
00:03:54,368 --> 00:03:56,204
20, 2017.

56
00:03:57,444 --> 00:04:01,100
Amazon S three, if you're not familiar with it, is an object storage.

57
00:04:01,172 --> 00:04:05,424
And it was one of the early services that AWS launched.

58
00:04:06,284 --> 00:04:09,876
And most of the customers directly or indirectly

59
00:04:09,940 --> 00:04:13,836
uses Amazon S three. So when s three had

60
00:04:13,860 --> 00:04:17,852
an interruption in 2017, many of the big customers

61
00:04:18,028 --> 00:04:22,076
were directly impacted. So this was a big deal.

62
00:04:22,260 --> 00:04:26,012
If you look at the root cause, again, root cause analysis from

63
00:04:26,068 --> 00:04:28,784
AWS, this was a human error.

64
00:04:29,324 --> 00:04:33,476
One of the engineers who was performing

65
00:04:33,620 --> 00:04:36,996
some commands typed something incorrect,

66
00:04:37,140 --> 00:04:40,820
which deleted some of the tables. And these are

67
00:04:40,852 --> 00:04:44,500
just some of the examples. I have a few more here from

68
00:04:44,652 --> 00:04:48,264
companies like Starbucks and Akamai and British Airways.

69
00:04:49,364 --> 00:04:52,780
In fact, these are so common that if you search for

70
00:04:52,852 --> 00:04:56,094
any date with outage, chances are that

71
00:04:56,134 --> 00:04:59,634
you may find one or more such incidents.

72
00:05:00,534 --> 00:05:04,606
And these outages have significant business and financial

73
00:05:04,710 --> 00:05:08,034
impact to organizations. I have some numbers here.

74
00:05:09,134 --> 00:05:12,754
For example, the cost of

75
00:05:13,094 --> 00:05:17,470
an hour of downtime for a business critical application can

76
00:05:17,502 --> 00:05:20,722
be around 1 million. So the question

77
00:05:20,778 --> 00:05:24,762
is why these issues are not being surfaced

78
00:05:24,898 --> 00:05:26,854
during the testing phase.

79
00:05:27,594 --> 00:05:32,146
Companies like Southwest or AWS or Starbucks,

80
00:05:32,330 --> 00:05:35,690
they don't put things into production without proper validation.

81
00:05:35,842 --> 00:05:39,418
So it does go through testing. So why

82
00:05:39,466 --> 00:05:42,294
are they not capturing these issues?

83
00:05:43,194 --> 00:05:46,478
The reason is that when we do the testing,

84
00:05:46,566 --> 00:05:50,674
whether it's unit testing, integration, or regression,

85
00:05:51,054 --> 00:05:55,102
we know the input to the test scenario and the expected

86
00:05:55,198 --> 00:05:58,998
output, right? So what the test case

87
00:05:59,046 --> 00:06:03,594
or test scenario does is to provide that input

88
00:06:03,894 --> 00:06:07,550
and to validate that the actual output matches

89
00:06:07,582 --> 00:06:11,594
the expected output. So if we

90
00:06:11,674 --> 00:06:15,442
plot the input and output into this

91
00:06:15,498 --> 00:06:19,378
framework, the testing focuses

92
00:06:19,466 --> 00:06:23,034
on the top left side, the one in green circle,

93
00:06:23,154 --> 00:06:26,134
where both input and output are known.

94
00:06:27,194 --> 00:06:30,826
But most of the situations that we discussed

95
00:06:30,850 --> 00:06:34,002
earlier in the session, they fall

96
00:06:34,098 --> 00:06:36,974
into the two right side quadrants.

97
00:06:37,884 --> 00:06:40,744
In some scenarios, like Southwest,

98
00:06:41,684 --> 00:06:45,244
the input is known. Southwest probably knew that

99
00:06:45,364 --> 00:06:49,316
weather could cause some interruptions, but they

100
00:06:49,340 --> 00:06:52,264
didn't know the output. They didn't know the impact of that.

101
00:06:53,204 --> 00:06:57,084
In other situations, like AWS and meta, it's very hard

102
00:06:57,124 --> 00:07:00,460
to predict human, the exact human error

103
00:07:00,652 --> 00:07:04,344
and the impact of that particular action.

104
00:07:05,164 --> 00:07:08,436
So in order to dive deep

105
00:07:08,500 --> 00:07:12,996
into the known unknowns and uncover

106
00:07:13,060 --> 00:07:16,344
some of the unknown unknowns, we need a different approach,

107
00:07:17,244 --> 00:07:21,068
regular testing. And this is where chaos

108
00:07:21,116 --> 00:07:25,228
engineering comes into picture. So let's talk about what is chaos

109
00:07:25,276 --> 00:07:28,664
engineering. Let's start with a bit of history.

110
00:07:30,374 --> 00:07:32,994
Chaos engineering came from Netflix.

111
00:07:33,694 --> 00:07:36,958
Netflix was one of the early adopters of cloud.

112
00:07:37,086 --> 00:07:40,314
They moved workloads to AWS in 2008.

113
00:07:41,094 --> 00:07:45,222
What Netflix realized is that in

114
00:07:45,238 --> 00:07:49,078
the cloud, they have to make applications more

115
00:07:49,126 --> 00:07:52,934
resilient to underlying infrastructure

116
00:07:52,974 --> 00:07:56,770
failures. In order to do that, they created a

117
00:07:56,802 --> 00:08:00,978
tool called Chaos monkey. What chaos monkey did

118
00:08:01,146 --> 00:08:04,794
was to run in prediction

119
00:08:04,874 --> 00:08:08,954
and randomly terminate compute instances.

120
00:08:09,074 --> 00:08:13,694
EC, two instances. This was hugely unpopular

121
00:08:15,154 --> 00:08:18,978
in the beginning within Netflix, because many

122
00:08:19,026 --> 00:08:22,624
application teams realized that this is impacting

123
00:08:22,664 --> 00:08:26,224
their workloads in production. But that was actually

124
00:08:26,304 --> 00:08:30,184
intent of this tool. The intent was to

125
00:08:30,224 --> 00:08:34,404
uncover issues in a controlled manner and

126
00:08:34,904 --> 00:08:38,924
move the ownership of building resilient applications

127
00:08:39,624 --> 00:08:42,204
to the application teams.

128
00:08:42,624 --> 00:08:46,384
Now, that worked. Gradually, the resiliency

129
00:08:46,424 --> 00:08:50,592
posture of these applications improved, and Netflix

130
00:08:50,648 --> 00:08:54,044
started creating more tools like that.

131
00:08:55,184 --> 00:08:58,936
They call this simian army. There was

132
00:08:58,960 --> 00:09:03,256
a tool which, for example, simulated availability

133
00:09:03,320 --> 00:09:07,144
zone failure, and there was one which even dropped

134
00:09:07,224 --> 00:09:10,952
the entire region, simulating a doctor

135
00:09:11,008 --> 00:09:14,324
scenario. Netflix open source these tools,

136
00:09:14,624 --> 00:09:17,902
and more and more organizations started

137
00:09:18,038 --> 00:09:22,474
adopting these tools for their workloads.

138
00:09:23,454 --> 00:09:26,998
So Netflix teamed up with some of these early adopters

139
00:09:27,166 --> 00:09:31,634
and created a manifesto

140
00:09:32,374 --> 00:09:34,794
called principles of chaos engineering.

141
00:09:36,134 --> 00:09:40,074
This is how the manifesto defines chaos engineering.

142
00:09:40,414 --> 00:09:44,088
Chaos engineering is the discipline of experimenting

143
00:09:44,176 --> 00:09:48,256
on a system in order to build confidence in the system's capability

144
00:09:48,440 --> 00:09:51,844
to withstand turbulent conditions in production.

145
00:09:52,504 --> 00:09:54,724
Now, let's unpack that a little bit.

146
00:09:55,664 --> 00:10:00,604
So, first, chaos engineering is about experimentation,

147
00:10:01,704 --> 00:10:05,336
not testing. What's the difference

148
00:10:05,440 --> 00:10:09,056
in testing? As we know, both input and outputs are

149
00:10:09,080 --> 00:10:13,642
known, right? So all you're doing is validating the actual output

150
00:10:13,818 --> 00:10:18,094
with the expected output. But in experiments,

151
00:10:18,474 --> 00:10:22,774
the output is unknown. So you start with a hypothesis,

152
00:10:23,114 --> 00:10:26,954
and then you create experiments to validate

153
00:10:26,994 --> 00:10:30,530
your hypothesis and make sure whether your

154
00:10:30,562 --> 00:10:32,774
hypothesis is valid or invalid.

155
00:10:33,314 --> 00:10:37,326
Now, the goal of chaos engineering is

156
00:10:37,350 --> 00:10:41,750
to build confidence in the system's ability to

157
00:10:41,782 --> 00:10:45,534
withstand chaos in production.

158
00:10:45,654 --> 00:10:49,462
Now, that's important. Think of

159
00:10:49,478 --> 00:10:53,674
chaos engineering like a vaccine. You are injecting

160
00:10:54,454 --> 00:10:58,694
a little bit of chaos in a controlled fashion to

161
00:10:58,734 --> 00:11:01,674
build resiliency or to build immunity.

162
00:11:02,104 --> 00:11:03,564
So that's the goal.

163
00:11:05,184 --> 00:11:09,000
Many would think that chaos engineering is all about breaking

164
00:11:09,032 --> 00:11:11,204
things in production, terminating instances,

165
00:11:12,704 --> 00:11:16,884
but that's not the case. Chaos engineering

166
00:11:17,304 --> 00:11:20,656
is all about uncovering chaos, which is

167
00:11:20,680 --> 00:11:24,168
already inherent. It's already there in the system, right.

168
00:11:24,336 --> 00:11:28,244
All you're doing is to perform controlled experiments

169
00:11:28,684 --> 00:11:32,076
to uncover those scenarios so that you can proactively address

170
00:11:32,140 --> 00:11:35,744
them before the actual outage happens.

171
00:11:37,164 --> 00:11:40,676
With that, let's look at how to approach

172
00:11:40,740 --> 00:11:43,824
chaos engineering and perform your experiments.

173
00:11:45,724 --> 00:11:49,572
This diagram shows the high level steps involved

174
00:11:49,748 --> 00:11:53,420
in building your experiments. It all starts

175
00:11:53,452 --> 00:11:56,896
with understanding the steady state behavior

176
00:11:57,000 --> 00:12:00,520
of your application. You need to know what a steady

177
00:12:00,552 --> 00:12:04,456
state looks like for your application before you can build your hypothesis.

178
00:12:04,560 --> 00:12:08,488
Right. So, for this, you need to have a solid observability

179
00:12:08,656 --> 00:12:12,672
framework. Once you observe the steady state,

180
00:12:12,808 --> 00:12:16,696
the next step is to build your hypothesis. This is where

181
00:12:16,840 --> 00:12:20,296
you're building multiple hypotheses that you want to one

182
00:12:20,320 --> 00:12:23,672
day experiment to validate. Once you have the

183
00:12:23,688 --> 00:12:26,768
hypothesis, we will run experiments,

184
00:12:26,936 --> 00:12:30,648
and the goal for the experiment is to

185
00:12:30,816 --> 00:12:34,832
verify the behavior and validate that

186
00:12:34,888 --> 00:12:38,884
against the hypothesis that you have. And if there is a deviation,

187
00:12:39,424 --> 00:12:43,352
this is where you need to act. Make the necessary changes

188
00:12:43,408 --> 00:12:46,534
to improve the resiliency of your,

189
00:12:46,704 --> 00:12:50,842
of your application and then repeat the process. Right. So this is

190
00:12:50,898 --> 00:12:54,146
a cycle, this is a continuous cycle to improve,

191
00:12:54,250 --> 00:12:58,494
to understand your applications resiliency

192
00:12:59,274 --> 00:13:03,522
incrementally, continuously improve them. Let's double

193
00:13:03,578 --> 00:13:07,450
click on each of these faces and understand that

194
00:13:07,522 --> 00:13:10,978
a little bit better. So the first step,

195
00:13:11,066 --> 00:13:14,636
as I mentioned, is all about observing the

196
00:13:14,660 --> 00:13:18,212
steady state of your application. When I say observing

197
00:13:18,308 --> 00:13:21,868
an application, what you need to know is to collect

198
00:13:21,916 --> 00:13:25,252
all the signals from your application and build

199
00:13:25,308 --> 00:13:28,884
an end to end view so that you can understand the state and health

200
00:13:28,924 --> 00:13:33,020
of your system. When talking about observability, there are

201
00:13:33,092 --> 00:13:36,564
three key telemetry data that you need to collect.

202
00:13:36,644 --> 00:13:39,384
One is logs,

203
00:13:40,104 --> 00:13:44,000
logs across your stack. The other is metrics and the

204
00:13:44,032 --> 00:13:47,760
third is traces. Now the key is not

205
00:13:47,832 --> 00:13:50,864
only collecting this, but also correlating or mapping

206
00:13:50,904 --> 00:13:54,920
these signals so that you have an overall understanding of

207
00:13:54,952 --> 00:13:58,904
the steady state and the health of the system. Now once

208
00:13:58,944 --> 00:14:03,096
you have the steady state behavior, you can

209
00:14:03,200 --> 00:14:06,482
start building your hypothesis around the steady

210
00:14:06,498 --> 00:14:10,298
state. Here are a couple of examples of

211
00:14:10,346 --> 00:14:13,770
hypothesis around different goals, right?

212
00:14:13,842 --> 00:14:16,974
So if you want to validate the availability of the system,

213
00:14:17,674 --> 00:14:21,202
a hypothesis can be under certain circumstances

214
00:14:21,298 --> 00:14:25,094
that you want to validate. Customer still has a good time

215
00:14:25,554 --> 00:14:28,134
or the application is still available.

216
00:14:28,754 --> 00:14:31,822
Now for a security hypothesis, it can be

217
00:14:31,878 --> 00:14:35,614
if certain scenario happens, under certain situations,

218
00:14:35,774 --> 00:14:38,974
the security team gets paged or

219
00:14:39,014 --> 00:14:42,630
a certain alarm goes off. Now you build these

220
00:14:42,662 --> 00:14:46,158
high level hypothesis because you don't have a clear understanding of

221
00:14:46,206 --> 00:14:50,270
the output, but you know what the desired behavior looks like,

222
00:14:50,342 --> 00:14:54,834
right? So once you have the hypothesis, you can start planning your experiments.

223
00:14:55,574 --> 00:14:59,298
Now, choosing the right experiments is key to get

224
00:14:59,446 --> 00:15:03,018
most out of the investment that you're putting into

225
00:15:03,066 --> 00:15:06,402
chaos engineering. So start with the most

226
00:15:06,458 --> 00:15:10,050
common scenarios that can impact your

227
00:15:10,082 --> 00:15:14,130
application with the goal of identifying the

228
00:15:14,162 --> 00:15:18,194
expected behavior and improving your applications resiliency

229
00:15:18,354 --> 00:15:21,882
against those failures. Right. Here are some

230
00:15:21,898 --> 00:15:25,070
of the common common

231
00:15:25,102 --> 00:15:28,714
failures. Common scenarios that you can build your experiments around.

232
00:15:29,054 --> 00:15:32,510
Single point of failures identify single point of failures within your

233
00:15:32,542 --> 00:15:35,394
application stack and build your experiments around it.

234
00:15:35,934 --> 00:15:39,434
Excessive load to different components and see how they react.

235
00:15:40,294 --> 00:15:43,886
Introduce artificial latency between components and see the

236
00:15:43,910 --> 00:15:46,954
overall application behavior when such things happen.

237
00:15:47,454 --> 00:15:51,334
Misconfiguration, bugs, etcetera are all common scenarios

238
00:15:51,494 --> 00:15:54,998
to get started. So the end goal for chaos

239
00:15:55,046 --> 00:15:57,914
engineering is to perform these experiments in production.

240
00:15:58,494 --> 00:16:00,714
But for many organization,

241
00:16:01,454 --> 00:16:04,726
starting with running these experiments in

242
00:16:04,750 --> 00:16:08,950
production is a great risk. What I recommend

243
00:16:09,062 --> 00:16:12,590
is to start experiments running

244
00:16:12,622 --> 00:16:15,718
these experiments in the lower environment. Choose a very

245
00:16:15,766 --> 00:16:18,934
limited control scope that you have better handle on,

246
00:16:19,234 --> 00:16:23,162
and run these experiments in lower environments and observe the

247
00:16:23,178 --> 00:16:27,018
behavior. Now I would also highly recommend

248
00:16:27,146 --> 00:16:30,858
adding guardrails to these experiments so that

249
00:16:30,986 --> 00:16:34,490
in case you are seeing an

250
00:16:34,522 --> 00:16:37,994
unexpected behavior in the system, you have a plan to roll it back,

251
00:16:38,074 --> 00:16:41,546
roll back the experiment and get the system back to its previous

252
00:16:41,610 --> 00:16:44,750
state. Now, once you run these experiments, as you

253
00:16:44,782 --> 00:16:48,566
gain confidence, start moving these experiments to production

254
00:16:48,630 --> 00:16:52,550
and start running them. Now once as you gain more and more

255
00:16:52,582 --> 00:16:56,990
confidence, you can increase the scope, add more experiments and

256
00:16:57,182 --> 00:17:00,686
iterate over it and make sure that

257
00:17:00,710 --> 00:17:04,366
you're automating these experiments, because systems

258
00:17:04,430 --> 00:17:08,534
do change, they do evolve. So you have to continuously run these

259
00:17:08,574 --> 00:17:12,260
hypotheses to make sure that the system behavior

260
00:17:12,332 --> 00:17:14,984
is not deviating from your hypothesis.

261
00:17:15,844 --> 00:17:19,172
And the last step is to verify the results

262
00:17:19,228 --> 00:17:22,516
of your experiments and then act upon it.

263
00:17:22,700 --> 00:17:26,548
And in this step, it's critical that you assess

264
00:17:26,716 --> 00:17:30,224
the impact of your findings,

265
00:17:30,524 --> 00:17:34,052
the business impact of your finding, and then

266
00:17:34,188 --> 00:17:37,922
prioritize the findings accordingly.

267
00:17:38,058 --> 00:17:41,174
So this way, if it's, for example, a security

268
00:17:42,314 --> 00:17:45,730
impacting issue, then that gets higher priority and it

269
00:17:45,762 --> 00:17:50,774
needs to be addressed immediately compared to some of the other findings.

270
00:17:51,794 --> 00:17:55,530
I want to wrap up this session by giving you pointers to

271
00:17:55,562 --> 00:17:58,738
some of the tools available to automate your chaos

272
00:17:58,786 --> 00:18:02,734
engineering experiments. If you're on

273
00:18:02,854 --> 00:18:06,646
AWS, if your workloads are running on AWS, AWS has a managed

274
00:18:06,670 --> 00:18:11,286
service called AWS fault Injection service which

275
00:18:11,470 --> 00:18:14,914
allows you to build hypothesis and run experiments.

276
00:18:15,494 --> 00:18:19,470
The great thing about FIS fault injection service is

277
00:18:19,502 --> 00:18:22,606
that it has native integration to many of the AWS

278
00:18:22,630 --> 00:18:26,754
services, so it makes it very easy for you to

279
00:18:27,304 --> 00:18:29,640
build experiments and run experiments.

280
00:18:29,832 --> 00:18:33,104
Similarly, if you're on Azure assure has Azure

281
00:18:33,144 --> 00:18:36,448
K Studio, which you can explore. There is

282
00:18:36,496 --> 00:18:39,320
also a commercial offering called Gremlin,

283
00:18:39,392 --> 00:18:43,024
which is very popular and again allows you to build

284
00:18:43,064 --> 00:18:46,288
hypothesis and run experiments. The last one in

285
00:18:46,296 --> 00:18:49,324
the list here, litmus, is an open source option.

286
00:18:50,064 --> 00:18:53,684
So if you are leaning towards exploring open

287
00:18:53,764 --> 00:18:57,524
source tools to automate your experiments, that's a tool

288
00:18:57,564 --> 00:19:01,212
to consider. All right, that's it.

289
00:19:01,388 --> 00:19:04,484
I hope you found this session useful. Thank you so much for watching.

