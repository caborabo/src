1
00:00:27,200 --> 00:00:30,748
Hello everyone. Today I would like to talk to you about the various recent advancements

2
00:00:30,796 --> 00:00:34,044
in generative AI and how we can leverage generative AI

3
00:00:34,124 --> 00:00:37,644
to unleash the creative potential while mitigating the risks.

4
00:00:37,764 --> 00:00:41,252
So, as a brief outline, here are the various topics that I would like to

5
00:00:41,268 --> 00:00:44,876
discuss today. First of all, I would like to introduce

6
00:00:45,020 --> 00:00:48,972
what exactly generated AI means. And then I would like to move on

7
00:00:48,988 --> 00:00:52,124
to talk about the different types of concrete

8
00:00:52,204 --> 00:00:55,996
impact business impact generative AI has been having in

9
00:00:56,020 --> 00:00:59,982
different industries. And specifically, I would like to focus today

10
00:01:00,068 --> 00:01:03,426
discussion on the deep fake challenge. I'm sure

11
00:01:03,450 --> 00:01:06,882
you have seen some recent news articles about the various issues that

12
00:01:06,938 --> 00:01:10,810
people have been having because of deepfakes. So we'll dive deeper into

13
00:01:10,842 --> 00:01:14,786
that, and then we can talk about various

14
00:01:14,890 --> 00:01:18,346
ways in which we can mitigate these problems. So we can

15
00:01:18,370 --> 00:01:21,954
approach the mitigation from a technical point of view, but also from

16
00:01:22,034 --> 00:01:26,026
a regulation point of view, or from a media and education point

17
00:01:26,050 --> 00:01:29,492
of view. So we'll talk about all of that, and then finally

18
00:01:29,588 --> 00:01:34,396
we'll talk about how all these things can play a role together in

19
00:01:34,420 --> 00:01:37,624
collaboration to achieve the outcomes desired.

20
00:01:38,084 --> 00:01:41,908
So, yeah, let's get into it. So what exactly is generative

21
00:01:41,956 --> 00:01:45,556
AI? In general? AI comprises of many

22
00:01:45,620 --> 00:01:49,668
different sub fields, of which generative AI is the most recent

23
00:01:49,796 --> 00:01:53,506
offshoot. So when we talk about artificial intelligence,

24
00:01:53,620 --> 00:01:58,150
we can categorize artificial intelligence into various buckets

25
00:01:58,342 --> 00:02:02,214
using many different dimensions. For example, we can

26
00:02:02,334 --> 00:02:05,782
look at artificial intelligence and talk about what kind of techniques we

27
00:02:05,798 --> 00:02:08,814
are using to achieve our goals and classify

28
00:02:08,854 --> 00:02:12,206
it that way. For example, you have the general machine learning techniques,

29
00:02:12,350 --> 00:02:15,534
including linear classifiers, nonlinear classifiers,

30
00:02:15,654 --> 00:02:19,310
etcetera, that have been in existence for many, many years before genetic

31
00:02:19,342 --> 00:02:23,052
AI. But you can also talk about deep learning, which uses

32
00:02:23,148 --> 00:02:26,932
artificial neural networks to achieve the same goals. So this is the kind

33
00:02:26,948 --> 00:02:30,748
of classification you can make based on the techniques that we are using. But you

34
00:02:30,756 --> 00:02:34,388
can also classify artificial intelligence based on the type of

35
00:02:34,476 --> 00:02:38,636
data dependencies it has. For example, you have supervised

36
00:02:38,820 --> 00:02:42,252
artificial intelligence, or machine learning that depends on

37
00:02:42,348 --> 00:02:45,632
lots and lots of label data. We call it supervised learning.

38
00:02:45,748 --> 00:02:48,736
But you can also talk about unsupervised learning,

39
00:02:48,840 --> 00:02:52,136
where the algorithms do not need any labeled data,

40
00:02:52,280 --> 00:02:55,680
but you are trying to discover patterns in

41
00:02:55,712 --> 00:02:58,888
that unlabeled data. And then there is a mixture of these

42
00:02:58,936 --> 00:03:02,712
two, which is semi supervised learning that uses a little bit of labeled data,

43
00:03:02,808 --> 00:03:06,520
but also leverages huge amounts of unlabeled data to achieve

44
00:03:06,552 --> 00:03:09,200
various codes. So this is another type of classification.

45
00:03:09,312 --> 00:03:13,044
So the final type of classification that is more pertinent

46
00:03:13,144 --> 00:03:16,612
generative AI is whether we are trying to build a

47
00:03:16,628 --> 00:03:20,180
model that differentiates between various groups of data, which we

48
00:03:20,212 --> 00:03:23,364
call discriminative AI, or we are trying to use

49
00:03:23,404 --> 00:03:27,468
these models to generate new data, which we call generative

50
00:03:27,516 --> 00:03:31,556
AI. So all the new and exciting techniques

51
00:03:31,580 --> 00:03:34,436
that we have been hearing route, such as, you know, LLMs,

52
00:03:34,540 --> 00:03:38,188
and genetic models for images and videos and 3d

53
00:03:38,236 --> 00:03:42,184
artifacts and so on, they are all in the category of generative

54
00:03:42,224 --> 00:03:46,192
AI. So what these models essentially do is they

55
00:03:46,248 --> 00:03:49,608
use a neural network architecture called transformers.

56
00:03:49,736 --> 00:03:53,512
And lots of, lots of data is used to

57
00:03:53,648 --> 00:03:57,280
train these transformer models to create what

58
00:03:57,312 --> 00:04:01,072
we call foundational models. The idea of the foundational models

59
00:04:01,128 --> 00:04:04,608
is that these models learn the statistical properties

60
00:04:04,696 --> 00:04:08,720
of a certain type of data, for example, language data. And then

61
00:04:08,832 --> 00:04:12,416
when you provide prompts to these models, they generate data.

62
00:04:12,480 --> 00:04:15,792
They generate, for example, in the case of LLMs,

63
00:04:15,888 --> 00:04:19,104
they generate text that appears

64
00:04:19,184 --> 00:04:22,564
to be created by a human being.

65
00:04:23,664 --> 00:04:27,736
So, generative AI has an impact on

66
00:04:27,800 --> 00:04:31,272
so many industries than we have time to talk about today. But I would like

67
00:04:31,288 --> 00:04:35,432
to just focus on three of the most important types of industries

68
00:04:35,488 --> 00:04:38,516
that generative AI has been transforming of late.

69
00:04:38,620 --> 00:04:41,916
So the first industry is healthcare.

70
00:04:42,060 --> 00:04:45,708
So, healthcare is challenging in the sense

71
00:04:45,756 --> 00:04:48,852
that it is very expensive to develop new drugs,

72
00:04:48,988 --> 00:04:52,748
and it takes a lot of regulatory clearances for

73
00:04:52,796 --> 00:04:57,116
something to become a product. And by reducing

74
00:04:57,300 --> 00:05:00,532
the search space of problems using generative AI,

75
00:05:00,668 --> 00:05:04,284
the expense that one has to spare to come up with a successful

76
00:05:04,324 --> 00:05:07,764
product is drastically reduced. So the second use

77
00:05:07,804 --> 00:05:11,500
case, and probably one that is more relevant

78
00:05:11,532 --> 00:05:14,624
to today's talk, is media. So, in media,

79
00:05:15,164 --> 00:05:18,788
generative AI helps content creators to create

80
00:05:18,876 --> 00:05:22,388
artifacts using a lot less effort

81
00:05:22,516 --> 00:05:26,044
than it was possible before the advent

82
00:05:26,084 --> 00:05:29,724
of generative AI. So the final industry that

83
00:05:29,884 --> 00:05:32,784
I would briefly allude to is design,

84
00:05:33,124 --> 00:05:36,664
for example, architecture and other creative disciplines like

85
00:05:36,704 --> 00:05:40,304
that, where we have been seeing many, many advancements in the field of generative

86
00:05:40,344 --> 00:05:41,004
AI,

87
00:05:43,664 --> 00:05:47,064
healthcare is one of the most exciting application areas for generative

88
00:05:47,104 --> 00:05:50,720
AI. The reason why genitive AI has a potentially

89
00:05:50,752 --> 00:05:55,024
very big impact on healthcare is due to the nature of drug discovery.

90
00:05:55,104 --> 00:05:58,104
The process of drug discovery is a very time consuming,

91
00:05:58,184 --> 00:06:01,480
laborious, and expensive process. The way the

92
00:06:01,512 --> 00:06:05,304
process, this process usually works is that the scientists would

93
00:06:05,384 --> 00:06:08,976
guess what kind of molecules could potentially make for useful

94
00:06:09,040 --> 00:06:12,184
drugs, and then they will have to manufacture or

95
00:06:12,224 --> 00:06:15,504
create each of these molecules in the lab, and then those molecules

96
00:06:15,544 --> 00:06:19,624
will have to go through various phases of testing and approval before they can

97
00:06:19,664 --> 00:06:23,080
be marketed as useful drugs. So what genetic AI does

98
00:06:23,192 --> 00:06:26,800
is it consumes all the information regarding

99
00:06:26,872 --> 00:06:30,458
the protein structure of various pathogens and naturally

100
00:06:30,506 --> 00:06:33,986
occurring molecules in your body, and also the existing drugs

101
00:06:34,090 --> 00:06:37,674
to create a subset of molecules that have the

102
00:06:37,754 --> 00:06:40,634
best chance of working as useful drugs.

103
00:06:40,714 --> 00:06:43,978
So what then happens is scientists can just purely focus

104
00:06:44,066 --> 00:06:47,354
on this subset of molecules, which is a much smaller

105
00:06:47,394 --> 00:06:50,474
set that they have to work with, and hence reduce the time

106
00:06:50,554 --> 00:06:53,938
and various costs that are involved in developing useful

107
00:06:53,986 --> 00:06:57,844
drugs. The next major application area

108
00:06:57,924 --> 00:07:01,540
that I want to focus on is media. This is

109
00:07:01,572 --> 00:07:05,252
the kind of application area that shows up most frequently in the

110
00:07:05,268 --> 00:07:08,704
news as well, just because of the societal impact that it could have.

111
00:07:10,004 --> 00:07:13,988
We are looking at a picture that is generated by one of these foundation models.

112
00:07:14,116 --> 00:07:17,500
It's called pseudonymnesia, and it ended up winning the Sony

113
00:07:17,532 --> 00:07:21,268
Award for the best picture in photography. Obviously, that award

114
00:07:21,396 --> 00:07:24,562
was withdrawn once the organizers came to

115
00:07:24,578 --> 00:07:28,042
know that it was actually generated by a model and not an individual.

116
00:07:28,138 --> 00:07:31,666
But you can see the kind of impact it could have in

117
00:07:31,690 --> 00:07:35,418
the media, because it is very hard to distinguish, and the

118
00:07:35,426 --> 00:07:38,786
models are getting better and better by the day, and it's becoming harder and

119
00:07:38,810 --> 00:07:43,314
harder to distinguish the content generated by models versus individuals.

120
00:07:43,434 --> 00:07:46,994
So the final focus area, or the application area

121
00:07:47,034 --> 00:07:50,494
that I want to talk about, is design. So in this example,

122
00:07:50,534 --> 00:07:54,022
we are looking at an architecture that is actually generated

123
00:07:54,118 --> 00:07:57,286
by one of the foundational models. So previously,

124
00:07:57,350 --> 00:08:00,726
when an architecture has to come up with a certain type of design,

125
00:08:00,910 --> 00:08:04,566
given the requirements and parameters of a given building, it would take

126
00:08:04,630 --> 00:08:08,206
him or her months and even longer to satisfy

127
00:08:08,270 --> 00:08:11,662
all the conditions and all the requirements that the particular structure

128
00:08:11,718 --> 00:08:15,470
needs. But with generative AI, you can just encode all these parameters

129
00:08:15,502 --> 00:08:19,342
and the requirements and constraints that a particular building needs to

130
00:08:19,358 --> 00:08:23,260
adhere to. And then generative AI, I can create candidate

131
00:08:23,332 --> 00:08:25,828
architectures in a matter of seconds.

132
00:08:25,916 --> 00:08:29,684
So as you can see, it could have a huge impact on the profession

133
00:08:29,724 --> 00:08:33,452
of not just architecture, but other similarly creative fields.

134
00:08:33,588 --> 00:08:37,524
So we've looked at three different examples or

135
00:08:37,684 --> 00:08:41,300
application areas where genetic AI can have a potentially

136
00:08:41,332 --> 00:08:45,276
huge impact, both in a positive and negative manner. But today

137
00:08:45,340 --> 00:08:48,802
I would like to focus more on some of the more

138
00:08:48,938 --> 00:08:53,066
tricky and precarious aspects of generative AI,

139
00:08:53,250 --> 00:08:57,026
specifically in the form of deepfakes. So what exactly are

140
00:08:57,050 --> 00:09:00,866
deepfakes? Deepfakes are essentially hyper realistic

141
00:09:00,930 --> 00:09:05,330
fabrications of content. So what

142
00:09:05,362 --> 00:09:09,602
that means is that the content generated by these models, at first glance

143
00:09:09,738 --> 00:09:13,354
looks like a very genuine human created content.

144
00:09:13,514 --> 00:09:17,938
But if you take a closer look, you will find inconsistencies

145
00:09:18,026 --> 00:09:21,642
that are, by the way, getting harder and harder to detect. The various

146
00:09:21,698 --> 00:09:25,562
problems that this type of content creates is, I mean, you may

147
00:09:25,578 --> 00:09:29,194
have seen some of these articles and some of these examples in the news yourself.

148
00:09:29,314 --> 00:09:32,414
So at an individual level,

149
00:09:32,914 --> 00:09:36,338
this kind of generative, AI created content could lead to

150
00:09:36,466 --> 00:09:39,690
id theft. So someone could imitate your voice and call

151
00:09:39,722 --> 00:09:43,094
your bank and ask the bank to transfer money,

152
00:09:43,134 --> 00:09:46,390
etcetera. So it could, it could cause to, it could cause

153
00:09:46,422 --> 00:09:50,142
many problems to individuals in terms of fraud and id theft.

154
00:09:50,238 --> 00:09:53,254
But also at a more higher macro level,

155
00:09:53,414 --> 00:09:57,118
there could be a lot of negative impact on the society in

156
00:09:57,126 --> 00:10:00,782
the form of misinformation, fake news, etcetera. So,

157
00:10:00,878 --> 00:10:04,310
for example, one of the most recent things that you may have seen in the

158
00:10:04,342 --> 00:10:07,654
news is how OpenAI has used a

159
00:10:07,694 --> 00:10:12,032
certain Hollywood celebrity twice without her consent. So obviously,

160
00:10:12,158 --> 00:10:15,220
as she had not provided her consent, they still

161
00:10:15,252 --> 00:10:19,188
were able to reproduce her voice and have that voice say whatever

162
00:10:19,236 --> 00:10:22,676
they want. So, as you can see, this could lead to

163
00:10:22,700 --> 00:10:26,596
many, many problems in terms of copyrights

164
00:10:26,660 --> 00:10:29,884
and things like that. So here is a quick example of

165
00:10:30,004 --> 00:10:33,636
what deep fake content could look like. It's just a short video,

166
00:10:33,780 --> 00:10:35,224
about 30 seconds.

167
00:10:37,584 --> 00:10:41,044
Now, you see, I would never say these things,

168
00:10:41,624 --> 00:10:45,644
at least not in a public address, but someone else would,

169
00:10:45,944 --> 00:10:49,004
someone like Jordan Peele.

170
00:10:50,104 --> 00:10:51,724
This is a dangerous time.

171
00:10:53,504 --> 00:10:56,856
Moving forward, we need to be more vigilant with what we trust from the

172
00:10:56,880 --> 00:11:00,440
Internet. That's a time when we need to rely on

173
00:11:00,472 --> 00:11:01,844
trusted news sources.

174
00:11:03,324 --> 00:11:07,308
May sound basic, but how we move forward in

175
00:11:07,356 --> 00:11:10,908
the age of information is going

176
00:11:10,916 --> 00:11:13,184
to be the difference between whether we survive.

177
00:11:15,284 --> 00:11:18,740
Yeah. So as you just saw, it's one of

178
00:11:18,772 --> 00:11:22,188
the more popular examples of what deep fakes

179
00:11:22,276 --> 00:11:25,740
and generative AI could do. And this

180
00:11:25,772 --> 00:11:29,052
video is actually a few years old, is, I think, six years old.

181
00:11:29,188 --> 00:11:31,544
So it's still pretty convincing.

182
00:11:32,804 --> 00:11:36,252
But as you know, six years in the field of generative AI is almost like

183
00:11:36,268 --> 00:11:39,620
a lifetime. So the models have improved so much in these

184
00:11:39,652 --> 00:11:43,636
six years that anyone that has access to a reasonably fast computer,

185
00:11:43,660 --> 00:11:47,332
you don't need gps or anything, you can, you just need access to a

186
00:11:47,348 --> 00:11:51,104
reasonably fast computer and a browser, and you don't have to be

187
00:11:51,404 --> 00:11:55,646
technically advanced or anything like that, or be good at imitating, like Jordan

188
00:11:55,670 --> 00:11:59,278
Peele, and you can still produce something like this. So you

189
00:11:59,286 --> 00:12:03,126
can see how quickly this can scale and how bad

190
00:12:03,190 --> 00:12:07,274
the problem of misinformation and fake information

191
00:12:08,014 --> 00:12:11,550
can get. So what can we do about all these problems?

192
00:12:11,582 --> 00:12:15,054
And given the pace at which generative AI is advancing, how can we

193
00:12:15,094 --> 00:12:18,510
keep up with protecting, you know,

194
00:12:18,582 --> 00:12:22,186
ourselves as individuals, but also the society

195
00:12:22,250 --> 00:12:26,442
as a whole? So I would like to propose three

196
00:12:26,498 --> 00:12:30,674
different points of view and three different approaches

197
00:12:30,834 --> 00:12:34,426
that we could potentially use to mitigate

198
00:12:34,530 --> 00:12:37,714
these problems. So the first approach that

199
00:12:37,794 --> 00:12:40,842
I want to talk about is technical approaches.

200
00:12:40,938 --> 00:12:44,978
So just as we are using technology to improve generative AI every

201
00:12:45,026 --> 00:12:48,760
day, we can use the same type of technology to actually

202
00:12:48,872 --> 00:12:51,856
combat deepfakes. So what do I mean by that?

203
00:12:51,920 --> 00:12:55,208
So we have, we already have seen a, quite a

204
00:12:55,216 --> 00:12:58,136
few papers published in the field of detecting deepfakes.

205
00:12:58,240 --> 00:13:01,832
So these can take the form of, you know, the, you know, the forms

206
00:13:01,848 --> 00:13:05,792
that we talked about in the beginning, these could be either supervised or unsupervised.

207
00:13:05,888 --> 00:13:09,664
So in terms of supervised detection of deepfakes,

208
00:13:09,784 --> 00:13:14,604
what we're going to need is lots of labeled data labeled by humans that,

209
00:13:14,964 --> 00:13:18,332
you know, that show which content is human generated

210
00:13:18,388 --> 00:13:22,260
versus which content is AI generated.

211
00:13:22,372 --> 00:13:25,980
So, as we discussed in the beginning, this is going to be

212
00:13:26,092 --> 00:13:29,908
expensive, and it's going to be hard,

213
00:13:30,036 --> 00:13:33,996
even for humans sometimes to distinguish AI generated content from

214
00:13:34,060 --> 00:13:37,476
human generated content just because how far the

215
00:13:37,500 --> 00:13:41,468
models have come. So what else can we do? So we have other deep learning

216
00:13:41,516 --> 00:13:44,704
based methods. For example, we have what we call a factor

217
00:13:44,744 --> 00:13:48,872
method. So what this method does is that instead of looking at the actual

218
00:13:48,968 --> 00:13:52,104
artifact that is created by the generative AI,

219
00:13:52,224 --> 00:13:55,888
it looks at the context in which this artifact is being presented.

220
00:13:55,936 --> 00:13:59,880
Like, the meta information on this generative AI artifacts,

221
00:13:59,952 --> 00:14:03,564
for example, like the video that we have saw or that we have just seen.

222
00:14:04,184 --> 00:14:08,328
You know, we all know it's Obama. But the video itself does not say anything

223
00:14:08,376 --> 00:14:10,960
about that video being of Obama.

224
00:14:11,112 --> 00:14:14,280
But there will be, like, the title of the video on YouTube,

225
00:14:14,312 --> 00:14:17,766
for example, that says something that points to the

226
00:14:17,790 --> 00:14:21,614
fact that this is supposed to be a video of Obama. So the

227
00:14:21,654 --> 00:14:25,542
idea of factor methods is to incorporate information like that

228
00:14:25,718 --> 00:14:28,798
instead of, you know, using hand coded human

229
00:14:28,846 --> 00:14:32,390
generated labels to train models to detect deep

230
00:14:32,422 --> 00:14:36,366
fake content. So, and then there are more technical

231
00:14:36,430 --> 00:14:39,334
approaches. For example, there are new, you know,

232
00:14:39,494 --> 00:14:42,406
new and novel neural networks architectures. For example,

233
00:14:42,470 --> 00:14:45,934
exceptionnet is one of the newer convolutional neural network

234
00:14:46,014 --> 00:14:49,414
variations that lets you detect defect content.

235
00:14:49,534 --> 00:14:53,022
So what this basically does is it modifies

236
00:14:53,118 --> 00:14:57,638
traditional combination neural networks to operate

237
00:14:57,686 --> 00:15:00,870
more effectively, and it also leverages vision,

238
00:15:00,902 --> 00:15:04,422
transformers, etcetera, to create feature extractions,

239
00:15:04,558 --> 00:15:07,860
which can then be used to detect defects. So,

240
00:15:07,892 --> 00:15:11,420
yeah, there are many, many approaches like that, but the general idea

241
00:15:11,492 --> 00:15:15,276
is that is to leverage the same type of technology

242
00:15:15,420 --> 00:15:18,948
to detect these deep fakes. So, but we cannot just

243
00:15:18,996 --> 00:15:23,212
focus only on technical approaches because of its

244
00:15:23,268 --> 00:15:26,492
inherent limitations. We need to have a more comprehensive

245
00:15:26,588 --> 00:15:30,624
approach that involves other stakeholders as well. So one of

246
00:15:30,964 --> 00:15:34,096
one such big stakeholder is media.

247
00:15:34,240 --> 00:15:37,712
So what do we mean by that? So, as we, as we

248
00:15:37,728 --> 00:15:41,600
have been seeing more and more of this fake content, right? So the media

249
00:15:41,672 --> 00:15:44,720
also has a responsibility to fact check, for example,

250
00:15:44,792 --> 00:15:48,336
if some video, like the one we have seen shows up on social

251
00:15:48,400 --> 00:15:52,080
media or something like that, the media cannot just assume

252
00:15:52,112 --> 00:15:55,776
or publish this. They have to kind of fact check and look at the

253
00:15:55,800 --> 00:15:59,360
origin of the video and do their due

254
00:15:59,392 --> 00:16:03,008
diligence to establish that it comes from legitimate sources.

255
00:16:03,136 --> 00:16:06,640
So we call this first party verification. But it's

256
00:16:06,672 --> 00:16:10,056
not just on the media. It's a combined effort both by the media as

257
00:16:10,080 --> 00:16:13,160
well as the consumers of media. So what do we. What do we mean by

258
00:16:13,192 --> 00:16:16,976
that? So here we refer to individuals who are consuming

259
00:16:17,000 --> 00:16:20,552
the media. So it is, the general public would

260
00:16:20,568 --> 00:16:24,472
be well served to not consume anything that they

261
00:16:24,488 --> 00:16:27,770
come across on the Internet, because anybody can post anything on the Internet, right?

262
00:16:27,872 --> 00:16:31,874
So it is a good idea for individuals who are consuming the media

263
00:16:32,294 --> 00:16:37,254
to kind of rely only on authentic

264
00:16:37,294 --> 00:16:40,794
sources and not attach the same

265
00:16:41,174 --> 00:16:44,714
level of weight to the content

266
00:16:45,094 --> 00:16:49,982
that they see, for example, on social media, because it's

267
00:16:50,038 --> 00:16:53,638
not necessarily clear what the source of the content that you see on social

268
00:16:53,686 --> 00:16:57,784
media is. And finally, there is the regulatory aspect

269
00:16:57,824 --> 00:17:01,352
of it. In fact, there have been many, many advancements

270
00:17:01,488 --> 00:17:05,176
over the second half of 2023 and during the

271
00:17:05,200 --> 00:17:08,752
first few months of 2024, particularly because of the election that

272
00:17:08,768 --> 00:17:11,992
is coming up. So, in fact, there have been already 14

273
00:17:12,088 --> 00:17:18,120
states that have introduced some form of legislation that

274
00:17:18,152 --> 00:17:20,004
address the problem of defects.

275
00:17:21,554 --> 00:17:24,986
So we talked about how various regulatory frameworks are being put

276
00:17:25,010 --> 00:17:29,322
into place to detect deepfakes and make sure that the media that

277
00:17:29,458 --> 00:17:31,994
the society is consuming is authentic,

278
00:17:32,034 --> 00:17:35,466
etcetera. But as you might have noticed

279
00:17:35,490 --> 00:17:39,498
in the previous slide, many of these regulatory frameworks depend

280
00:17:39,586 --> 00:17:43,650
on some kind of mechanism to track the authenticity

281
00:17:43,722 --> 00:17:48,190
of the content that is being put out there. So this

282
00:17:48,262 --> 00:17:51,590
turns out to be a non trivial issue to solve.

283
00:17:51,702 --> 00:17:55,414
So when it comes to establishing the identity

284
00:17:55,454 --> 00:17:59,102
and authenticity of content, there are two main

285
00:17:59,158 --> 00:18:02,366
ways or two main aspects that we need to talk about.

286
00:18:02,470 --> 00:18:06,470
The first one is provenance, and the second one is verification.

287
00:18:06,582 --> 00:18:10,526
So what exactly is prominence? So, prominence just talks about the

288
00:18:10,590 --> 00:18:14,854
origin of the content. So it answers questions such as who created

289
00:18:14,974 --> 00:18:18,382
this content? When was it created and how was it created?

290
00:18:18,438 --> 00:18:22,174
And who owns it? Etcetera. All these questions fall under the purview

291
00:18:22,294 --> 00:18:26,350
of problems. So verification is a related

292
00:18:26,462 --> 00:18:29,782
but a different concept. So what verification

293
00:18:29,838 --> 00:18:33,190
talks about is, is this content original or modified

294
00:18:33,262 --> 00:18:36,854
or copied in some form? Is it, is it authentic? Is it,

295
00:18:36,894 --> 00:18:40,554
is it, is it real or fake? Or is this accurate

296
00:18:40,594 --> 00:18:44,458
or inaccurate? Or is this consistent? Or are there internal

297
00:18:44,506 --> 00:18:48,186
inconsistencies between the content that

298
00:18:48,210 --> 00:18:50,690
is supposed to be the same, but not really?

299
00:18:50,842 --> 00:18:55,490
So these, all these aspects fall under the purview of verification.

300
00:18:55,642 --> 00:19:00,194
So there are some open source industry collaborations,

301
00:19:00,354 --> 00:19:04,598
such as C two PA, which stands for

302
00:19:04,746 --> 00:19:08,246
coalition for content provenance and authenticity. And, you know,

303
00:19:08,270 --> 00:19:12,318
there are, there are other, you know, industry bodies like that that are

304
00:19:12,366 --> 00:19:16,038
trying to build a commonly accepted format

305
00:19:16,126 --> 00:19:19,582
and structure for metadata that needs to

306
00:19:19,598 --> 00:19:23,198
be attached to various forms of content, which can then

307
00:19:23,326 --> 00:19:26,814
be examined by end users and end consumers by

308
00:19:26,854 --> 00:19:30,830
using various tools, which are also expected to be open, open source.

309
00:19:30,942 --> 00:19:34,270
But the key problem with these type of

310
00:19:34,302 --> 00:19:37,782
approaches is that someone who is motivated enough can

311
00:19:37,838 --> 00:19:41,006
actually mess with the cryptographic signatures.

312
00:19:41,030 --> 00:19:44,694
They can mess with all this metadata that we're talking about that is supposed to

313
00:19:44,734 --> 00:19:47,982
establish the authenticity that the provenance and the

314
00:19:47,998 --> 00:19:51,646
veracity, or, you know, the verification of the content.

315
00:19:51,790 --> 00:19:55,758
So one way to solve this problem is blockchain. So blockchain

316
00:19:55,806 --> 00:19:58,562
or other distribution, distributed ledger technologies, as you know,

317
00:19:58,738 --> 00:20:02,778
depend not on a source of truth that is centralized,

318
00:20:02,906 --> 00:20:06,178
but they establish the source of truth via

319
00:20:06,226 --> 00:20:09,962
consensus in a distributed form. So by

320
00:20:10,098 --> 00:20:14,090
leveraging, you know, blockchain technologies to

321
00:20:14,122 --> 00:20:18,330
incorporate this information, this meta information on

322
00:20:18,362 --> 00:20:21,970
various types of content in a distributed fashion, we have

323
00:20:22,002 --> 00:20:25,516
a more, much better chance of coming to a

324
00:20:25,540 --> 00:20:29,668
consensus on the provenance and the authenticity

325
00:20:29,756 --> 00:20:34,572
of digital content. So we talked about technological

326
00:20:34,628 --> 00:20:38,292
approaches, we talked about media consciousness,

327
00:20:38,348 --> 00:20:42,468
and also regulatory approaches that can all address the problem

328
00:20:42,596 --> 00:20:46,660
of deepfakes. But what we need to keep in mind is any

329
00:20:46,692 --> 00:20:49,500
of these approaches on a standalone basis,

330
00:20:49,572 --> 00:20:53,674
or even when they are all in motion, even if they're operating

331
00:20:53,794 --> 00:20:57,666
in their own silos, it's very hard to achieve the

332
00:20:57,690 --> 00:21:00,946
final, ultimate goals that we all desire,

333
00:21:01,050 --> 00:21:04,162
which is establishing, you know, the authenticity of content

334
00:21:04,258 --> 00:21:08,258
and having only authentic information out there

335
00:21:08,346 --> 00:21:11,930
for people to consume, etcetera. So in order

336
00:21:11,962 --> 00:21:15,186
to, you know, in order for all these approaches to

337
00:21:15,210 --> 00:21:18,736
work in tandem and achieve the desired goals necessary,

338
00:21:18,850 --> 00:21:21,304
and collaboration is very important.

339
00:21:22,084 --> 00:21:26,092
For example, we talked about how we can leverage blockchain technologies to

340
00:21:26,148 --> 00:21:29,708
enable the regulatory frameworks that are being put in place.

341
00:21:29,796 --> 00:21:34,788
So these type of interactions are crucial for

342
00:21:34,836 --> 00:21:39,444
each of these stakeholders to understand the various advancements

343
00:21:39,604 --> 00:21:43,076
in other disciplines and kind of coordinate with each

344
00:21:43,100 --> 00:21:46,508
other to come up with a coherent approach to address

345
00:21:46,556 --> 00:21:49,892
the problem of deepfakes. So, so, as a

346
00:21:49,908 --> 00:21:53,740
conclusion, you know, so if you

347
00:21:53,892 --> 00:21:58,004
take home one thing from today's discussion,

348
00:21:58,164 --> 00:22:01,956
I would say that generative AI has a lot of potential,

349
00:22:02,140 --> 00:22:05,836
as we talked about, we only talked about three examples, but there's a whole lot

350
00:22:05,860 --> 00:22:09,052
going on in the field of generating AI in terms of applications, et cetera.

351
00:22:09,148 --> 00:22:12,252
So the future is bright, but at

352
00:22:12,268 --> 00:22:16,150
the same time, we also need to keep in mind the various risks associated

353
00:22:16,292 --> 00:22:20,082
and innovation needs to progress not only on

354
00:22:20,098 --> 00:22:23,814
the side of building new applications, coming up with new techniques

355
00:22:24,274 --> 00:22:28,074
to improve and enhance the abilities of generative AI,

356
00:22:28,194 --> 00:22:31,514
but we also need to keep the risks in mind and

357
00:22:31,634 --> 00:22:35,194
make a concomitant progress on the security

358
00:22:35,274 --> 00:22:38,894
side of generative AI as well. So it is crucial to

359
00:22:39,554 --> 00:22:43,122
put the right incentives in place for

360
00:22:43,218 --> 00:22:46,962
continued research and investments that are necessary

361
00:22:47,098 --> 00:22:50,450
for these two things to progress, you know,

362
00:22:50,602 --> 00:22:54,186
in lockstep with each other. So that's

363
00:22:54,210 --> 00:22:57,290
all I have to share today. Thanks a lot for, you know,

364
00:22:57,322 --> 00:23:01,586
listening to my talk today, and I hope you

365
00:23:01,610 --> 00:23:04,970
got something out of it. I hope you got some insights that,

366
00:23:05,162 --> 00:23:08,610
you know, that could be potentially useful in your work.

367
00:23:08,722 --> 00:23:11,586
But I'll be in the hallway track and I would love to, you know,

368
00:23:11,610 --> 00:23:14,862
discuss further and, you know, I would like to hear your opinions

369
00:23:14,958 --> 00:23:18,646
and inputs on what you think are the,

370
00:23:18,750 --> 00:23:22,794
are the most pressing problems in the field of generative AI

371
00:23:23,174 --> 00:23:26,514
and how you think we can collaborate

372
00:23:27,214 --> 00:23:31,222
from various disciplines to mitigate all these problems. Thank you.

373
00:23:31,358 --> 00:23:32,294
I hope to talk to you soon.

