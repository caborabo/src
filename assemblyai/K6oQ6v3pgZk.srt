1
00:00:20,720 --> 00:00:24,630
Hello everyone, and welcome to this session about how to

2
00:00:24,662 --> 00:00:28,170
make LLM app sane again for getting incorrect

3
00:00:28,202 --> 00:00:31,490
data in real time. So today you're going to learn how

4
00:00:31,522 --> 00:00:34,374
to write your real time LLM app with pathway.

5
00:00:34,914 --> 00:00:37,934
So we're going to see how to create your chatbot,

6
00:00:38,394 --> 00:00:41,778
make it learn on real time data, and in particular

7
00:00:41,866 --> 00:00:44,814
how to forget incorrect data in real time.

8
00:00:45,394 --> 00:00:49,210
First we're going to see it is important

9
00:00:49,362 --> 00:00:53,676
to learn and forget in real time. Then the

10
00:00:53,700 --> 00:00:57,148
common solution, fine tuning and rag before

11
00:00:57,316 --> 00:01:00,644
seeing how to do a rag pipeline

12
00:01:00,684 --> 00:01:04,104
with pathway and its reactive vector index

13
00:01:04,444 --> 00:01:07,788
with a live demo at the

14
00:01:07,796 --> 00:01:11,748
end. So today we

15
00:01:11,756 --> 00:01:15,780
have the chance to have access to really powerful LLM

16
00:01:15,812 --> 00:01:18,824
model really easily through APIs.

17
00:01:19,754 --> 00:01:23,106
And in my example I will

18
00:01:23,250 --> 00:01:27,214
use OpenAI API. But everything is

19
00:01:28,674 --> 00:01:32,274
model agnostic. You can use any model you want from

20
00:01:32,314 --> 00:01:35,466
meta Mistral, or you can

21
00:01:35,490 --> 00:01:39,066
even host your own. And we're going to use LLM

22
00:01:39,090 --> 00:01:41,818
models for two operations. First,

23
00:01:41,946 --> 00:01:46,234
embeddings text into vectors

24
00:01:46,894 --> 00:01:50,510
and then chat completion to answer

25
00:01:50,582 --> 00:01:54,114
question okay, so what's wrong with our LLM?

26
00:01:54,614 --> 00:01:58,270
So LLM are very good at answering questions,

27
00:01:58,462 --> 00:02:02,262
but only on the topic they know about.

28
00:02:02,358 --> 00:02:05,654
And it's like us, right? If I didn't learn about the subject

29
00:02:05,734 --> 00:02:09,486
and you asked me a question about this, I would have troubles to

30
00:02:09,510 --> 00:02:12,223
answer it. And that's the same for LLM.

31
00:02:12,683 --> 00:02:17,091
So the first issue is that they are not very good at answering

32
00:02:17,147 --> 00:02:21,211
question about unfamiliar topics. For example, on OpenAI,

33
00:02:21,387 --> 00:02:26,131
the model are not able

34
00:02:26,187 --> 00:02:29,263
to answer a question about 2024.

35
00:02:29,843 --> 00:02:34,427
All the training data from

36
00:02:34,475 --> 00:02:37,410
before this year in particular,

37
00:02:37,562 --> 00:02:39,814
it's not going to work with real time data.

38
00:02:40,674 --> 00:02:44,946
And another kind of data it is unfamiliar with is personal

39
00:02:45,010 --> 00:02:48,514
and confusion. Don't share data, right? The data you didn't share,

40
00:02:48,554 --> 00:02:50,854
your non public document, your personal data,

41
00:02:53,394 --> 00:02:57,266
didn't end up in the training data of those models. So the model

42
00:02:57,290 --> 00:02:59,614
has no way to learn to know about that.

43
00:03:01,074 --> 00:03:04,744
And the second issue is that what is learned is learn, right?

44
00:03:04,824 --> 00:03:08,600
Ll models cannot forget, and this might be

45
00:03:08,632 --> 00:03:12,360
a problem if what they learn or is

46
00:03:12,392 --> 00:03:15,320
something they should not know. For example,

47
00:03:15,352 --> 00:03:18,936
if it's outdated data, right? Such as Pluto

48
00:03:18,960 --> 00:03:22,840
statue, right? Pluto statue has changed a lot. So is it a planet or not?

49
00:03:22,992 --> 00:03:25,664
Right? If it changes this year,

50
00:03:25,704 --> 00:03:28,836
the LLM

51
00:03:28,860 --> 00:03:32,084
has no way to know. And the problem is that he assumed that the last

52
00:03:32,244 --> 00:03:36,864
statue is the ground truth, right? And more

53
00:03:38,164 --> 00:03:42,504
similarly, we have fake news and deliberate misinformation.

54
00:03:44,244 --> 00:03:46,264
Everything which is seen,

55
00:03:47,484 --> 00:03:51,084
it was used to be seen as ground truth in the past, and it's

56
00:03:51,124 --> 00:03:55,076
not true anymore. But LLM model has no way to know if

57
00:03:55,100 --> 00:03:59,180
it is not true anymore. And you have also the cases

58
00:03:59,252 --> 00:04:01,624
of copyrighted and personal data.

59
00:04:02,124 --> 00:04:05,084
If by mistake it end up in the LL model,

60
00:04:05,164 --> 00:04:08,104
the fact that it cannot forget is a problem.

61
00:04:09,404 --> 00:04:11,704
So how to correct the model?

62
00:04:13,084 --> 00:04:16,876
You can improve the knowledge of the model by providing additional data,

63
00:04:17,020 --> 00:04:17,864
new information.

64
00:04:19,284 --> 00:04:23,009
Pluto now is a planet or is not and

65
00:04:23,161 --> 00:04:26,573
patches. Okay, this information was not correct

66
00:04:27,833 --> 00:04:31,853
and you can add this extra

67
00:04:32,673 --> 00:04:36,025
knowledge by two operations. You don't want to

68
00:04:36,049 --> 00:04:39,009
wait the new version of the model, right?

69
00:04:39,081 --> 00:04:42,465
Because maybe you don't have the time, you want to do it by

70
00:04:42,489 --> 00:04:45,641
yourself to be sure the data is included. So you have

71
00:04:45,657 --> 00:04:48,857
two way of doing that. The first one is fine tuning and the second one

72
00:04:48,865 --> 00:04:52,334
is prompt engineering. So we're going to discuss both

73
00:04:52,634 --> 00:04:55,454
right now. So first, fine tuning.

74
00:04:55,954 --> 00:04:59,754
Fine tuning is taking a pre

75
00:04:59,794 --> 00:05:03,578
trained model, such as a generic GPT model,

76
00:05:03,706 --> 00:05:07,938
and then adapt, personalize it on your own data.

77
00:05:08,106 --> 00:05:12,018
So you take an existing model and then

78
00:05:12,066 --> 00:05:16,110
you pursue the training over

79
00:05:16,262 --> 00:05:20,094
your data. So the same kind of data training that

80
00:05:20,174 --> 00:05:23,334
was used to obtain the generic model. So it's batch training.

81
00:05:23,494 --> 00:05:26,714
So you need all the data at once and you train on it.

82
00:05:27,054 --> 00:05:31,558
And one issue is that it requires an adaptive

83
00:05:31,606 --> 00:05:34,886
dataset, so you cannot train over any

84
00:05:34,910 --> 00:05:38,554
kind of data. It has to have a particular schema

85
00:05:39,014 --> 00:05:42,970
and preprocessing your data may be costly.

86
00:05:43,062 --> 00:05:47,162
Okay. And another issue that you

87
00:05:47,178 --> 00:05:51,210
cannot forget, right? We end up with the same kind

88
00:05:51,242 --> 00:05:55,146
of model. It's LLM model, fine tuned,

89
00:05:55,210 --> 00:05:58,414
but it's still LLM model, so you cannot forget.

90
00:05:59,514 --> 00:06:02,826
So if something changes in your data, you will

91
00:06:02,850 --> 00:06:06,842
have to retrain it from scratch. So you have to

92
00:06:06,938 --> 00:06:10,560
retake the pre trained model and do the fine tuning

93
00:06:10,592 --> 00:06:14,040
part again. And this can be really costly, so it's

94
00:06:14,072 --> 00:06:16,084
not suitable for real time data.

95
00:06:17,384 --> 00:06:19,724
The second solution is prompt engineering.

96
00:06:20,304 --> 00:06:24,808
Prompt engineering is to

97
00:06:24,856 --> 00:06:29,480
modify the query, such as all the data that

98
00:06:29,552 --> 00:06:33,000
the LLM might need to answer. The query is

99
00:06:33,032 --> 00:06:37,058
included in the query. The answer is in the question kind of

100
00:06:37,226 --> 00:06:40,574
operation. So for example,

101
00:06:41,194 --> 00:06:44,642
but Pluto, if you want to ask what's the statue of Pluto? You will

102
00:06:44,738 --> 00:06:48,202
add all the latest news about Pluto and

103
00:06:48,218 --> 00:06:51,546
say okay, given those articles about Pluto

104
00:06:51,690 --> 00:06:55,418
is Pluto planet or not? And then the LLM

105
00:06:55,466 --> 00:07:00,054
should be able to answer you with the latest statute.

106
00:07:00,674 --> 00:07:04,736
So what's wrong with that is that it's

107
00:07:04,760 --> 00:07:08,496
a tedious rock, right? You don't want to do that by hand because you have

108
00:07:08,520 --> 00:07:12,992
to fetch data and do

109
00:07:13,008 --> 00:07:16,840
it by yourself, is not scalable and it

110
00:07:16,872 --> 00:07:18,844
doesn't work well with real time data.

111
00:07:19,584 --> 00:07:23,120
So you want to automate it.

112
00:07:23,312 --> 00:07:26,608
And the way to do it automatically is called retrieval

113
00:07:26,656 --> 00:07:30,214
augmented generation. It's a three step process process.

114
00:07:30,754 --> 00:07:34,330
So first you transform the query the

115
00:07:34,362 --> 00:07:37,962
question in a vector using embeddings.

116
00:07:38,018 --> 00:07:41,394
Okay, you embed the query

117
00:07:41,434 --> 00:07:44,090
to obtain a vector, and then using these vectors.

118
00:07:44,202 --> 00:07:48,050
Second thing you do is to find automatically the most similar

119
00:07:48,122 --> 00:07:52,234
documents using a vector index. And now

120
00:07:52,274 --> 00:07:55,886
that you have the most relevant documents, you can

121
00:07:55,910 --> 00:07:58,614
do the prompt engineering. So,

122
00:07:58,654 --> 00:08:02,398
just one quick

123
00:08:02,446 --> 00:08:05,926
explanation about what are veto embeddings and why are

124
00:08:05,950 --> 00:08:08,034
they useful? So,

125
00:08:09,614 --> 00:08:13,834
embeddings are used to transform and unstructure

126
00:08:14,974 --> 00:08:17,434
data into vectors.

127
00:08:18,054 --> 00:08:21,134
And why are we doing that?

128
00:08:21,174 --> 00:08:24,486
It's because the raw data is unstructured. Data might be

129
00:08:24,510 --> 00:08:28,262
really hard to compare our example, which is text.

130
00:08:28,438 --> 00:08:31,654
Comparing two texts might be difficult, but comparing

131
00:08:31,694 --> 00:08:36,118
vectors is quite easy, right? We have a lot of optimized

132
00:08:36,286 --> 00:08:39,446
mathematical operation to do that. So the idea is to transform

133
00:08:39,510 --> 00:08:43,674
text into vectors so we can use all the

134
00:08:44,454 --> 00:08:48,726
optimized techniques so we can find the

135
00:08:48,750 --> 00:08:52,364
fastest way

136
00:08:52,904 --> 00:08:57,440
more similar documents. So what's

137
00:08:57,472 --> 00:09:00,512
good with the vector embedding? That it is done

138
00:09:00,568 --> 00:09:04,392
in a way that the more two

139
00:09:04,568 --> 00:09:09,284
texts are similar, the more similar

140
00:09:10,104 --> 00:09:13,344
their vector will be. Okay, so using those

141
00:09:13,384 --> 00:09:17,320
vectors, we will do really fast

142
00:09:17,512 --> 00:09:22,642
document retrieval. Okay, so the

143
00:09:22,738 --> 00:09:26,586
most popular hack use cases are chatbots. Over your

144
00:09:26,610 --> 00:09:31,218
own data. You can take a LLm chatbot

145
00:09:31,266 --> 00:09:34,494
and add your own data on it.

146
00:09:34,914 --> 00:09:38,370
And it's a good fit for real time

147
00:09:38,482 --> 00:09:42,034
data and also to correct,

148
00:09:42,114 --> 00:09:46,014
to provide context to queries to avoid potential incorrect answers.

149
00:09:47,694 --> 00:09:51,166
So let's take an example about confidential data. Let's assume your

150
00:09:51,190 --> 00:09:56,166
company has confidential data and you

151
00:09:56,190 --> 00:10:00,154
want to build a chatbot to query those,

152
00:10:01,534 --> 00:10:04,474
for example, and ask, what's this company's budget?

153
00:10:04,854 --> 00:10:08,518
And you will use rag to find all the most relevant

154
00:10:08,566 --> 00:10:13,032
data and then do the prompt engineering,

155
00:10:13,208 --> 00:10:16,824
doing a summary of the information you have found.

156
00:10:16,944 --> 00:10:20,672
Okay, but as

157
00:10:20,688 --> 00:10:23,976
you can see, there is still an issue. What's happened? If the RAC data is

158
00:10:24,000 --> 00:10:27,296
compromised, it's the same as the

159
00:10:27,320 --> 00:10:31,124
initial issue. What if the data is outdated

160
00:10:31,424 --> 00:10:35,644
or totally wrong or copyrighted or personal?

161
00:10:37,974 --> 00:10:41,870
The good news is the context can be forgotten. Every time

162
00:10:41,902 --> 00:10:45,910
you do query, you retrieve

163
00:10:46,022 --> 00:10:49,474
the most similar documents. So if you remove,

164
00:10:50,094 --> 00:10:52,114
it will not be taken into account.

165
00:10:54,374 --> 00:10:57,982
So hack not also supports the addition of new

166
00:10:58,078 --> 00:11:01,486
document, but it provides a really

167
00:11:01,550 --> 00:11:05,050
easy way to remove knowledge from

168
00:11:05,082 --> 00:11:08,418
your application so you can easily remove

169
00:11:08,466 --> 00:11:12,034
incorrect data or confidential data using

170
00:11:12,154 --> 00:11:14,934
HAC. So in our example,

171
00:11:15,554 --> 00:11:18,494
we have our chatbot about our confidential data.

172
00:11:19,114 --> 00:11:23,618
And as you know, confidential data is heavily regulated and

173
00:11:23,786 --> 00:11:27,442
if for legal reason you have to remove a document, you don't want

174
00:11:27,618 --> 00:11:31,624
your system to reflect this removal only

175
00:11:31,704 --> 00:11:35,608
one month later, right? It has to be removed from your

176
00:11:35,736 --> 00:11:37,964
chatbot as soon as possible,

177
00:11:38,824 --> 00:11:41,964
otherwise you may face some lawsuits.

178
00:11:42,504 --> 00:11:45,648
You want to have something really

179
00:11:45,776 --> 00:11:49,424
reactive, and here reactivity is key. You want

180
00:11:49,504 --> 00:11:52,808
to have a system

181
00:11:52,976 --> 00:11:56,480
which take into account new data as soon as

182
00:11:56,592 --> 00:12:00,444
it is inserted, and same way it will forget

183
00:12:01,664 --> 00:12:05,444
the data as soon as it is removed from your system.

184
00:12:06,144 --> 00:12:08,964
So the solution is to use a real time vector index.

185
00:12:11,504 --> 00:12:15,112
It will take into account any document

186
00:12:15,168 --> 00:12:18,564
whenever it is indexed, and by removing it,

187
00:12:20,104 --> 00:12:23,328
by removing the data your system will forget.

188
00:12:23,456 --> 00:12:26,824
And it's well adapted to LaV

189
00:12:26,864 --> 00:12:31,096
data stream, of course. And the main characteristic you

190
00:12:31,120 --> 00:12:34,600
want is to be reactive. Now a bit of practice,

191
00:12:34,712 --> 00:12:38,764
let's build a chatbot. We'll see how to build a chatbot over

192
00:12:39,264 --> 00:12:42,684
PDF. So here we'll take financial documents,

193
00:12:44,144 --> 00:12:47,536
we'll take a scenario where we need

194
00:12:47,560 --> 00:12:50,626
to remove one document and we want the chatbot

195
00:12:50,650 --> 00:12:54,218
to forget it as fast as possible. And we'll do that in pure Python

196
00:12:54,266 --> 00:12:58,570
with pathway. So the

197
00:12:58,602 --> 00:13:02,094
pipeline will look like this. So everything

198
00:13:03,274 --> 00:13:07,226
can be separated into parts. You have the prompt construction and retrieval which will

199
00:13:07,250 --> 00:13:11,018
be done in Python, and you have all the

200
00:13:11,146 --> 00:13:15,014
LLM operation which will be handled by open

201
00:13:15,314 --> 00:13:18,550
AI API. So the first step

202
00:13:18,582 --> 00:13:23,074
would be to index the documents. So all your documents are on your storage

203
00:13:23,614 --> 00:13:27,846
and for each document you

204
00:13:27,870 --> 00:13:31,478
call the API to obtain the embeddings

205
00:13:31,566 --> 00:13:35,438
and you will do the indexing of

206
00:13:35,606 --> 00:13:40,678
all the documents with the embeddings. Then whenever

207
00:13:40,766 --> 00:13:43,954
user will use a search bar to do a query,

208
00:13:44,704 --> 00:13:47,256
you will do the hag approach.

209
00:13:47,400 --> 00:13:51,184
You will call the API to

210
00:13:51,304 --> 00:13:54,284
compute the embeddings of the query.

211
00:13:54,824 --> 00:13:58,272
And using this vector you will query

212
00:13:58,328 --> 00:14:02,312
the vector index to retrieve the most relevant documents

213
00:14:02,368 --> 00:14:05,364
from your documentation.

214
00:14:05,744 --> 00:14:10,032
And with those documentation you will do the prompt end engineering. Given those documents,

215
00:14:10,168 --> 00:14:13,944
please answer this query and you will call

216
00:14:14,064 --> 00:14:16,832
the API to do the chat completion over this query.

217
00:14:16,968 --> 00:14:19,964
Then you can post process or directly forward.

218
00:14:21,464 --> 00:14:24,604
Okay, so we'll do that with pathway which is

219
00:14:25,104 --> 00:14:28,684
a data processing framework in Python

220
00:14:29,984 --> 00:14:34,168
to do batch and streaming data which is meant

221
00:14:34,216 --> 00:14:37,780
to to allow LLM application to work on real

222
00:14:37,812 --> 00:14:41,396
time at the time. So it's a Python framework, but there

223
00:14:41,420 --> 00:14:44,988
is a scalable rest engine behind

224
00:14:45,036 --> 00:14:49,036
it. So it will

225
00:14:49,060 --> 00:14:52,544
look like that password will under all the calls

226
00:14:53,004 --> 00:14:56,532
to OpenAI, so it will under embeddings and

227
00:14:56,548 --> 00:15:00,244
the victor index search and also the prompting.

228
00:15:00,284 --> 00:15:04,036
So it provides you the tools to to

229
00:15:04,060 --> 00:15:07,428
query all kind of documents from file kafka topics

230
00:15:07,476 --> 00:15:11,384
or even g drive or sharepoint.

231
00:15:12,364 --> 00:15:16,332
And we'll provide you all the tools to

232
00:15:16,348 --> 00:15:20,540
do a rack really easily with OpenAI or any

233
00:15:20,612 --> 00:15:23,424
other you want.

234
00:15:23,884 --> 00:15:27,660
So let's see how it is

235
00:15:27,692 --> 00:15:32,544
done. First thing you do is you connect

236
00:15:32,584 --> 00:15:36,192
to your document. So here we use connectors,

237
00:15:36,288 --> 00:15:39,872
so you can connect to your data source using connectors. And we're

238
00:15:39,888 --> 00:15:43,808
going to read on the file system this folder

239
00:15:43,856 --> 00:15:47,524
document. So all the documents

240
00:15:48,024 --> 00:15:52,440
will be PDF put into this folder and

241
00:15:52,512 --> 00:15:57,621
then you need to define the model. So password provides you all the tools

242
00:15:58,121 --> 00:16:02,594
to really time, easily configure the model you want to use by

243
00:16:02,754 --> 00:16:06,370
pre configuring everything for you and

244
00:16:06,522 --> 00:16:10,374
you can define them better. The LLM for chat completion,

245
00:16:11,754 --> 00:16:16,090
the splitters similarly, you just

246
00:16:16,122 --> 00:16:19,426
have to initialize a vector store with the

247
00:16:19,450 --> 00:16:23,054
documents and everything is

248
00:16:23,394 --> 00:16:25,974
configured for you by password.

249
00:16:26,794 --> 00:16:31,414
We need to define a web server for the query answer,

250
00:16:31,714 --> 00:16:36,734
so everything is customizable.

251
00:16:37,754 --> 00:16:41,494
And using a rest connector we obtain the queries.

252
00:16:42,274 --> 00:16:46,290
Now we can do the rag, so we retrieve for

253
00:16:46,362 --> 00:16:49,818
each query the most similar document. So here

254
00:16:49,946 --> 00:16:53,610
we retrieve only one document, but in practice, depending on your use case,

255
00:16:53,642 --> 00:16:55,774
it might be 10, 20, 30.

256
00:16:57,394 --> 00:17:01,074
Then we do the prompt engineering. So as you see,

257
00:17:01,194 --> 00:17:03,374
everything is already,

258
00:17:04,514 --> 00:17:07,970
all the functions are done

259
00:17:08,002 --> 00:17:11,922
for you. So it's very simple. And then we

260
00:17:11,938 --> 00:17:15,346
do the chat completion with the prompt, we send back

261
00:17:15,370 --> 00:17:17,774
the result, and then we run the pipeline.

262
00:17:18,954 --> 00:17:22,597
Okay, let's see, let's see how it works.

263
00:17:22,645 --> 00:17:26,445
So first we just check what kind

264
00:17:26,469 --> 00:17:28,833
of documents we have. We have two documents,

265
00:17:29,493 --> 00:17:32,917
Alphabet financial document

266
00:17:32,965 --> 00:17:36,925
and another document about we

267
00:17:36,949 --> 00:17:40,005
launch the pipeline, we run the pipeline

268
00:17:40,069 --> 00:17:43,277
and it might take a while, because the

269
00:17:43,285 --> 00:17:46,536
first thing the pipeline will do is to index all the

270
00:17:46,560 --> 00:17:50,256
documents, right? So it builds the

271
00:17:50,360 --> 00:17:53,364
vector index using OpenAI,

272
00:17:54,184 --> 00:17:57,872
and then we can query documents.

273
00:17:57,928 --> 00:18:01,024
So we want to ask a question about Alphabet.

274
00:18:01,104 --> 00:18:04,568
So what is the revenue of Alphabet in 2022

275
00:18:04,616 --> 00:18:08,176
in millions of dollars? Okay, let's see,

276
00:18:08,360 --> 00:18:12,782
what's the answer? So it answer this number.

277
00:18:12,958 --> 00:18:16,366
So let's check if the

278
00:18:16,510 --> 00:18:20,262
answer is correct. So this is a document which is

279
00:18:20,318 --> 00:18:24,094
indexed. If we go to the revenue page, we can

280
00:18:24,134 --> 00:18:27,674
see that the number is correct.

281
00:18:28,294 --> 00:18:31,718
Now let's assume that this document,

282
00:18:31,846 --> 00:18:35,886
for some legal reason has to be removed. So we remove

283
00:18:35,910 --> 00:18:39,946
it from our, and let's see what

284
00:18:39,970 --> 00:18:43,854
the chatbot says. Now what the revenue of Alphabet

285
00:18:44,354 --> 00:18:47,818
no information form. So our

286
00:18:47,866 --> 00:18:51,978
chatbot is really reactive, right? Whenever the document

287
00:18:52,106 --> 00:18:55,818
is removed, the information, if you

288
00:18:55,866 --> 00:18:59,162
do another query, the removal has been taken

289
00:18:59,218 --> 00:19:02,898
into account. So reactivity is key,

290
00:19:02,986 --> 00:19:05,808
right? As we say, garbage in, garbage out,

291
00:19:05,906 --> 00:19:09,024
so you need to update your index as soon as possible.

292
00:19:10,444 --> 00:19:13,892
You should not. Your system to

293
00:19:13,988 --> 00:19:17,984
take the removal into account has to be

294
00:19:18,484 --> 00:19:21,900
very quick and for that streaming is the way to go.

295
00:19:22,012 --> 00:19:25,700
If you do randexing by

296
00:19:25,732 --> 00:19:29,716
batch, so I don't know, every hour, 20 minutes or

297
00:19:29,740 --> 00:19:33,022
so in between the two re

298
00:19:33,038 --> 00:19:36,430
indexing all the queries might

299
00:19:36,462 --> 00:19:40,070
be inconsistent, right? Because the document has

300
00:19:40,102 --> 00:19:43,914
been removed but it was not.

301
00:19:44,974 --> 00:19:48,486
The changes has not been forward until the

302
00:19:48,550 --> 00:19:52,422
whole system. And that's why we need

303
00:19:52,438 --> 00:19:56,166
to have an event based approach

304
00:19:56,350 --> 00:20:00,570
and for that reactive real time vector index

305
00:20:00,682 --> 00:20:04,978
these are the way to go, such as the one of pathway

306
00:20:05,146 --> 00:20:09,134
which is very reactive to any updates with

307
00:20:10,314 --> 00:20:12,534
an additional removal.

308
00:20:13,474 --> 00:20:17,210
So to conclude while LLM

309
00:20:17,242 --> 00:20:20,378
can be wrong is the solution.

310
00:20:20,426 --> 00:20:24,258
Like most of the problem with LLM

311
00:20:24,306 --> 00:20:27,610
is coming from the training data bit because it's missing

312
00:20:27,642 --> 00:20:31,374
some data or because some data is incorrect and

313
00:20:31,994 --> 00:20:35,586
Rag is the only existing solution to correct this limited

314
00:20:35,650 --> 00:20:39,018
knowledge that can adapt in real time.

315
00:20:39,146 --> 00:20:42,530
Fine tuning is nice, but it's done

316
00:20:42,682 --> 00:20:47,106
on batch data so you

317
00:20:47,130 --> 00:20:50,738
cannot forget. So you have to redo it every time while

318
00:20:50,826 --> 00:20:54,224
hack can maintain in real time index and

319
00:20:54,844 --> 00:20:59,116
your system will be really reactive to all the changes and

320
00:20:59,220 --> 00:21:02,820
reactivity is key. Your index should be reflecting the

321
00:21:02,852 --> 00:21:06,944
changes in your data in real time.

322
00:21:08,804 --> 00:21:12,692
So thank you for listening to

323
00:21:12,708 --> 00:21:16,900
this session. You can try the demo yourself and

324
00:21:16,972 --> 00:21:21,524
please don't hesitate to reach out to me if you have any questions.

