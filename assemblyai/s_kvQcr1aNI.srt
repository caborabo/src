1
00:00:27,594 --> 00:00:31,974
Welcome to this talk on design thinking before you develop your next AI product.

2
00:00:32,314 --> 00:00:36,218
Before we go further, let me tell you a bit about myself and

3
00:00:36,306 --> 00:00:39,778
my background in this field. I have been in software consulting,

4
00:00:39,826 --> 00:00:43,254
design and development for the last 20 years.

5
00:00:43,634 --> 00:00:47,642
After doing some Java J two e development at

6
00:00:47,658 --> 00:00:51,730
the start of my career, I got several opportunities in

7
00:00:51,762 --> 00:00:56,164
entity data to lead emerging technologies,

8
00:00:56,784 --> 00:01:00,984
research, design development and now deployment.

9
00:01:01,144 --> 00:01:05,400
So right from big data around twelve years ago, I have been with

10
00:01:05,552 --> 00:01:08,384
these technologies like IoT, gamification,

11
00:01:08,504 --> 00:01:12,404
enterprise, social blockchain, data science,

12
00:01:12,744 --> 00:01:16,480
and now AI and ML. So now in my current

13
00:01:16,552 --> 00:01:20,916
role, I actually lead fantastic

14
00:01:21,060 --> 00:01:24,524
team of designers, developers and

15
00:01:24,564 --> 00:01:27,884
solution architects who design,

16
00:01:27,964 --> 00:01:31,412
deploy and scale these products for large enterprise

17
00:01:31,468 --> 00:01:34,780
grade customers. And that really gives me

18
00:01:34,852 --> 00:01:38,252
some of the real world experience in this field which I'm going

19
00:01:38,268 --> 00:01:41,804
to share with you all today. In today's talk,

20
00:01:41,884 --> 00:01:45,620
we will dive deeper into the current state of AI,

21
00:01:45,692 --> 00:01:49,688
where things are, what are some of the issues that

22
00:01:49,776 --> 00:01:53,200
haunt AI application in the absence of approaches

23
00:01:53,232 --> 00:01:56,504
like design thinking? And then how can design thinking

24
00:01:56,544 --> 00:02:00,320
really help? And how

25
00:02:00,352 --> 00:02:03,604
do you design things for AI applications?

26
00:02:03,904 --> 00:02:06,284
So with that, let's start the talk today.

27
00:02:06,864 --> 00:02:10,008
To start, let's get started from the big picture.

28
00:02:10,176 --> 00:02:13,204
What is the ultimate aim of AI products?

29
00:02:14,034 --> 00:02:17,946
In my opinion, it is to be a force multiplier

30
00:02:18,130 --> 00:02:22,474
for people and organizations taking over repetitive tasks

31
00:02:22,554 --> 00:02:25,890
or very complex tasks that take time for

32
00:02:25,922 --> 00:02:29,530
humans to do and that can complement human

33
00:02:29,642 --> 00:02:33,978
effort. Things that are right now based on

34
00:02:34,146 --> 00:02:37,706
subjective knowledge and experience

35
00:02:37,890 --> 00:02:41,730
and which often result in inconsistent outcomes,

36
00:02:41,802 --> 00:02:45,602
can be changed with AI. They can be made to be encompassing

37
00:02:45,658 --> 00:02:49,506
of all the possible data and all the possible context

38
00:02:49,570 --> 00:02:53,610
around that data to result in the best possible decisions

39
00:02:53,762 --> 00:02:56,534
with consistency and reliability.

40
00:02:56,914 --> 00:03:00,370
And the data that we are talking about changes with time.

41
00:03:00,562 --> 00:03:03,770
In my opinion, the biggest promise of AI

42
00:03:03,842 --> 00:03:07,018
is to ensure that the technology supporting our

43
00:03:07,066 --> 00:03:10,922
society keeps updating itself continuously

44
00:03:11,018 --> 00:03:14,330
with the changes that come with time, with place,

45
00:03:14,442 --> 00:03:18,266
with the surrounding context, and it

46
00:03:18,290 --> 00:03:22,522
keeps updating this knowledge and understanding with this continuous learning.

47
00:03:22,698 --> 00:03:26,450
So lofty goals for sure, but I would say not too

48
00:03:26,482 --> 00:03:30,298
far. And in fact these goals are not at all new.

49
00:03:30,426 --> 00:03:34,458
Over the last 70 or so years we have made incremental

50
00:03:34,506 --> 00:03:38,144
progress in AI. After coining the term and making

51
00:03:38,184 --> 00:03:42,576
some early experiments, there was a long gap in which we did not really

52
00:03:42,720 --> 00:03:46,232
do much and it is often called as AI winter.

53
00:03:46,408 --> 00:03:50,064
But since 1980s, focused academic

54
00:03:50,104 --> 00:03:53,704
research as well as development of surrounding ecosystem of

55
00:03:53,744 --> 00:03:57,400
technologies has led to tremendous investments.

56
00:03:57,552 --> 00:04:01,024
But nothing has been as big

57
00:04:01,104 --> 00:04:04,748
as the last two, three years where we saw the first big use

58
00:04:04,796 --> 00:04:09,428
case of generative AI through chat GPT and

59
00:04:09,556 --> 00:04:13,444
that has rightly deserved its place on every next LinkedIn

60
00:04:13,484 --> 00:04:16,428
post that you see or the conference talks,

61
00:04:16,556 --> 00:04:20,304
newspaper headlines, and more often than not

62
00:04:20,684 --> 00:04:24,108
on the minds of worried policymakers.

63
00:04:24,276 --> 00:04:27,660
That said, it is not just OpenAI and chat GPT.

64
00:04:27,772 --> 00:04:31,246
AI startups are sprouting in every

65
00:04:31,310 --> 00:04:35,190
major industry vertical cross industry applications

66
00:04:35,302 --> 00:04:38,518
such as vision, natural language processing and search

67
00:04:38,566 --> 00:04:41,726
are still ruling the charts, but use case

68
00:04:41,790 --> 00:04:45,190
focused development which is application of these

69
00:04:45,222 --> 00:04:48,834
cross industry applications in a given business context,

70
00:04:49,134 --> 00:04:52,758
such as manufacturing plant floor machine vision

71
00:04:52,926 --> 00:04:56,754
or say traffic condition analysis on a highway.

72
00:04:57,234 --> 00:05:00,706
Those are the ones that are actually resulting in real business

73
00:05:00,770 --> 00:05:04,946
value for businesses, but none

74
00:05:04,970 --> 00:05:08,634
of them are as feasible or even deployable

75
00:05:08,714 --> 00:05:12,538
without the supporting platforms and processes. Where we are seeing a

76
00:05:12,586 --> 00:05:15,946
lot of development recently, such as

77
00:05:16,010 --> 00:05:19,026
those that are needed for Mlops or

78
00:05:19,090 --> 00:05:22,970
AI Ops. But with around all this buzz

79
00:05:23,002 --> 00:05:26,274
that we are hearing about AI, there are a few things which it is really

80
00:05:26,354 --> 00:05:29,884
good at and some things where it is still very niche,

81
00:05:29,924 --> 00:05:33,932
very new things that it is good at. First I would say is

82
00:05:34,028 --> 00:05:38,236
the NLP based chatbots BI has really shined great

83
00:05:38,340 --> 00:05:42,380
in these areas. Or inculcating knowledge from

84
00:05:42,492 --> 00:05:46,020
heterogeneous data sources, detecting anomalies and

85
00:05:46,052 --> 00:05:49,044
fraud in say, insurance claims,

86
00:05:49,084 --> 00:05:52,964
etcetera, and then predicting based on

87
00:05:53,084 --> 00:05:57,442
historical trends and creating better personalized recommendations.

88
00:05:57,588 --> 00:06:01,182
Something that you see when you shop on Amazon or Instacart,

89
00:06:01,238 --> 00:06:05,670
etcetera. It is not just based on your historic

90
00:06:05,782 --> 00:06:09,294
information, but also the context, people like you

91
00:06:09,374 --> 00:06:12,670
and where could your interests lead you to spend

92
00:06:12,702 --> 00:06:16,214
your next big part? Right, but where are

93
00:06:16,254 --> 00:06:20,078
we heading with all these developments? Around 90%

94
00:06:20,166 --> 00:06:23,994
of the AI experts when surveys felt that

95
00:06:24,154 --> 00:06:27,362
AI advancements will lead to human like

96
00:06:27,418 --> 00:06:31,026
intelligence in the next 100 years. I don't know what

97
00:06:31,050 --> 00:06:34,618
will happen decades later from now,

98
00:06:34,746 --> 00:06:38,106
but one thing we can say for sure is that it

99
00:06:38,130 --> 00:06:42,274
is here to get better, with over 80% of businesses

100
00:06:42,354 --> 00:06:45,986
adopting it by 2025. But some of the

101
00:06:46,010 --> 00:06:49,858
most prominent area of challenges in the current state of AI

102
00:06:49,946 --> 00:06:53,378
still haunt us, and one of the biggest one is

103
00:06:53,426 --> 00:06:56,946
scaling of AI application. Currently, the kind

104
00:06:57,010 --> 00:07:01,214
of energy requirements that large scale AI

105
00:07:01,834 --> 00:07:06,186
applications have are simply not sustainable

106
00:07:06,330 --> 00:07:10,202
and feasible for most of the businesses and people.

107
00:07:10,378 --> 00:07:13,730
That is something that still needs to be worked out.

108
00:07:13,882 --> 00:07:17,274
And there are some very good advancements being made

109
00:07:17,314 --> 00:07:20,580
which which tell us that this is something which will

110
00:07:20,612 --> 00:07:22,704
be solved in next few years.

111
00:07:23,484 --> 00:07:26,900
The next is about adoption of AI. Yes, it is

112
00:07:26,932 --> 00:07:31,224
related closely to the cost of AI right now, but there are other issues

113
00:07:31,684 --> 00:07:34,660
that are stopping AI from being adopted.

114
00:07:34,732 --> 00:07:38,716
Generally they are related to the quality

115
00:07:38,780 --> 00:07:42,508
of the outcomes, as well as the biggest

116
00:07:42,556 --> 00:07:46,100
thing, which is again, closely related to adoption and scaling, is the

117
00:07:46,132 --> 00:07:49,978
trust. The users and the businesses

118
00:07:50,146 --> 00:07:53,874
are not able to put their complete trust in the

119
00:07:53,914 --> 00:07:57,402
AI solutions. We will look at all these issues in detail in

120
00:07:57,418 --> 00:08:00,954
a little bit, but having said that,

121
00:08:01,074 --> 00:08:04,290
I think there are some things that you can solve

122
00:08:04,402 --> 00:08:08,034
with approaches like design thinking, and there are of course,

123
00:08:08,114 --> 00:08:11,610
a whole gamut of other frameworks and approaches that

124
00:08:11,642 --> 00:08:15,062
need to be applied in order to solve the others. That said,

125
00:08:15,158 --> 00:08:18,710
some of the most prominent challenges in scaling adoption of

126
00:08:18,742 --> 00:08:22,414
AI this is a report from Everest group. I'm not

127
00:08:22,454 --> 00:08:26,998
going to go through all the numbers here, but the key point is that organizations

128
00:08:27,046 --> 00:08:30,798
do not really have talent and skill to make informed decisions

129
00:08:30,846 --> 00:08:34,942
about their AI investments. The way they manage their data

130
00:08:35,118 --> 00:08:38,702
and the computational infrastructure right now is just

131
00:08:38,758 --> 00:08:42,385
not suitable for enterprise grade implementation of AI,

132
00:08:42,549 --> 00:08:46,417
and everything that would be useful for them

133
00:08:46,545 --> 00:08:49,945
is extremely expensive right now, at least at the current

134
00:08:50,009 --> 00:08:53,993
state of technology, and the skills to maintain that

135
00:08:54,073 --> 00:08:58,153
is just not there. There is also inadequate policy

136
00:08:58,273 --> 00:09:02,633
and compliance infrastructure, so businesses do not know if

137
00:09:02,793 --> 00:09:06,985
what they are going to do is going to be acceptable in the regulatory framework

138
00:09:07,049 --> 00:09:10,990
or not. And then there's simply just unclear ROI

139
00:09:11,142 --> 00:09:14,718
of the new things that organizations do want to do.

140
00:09:14,846 --> 00:09:18,270
But the kind of costs that it takes, whether it is really

141
00:09:18,342 --> 00:09:21,550
going to result, the ROI that they are

142
00:09:21,582 --> 00:09:25,382
expecting or not, is unclear. But that happens with every new technology,

143
00:09:25,478 --> 00:09:28,686
isn't it? Having talked about scaling and adoption,

144
00:09:28,790 --> 00:09:32,558
let us look at the last pillar, which was really trusting

145
00:09:32,606 --> 00:09:36,582
AI and why businesses and users are

146
00:09:36,758 --> 00:09:40,366
having trust issues with it. So the first one that

147
00:09:40,390 --> 00:09:43,798
is most talked about is bias. It occurs when an

148
00:09:43,926 --> 00:09:48,534
algorithm produces results that are systemically prejudiced

149
00:09:48,654 --> 00:09:52,950
due to either erroneous assumptions in the machine learning process

150
00:09:53,142 --> 00:09:56,294
or the lack of context, lack of

151
00:09:56,334 --> 00:09:59,846
data sources, lack of all the possible parameters

152
00:09:59,950 --> 00:10:03,778
that can inform better decisions. And algorithms

153
00:10:03,826 --> 00:10:06,954
can usually have this built in biases because they're

154
00:10:06,994 --> 00:10:10,938
created by individuals who have conscious or unconscious preferences

155
00:10:10,986 --> 00:10:15,174
themselves that may go completely undiscovered

156
00:10:16,554 --> 00:10:19,334
until the algorithms are actually used.

157
00:10:19,634 --> 00:10:23,682
Bias is a big issue which is haunting AI right now.

158
00:10:23,818 --> 00:10:27,162
The second one is accuracy and real time intelligence.

159
00:10:27,338 --> 00:10:31,074
All the AI systems that have been created either have supervised

160
00:10:31,114 --> 00:10:34,602
or unsupervised learning in them. The data sources that they

161
00:10:34,618 --> 00:10:37,534
are considering keep changing all the time,

162
00:10:37,834 --> 00:10:41,954
and depending on how the AI system has been implemented,

163
00:10:42,074 --> 00:10:45,930
it may or may not be taking updating its learning

164
00:10:46,002 --> 00:10:49,810
sources continuously or not, and which greatly affects the

165
00:10:49,842 --> 00:10:53,762
accuracy of outcomes. The third one is a big one, as it

166
00:10:53,778 --> 00:10:57,190
is about explainability the AI system, the kind

167
00:10:57,222 --> 00:11:00,982
of recommendation it has given you, on what basis has it said

168
00:11:01,038 --> 00:11:04,902
so? Again, coming back to the example of doing

169
00:11:04,958 --> 00:11:08,314
online shopping, your new recommendations,

170
00:11:08,614 --> 00:11:11,834
have they been shown just based on your past history,

171
00:11:12,254 --> 00:11:15,910
or has it also considered people like you? Has it

172
00:11:15,942 --> 00:11:19,518
considered what is the next big fashion trend that is going

173
00:11:19,566 --> 00:11:23,494
to happen? Or has it considered the weather implications

174
00:11:23,574 --> 00:11:28,424
in your area in the next few months?

175
00:11:28,544 --> 00:11:32,056
So, there is a lot of factors that AI algorithms can consider,

176
00:11:32,240 --> 00:11:36,124
but communicating them to the end customers are extremely important,

177
00:11:36,464 --> 00:11:39,724
which is right now failing that is one part of it,

178
00:11:40,024 --> 00:11:43,640
things that can be easily explained. There is also

179
00:11:43,792 --> 00:11:47,768
inherent in explainability, or the lack of explainability within

180
00:11:47,816 --> 00:11:51,288
the AI models themselves, because they have been built

181
00:11:51,336 --> 00:11:54,668
upon the understanding that we do not understand our

182
00:11:54,716 --> 00:11:58,596
brain very much. We created these neural networks which mimic

183
00:11:58,700 --> 00:12:02,452
the outcomes that our brain create, but we do not

184
00:12:02,508 --> 00:12:06,492
know how the process itself works. The data scientists have a very hard

185
00:12:06,548 --> 00:12:10,172
time explaining why did their model choose

186
00:12:10,268 --> 00:12:13,444
outcome a versus outcome b in two

187
00:12:13,484 --> 00:12:17,988
different scenarios. And that is a little bit harder

188
00:12:18,036 --> 00:12:22,004
problem to solve than the first one, where there are parameters

189
00:12:22,044 --> 00:12:24,264
that just need to be explained to the users.

190
00:12:24,884 --> 00:12:27,980
The next one is about the security,

191
00:12:28,172 --> 00:12:32,060
copyright and IP infringement. And as we have all seen with some of

192
00:12:32,092 --> 00:12:36,196
the image generation or the video generation softwares,

193
00:12:36,340 --> 00:12:40,540
with the generative AI, it does collect millions

194
00:12:40,572 --> 00:12:43,148
of data. So data sets, right?

195
00:12:43,276 --> 00:12:46,580
And where has it take? What influence,

196
00:12:46,652 --> 00:12:49,836
if, uh, what has affected the result? It is

197
00:12:49,860 --> 00:12:53,356
just simply not humanly possible to cite all

198
00:12:53,380 --> 00:12:56,732
of them with every outcome that is created.

199
00:12:56,868 --> 00:13:00,236
And it is important for it to look at all the data

200
00:13:00,300 --> 00:13:03,684
sources. But that also heavily weighs

201
00:13:03,724 --> 00:13:07,300
in on some of the creative ideas that original

202
00:13:07,372 --> 00:13:10,748
creators have put in, and they are simply not

203
00:13:10,796 --> 00:13:15,084
getting credited for it. Forget the, the reimbursement

204
00:13:15,204 --> 00:13:18,876
for their well thought work put in. So that is,

205
00:13:18,900 --> 00:13:22,196
again, some of the things from which the creative community has

206
00:13:22,220 --> 00:13:25,812
a lot of problem with AI. The next one is the quality.

207
00:13:25,988 --> 00:13:29,428
So a lot of you who have, who may have used

208
00:13:29,516 --> 00:13:33,500
chat GPT for must have seen this, right? I have seen this myself.

209
00:13:33,692 --> 00:13:37,624
If I ask to create the script for this talk,

210
00:13:38,044 --> 00:13:41,308
I would see a lot of things being repeated, which is

211
00:13:41,356 --> 00:13:44,960
known as parroting. I would see the answer that I

212
00:13:44,992 --> 00:13:48,752
get today for a talk on design thinking would be

213
00:13:48,768 --> 00:13:52,808
very different from what I may have got a year ago and maybe very

214
00:13:52,856 --> 00:13:56,376
different the next year, depending on what data sources and

215
00:13:56,400 --> 00:13:59,564
sets it has considered. And sometimes it

216
00:14:00,384 --> 00:14:04,160
will be very outdated as well, creating these drifts.

217
00:14:04,192 --> 00:14:07,952
And the outcome. The last one is hallucinations. Sometimes when

218
00:14:08,008 --> 00:14:11,384
AI is not able to bridge the gap between

219
00:14:11,464 --> 00:14:15,072
what it knows, it puts in things which are

220
00:14:15,208 --> 00:14:18,360
totally out of the context. Right? Just the other day I was

221
00:14:18,392 --> 00:14:22,240
hearing on NPR an example where they said a

222
00:14:22,272 --> 00:14:26,080
newspaper ad agency or a product review

223
00:14:26,232 --> 00:14:30,084
company was using AI, and they were talking about

224
00:14:30,824 --> 00:14:34,864
the gym belts. And suddenly the AI hallucinated and

225
00:14:34,904 --> 00:14:38,470
started putting in text, which was talking about the belts,

226
00:14:38,542 --> 00:14:41,454
the waist belts that you use, or the fashion belts that you use.

227
00:14:41,494 --> 00:14:45,614
And it doesn't take a human to understand when the

228
00:14:45,654 --> 00:14:49,390
AI has done so, and it just creates that cringeworthy experience,

229
00:14:49,502 --> 00:14:52,870
where would you really want to consider it for any serious

230
00:14:52,982 --> 00:14:56,262
discussion or not? So, all these issues

231
00:14:56,438 --> 00:15:00,414
are prominent challenges, because of which users are

232
00:15:00,454 --> 00:15:04,216
not able to trust AI completely. But what is the

233
00:15:04,240 --> 00:15:07,456
root cause? We've talked about some of them, the lack

234
00:15:07,480 --> 00:15:11,320
of explainability, etcetera. But one of the key reasons

235
00:15:11,352 --> 00:15:14,768
this is happening is because while the development of this

236
00:15:14,816 --> 00:15:18,792
technology, people were extremely focused on

237
00:15:18,968 --> 00:15:22,392
the technology itself and how to make

238
00:15:22,528 --> 00:15:26,160
the algorithm work, they were not really taking

239
00:15:26,272 --> 00:15:29,772
the end user in consideration, or society

240
00:15:29,828 --> 00:15:33,820
in consideration, etcetera. But now, when the business applications are

241
00:15:33,852 --> 00:15:37,796
out there in the world, this tech focused tunnel vision is

242
00:15:37,820 --> 00:15:41,740
not working at all. So that is one of the key reasons. Another one

243
00:15:41,772 --> 00:15:45,948
is that when people think from business applications perspective,

244
00:15:46,116 --> 00:15:48,624
they think about people, process and technology.

245
00:15:49,124 --> 00:15:52,868
And now data is a new participant in it.

246
00:15:52,916 --> 00:15:56,404
It is more than the technology itself, it is

247
00:15:56,444 --> 00:16:00,258
more than the process, it is more than the people. It is also an

248
00:16:00,306 --> 00:16:03,546
active participant in an AI ecosystem, in an

249
00:16:03,570 --> 00:16:07,786
AI application. So now, thinking from the

250
00:16:07,930 --> 00:16:10,854
data's point of view is extremely important.

251
00:16:11,274 --> 00:16:14,122
What data sources should you consider?

252
00:16:14,258 --> 00:16:18,314
What should be the quality of those data sources is something

253
00:16:18,474 --> 00:16:22,762
that have to be rethought of when you are developing an AI application,

254
00:16:22,898 --> 00:16:26,528
which right now are not being considered by every product,

255
00:16:26,656 --> 00:16:30,240
and which is where, again, which creates some of these

256
00:16:30,392 --> 00:16:33,568
biases, drift, etcetera. Lack of explainability.

257
00:16:33,696 --> 00:16:37,016
I think we've talked about it enough. There is, of course, the two kinds.

258
00:16:37,080 --> 00:16:40,432
The one that is just not done

259
00:16:40,528 --> 00:16:44,368
by the developers and implementers, and informing the users enough.

260
00:16:44,496 --> 00:16:48,048
But then there is also an inherent explainability in the implementation

261
00:16:48,096 --> 00:16:51,528
of these models. The last, but something that

262
00:16:51,576 --> 00:16:54,628
is very true for any new technology, is that

263
00:16:54,716 --> 00:16:58,132
it is just simply new. We have not lived in an AI

264
00:16:58,188 --> 00:17:01,732
world before. Some of the problems that we are seeing right now

265
00:17:01,868 --> 00:17:06,076
will not exist in 2040. So there will be new problems.

266
00:17:06,100 --> 00:17:09,308
Of course. I think the problems that we would be talking ten

267
00:17:09,356 --> 00:17:12,732
or 15 years from now would be about the supervised

268
00:17:12,828 --> 00:17:16,452
learning, the sustainability of quantum computing,

269
00:17:16,508 --> 00:17:20,210
etcetera. But probably we as a society would

270
00:17:20,242 --> 00:17:23,978
have found out solutions to problems related to bias,

271
00:17:24,026 --> 00:17:28,178
crypt, etcetera. So remember these four reasons

272
00:17:28,266 --> 00:17:31,554
for the rest of the talk. Now that there is

273
00:17:31,594 --> 00:17:35,338
extremely tech focus, the new participant and

274
00:17:35,386 --> 00:17:38,770
data is not being considered in designing.

275
00:17:38,922 --> 00:17:42,506
When the user experience people look at AI

276
00:17:42,570 --> 00:17:46,346
systems and then there is lack of explainability, both from

277
00:17:46,370 --> 00:17:50,452
the technology side as well as from the design side. And overall,

278
00:17:50,508 --> 00:17:54,660
it is a new word for all, not only the implementers and designers,

279
00:17:54,772 --> 00:17:58,580
but also for the users and customers. So nobody really

280
00:17:58,652 --> 00:18:02,744
knows what they expect from the ultimate system.

281
00:18:03,444 --> 00:18:07,036
Why I think design thinking can really help

282
00:18:07,180 --> 00:18:10,484
because if you look at the key tenets of design

283
00:18:10,564 --> 00:18:14,056
thinking, it is, it always keeps it's

284
00:18:14,080 --> 00:18:17,680
focused on user at the center of it.

285
00:18:17,872 --> 00:18:21,600
Everything that you design is for a specific

286
00:18:21,792 --> 00:18:26,048
Persona, and that is extremely important for AI systems, because when

287
00:18:26,096 --> 00:18:29,896
you consider the feelings and emotions

288
00:18:30,000 --> 00:18:34,272
of a person who's going to interact with the system that you have implemented,

289
00:18:34,408 --> 00:18:38,120
you are always going to do a better job than

290
00:18:38,192 --> 00:18:42,184
by throwing technology at them, by perceiving what

291
00:18:42,224 --> 00:18:46,128
they might or might not like. And then the next is it

292
00:18:46,176 --> 00:18:49,952
also involves around. It very much relies on

293
00:18:50,128 --> 00:18:53,568
cut. It also relies on cross functional

294
00:18:53,616 --> 00:18:56,832
collaboration. You can avoid some of or

295
00:18:56,928 --> 00:19:00,672
many of the biases and tech only vision problems

296
00:19:00,768 --> 00:19:04,912
by involving various groups, diverse backgrounds,

297
00:19:05,048 --> 00:19:08,320
diverse data sets, diverse context.

298
00:19:08,472 --> 00:19:11,484
In your AI solution design,

299
00:19:12,264 --> 00:19:15,360
there is also a focus

300
00:19:15,512 --> 00:19:19,096
on iterative development which can really help you to

301
00:19:19,120 --> 00:19:23,004
solve problems incrementally and then reduce risks.

302
00:19:24,024 --> 00:19:27,884
The way it does it is by taking

303
00:19:28,184 --> 00:19:32,072
the solution back to the user sooner than

304
00:19:32,248 --> 00:19:35,894
what traditionally system implementations too.

305
00:19:36,234 --> 00:19:39,778
You cannot solve all the issues that plague AI

306
00:19:39,826 --> 00:19:43,650
currently, such as the talent and skills gap,

307
00:19:43,762 --> 00:19:47,322
or the hardware sustainability and inefficiencies,

308
00:19:47,378 --> 00:19:51,370
etcetera. But there are definitely these application

309
00:19:51,442 --> 00:19:55,378
adoption and trust issues that could benefit

310
00:19:55,466 --> 00:19:59,026
from the approach of design thinking. So before we

311
00:19:59,050 --> 00:20:02,614
go deeper into how, let's see what is design thinking?

312
00:20:03,224 --> 00:20:06,872
What design thinking actually does is that it makes sure

313
00:20:06,928 --> 00:20:10,792
that whatever product or service you're trying to implement is

314
00:20:10,848 --> 00:20:14,296
actually viable for your business and it is feasible with the

315
00:20:14,320 --> 00:20:18,464
technology at hand. And most important, it is desirable

316
00:20:18,584 --> 00:20:22,424
by the users that are going to use it. It is centered

317
00:20:22,544 --> 00:20:26,084
for most of the innovation that has happened recently.

318
00:20:26,464 --> 00:20:30,184
Many of the successful apps, companies, products and services

319
00:20:30,344 --> 00:20:34,000
have benefited from it because of these key

320
00:20:34,032 --> 00:20:38,000
qualities of design thinking approach. But again,

321
00:20:38,072 --> 00:20:41,536
it is not new at all. It is not something that

322
00:20:41,560 --> 00:20:45,072
has only recently come and no products or services have

323
00:20:45,128 --> 00:20:48,872
become widely popular because of this. Without design thinking at the

324
00:20:48,888 --> 00:20:53,048
center, it has always been there. Everything has been said about

325
00:20:53,136 --> 00:20:56,892
this. You take up any industry is going to take a successful

326
00:20:56,948 --> 00:21:00,484
book, take a successful movie, take a successful song.

327
00:21:00,604 --> 00:21:04,524
It has always had these coordinates at its center.

328
00:21:04,644 --> 00:21:08,676
But the reason why we need to talk about it now about

329
00:21:08,740 --> 00:21:12,556
this again is because AI is new and it is showing some of

330
00:21:12,580 --> 00:21:15,996
the problems that other products and services have shown in the past,

331
00:21:16,180 --> 00:21:20,464
which when they use design thinking, help them get better about themselves.

332
00:21:21,084 --> 00:21:24,528
That is why AI could really benefit some of the key tenants

333
00:21:24,576 --> 00:21:28,368
of design thinking. For those of you who go by the definition,

334
00:21:28,496 --> 00:21:32,248
here is the definition for you. It is an approach to solving bigger

335
00:21:32,296 --> 00:21:36,184
problems by understanding users needs and developing insights

336
00:21:36,224 --> 00:21:39,856
to solve those needs, resulting in an AHA experience

337
00:21:40,040 --> 00:21:43,632
for not only the users, but creators and stakeholders

338
00:21:43,688 --> 00:21:47,648
as well. Now before, let me go back before we

339
00:21:47,776 --> 00:21:51,522
move on to when should you use design thinking or not? If you

340
00:21:51,538 --> 00:21:55,194
just look at the highlighted words here, it would give you a

341
00:21:55,234 --> 00:21:58,802
very simple guide on when to use

342
00:21:58,858 --> 00:22:01,854
design thinking or why is it important? Right?

343
00:22:02,234 --> 00:22:05,930
Wicked problems. Problems which are not easily solvable

344
00:22:06,002 --> 00:22:09,610
by simple if then else in for loops, right? When you have

345
00:22:09,642 --> 00:22:13,738
to consider users needs again, think of trust and adoption

346
00:22:13,786 --> 00:22:17,362
issues. When you develop deeper insights, think about

347
00:22:17,418 --> 00:22:21,306
all the data and context that we have been talking about till now and create

348
00:22:21,370 --> 00:22:25,214
an AHA experience, something that people have not experienced before.

349
00:22:25,514 --> 00:22:28,618
And that is what your AI systems is

350
00:22:28,786 --> 00:22:32,258
supposed to do, which we all saw that aha experience

351
00:22:32,346 --> 00:22:36,586
when we use that GPD for the first time, right? So coming

352
00:22:36,650 --> 00:22:39,890
back to when to use it, right? Whenever you need to

353
00:22:39,922 --> 00:22:43,082
understand user needs and develop insights into

354
00:22:43,138 --> 00:22:46,464
those needs, that is when design thinking should be used.

355
00:22:47,004 --> 00:22:50,444
When the problems are really wicked, then that

356
00:22:50,484 --> 00:22:53,540
means they are extremely complex. The answer

357
00:22:53,572 --> 00:22:57,404
is not straightforward. And when you

358
00:22:57,444 --> 00:23:00,772
have, you need to create a unique experience. Now let me give an example

359
00:23:00,828 --> 00:23:04,372
here. For example, in healthcare industry vertical,

360
00:23:04,428 --> 00:23:07,988
there is a lot of talk about analyzing the electronic

361
00:23:08,036 --> 00:23:11,294
health records and creating these use cases

362
00:23:11,454 --> 00:23:14,654
which come up with recommended treatment plans.

363
00:23:14,814 --> 00:23:18,674
The basis of that is that there are certain conditions

364
00:23:18,974 --> 00:23:22,966
which only involve looking at certain parameters and depending

365
00:23:23,070 --> 00:23:26,838
on the context of the patient, the recommended treatment

366
00:23:26,886 --> 00:23:30,334
plans are a few and you cannot

367
00:23:30,374 --> 00:23:33,950
really go wrong. It is really just a few options

368
00:23:34,022 --> 00:23:35,954
that a provider has to consider.

369
00:23:36,634 --> 00:23:40,090
So here you need to understand the

370
00:23:40,122 --> 00:23:44,266
user needs. When you're creating the system, you have to think about the patient

371
00:23:44,330 --> 00:23:48,494
trust issues, right? What if you give the information

372
00:23:49,394 --> 00:23:52,962
of these recommended treatment plans to the customers

373
00:23:53,058 --> 00:23:56,674
or to the patients directly? Right? Are you ready for that

374
00:23:56,714 --> 00:24:00,474
kind of world where people are getting their treatment plans

375
00:24:00,514 --> 00:24:03,644
from an AI solution? Would they be able to

376
00:24:03,684 --> 00:24:07,436
trust it or would it create us? Could it create distrust

377
00:24:07,500 --> 00:24:10,892
in the physicians and providers themselves that

378
00:24:10,988 --> 00:24:14,948
oh, if these are so simple things, there is

379
00:24:15,076 --> 00:24:18,916
a possibility of misuse of that information by the

380
00:24:18,940 --> 00:24:22,316
patients themselves, right? If they have just considered two

381
00:24:22,340 --> 00:24:26,380
or three parameters and coming up with their own treatment plans

382
00:24:26,492 --> 00:24:30,622
and maybe ignored a larger health health condition which

383
00:24:30,678 --> 00:24:34,086
a physician would have been able to look at much more in detail,

384
00:24:34,230 --> 00:24:37,598
right? So maybe for this particular use case,

385
00:24:37,726 --> 00:24:41,022
patients are not the user group that you should be focusing on.

386
00:24:41,158 --> 00:24:45,302
Maybe it still needs physicians or the human supervisions

387
00:24:45,398 --> 00:24:48,974
of certified nurse practitioners or

388
00:24:49,014 --> 00:24:53,030
any other provider Persona type, depending on

389
00:24:53,182 --> 00:24:56,768
where one group is permitted versus other. For example,

390
00:24:56,926 --> 00:25:01,060
nurses should get only these aspects of recommendations.

391
00:25:01,252 --> 00:25:05,052
Probably doctors can look at, or the high level of

392
00:25:05,108 --> 00:25:08,420
recommendations, etcetera. So you probably

393
00:25:08,492 --> 00:25:12,180
need to rethink about your user groups and the needs of that user

394
00:25:12,212 --> 00:25:16,132
groups. Maybe patients are not at all a user group in this particular use case

395
00:25:16,268 --> 00:25:19,732
at this time, right? The next one is why is it

396
00:25:19,748 --> 00:25:23,144
a wicked problem? No one knows the right answer here.

397
00:25:23,904 --> 00:25:27,376
No long term study has been yet done

398
00:25:27,560 --> 00:25:31,376
on automated recommendations of treatment plans

399
00:25:31,480 --> 00:25:34,944
in various conditions. Maybe recommended

400
00:25:35,024 --> 00:25:39,056
test plan may be good for weight management type of issues,

401
00:25:39,240 --> 00:25:43,336
maybe not for a diabetic's weight management,

402
00:25:43,520 --> 00:25:46,856
maybe not a weight management in pregnant people, etcetera,

403
00:25:46,920 --> 00:25:50,326
right? The correctness of this has not yet been been studied

404
00:25:50,390 --> 00:25:53,910
in long term, it is fairly wicked problem, right,

405
00:25:54,022 --> 00:25:57,302
that you have also not studied it. And what happens

406
00:25:57,358 --> 00:26:01,350
to the experience of the end customers and the

407
00:26:01,382 --> 00:26:03,274
provider spaces as well?

408
00:26:04,414 --> 00:26:07,870
You have to create the unique experience here. No one has lived

409
00:26:07,902 --> 00:26:11,222
in the true AI world and you do not know the repercussions

410
00:26:11,278 --> 00:26:15,078
of this, right? So you have to create an experience that

411
00:26:15,126 --> 00:26:20,744
fits their requirements right now that increases

412
00:26:20,784 --> 00:26:24,728
their productivity without compromising on the quality of

413
00:26:24,776 --> 00:26:27,872
care that the end customer is getting. It is

414
00:26:27,888 --> 00:26:31,368
a perfect use case where design

415
00:26:31,456 --> 00:26:35,312
thinking should come in before any actual AI

416
00:26:35,368 --> 00:26:39,112
application is launched in this particular area.

417
00:26:39,288 --> 00:26:42,496
There are several school of thoughts in traditional design

418
00:26:42,560 --> 00:26:46,644
thinking now. Literally, they come from schools,

419
00:26:46,764 --> 00:26:50,196
many universities. The one that is most popular, which we will

420
00:26:50,220 --> 00:26:53,824
be going in detail today, has come from Stanford's

421
00:26:54,284 --> 00:26:56,924
school's framework for design thinking,

422
00:26:57,044 --> 00:27:00,144
and we would look at it in a bit more detail.

423
00:27:00,484 --> 00:27:04,244
But all these different frameworks that exist out there, their key

424
00:27:04,284 --> 00:27:07,900
goals remains the same. They all focus on

425
00:27:07,932 --> 00:27:11,478
understanding the user. They all rely on radical

426
00:27:11,606 --> 00:27:14,554
brainstorming in cross functional groups,

427
00:27:14,974 --> 00:27:18,358
they all promote rapid experimentation and going

428
00:27:18,406 --> 00:27:22,198
back to the users with the test of those. And they all believe

429
00:27:22,326 --> 00:27:26,198
in co creation and collaboration between user groups,

430
00:27:26,246 --> 00:27:29,494
different teams, different sets of data, etcetera.

431
00:27:29,574 --> 00:27:33,398
So doesn't matter which framework you are picking up

432
00:27:33,526 --> 00:27:36,430
until it is these coordinates in it.

433
00:27:36,622 --> 00:27:40,714
But we've talked about all what design thinking is.

434
00:27:41,254 --> 00:27:44,614
Let us also see what it is not. It is

435
00:27:44,654 --> 00:27:48,358
definitely not a quick fix or a band aid type of approach,

436
00:27:48,446 --> 00:27:52,694
right? Where you are seeing certain issues in production

437
00:27:52,814 --> 00:27:56,358
and there is a big user trust problem which you need to fix

438
00:27:56,406 --> 00:27:59,438
in a week or so. Design thinking is not the answer.

439
00:27:59,486 --> 00:28:03,610
You need to do something else about it. Design thinking is generally

440
00:28:03,682 --> 00:28:07,522
needed when you are starting a new product or service

441
00:28:07,698 --> 00:28:11,594
because it takes time to do the iterations. It takes

442
00:28:11,634 --> 00:28:14,834
time to come up with the final solution. It is also

443
00:28:14,914 --> 00:28:18,578
not an approach where you have the technology at hand

444
00:28:18,626 --> 00:28:22,274
and you are looking for where to apply it. Right? What is

445
00:28:22,314 --> 00:28:25,834
happening with most organizations right now? They have

446
00:28:25,954 --> 00:28:29,408
AI and machine learning. Oh, how can we use generative AI

447
00:28:29,496 --> 00:28:32,904
in insurance right now? How can we use generative AI?

448
00:28:32,944 --> 00:28:35,904
So there again, hammer looking for nails.

449
00:28:35,944 --> 00:28:39,600
Right? This is not where, again, design thinking would help you.

450
00:28:39,632 --> 00:28:43,512
Design thinking would really help you. If you have a problem space,

451
00:28:43,608 --> 00:28:47,384
if you have a user group in mind and you want to solve

452
00:28:47,424 --> 00:28:51,352
a specific problem, it is also not a quick response

453
00:28:51,408 --> 00:28:55,486
to competition. Again, enterprises these days are saying,

454
00:28:55,590 --> 00:28:58,814
oh, our competitor a is not yet

455
00:28:58,894 --> 00:29:02,422
using generative AI. Maybe if we do, we will have a

456
00:29:02,438 --> 00:29:05,286
better edge over them. Maybe not,

457
00:29:05,350 --> 00:29:09,342
right? Nobody knows the answer. But definitely design thinking

458
00:29:09,398 --> 00:29:13,326
is not a way to get that competitive edge.

459
00:29:13,390 --> 00:29:16,838
You probably need to look at the strategy a little differently.

460
00:29:16,966 --> 00:29:20,718
You need to look at your industry specific use cases.

461
00:29:20,886 --> 00:29:24,714
You need to look at where your business is differentiated,

462
00:29:24,754 --> 00:29:28,162
etcetera. Again, a whole side of business that

463
00:29:28,258 --> 00:29:32,338
cannot be solved by design thinking. And then it is definitely

464
00:29:32,386 --> 00:29:36,058
not a foolproof formula for sure. Short business success. Again,

465
00:29:36,106 --> 00:29:39,298
related to the third point here, there is,

466
00:29:39,466 --> 00:29:42,986
it's not that you can solve every problem that your business

467
00:29:43,050 --> 00:29:47,234
is currently facing or your use case has with just design thinking.

468
00:29:47,314 --> 00:29:51,030
It is really about creating new products

469
00:29:51,142 --> 00:29:54,670
or experiences or services for known user

470
00:29:54,702 --> 00:29:58,634
groups. So let's look at traditional

471
00:29:58,974 --> 00:30:02,862
design thinking as well as what is emerging for AI in

472
00:30:02,958 --> 00:30:06,894
the market space right now. So as we talked about this

473
00:30:07,014 --> 00:30:09,354
Stanford D school design thinking approach,

474
00:30:09,974 --> 00:30:13,274
there are five modes to this approach, right?

475
00:30:14,334 --> 00:30:18,272
You go from understanding the user, which is the empathize

476
00:30:18,328 --> 00:30:21,992
mode, and we will go into the depth of these modes again

477
00:30:22,048 --> 00:30:25,672
in a little bit. But there are these five modes where you understand

478
00:30:25,768 --> 00:30:29,524
the user, you define the problem space, you collect the ideas,

479
00:30:29,984 --> 00:30:33,744
then you try to solve the problem with rapid prototyping,

480
00:30:33,824 --> 00:30:37,264
and then you test it with the user. Again, this is how

481
00:30:37,304 --> 00:30:41,304
traditional design thinking works. Again, it may look like a waterfall approach,

482
00:30:41,424 --> 00:30:45,016
but there are several iterations that can happen between

483
00:30:45,080 --> 00:30:48,976
the modes themselves. For example, between empathize and define.

484
00:30:49,120 --> 00:30:52,784
You might go back to the user to understand the problem

485
00:30:52,864 --> 00:30:56,640
space. Again, you might want to refine the problem statement again

486
00:30:56,712 --> 00:31:00,128
by talking to the users again, and you can do several such cycle

487
00:31:00,176 --> 00:31:03,952
back and forth, similarly between prototype and test, or between ideate and

488
00:31:03,968 --> 00:31:07,164
prototype, or the whole cycle itself.

489
00:31:07,664 --> 00:31:11,466
It is a very iterative approach, although it may look like waterfall in

490
00:31:11,490 --> 00:31:15,762
this diagram. So this is what traditional design

491
00:31:15,818 --> 00:31:19,002
thinking approach look like. If you look at what is

492
00:31:19,098 --> 00:31:23,026
out there in the world right now, this is an example.

493
00:31:23,090 --> 00:31:26,714
It has come from IBM's designed for AI framework.

494
00:31:26,834 --> 00:31:30,354
Now, again, they are looking at design thinking from the AI's

495
00:31:30,394 --> 00:31:34,554
point of view. From what, how AI is making you think about

496
00:31:34,634 --> 00:31:37,994
the business, about data, about the

497
00:31:38,034 --> 00:31:41,534
understanding of the users. How should

498
00:31:41,574 --> 00:31:45,238
you prototype about it? What knowledge have you created?

499
00:31:45,366 --> 00:31:48,174
What knowledge would you continuously learn about,

500
00:31:48,254 --> 00:31:52,190
etcetera? Again, looking at the problem space from the

501
00:31:52,222 --> 00:31:55,646
AI's lens. Now I want

502
00:31:55,670 --> 00:31:59,270
to explain this for the benefit of those of you who have worked

503
00:31:59,302 --> 00:32:03,358
with design thinking approach, as well as for those who

504
00:32:03,366 --> 00:32:07,238
are doing it for the first time. The traditional design thinking approach

505
00:32:07,326 --> 00:32:11,102
has been there for some time now. People are comfortable with it,

506
00:32:11,158 --> 00:32:14,462
they have deployed it, you've seen it succeed multiple

507
00:32:14,518 --> 00:32:18,302
times. So it is very much possible for people

508
00:32:18,358 --> 00:32:22,014
to just fall back on the traditional approach without really considering

509
00:32:22,054 --> 00:32:26,110
these new frameworks. And then this new framework that

510
00:32:26,142 --> 00:32:29,302
you have seen, for example, that of IBM, requires you

511
00:32:29,358 --> 00:32:33,326
to again practice it, learn it, and without really

512
00:32:33,510 --> 00:32:37,346
implementing it in the real world, it is difficult to be

513
00:32:37,370 --> 00:32:40,986
an expert in these new frameworks and approaches.

514
00:32:41,130 --> 00:32:45,090
So that's why what I want to talk about today is

515
00:32:45,282 --> 00:32:48,674
how you can train the traditional approach and apply some

516
00:32:48,714 --> 00:32:52,114
AI considerations on it so that your learning curve

517
00:32:52,154 --> 00:32:55,586
is not as deep. So that is what approach

518
00:32:55,650 --> 00:32:59,442
I'm going to take for today's talk. If you all have different ideas,

519
00:32:59,538 --> 00:33:02,330
you have any discussion point about it,

520
00:33:02,402 --> 00:33:05,708
we can talk about it later through questions as well.

521
00:33:05,836 --> 00:33:09,332
But right now, for the purposes of this discussion,

522
00:33:09,388 --> 00:33:13,332
let's just take traditional approaches and then apply

523
00:33:13,468 --> 00:33:17,012
AI considerations on them. So again,

524
00:33:17,068 --> 00:33:20,740
I'm going back to the five step or the five

525
00:33:20,812 --> 00:33:24,524
mode framework, the Stanford schools, and for

526
00:33:24,564 --> 00:33:27,836
each of the steps that we would talk about, I would talk about the goal,

527
00:33:27,940 --> 00:33:31,260
the process that is generally taken, and the tools that are available in

528
00:33:31,292 --> 00:33:35,404
traditional approach. So first of all, mode one for empathize.

529
00:33:35,564 --> 00:33:39,180
The aim is to understand the users within the

530
00:33:39,212 --> 00:33:41,780
context of your design challenge.

531
00:33:41,932 --> 00:33:45,140
The process is basically to observe, engage,

532
00:33:45,172 --> 00:33:48,452
and immerse with these users. Some of the tools that are available

533
00:33:48,548 --> 00:33:52,860
for you are interviews, empathy, maps, and in context immersion.

534
00:33:52,972 --> 00:33:56,852
Now, let me give you a quick example of how this traditionally works.

535
00:33:56,948 --> 00:34:01,208
So, for example, if you are creating

536
00:34:01,256 --> 00:34:03,644
an application for kindergarteners,

537
00:34:04,144 --> 00:34:07,480
generally what the designers would do is

538
00:34:07,592 --> 00:34:11,488
be in the classroom at the level of kindergarteners,

539
00:34:11,616 --> 00:34:15,560
sit in that space, see what are their physical constraints,

540
00:34:15,712 --> 00:34:19,696
what do the kids need, what frustrates them, what excites them

541
00:34:19,800 --> 00:34:23,128
in the classroom. And then look at that,

542
00:34:23,176 --> 00:34:26,828
and then see all these concentrations are met when they

543
00:34:26,836 --> 00:34:30,268
are designing the final solution. So they would actually immerse

544
00:34:30,316 --> 00:34:33,852
themselves, they will interview their end users, and they

545
00:34:33,868 --> 00:34:37,444
would create the empathy maps of saying, okay, what do they say?

546
00:34:37,524 --> 00:34:41,308
What do they do, what do they feel, what do they like, what do they

547
00:34:41,396 --> 00:34:45,476
not like? Etcetera. And this in context immersion becomes

548
00:34:45,540 --> 00:34:48,804
extremely important, even for

549
00:34:48,884 --> 00:34:52,605
non AI applications. So let's now, having said that, let's look

550
00:34:52,629 --> 00:34:56,309
at the AI considerations that any of

551
00:34:56,341 --> 00:34:59,353
these modes need to take off. So first we'll go to empathize.

552
00:34:59,773 --> 00:35:03,485
So it is extremely important to observe

553
00:35:03,509 --> 00:35:07,333
the user in non AI word for, again, going deeper

554
00:35:07,373 --> 00:35:10,373
into it. If you are creating an AI application,

555
00:35:10,453 --> 00:35:14,197
it is always very much possible that you would just

556
00:35:14,245 --> 00:35:18,053
interview a user virtually and say, okay,

557
00:35:18,173 --> 00:35:21,574
this is how you are going to use a treatment plan,

558
00:35:21,694 --> 00:35:25,534
right? Maybe somebody who is considering using treatment

559
00:35:25,574 --> 00:35:28,814
plan, somebody who is going through the diagnosis right now,

560
00:35:28,854 --> 00:35:32,366
or somebody who is going through the treatment plan itself

561
00:35:32,430 --> 00:35:35,798
right now. Until, unless you sit with the

562
00:35:35,846 --> 00:35:39,630
user, when the diagnosis is communicated

563
00:35:39,702 --> 00:35:43,174
to them, what do they go through during that time? What kind

564
00:35:43,214 --> 00:35:46,630
of questions do come to their mind? What do they ask

565
00:35:46,782 --> 00:35:50,894
their provider? What does the provider explain to you? Until, unless you

566
00:35:50,934 --> 00:35:54,478
sit in that context and you observe all your user groups

567
00:35:54,526 --> 00:35:57,846
properly, it is often possible to leave

568
00:35:57,910 --> 00:36:01,034
out certain data, certain key consideration,

569
00:36:02,174 --> 00:36:06,030
the point of explainability and all

570
00:36:06,062 --> 00:36:09,754
this is extremely important for your AI applications to create the trust.

571
00:36:10,584 --> 00:36:14,024
Also, you have to select the data sources based on the

572
00:36:14,064 --> 00:36:16,728
authenticity and accuracy, right? Again,

573
00:36:16,856 --> 00:36:20,576
considering the patient's questions, considering what

574
00:36:20,680 --> 00:36:24,112
do the providers or the doctors look at most,

575
00:36:24,168 --> 00:36:28,088
and what do they believe in? Making sure you prioritize that

576
00:36:28,216 --> 00:36:32,008
in your AI application is important. And then you

577
00:36:32,016 --> 00:36:35,500
have to ally the AI solutions to users context,

578
00:36:35,632 --> 00:36:38,884
something that may work very well in,

579
00:36:39,044 --> 00:36:43,100
say, a generative AI chat bot that is general

580
00:36:43,172 --> 00:36:46,268
purpose may not be right for

581
00:36:46,396 --> 00:36:49,836
what a diabetic patient is looking at to consider

582
00:36:49,900 --> 00:36:53,828
its next treatment plan, right? You cannot

583
00:36:53,916 --> 00:36:57,364
give them the same kind of disclaimer

584
00:36:57,484 --> 00:37:00,604
that AI results may be wrong. Here, right? What you

585
00:37:00,684 --> 00:37:03,820
maybe again, as we said, patient may not be the right user group

586
00:37:03,852 --> 00:37:07,076
at all. If it is the doctors and physicians

587
00:37:07,140 --> 00:37:10,548
that are your writer user group, you might consider giving them a confidence

588
00:37:10,636 --> 00:37:14,828
score, saying that this is what algorithm thinks is 90%

589
00:37:14,916 --> 00:37:18,204
accurate or this is 60% accurate, etcetera, so that

590
00:37:18,244 --> 00:37:22,148
it increases their productivity, but also tells them that

591
00:37:22,276 --> 00:37:25,984
how much relevance they should give to a particular again,

592
00:37:26,324 --> 00:37:30,092
it is extremely important to align a general purpose

593
00:37:30,148 --> 00:37:33,730
or a cross function AI application to the user's

594
00:37:33,762 --> 00:37:37,786
context. Going to the second mode, the define generally

595
00:37:37,810 --> 00:37:41,554
in traditional design thinking, you capture the findings of

596
00:37:41,594 --> 00:37:45,234
your empathize mode and you create a deeper understanding by

597
00:37:45,274 --> 00:37:48,890
creating a Persona definition of your users. You craft a

598
00:37:48,922 --> 00:37:52,974
meaningful statement, an actionable problem statement really,

599
00:37:53,674 --> 00:37:57,554
which is generally said like this, right? A user needs

600
00:37:57,674 --> 00:38:00,996
something in a way that they are able to do something.

601
00:38:01,060 --> 00:38:05,092
For example, they may something like diabetes

602
00:38:05,228 --> 00:38:09,340
especially needs to provide treatment plans

603
00:38:09,492 --> 00:38:14,144
in a way that increases their efficiency

604
00:38:14,724 --> 00:38:18,804
and productivity. So that could be our high level problem statement.

605
00:38:18,964 --> 00:38:22,460
And then you might be able to come back with more how

606
00:38:22,492 --> 00:38:25,584
might we statements saying that how

607
00:38:25,624 --> 00:38:29,160
might we be able to increase their productivity through AI?

608
00:38:29,312 --> 00:38:32,832
How might we be able to increase the trust

609
00:38:32,888 --> 00:38:35,752
of patients in the solution through AI?

610
00:38:35,848 --> 00:38:39,736
So there can be several how might we statements that could be horrible.

611
00:38:39,880 --> 00:38:43,376
The next one is really about storytelling, journey mapping

612
00:38:43,400 --> 00:38:47,048
and Personas. These are the tools that are available that people

613
00:38:47,096 --> 00:38:50,704
often use to create the problem statements,

614
00:38:50,744 --> 00:38:54,146
or create these how. How might we statements. These are the

615
00:38:54,170 --> 00:38:58,178
supporting tools that they use now. These are the considerations that people

616
00:38:58,226 --> 00:39:01,930
should keep in mind. They should go back to the user for

617
00:39:01,962 --> 00:39:05,730
validation of these problem statements. Are we considering the

618
00:39:05,762 --> 00:39:08,946
right things for this solution? Is this what you

619
00:39:08,970 --> 00:39:12,538
would really like to see in the final how

620
00:39:12,586 --> 00:39:16,154
would you act if you were able to solve this particular problem

621
00:39:16,234 --> 00:39:20,502
for you, etcetera? And you have to develop these insight

622
00:39:20,678 --> 00:39:23,902
questions for non functional requirements also.

623
00:39:23,998 --> 00:39:27,694
And this is where cross collaboration happens between teams. You need

624
00:39:27,734 --> 00:39:31,014
to bring the security and privacy teams,

625
00:39:31,054 --> 00:39:34,742
the technology team, the infrastructure team, and you have to also

626
00:39:34,838 --> 00:39:37,902
bring in. So when we are talking about efficiency and

627
00:39:37,918 --> 00:39:41,550
productivity of the provider, you also have to

628
00:39:41,742 --> 00:39:45,022
look at what are the security and privacy concerns that you need

629
00:39:45,038 --> 00:39:48,550
to take in mind. What are some of the technology concerns,

630
00:39:48,622 --> 00:39:52,126
investment concerns, etcetera, that you need to take in mind. So again, you have to

631
00:39:52,150 --> 00:39:55,374
cross collaborate, not just with end user, but also

632
00:39:55,414 --> 00:39:59,286
with internal business team. You have to define the

633
00:39:59,310 --> 00:40:02,598
values of your solution that should be tested with each phase.

634
00:40:02,726 --> 00:40:06,638
So this is also extremely important that especially for

635
00:40:06,686 --> 00:40:10,070
AI, you have to put in the values,

636
00:40:10,182 --> 00:40:14,170
for example, quality. What is the targeted accuracy

637
00:40:14,242 --> 00:40:17,866
of the solution? How much drift should be allowed?

638
00:40:17,930 --> 00:40:21,546
What is the continuous learning approach to

639
00:40:21,570 --> 00:40:25,042
this, of your overall? What are

640
00:40:25,098 --> 00:40:28,490
some of the privacy guidelines that the solution should always follow?

641
00:40:28,642 --> 00:40:32,458
And you would see, if you do that in this particular mode,

642
00:40:32,546 --> 00:40:36,002
it's really going to help you when you are coming up

643
00:40:36,018 --> 00:40:39,646
with ideas. It is also going to help you when

644
00:40:39,670 --> 00:40:43,318
you are coming up with prototypes, the final solutions,

645
00:40:43,406 --> 00:40:47,038
etcetera. And this is where again, design thinking can

646
00:40:47,086 --> 00:40:51,022
greatly help AI solutions in maintaining

647
00:40:51,158 --> 00:40:54,966
their quality issues, as well as increasing

648
00:40:55,070 --> 00:40:58,234
the consumers trust in the overall solution.

649
00:40:58,814 --> 00:41:02,366
Coming to mode three idea

650
00:41:02,550 --> 00:41:05,732
in this mode, you basically try to

651
00:41:05,838 --> 00:41:09,184
get as much ideas for solution as

652
00:41:09,224 --> 00:41:13,136
possible. The main aim is to create both

653
00:41:13,200 --> 00:41:17,376
volume and variety. You are totally non judgmental

654
00:41:17,480 --> 00:41:21,288
about the ideas that you are collecting. Usually people use

655
00:41:21,336 --> 00:41:23,872
brainstorms, brainstorming sessions,

656
00:41:24,048 --> 00:41:27,552
and looking at existing solutions with cross

657
00:41:27,608 --> 00:41:31,008
functional teams and creating. They use

658
00:41:31,056 --> 00:41:34,542
mind maps and notes, clouds tools, collect as many ideas

659
00:41:34,638 --> 00:41:38,494
as possible. Now, again, from AI perspective,

660
00:41:38,534 --> 00:41:42,638
if you think if we really actively seek out alternative

661
00:41:42,726 --> 00:41:46,246
sources of data and perhaps even conflicting

662
00:41:46,310 --> 00:41:49,718
point of views to include in our models

663
00:41:49,766 --> 00:41:53,234
of the work, perhaps the algorithms

664
00:41:53,654 --> 00:41:57,294
will be less biased, right? They will be less open to

665
00:41:57,334 --> 00:42:01,026
manipulation. And this is all true for supervised

666
00:42:01,090 --> 00:42:04,186
learning. We are not yet talking about the

667
00:42:04,210 --> 00:42:07,290
future where AI is taking decision of its own. Remember,

668
00:42:07,362 --> 00:42:10,578
singularity is still not here and humans

669
00:42:10,666 --> 00:42:15,154
still have the unique ability to engage in the

670
00:42:15,194 --> 00:42:18,650
decision making. So this is where, from AI's point of

671
00:42:18,682 --> 00:42:21,706
view, you have to involve cross functional teams,

672
00:42:21,810 --> 00:42:25,322
get their context, their alternative views of the world.

673
00:42:25,458 --> 00:42:29,578
You do not have to really solve the problem here. You're just collecting

674
00:42:29,706 --> 00:42:33,042
the data sources, the data sets, the values that you

675
00:42:33,058 --> 00:42:36,626
should consider, things that are important for other teams and groups,

676
00:42:36,770 --> 00:42:40,162
and you're just collecting those ideas,

677
00:42:40,218 --> 00:42:43,570
those solutions. And here you have to surface all

678
00:42:43,602 --> 00:42:47,378
the AI opportunities and pitfalls. Again, this is important.

679
00:42:47,506 --> 00:42:51,218
Traditional design thinkers do not do this, but AI

680
00:42:51,306 --> 00:42:54,770
has almost made it most important that you consider

681
00:42:54,882 --> 00:42:58,896
the security aspects, the privacy aspects,

682
00:42:59,000 --> 00:43:02,784
the infrastructure constraints that you may have on your solution.

683
00:43:02,944 --> 00:43:06,184
And you involved technology, not just business people,

684
00:43:06,264 --> 00:43:09,496
in coming up with the ideas for the solutions. You also

685
00:43:09,520 --> 00:43:13,088
have to consider the pitfalls that your AI solutions may have,

686
00:43:13,136 --> 00:43:17,704
right, the risks that are associated with

687
00:43:17,784 --> 00:43:20,684
getting the solution in the hands of the end user.

688
00:43:21,144 --> 00:43:24,418
And then you have to draw, and this is really

689
00:43:24,466 --> 00:43:28,306
an example, this is really a tool, I would say, which works

690
00:43:28,330 --> 00:43:32,050
for anything, AI or not. When you're doing such brainstorming and just

691
00:43:32,082 --> 00:43:35,786
collecting ideas is to draw some inferences

692
00:43:35,890 --> 00:43:39,466
and some motivations from parallel universes,

693
00:43:39,610 --> 00:43:44,054
which in our context, we would say, for example, if you are developing

694
00:43:44,434 --> 00:43:47,746
something for healthcare, you may look at retail,

695
00:43:47,810 --> 00:43:51,356
or you may look at finance to get some ideas as well. From there,

696
00:43:51,420 --> 00:43:54,860
how people have used AI solutions in those contexts,

697
00:43:54,892 --> 00:43:59,068
there may be a few ideas hidden there that may be applicable in your

698
00:43:59,196 --> 00:44:02,612
context as well. The next mode is prototype.

699
00:44:02,708 --> 00:44:06,404
Here, people create the physical form of the best ideas,

700
00:44:06,444 --> 00:44:10,532
the prioritized ideas, and then they allow people to experience and interact

701
00:44:10,588 --> 00:44:13,876
with them. And that is where they again record their emotions

702
00:44:13,940 --> 00:44:17,392
of how people would react if a

703
00:44:17,448 --> 00:44:20,856
service or product, as hypothesized, would be presented

704
00:44:20,880 --> 00:44:25,160
to them. The process that people use, they generally learn

705
00:44:25,192 --> 00:44:28,864
and explore. They solve any disagreements that could

706
00:44:28,904 --> 00:44:32,032
be there between ideas, between teams. Here, this is a

707
00:44:32,048 --> 00:44:35,976
great opportunity for solving that. They would start conversations about

708
00:44:36,040 --> 00:44:39,992
things that have yet not been talked about between teams or inside

709
00:44:40,048 --> 00:44:43,504
teams yet. And for example, in AI's world,

710
00:44:43,584 --> 00:44:47,536
we can talk about policies that have not yet been considered,

711
00:44:47,640 --> 00:44:51,344
the fears of the users that have not been yet taken into account.

712
00:44:51,424 --> 00:44:54,768
And how could technologists look at solving that?

713
00:44:54,816 --> 00:44:58,368
Right. Breaking the larger problem into smaller

714
00:44:58,416 --> 00:45:01,752
components is again a key tenet here, which really

715
00:45:01,808 --> 00:45:06,024
means that if you are creating, say, generative AI

716
00:45:06,104 --> 00:45:09,976
based chatbot, what are the different aspects

717
00:45:10,040 --> 00:45:13,684
that it would have? What are the different modules that it is going to

718
00:45:13,724 --> 00:45:17,212
have? And you can create a prototype for each different subset of

719
00:45:17,228 --> 00:45:20,636
the problem and test it individually before actually

720
00:45:20,700 --> 00:45:24,460
bringing everything together to see whether.

721
00:45:24,612 --> 00:45:27,836
And again, this is to reduce your risk, reduce your investments of

722
00:45:27,860 --> 00:45:31,532
future. You can fail quickly if people do not like something

723
00:45:31,628 --> 00:45:36,164
at the prototype stage itself. Generally, people use catching

724
00:45:36,244 --> 00:45:40,228
physical mock ups, wireframes, they use interaction

725
00:45:40,276 --> 00:45:43,660
flows, storyboards, prototypes, which are

726
00:45:43,732 --> 00:45:46,984
again, something that you can very quickly create

727
00:45:47,924 --> 00:45:52,100
and test with your end users. For AI,

728
00:45:52,292 --> 00:45:56,196
you have to now think of your prototypes

729
00:45:56,380 --> 00:45:59,908
a little more advanced than they have been before. You have to focus

730
00:45:59,996 --> 00:46:03,740
on the technology a little bit more. You have to set clear

731
00:46:03,812 --> 00:46:07,532
test goals for each prototype. Now, these test goals, remember the define

732
00:46:07,588 --> 00:46:11,128
mode. You are deriving your

733
00:46:11,176 --> 00:46:14,784
test goals from that defined mode. Again, the values that your

734
00:46:14,824 --> 00:46:17,968
system should consider and are your prototypes.

735
00:46:18,056 --> 00:46:21,400
Considering those values, what and what not should

736
00:46:21,432 --> 00:46:24,920
be presented to user in the solution. Have you considered

737
00:46:24,952 --> 00:46:28,328
all that? So test for those goals again

738
00:46:28,416 --> 00:46:32,592
with each prototype. Think how the user will test this

739
00:46:32,688 --> 00:46:36,478
when you are presenting an AI solution to them, right? What are

740
00:46:36,486 --> 00:46:40,166
the things that they could break? What are the lines

741
00:46:40,310 --> 00:46:44,030
outside which they are going to color the solution and

742
00:46:44,062 --> 00:46:47,422
then test the values again and again. I would explain

743
00:46:47,478 --> 00:46:51,054
this. I would emphasize on this rather that

744
00:46:51,134 --> 00:46:54,846
you have to think of explainability, you have to think of bias.

745
00:46:54,910 --> 00:46:58,502
You have to test those values in your prototypes as well as in

746
00:46:58,518 --> 00:47:02,290
the real solution. Next is the

747
00:47:02,322 --> 00:47:06,146
test mode, the final mode of traditional design thinking where you

748
00:47:06,170 --> 00:47:09,738
solicit feedback on prototypes by putting them into the context

749
00:47:09,786 --> 00:47:13,402
of use. You define these prototypes and solution and

750
00:47:13,458 --> 00:47:16,826
learn more about the user. You continue to ask

751
00:47:16,850 --> 00:47:19,842
the why questions and refine your point of view.

752
00:47:20,018 --> 00:47:23,466
Sometimes you create things which and by the time you have

753
00:47:23,490 --> 00:47:27,328
created there is additional insight available to you which

754
00:47:27,376 --> 00:47:31,288
might ask you to pivot totally or change your initial

755
00:47:31,336 --> 00:47:35,904
goals again. So sometimes the iteration can happen after

756
00:47:35,984 --> 00:47:38,680
you reach this end state as well.

757
00:47:38,872 --> 00:47:42,336
So be ready for that. Continue to ask those why

758
00:47:42,400 --> 00:47:46,120
questions. They would really help you to do that. Some of the things

759
00:47:46,152 --> 00:47:49,920
that people use here are again, desirability testing,

760
00:47:49,992 --> 00:47:53,540
field studies, feasibility testing. They do cost

761
00:47:53,612 --> 00:47:57,268
analysis, they do swot analysis, etcetera. Again, some of your

762
00:47:57,316 --> 00:48:01,012
MBA friends can actually help you with that, but this is where

763
00:48:01,068 --> 00:48:04,676
you actually test the feasibility, desirability and viability of

764
00:48:04,700 --> 00:48:08,460
your solution alongside your competition, right? An extremely

765
00:48:08,492 --> 00:48:11,764
important stage. This is what you do before you actually go

766
00:48:11,804 --> 00:48:14,724
ahead and code in a real prototype.

767
00:48:14,884 --> 00:48:17,988
And then some of the AI considerations for this mode

768
00:48:18,036 --> 00:48:21,094
are again, show the user something

769
00:48:21,174 --> 00:48:24,926
that they could test. Don't just tell them an idea and

770
00:48:25,110 --> 00:48:28,994
try to gain their feedback from that verbal idea.

771
00:48:29,334 --> 00:48:32,718
Regard their reactions when they see something for the first time

772
00:48:32,766 --> 00:48:35,670
and how they use it, right? Not just yes,

773
00:48:35,742 --> 00:48:39,030
they like solution and yes, it is passed, right?

774
00:48:39,222 --> 00:48:42,910
Test with a new set of demographics. You may have

775
00:48:43,062 --> 00:48:47,134
collected your requirements or ideas, or even you

776
00:48:47,174 --> 00:48:50,990
may have studied a certain type of demographics and

777
00:48:51,022 --> 00:48:53,990
user group along with testing with them.

778
00:48:54,102 --> 00:48:57,814
Consider alternative sources if you have till now for

779
00:48:57,854 --> 00:49:01,694
your say again going back to our healthcare treatment

780
00:49:01,774 --> 00:49:05,318
plan recommendations use case, suppose you have

781
00:49:05,366 --> 00:49:08,934
tested it with older demographics till now.

782
00:49:09,054 --> 00:49:12,608
Think of what happens when you go to teens or when you go to

783
00:49:12,726 --> 00:49:16,500
young mothers, etcetera, right? So that is where this can really

784
00:49:16,572 --> 00:49:20,636
help when your test mode expands. And this is where again those

785
00:49:20,700 --> 00:49:24,636
trust issues could be avoided. Because till now you might have been considering

786
00:49:24,660 --> 00:49:28,628
a view of the world that was not encompassing of certain things that you

787
00:49:28,636 --> 00:49:32,260
have not considered. The last is test with

788
00:49:32,292 --> 00:49:35,704
newer versions of data sets. This is again just given

789
00:49:36,204 --> 00:49:39,666
how time boxed things generally are in our industry,

790
00:49:39,860 --> 00:49:43,830
you may not have the freedom and ability to test

791
00:49:43,902 --> 00:49:47,774
all the data sets at every stage, even at the prototype stage.

792
00:49:47,814 --> 00:49:51,406
Or before that, when you finally are in test mode

793
00:49:51,470 --> 00:49:54,806
with a real prototype, open it up to newer data source

794
00:49:54,830 --> 00:49:58,350
and then see how it fares against them. See whether it

795
00:49:58,382 --> 00:50:02,438
suffers from hallucinations, drift, or those parroting issues

796
00:50:02,486 --> 00:50:05,886
that we talked about. So this is a place where I've put all

797
00:50:05,910 --> 00:50:09,462
the considerations together for those of you taking screenshots,

798
00:50:09,598 --> 00:50:13,254
so that when you are practicing traditional design

799
00:50:13,334 --> 00:50:16,478
thinking, you have something to go back to and

800
00:50:16,526 --> 00:50:20,246
look at all the recommendations at one place beyond

801
00:50:20,310 --> 00:50:23,582
the design stages too. This work does not stop

802
00:50:23,718 --> 00:50:27,070
and you have to keep these things in

803
00:50:27,102 --> 00:50:30,862
mind when you are actually designing the user interface,

804
00:50:30,958 --> 00:50:35,054
the final solution for your customers. You have to keep transparency in

805
00:50:35,094 --> 00:50:39,168
mind, you have to keep explainability in mind,

806
00:50:39,216 --> 00:50:42,528
and you have to keep testing for alternates users

807
00:50:42,576 --> 00:50:46,032
more than ever, right users, data sets, how your

808
00:50:46,088 --> 00:50:49,736
different releases of your software are acting and you will keep doing

809
00:50:49,800 --> 00:50:53,824
that, not just during the development of your product or service,

810
00:50:53,984 --> 00:50:57,904
but even after it has gone into

811
00:50:57,984 --> 00:51:01,408
production. And when can you do this is

812
00:51:01,496 --> 00:51:05,680
a question I often get right when I introduce these concepts to

813
00:51:05,792 --> 00:51:08,684
senior leaders, cxos, etcetera.

814
00:51:09,344 --> 00:51:12,736
They are like okay, we have already started a pilot or we

815
00:51:12,760 --> 00:51:16,648
do not know anything about AI or we are already

816
00:51:16,776 --> 00:51:20,216
on this journey, are we too late? So you

817
00:51:20,240 --> 00:51:24,064
can start it before you start prototyping your next idea.

818
00:51:24,184 --> 00:51:27,662
You can also employ it in your current project

819
00:51:27,848 --> 00:51:31,154
as a parallel stream and you can

820
00:51:31,234 --> 00:51:34,842
always look at any weird wicked problem and

821
00:51:34,938 --> 00:51:39,090
apply it there. Start wherever you are and you will be fine.

822
00:51:39,282 --> 00:51:42,450
Again, thank you so much for your time today. I hope

823
00:51:42,482 --> 00:51:45,618
you liked this session. Let me know if you have any questions.

824
00:51:45,666 --> 00:51:49,770
You can always shoot me a note at arushi dot shivastava

825
00:51:49,842 --> 00:51:54,226
mail.com or arushi dot shivaswapntdata.com and

826
00:51:54,330 --> 00:51:58,330
I would be happy to discuss this further, especially if you have

827
00:51:58,362 --> 00:52:02,274
any alternate views about this because just like this

828
00:52:02,314 --> 00:52:05,850
approach, I would like to consider diverse data sets in

829
00:52:05,882 --> 00:52:09,226
my thinking as well. Thank you so much again and

830
00:52:09,290 --> 00:52:11,474
hope you enjoy the rest of the conference. Thank you.

