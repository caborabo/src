1
00:00:25,010 --> 00:00:29,314
Welcome. Today we are going to talk about machine

2
00:00:29,362 --> 00:00:33,000
learning in observability and what we did in the data.

3
00:00:33,370 --> 00:00:37,478
The data is a monitoring tool that was born out of a need,

4
00:00:37,564 --> 00:00:41,654
out of frustration. I got frustrated by the monitoring solutions that

5
00:00:41,692 --> 00:00:45,974
existed a few years ago, and I said,

6
00:00:46,092 --> 00:00:48,790
okay, what is wrong?

7
00:00:48,940 --> 00:00:52,746
Why monitoring have these limitations? Why it's

8
00:00:52,778 --> 00:00:56,494
so much of a problem to have high

9
00:00:56,532 --> 00:01:00,574
resolution monitoring, high fidelity monitoring across the board.

10
00:01:00,772 --> 00:01:03,470
And I try to redesign,

11
00:01:03,630 --> 00:01:07,038
rethink, let's say, how monitoring systems

12
00:01:07,134 --> 00:01:10,514
work. So traditionally, a monitoring system looks

13
00:01:10,552 --> 00:01:14,050
like this. So you have some applications or systems

14
00:01:14,550 --> 00:01:18,440
exposing metrics and logs. You push these

15
00:01:18,970 --> 00:01:22,226
metrics and logs to some database servers.

16
00:01:22,338 --> 00:01:26,134
So a time series database like Prometheus, or a monitoring provider like

17
00:01:26,172 --> 00:01:29,590
datadoc, dynatrace, new Relic, et cetera,

18
00:01:30,010 --> 00:01:34,422
or even for logs, elastic or

19
00:01:34,556 --> 00:01:38,086
lockey, or even their splunk commercial providers,

20
00:01:38,118 --> 00:01:41,950
et cetera. So you push all your metrics and logs to these

21
00:01:42,020 --> 00:01:46,014
databases, and then you use the tools that these

22
00:01:46,052 --> 00:01:48,922
databases have available in order to create dashboards,

23
00:01:48,986 --> 00:01:52,410
alerts to explore the logs, the metrics, et cetera.

24
00:01:52,570 --> 00:01:56,594
This has some issues. The biggest issues is

25
00:01:56,792 --> 00:02:00,494
the biggest issues come from the fact that as you push

26
00:02:00,542 --> 00:02:04,062
metrics to them, as you push data, not just metrics, but also logs

27
00:02:04,126 --> 00:02:07,334
to them, they become a lot more

28
00:02:07,372 --> 00:02:10,934
expensive. So you have to be very careful, you have to

29
00:02:11,052 --> 00:02:14,290
carefully select which metrics you centralize,

30
00:02:14,450 --> 00:02:18,650
how you collect and visualize how frequently

31
00:02:19,390 --> 00:02:22,998
you gave, to be very careful about the log streams that you push

32
00:02:23,014 --> 00:02:26,342
to them, the fields that you index, et cetera.

33
00:02:26,486 --> 00:02:30,702
And then, of course, in order to have

34
00:02:30,756 --> 00:02:34,510
a meaningful monitoring solution, you have to learn query languages.

35
00:02:35,170 --> 00:02:39,690
You have to build dashboards, metric by metric and alerts,

36
00:02:39,850 --> 00:02:43,460
metric by metric, et cetera. So the whole process,

37
00:02:44,710 --> 00:02:48,546
the first is that it requires skills. So you have to know what you

38
00:02:48,568 --> 00:02:52,500
are doing. You gave to have experience in doing that thing,

39
00:02:53,830 --> 00:02:57,230
knowing what you need to collect, how frequently

40
00:02:57,310 --> 00:02:58,840
you have to collect it,

41
00:03:00,010 --> 00:03:03,766
knowing what each metric means. Because at the end of

42
00:03:03,788 --> 00:03:06,854
the day, you need to provide dashboards, you need to provide alerts. So you need

43
00:03:06,892 --> 00:03:10,486
to have an understanding of the metrics and the logs,

44
00:03:10,518 --> 00:03:13,914
et cetera, that you have available. It has

45
00:03:13,952 --> 00:03:16,922
a lot of moving parts, so a lot of,

46
00:03:17,056 --> 00:03:20,574
I don't know, integrations or stuff to

47
00:03:20,612 --> 00:03:24,366
install and maintain database servers, visualizers and

48
00:03:24,388 --> 00:03:27,934
the likes. And what happens for most of the companies

49
00:03:28,052 --> 00:03:32,778
is that the skills of the engineers they have reflect

50
00:03:32,874 --> 00:03:36,514
actually the quality of the monitoring they will get. So if

51
00:03:36,552 --> 00:03:40,386
you have good engineers that have a deep understanding of what

52
00:03:40,408 --> 00:03:43,554
they are doing, they have experience in what they do.

53
00:03:43,752 --> 00:03:47,042
You are going to have a good monitoring. But if your engineers

54
00:03:47,106 --> 00:03:51,346
are not that experienced or they don't have much time to spend on monitoring,

55
00:03:51,458 --> 00:03:55,330
then most likely your monitoring will be childish,

56
00:03:55,410 --> 00:03:58,698
will be primitive, it will not be

57
00:03:58,704 --> 00:04:02,362
able to help you when you need it. And of course the whole

58
00:04:02,416 --> 00:04:06,694
process on how to work with monitoring follows a development

59
00:04:06,742 --> 00:04:10,454
lifecycle. So you have to design things,

60
00:04:10,592 --> 00:04:13,774
put the requirement, design that thing,

61
00:04:13,892 --> 00:04:18,110
test it, develop it, test it and then consider

62
00:04:18,180 --> 00:04:21,854
its production quality. What I tried in the data is

63
00:04:21,892 --> 00:04:24,100
zap, everything. So I said, oh come on,

64
00:04:24,550 --> 00:04:28,594
monitoring cannot work like this. So can we find another way

65
00:04:28,712 --> 00:04:31,858
to do it? So I created this application.

66
00:04:32,024 --> 00:04:35,018
It's an open source application that we install everywhere.

67
00:04:35,134 --> 00:04:37,270
This application, Netdata,

68
00:04:38,730 --> 00:04:42,518
has all the moving parts inside it. So it has

69
00:04:42,684 --> 00:04:46,358
800 integrations to collect data from and actually

70
00:04:46,444 --> 00:04:50,170
auto discovers everything. So you don't need to go and configure

71
00:04:50,670 --> 00:04:54,490
everything. And it tries to collect as

72
00:04:54,560 --> 00:04:58,538
many metrics as possible. The second is that it collects everything

73
00:04:58,704 --> 00:05:01,966
per second. Of course there are metrics that are not per second, but this

74
00:05:01,988 --> 00:05:05,230
is because the data source does not expose

75
00:05:06,290 --> 00:05:10,126
this kind of granularity. So the

76
00:05:10,148 --> 00:05:14,334
data source updates the metrics that it exposes every five or

77
00:05:14,372 --> 00:05:17,394
every ten. Otherwise the data will get everything per

78
00:05:17,432 --> 00:05:20,660
second and it will get as many metrics as it can.

79
00:05:21,110 --> 00:05:24,862
It has a database, a time series database

80
00:05:24,926 --> 00:05:28,734
in it. So there is no separate application. It's just a library

81
00:05:28,782 --> 00:05:32,310
inside netdata that stores metrics in files.

82
00:05:34,330 --> 00:05:38,220
So it has a health engine that checks metrics against

83
00:05:39,790 --> 00:05:43,814
common issues that we know that exist

84
00:05:43,862 --> 00:05:47,674
for your applications and your systems. And it learns the

85
00:05:47,712 --> 00:05:51,086
behavior of metrics. And this is what we're going to discuss about machine learning and

86
00:05:51,108 --> 00:05:55,018
how it learns. And of course it provides all the APIs

87
00:05:55,194 --> 00:05:58,810
to query these metrics and these logs

88
00:05:58,890 --> 00:06:02,426
and visualize them and also has all the methodologies.

89
00:06:02,458 --> 00:06:06,254
So it's a nice citizen in the observability ecosystem.

90
00:06:06,302 --> 00:06:09,506
It has integrations to push metrics, even to Prometheus, even to

91
00:06:09,528 --> 00:06:13,570
other monitoring solutions, and to also stream metrics between

92
00:06:13,640 --> 00:06:17,894
the data servers. So it allows you to create centralization points within

93
00:06:18,092 --> 00:06:21,334
your infrastructure. So if

94
00:06:21,372 --> 00:06:24,486
we look how this works behind the

95
00:06:24,508 --> 00:06:28,262
scenes, it may seem complicated. It's not that much. So you have

96
00:06:28,316 --> 00:06:31,882
a local. Net data running on a Linux system, for example.

97
00:06:32,016 --> 00:06:35,654
It will discover all metrics using all the plugins

98
00:06:35,702 --> 00:06:39,226
that it has. It will collect data from these

99
00:06:39,248 --> 00:06:42,794
sources. This is with zero configuration. This is just the default behavior.

100
00:06:42,842 --> 00:06:46,458
This is what it does. It will automatically

101
00:06:46,554 --> 00:06:50,206
detect anomalies and store everything in

102
00:06:50,228 --> 00:06:53,538
its own time series database. Once the data are stored in the

103
00:06:53,544 --> 00:06:57,442
time series database, it provides all these

104
00:06:57,576 --> 00:07:01,438
features, so it learns from the metrics

105
00:07:01,534 --> 00:07:04,938
in order to detect, to feed the trained

106
00:07:04,974 --> 00:07:09,158
models into the anomaly detection. It checks the metrics for

107
00:07:09,244 --> 00:07:11,240
common issues,

108
00:07:12,010 --> 00:07:16,182
congestions, errors and the likes. It can

109
00:07:16,236 --> 00:07:19,834
score the metrics, so it can use different

110
00:07:19,952 --> 00:07:23,254
algorithms to find the needle in the haystack

111
00:07:23,302 --> 00:07:27,046
when you need it. It can query all the metrics

112
00:07:27,078 --> 00:07:30,334
and provide dashboards out of the box.

113
00:07:30,372 --> 00:07:33,726
And the data actually visualizes everything by itself.

114
00:07:33,908 --> 00:07:37,850
So every metric that is collected is also visualized,

115
00:07:37,930 --> 00:07:41,120
correlated and visualized in a meaningful way.

116
00:07:41,810 --> 00:07:45,902
It can export the metrics to third party time series databases

117
00:07:45,966 --> 00:07:49,390
and the likes. And it can also stream

118
00:07:49,470 --> 00:07:52,674
metrics to other netdata servers. So it can

119
00:07:52,712 --> 00:07:56,310
come here from one end data to another.

120
00:07:56,460 --> 00:07:58,994
So you can create metrics,

121
00:07:59,042 --> 00:08:03,030
centralization points on demand. You don't need to centralize everything

122
00:08:03,180 --> 00:08:07,074
on one node or across your infrastructure.

123
00:08:07,122 --> 00:08:11,002
You can have as many centralization points as required across

124
00:08:11,056 --> 00:08:15,254
your infrastructure. This provides both efficiency, cost efficiency,

125
00:08:15,382 --> 00:08:18,460
mainly because there are no egress costs, for example.

126
00:08:19,470 --> 00:08:23,066
But also it allows you to

127
00:08:23,088 --> 00:08:26,474
use netdata in cases where you have ephemeral service, for example.

128
00:08:26,512 --> 00:08:29,614
So you have a kubernetes cluster that nodes come up

129
00:08:29,652 --> 00:08:33,202
and go down all the time. Where are my data? If the data

130
00:08:33,256 --> 00:08:36,946
are in these servers and the data is offline, then where are

131
00:08:36,968 --> 00:08:40,706
my data? So you can have a parent a

132
00:08:40,728 --> 00:08:44,190
centralization point where your data are aggregated,

133
00:08:44,270 --> 00:08:48,102
that are permanently available even if the

134
00:08:48,156 --> 00:08:52,230
data collection server is not available. A similar

135
00:08:52,300 --> 00:08:56,338
methodology happens for logs. For logs, we rely

136
00:08:56,434 --> 00:08:59,706
on systemdjournal. Systemdjournal is an application that we

137
00:08:59,728 --> 00:09:02,458
all use. So even if we don't know,

138
00:09:02,544 --> 00:09:06,474
systemdjournal is there inside our systems. And system dig

139
00:09:06,512 --> 00:09:09,498
journal is amazing for logs. Why?

140
00:09:09,664 --> 00:09:14,586
Because it's

141
00:09:14,618 --> 00:09:18,318
the opposite of what all the other log

142
00:09:18,484 --> 00:09:21,502
solutions do. So for all log data,

143
00:09:21,636 --> 00:09:25,230
log database servers, the cardinality.

144
00:09:25,310 --> 00:09:28,578
So the number of fields that are there and the number of

145
00:09:28,664 --> 00:09:31,780
values that the fields have is important.

146
00:09:33,270 --> 00:09:36,662
And the more you have, the slower it gets,

147
00:09:36,716 --> 00:09:40,114
the more opensource are required, more memory, et cetera, et cetera.

148
00:09:40,242 --> 00:09:43,282
But for the system Dig journal,

149
00:09:43,426 --> 00:09:47,174
the cardinality of the logs is totally relevant. So the system

150
00:09:47,212 --> 00:09:51,194
dig journal is designed to actually index all

151
00:09:51,232 --> 00:09:55,002
fields and all values, even if all log

152
00:09:55,056 --> 00:09:58,726
lines, each log line has a different set of fields

153
00:09:58,758 --> 00:10:02,554
and a different set of values. So it doesn't care about the cardinality

154
00:10:02,602 --> 00:10:06,234
at all. It has been designed first to be secure.

155
00:10:06,362 --> 00:10:09,706
It has ceiling and tampering and a lot of features

156
00:10:09,738 --> 00:10:13,218
to improve security. And at the same time,

157
00:10:13,384 --> 00:10:17,822
it is designed to scale

158
00:10:17,966 --> 00:10:21,538
independently of the number of fields. The only, of course,

159
00:10:21,624 --> 00:10:24,894
drawback if you push huge cardinality

160
00:10:24,942 --> 00:10:28,386
to system digital is the disk footprint,

161
00:10:28,498 --> 00:10:30,866
but not CPU, not memory.

162
00:10:31,058 --> 00:10:33,942
So it's there,

163
00:10:33,996 --> 00:10:37,094
it's inside our system. So what we do is that

164
00:10:37,132 --> 00:10:40,886
we provide for the first is the data, can use journal

165
00:10:40,918 --> 00:10:44,410
files, can query journal files without storing, without moving

166
00:10:44,480 --> 00:10:48,246
the logs to another database server. So we don't have a logs

167
00:10:48,278 --> 00:10:51,370
database server, we rely on systemd journal.

168
00:10:51,870 --> 00:10:55,646
The first is this, and the second is that if you have text files and

169
00:10:55,668 --> 00:10:59,182
you want to push them to systemdjournal, we provide log

170
00:10:59,236 --> 00:11:02,974
to journal, a tool, a command line tool that you can configure to

171
00:11:03,012 --> 00:11:06,426
actually extract structured information from text log

172
00:11:06,468 --> 00:11:09,486
files and push them to systemdate journal.

173
00:11:09,598 --> 00:11:13,774
Systemdate Journal itself has the ability to create multiple

174
00:11:13,822 --> 00:11:17,474
centralization points across the infrastructure, much like Netdata.

175
00:11:17,602 --> 00:11:20,930
So while Netdata can do this for streaming,

176
00:11:21,010 --> 00:11:24,870
with streaming, to push metrics from one data

177
00:11:24,940 --> 00:11:28,970
agent to another, systemd journal has the same

178
00:11:29,040 --> 00:11:33,078
functionality. It provides system digital upload that pushes metrics

179
00:11:33,094 --> 00:11:36,282
to another journal d. And it provides also

180
00:11:36,336 --> 00:11:39,958
system digital remote that ingests this metric

181
00:11:40,054 --> 00:11:43,660
and stores them locally. If you put net data

182
00:11:44,210 --> 00:11:47,678
in a parent, let's say in a log centralization point,

183
00:11:47,764 --> 00:11:51,566
Netdata will automatically pick all your logs. So the

184
00:11:51,588 --> 00:11:54,894
idea with this setup is that you

185
00:11:54,932 --> 00:11:58,610
install netdata everywhere on all your servers.

186
00:11:59,830 --> 00:12:03,214
If you want to centralize, if you have ephemeral nodes, et cetera,

187
00:12:03,262 --> 00:12:06,150
you have the methodology to create centralization points,

188
00:12:06,220 --> 00:12:10,358
but not one. You can configure as many centralization points

189
00:12:10,444 --> 00:12:14,166
as is optimal for your setup in terms of cost or

190
00:12:14,188 --> 00:12:17,990
complexity or none if you don't require any.

191
00:12:18,140 --> 00:12:21,978
And then the whole infrastructure becomes one.

192
00:12:22,064 --> 00:12:25,722
How it becomes one. So it becomes one with the help of

193
00:12:25,776 --> 00:12:29,146
our SaaS offering that we have. Of course,

194
00:12:29,168 --> 00:12:32,874
it has a free tier too. So you install netdata everywhere

195
00:12:33,002 --> 00:12:36,522
and then all these are independent servers.

196
00:12:36,666 --> 00:12:40,986
But then Netdata cloud can provide dashboards

197
00:12:41,018 --> 00:12:44,650
and alerts for all of them for metrics and logs.

198
00:12:44,810 --> 00:12:48,306
And if you don't want to use the SaaS offering, you can

199
00:12:48,328 --> 00:12:51,362
do the same with any data parent. So the same

200
00:12:51,416 --> 00:12:55,138
software that is installed in your servers, you can build a

201
00:12:55,144 --> 00:12:59,538
centralization point. You can centralize here metrics and logs,

202
00:12:59,634 --> 00:13:03,430
and this thing will of course do all the

203
00:13:03,500 --> 00:13:05,800
mail stuff and whatever else needed.

204
00:13:06,570 --> 00:13:10,326
And from this point you can have fully automated

205
00:13:10,438 --> 00:13:14,294
multi node dashboards and fully automated alerts

206
00:13:14,342 --> 00:13:18,102
for the whole infrastructure. If your setup is more complex,

207
00:13:18,166 --> 00:13:22,042
you can do it like this. So in this setup

208
00:13:22,106 --> 00:13:26,378
there are different data centers or cloud providers.

209
00:13:26,474 --> 00:13:30,286
So this is a hybrid setup in this case or a

210
00:13:30,308 --> 00:13:34,030
multi cloud setup. Again, you have if you want

211
00:13:34,100 --> 00:13:37,350
multiple parents all over the place, and then you can use medada

212
00:13:37,370 --> 00:13:40,750
cloud to integrate the totally independent parents.

213
00:13:40,910 --> 00:13:44,050
If you don't want to use no data cloud, again you can use

214
00:13:44,120 --> 00:13:47,350
a data grandparent. But this time, this thing,

215
00:13:47,420 --> 00:13:50,600
the grandparent needs to centralize everything.

216
00:13:50,970 --> 00:13:54,162
Now what this setup

217
00:13:54,226 --> 00:13:57,818
provides is the following. The first thing is that we manage

218
00:13:57,904 --> 00:14:01,862
to decouple completely cardinality and granularity

219
00:14:02,006 --> 00:14:05,942
from the economics of monitoring of observability.

220
00:14:06,086 --> 00:14:09,706
So you can have as many metrics. The data is about having

221
00:14:09,808 --> 00:14:13,226
all the metrics available. If a metric is available, if there is a data opensource

222
00:14:13,258 --> 00:14:17,118
that expose a metric, this is the standard policies that we have.

223
00:14:17,204 --> 00:14:21,214
Grab it, bring it in, store it, analyze it, learn about it,

224
00:14:21,412 --> 00:14:25,170
attach alerts to it, et cetera. So all

225
00:14:25,240 --> 00:14:29,234
metrics in full resolution, everything is per second for

226
00:14:29,272 --> 00:14:32,290
all applications, for all components, for all systems,

227
00:14:33,190 --> 00:14:37,046
and even the visualization is per second. So the

228
00:14:37,068 --> 00:14:39,650
data collection to visualization latency.

229
00:14:39,810 --> 00:14:42,982
While in most monitoring solutions it's a problem

230
00:14:43,116 --> 00:14:46,262
in a data, you hit enter on a terminal to make a change and

231
00:14:46,316 --> 00:14:50,746
boom, it's immediately on the dashboard. It's less than a second.

232
00:14:50,928 --> 00:14:54,294
Data collection to visualization the time required

233
00:14:54,342 --> 00:14:57,946
from data collection to visualization. The second is that

234
00:14:58,048 --> 00:15:01,454
all metrics are visualized. So you don't need to do

235
00:15:01,492 --> 00:15:04,654
anything, you don't need to visualize metrics yourself.

236
00:15:04,772 --> 00:15:08,814
Everything is visualized, everything is correlated. So the moment

237
00:15:09,012 --> 00:15:11,470
we create plugins for Netdata,

238
00:15:13,570 --> 00:15:17,106
we attach to them all the metadata required in

239
00:15:17,128 --> 00:15:20,238
order for the fully automated dashboard and visualization

240
00:15:20,334 --> 00:15:23,730
to work out of the box for you. The next

241
00:15:23,800 --> 00:15:27,030
is that we're going to see this in a while.

242
00:15:27,100 --> 00:15:30,566
Our visualization is quite powerful. So you don't need to

243
00:15:30,588 --> 00:15:32,310
learn a query language.

244
00:15:34,890 --> 00:15:38,602
You can slice and dice the data, any data set

245
00:15:38,656 --> 00:15:42,074
actually on a data charts with just point and click.

246
00:15:42,192 --> 00:15:46,518
And actually in the data is the only tool that is totally transparent

247
00:15:46,614 --> 00:15:48,940
on where data are coming from,

248
00:15:50,130 --> 00:15:53,630
if there are missed samples somewhere.

249
00:15:54,210 --> 00:15:57,390
So all these work out of the box for you,

250
00:15:57,460 --> 00:16:01,722
including alerts. So in a data, when we build alerts,

251
00:16:01,786 --> 00:16:05,314
we create alert templates. We say

252
00:16:05,352 --> 00:16:09,790
for example, attach these alerts to all network interfaces,

253
00:16:09,870 --> 00:16:13,966
attach these alerts to all disk

254
00:16:13,998 --> 00:16:17,826
devices, to all mount points, attach these alerts to all NgINX

255
00:16:17,858 --> 00:16:22,514
servers or all postgres servers. So we create templates

256
00:16:22,642 --> 00:16:26,358
of alerts that are automatically attached to

257
00:16:26,444 --> 00:16:29,560
your instances, your data.

258
00:16:30,170 --> 00:16:33,282
And of course we don't use fixed thresholds.

259
00:16:33,346 --> 00:16:37,154
Everything is about rolling Windows and statistical analysis

260
00:16:37,202 --> 00:16:39,560
and the likes in order to figure out,

261
00:16:40,130 --> 00:16:43,440
ah, we should trigger an alert or not.

262
00:16:45,890 --> 00:16:50,080
Some people may think, okay, since this is an application that

263
00:16:50,850 --> 00:16:54,830
we should install everywhere on all our servers,

264
00:16:55,490 --> 00:16:58,754
and it has a database in it, it has a machine learning in it,

265
00:16:58,792 --> 00:17:02,820
then it must be heavy. No, it's not.

266
00:17:03,190 --> 00:17:05,894
Actually, it's lighter than everything.

267
00:17:06,012 --> 00:17:09,846
Here we have a comparison with Prometheus as a

268
00:17:09,868 --> 00:17:13,126
centralization point. And you can see that we

269
00:17:13,148 --> 00:17:16,434
tested it with 2.7 million metrics,

270
00:17:16,482 --> 00:17:20,806
samples per second, 2.7 time series,

271
00:17:20,918 --> 00:17:24,166
everything collected per second, 40,000 containers,

272
00:17:24,278 --> 00:17:28,058
500 servers. And you see that Netdata used

273
00:17:28,144 --> 00:17:32,430
one third less CPU compared to Prometheus, half the memory,

274
00:17:33,090 --> 00:17:37,118
10% less bandwidth, almost no

275
00:17:37,204 --> 00:17:40,366
disk I o compared to Prometheus. This means that my data, when it

276
00:17:40,388 --> 00:17:44,270
writes data to disk, it writes them at the right place. So it compresses everything.

277
00:17:44,340 --> 00:17:48,162
It writes in batches, small increments, and puts everything

278
00:17:48,216 --> 00:17:51,842
in the right place in one go. And at the same time,

279
00:17:51,896 --> 00:17:57,154
it has an amazing storage footprint. So the

280
00:17:57,192 --> 00:18:00,730
average sample for the high resolution tier,

281
00:18:00,910 --> 00:18:04,722
for the per second metrics is 0.6 bytes

282
00:18:04,786 --> 00:18:07,910
per sample. So every value that we collect,

283
00:18:09,070 --> 00:18:12,954
it just needs 0.6 bytes less than

284
00:18:12,992 --> 00:18:16,886
a byte, a little bit above half a byte

285
00:18:16,998 --> 00:18:20,518
on disk. Of course, this depends

286
00:18:20,534 --> 00:18:23,966
on compression, et cetera. And what I didn't tell you

287
00:18:23,988 --> 00:18:27,934
is that we have on our blog, we have

288
00:18:27,972 --> 00:18:31,886
a comparison with all the other agents that Datatrace has,

289
00:18:31,908 --> 00:18:34,610
or Datadog has, or Eurelli has, et cetera,

290
00:18:35,510 --> 00:18:39,010
a comparison on resources. What resources need data

291
00:18:39,080 --> 00:18:42,862
required from you when it is installed,

292
00:18:43,006 --> 00:18:46,626
and the data is among the lightest.

293
00:18:46,818 --> 00:18:49,686
So it's written in C,

294
00:18:49,788 --> 00:18:53,142
and the core is highly optimized with

295
00:18:53,196 --> 00:18:56,818
ML, with machine learning enabled. It's the lightest

296
00:18:56,914 --> 00:19:02,122
across all the agents. So this

297
00:19:02,176 --> 00:19:03,420
makes net data,

298
00:19:05,150 --> 00:19:08,234
let's say that you

299
00:19:08,272 --> 00:19:11,594
can now build with net data a distributed

300
00:19:11,642 --> 00:19:15,920
pipeline of all your metrics and logs without

301
00:19:16,450 --> 00:19:19,930
all the problems that you gave from centralizing

302
00:19:20,010 --> 00:19:23,826
metrics and logs. So you can have infinite scalability and

303
00:19:23,848 --> 00:19:27,730
at the same virtually infinite scalability. Don't be arrogant

304
00:19:28,230 --> 00:19:32,130
and at the same time have

305
00:19:32,280 --> 00:19:35,586
high fidelity monitoring and out of the box. So you

306
00:19:35,608 --> 00:19:39,334
don't need to know what you can install Netdata mid

307
00:19:39,372 --> 00:19:42,946
crisis, so you have a problem, you don't have a monitoring

308
00:19:42,978 --> 00:19:46,280
in place. Install net data, it will tell you what is wrong.

309
00:19:46,730 --> 00:19:49,770
So let's move on to AI

310
00:19:50,510 --> 00:19:52,170
in 2019.

311
00:19:53,310 --> 00:19:56,426
Google Todd Underwood from Google gave this

312
00:19:56,448 --> 00:19:59,722
speech about so what Google did is that

313
00:19:59,776 --> 00:20:03,706
they gathered several engineers, SREs, DevOps and the likes,

314
00:20:03,818 --> 00:20:07,006
and they asked them what they expect from machine learning to

315
00:20:07,028 --> 00:20:10,398
do for them. And it turned out that

316
00:20:10,564 --> 00:20:13,060
none of their ideas worked.

317
00:20:14,310 --> 00:20:17,970
Why it didn't work because when people hear

318
00:20:18,040 --> 00:20:21,682
machine learning, the expectations they have are a little bit different of what

319
00:20:21,736 --> 00:20:25,842
actually machine learning can do. So let's see, let's understand first

320
00:20:25,896 --> 00:20:29,142
how machine learning works. Machine learning. In machine learning,

321
00:20:29,196 --> 00:20:32,886
you train model based on sample data. So you

322
00:20:32,908 --> 00:20:36,646
give some samples to it, some old data to it,

323
00:20:36,748 --> 00:20:40,234
and it trains a model. Now, the idea is that

324
00:20:40,272 --> 00:20:43,926
if you give new data to it, it should detect

325
00:20:44,038 --> 00:20:47,786
if the new data are aligned with the patterns you

326
00:20:47,808 --> 00:20:51,286
saw in the past or if they are outliers.

327
00:20:51,478 --> 00:20:55,182
If they are outliers, then you have to train it more in order

328
00:20:55,236 --> 00:20:59,390
to learn the new patterns and repeat the process until

329
00:20:59,460 --> 00:21:03,522
you have the right model. What most people believe is

330
00:21:03,576 --> 00:21:07,342
that machine learning models can be served.

331
00:21:07,486 --> 00:21:10,722
So assume that you have a database server. You can train

332
00:21:10,776 --> 00:21:14,514
a machine learning on one database server and apply the

333
00:21:14,552 --> 00:21:18,322
trained machine learning model to another to detect

334
00:21:18,386 --> 00:21:22,230
outliers and anomalies. The truth is that it is not.

335
00:21:22,300 --> 00:21:25,640
So let's assume that we have two database servers, A and B.

336
00:21:27,390 --> 00:21:30,982
They run on identical hardware, they have the same operating

337
00:21:31,046 --> 00:21:34,374
system, they run the same application, a database server,

338
00:21:34,422 --> 00:21:37,594
postgres, same version. They have exactly the

339
00:21:37,632 --> 00:21:42,266
same data, so they are identical,

340
00:21:42,378 --> 00:21:45,662
both of them. Can we train a model on

341
00:21:45,716 --> 00:21:49,278
A and apply this model

342
00:21:49,364 --> 00:21:52,618
on B? Will it be reliable? Most likely not.

343
00:21:52,724 --> 00:21:57,566
Why not? Because the trained model has incorporated

344
00:21:57,678 --> 00:22:01,154
into it the workload. So if the

345
00:22:01,192 --> 00:22:05,086
workload on B is slightly different, it runs some

346
00:22:05,208 --> 00:22:09,030
statistical queries, some reports that a doesn't,

347
00:22:10,250 --> 00:22:13,526
or if the load balancer, that is, if there

348
00:22:13,548 --> 00:22:17,454
is a load balancer or the clustering software, it spreads

349
00:22:17,522 --> 00:22:21,686
the stuff a little bit, not completely equally

350
00:22:21,798 --> 00:22:25,274
among the two. Then the

351
00:22:25,312 --> 00:22:29,674
behavior that the machine learning model that was trained on a

352
00:22:29,792 --> 00:22:33,386
will not work on B. It will give false

353
00:22:33,418 --> 00:22:36,958
positive. So what can we do? If this

354
00:22:36,964 --> 00:22:38,640
is the case, what can we do?

355
00:22:40,050 --> 00:22:43,714
Let's understand the following. The first is that machine learning

356
00:22:43,832 --> 00:22:48,500
is the simplest solution for

357
00:22:49,190 --> 00:22:52,322
learning the behavior of metrics. So it can,

358
00:22:52,456 --> 00:22:55,182
given enough data, enough samples,

359
00:22:55,326 --> 00:22:58,258
it can learn the behavior of the metrics.

360
00:22:58,354 --> 00:23:02,118
So if you grab a new sample and you give it

361
00:23:02,124 --> 00:23:05,478
to it, it can tell you if this

362
00:23:05,564 --> 00:23:09,002
sample that you just collected is an outlier. Sorry,

363
00:23:09,056 --> 00:23:12,106
it's an outlier or not. This is

364
00:23:12,128 --> 00:23:15,722
what anomaly detection is. So you train the model,

365
00:23:15,856 --> 00:23:19,370
you collect a new sample, you check against the model

366
00:23:19,520 --> 00:23:22,960
you have true or false, if it is an anomaly or not.

367
00:23:23,330 --> 00:23:27,294
Now, the whole point of

368
00:23:27,332 --> 00:23:31,134
this is how reliable it is, if it is

369
00:23:31,172 --> 00:23:34,478
accurate, so it's not accurate.

370
00:23:34,654 --> 00:23:38,594
So if you have one machine learning model and you

371
00:23:38,632 --> 00:23:42,180
actually train

372
00:23:43,350 --> 00:23:46,978
one machine learning model and you actually give samples

373
00:23:46,994 --> 00:23:50,738
to it, just collect samples to it, it has some noise.

374
00:23:50,914 --> 00:23:55,000
So by itself, machine learning should not be something,

375
00:23:55,370 --> 00:23:58,794
an anomaly should not be something to wake up

376
00:23:58,992 --> 00:24:01,866
at 03:00 a.m. Because it will happen.

377
00:24:02,048 --> 00:24:06,950
Of course, you can reduce the noise by learning multiple

378
00:24:07,030 --> 00:24:10,726
machine learning models. So you train multiple models,

379
00:24:10,838 --> 00:24:14,254
and then when you collect a sample, you check against

380
00:24:14,372 --> 00:24:17,918
all of them. If all of them agree that

381
00:24:18,004 --> 00:24:21,102
this sample is an anomaly, then you can say,

382
00:24:21,156 --> 00:24:24,900
okay, this is an anomaly. Still, there are

383
00:24:25,590 --> 00:24:28,994
false positives. Still, you should not wake up at 03:00

384
00:24:29,032 --> 00:24:31,700
a.m. But look what happens.

385
00:24:32,710 --> 00:24:36,610
What we realized is that if this

386
00:24:36,760 --> 00:24:40,370
inaccurate anomaly rates, these noisy

387
00:24:40,450 --> 00:24:43,734
anomaly rates, are triggering for

388
00:24:43,772 --> 00:24:48,150
a short period of time across multiple metrics.

389
00:24:48,510 --> 00:24:51,990
So it's not random anymore. It's multiple

390
00:24:52,070 --> 00:24:55,066
metrics that for a period of time,

391
00:24:55,248 --> 00:24:58,650
they all trigger together. They all say,

392
00:24:58,720 --> 00:25:02,334
I am anomalous together, then we know for

393
00:25:02,372 --> 00:25:06,046
sure that something anomalous is happening

394
00:25:06,228 --> 00:25:09,534
at a larger scale. It's not just a metric now.

395
00:25:09,652 --> 00:25:12,946
It's the system or the service level or the

396
00:25:12,968 --> 00:25:17,582
application level that triggers a lot anomalies

397
00:25:17,726 --> 00:25:22,962
across many metrics. So how

398
00:25:23,016 --> 00:25:26,486
this can help us? So we use,

399
00:25:26,588 --> 00:25:29,750
of course, this is what we did in the data. So we train

400
00:25:29,820 --> 00:25:34,390
multiple machine learning models, and we try to detect anomalies

401
00:25:36,570 --> 00:25:40,682
to make anomalies useful, not just because one

402
00:25:40,736 --> 00:25:44,186
metric had one anomaly at some point,

403
00:25:44,288 --> 00:25:48,214
this is nothing, but because a lot of metrics

404
00:25:48,262 --> 00:25:50,990
at the same time are anomalous,

405
00:25:52,050 --> 00:25:55,710
and we try to use this to help people

406
00:25:55,860 --> 00:26:00,030
troubleshoot problems more efficiently.

407
00:26:00,370 --> 00:26:04,366
So how

408
00:26:04,388 --> 00:26:07,922
we use it in the data, the first is, you understand, since the data

409
00:26:07,976 --> 00:26:11,762
is installed, the moment

410
00:26:11,816 --> 00:26:15,730
you run it, it comes up with hundreds of charts.

411
00:26:16,330 --> 00:26:20,326
You probably will see for the first time. So how do

412
00:26:20,348 --> 00:26:23,606
you know what is important? This is the first question that we

413
00:26:23,628 --> 00:26:27,606
try to answer. So you just see

414
00:26:27,788 --> 00:26:31,206
in front of this amazing dashboard, a lot of charts,

415
00:26:31,318 --> 00:26:34,918
hundreds of thousands of metrics, and hundreds of charts.

416
00:26:35,094 --> 00:26:38,634
And what is important, machine learning

417
00:26:38,672 --> 00:26:42,720
can help us with this, and we will see how. The second is,

418
00:26:43,330 --> 00:26:46,954
you face a problem. I know that the spike or dive

419
00:26:47,002 --> 00:26:50,814
or for this time frame, I know

420
00:26:50,852 --> 00:26:54,642
that there is a problem. Can you tell me what is there

421
00:26:54,776 --> 00:26:58,834
in most monitoring solutions to

422
00:26:58,872 --> 00:27:02,466
troubleshoot this issue? You go through speculation. So if you

423
00:27:02,488 --> 00:27:05,846
use, for example, Mithoso Grafana, you say okay. And you

424
00:27:05,868 --> 00:27:10,134
see for example a spike or a dive in your

425
00:27:10,332 --> 00:27:13,686
web server responses, or increased latency in

426
00:27:13,708 --> 00:27:17,990
your web server responses, a spike. There you start speculating.

427
00:27:18,650 --> 00:27:22,442
What if is the database server? Oh no, what if is the

428
00:27:22,496 --> 00:27:25,990
storage? And you start speculating,

429
00:27:26,150 --> 00:27:29,782
making assumptions and then trying to validate

430
00:27:29,846 --> 00:27:33,614
or drop these assumptions. What we tried with net

431
00:27:33,652 --> 00:27:36,014
data is to flip it completely.

432
00:27:36,212 --> 00:27:39,646
So you highlight the area, the time you

433
00:27:39,668 --> 00:27:42,846
are interested, and the data gives you an ordered

434
00:27:42,878 --> 00:27:47,122
list, a sorted list of what was most anomalous during

435
00:27:47,176 --> 00:27:50,962
that time, hoping that your

436
00:27:51,016 --> 00:27:55,038
aha moment is within the first 2030 entries.

437
00:27:55,134 --> 00:27:58,454
So the idea is that instead of speculating what could

438
00:27:58,492 --> 00:28:01,560
be wrong, in order to figure it out and solve it,

439
00:28:03,130 --> 00:28:06,998
what we do is we go to netdata and a data gives us a list

440
00:28:07,084 --> 00:28:10,730
of what is most anomalous during that time and our

441
00:28:10,800 --> 00:28:14,234
aha moment. The disk did that, the storage did that,

442
00:28:14,272 --> 00:28:17,546
or the database did that is in front

443
00:28:17,568 --> 00:28:22,942
of our eyes. And how

444
00:28:23,076 --> 00:28:26,606
we can find correlations between

445
00:28:26,788 --> 00:28:30,622
components. What happens when this thing

446
00:28:30,676 --> 00:28:34,162
runs? What happens when a user logs in?

447
00:28:34,216 --> 00:28:37,746
What happens when what is

448
00:28:37,768 --> 00:28:42,434
affected? Because if you have a

449
00:28:42,472 --> 00:28:46,114
steady workload and then suddenly you do something,

450
00:28:46,232 --> 00:28:49,574
a lot of metrics will become anomalous. And this allows you to

451
00:28:49,612 --> 00:28:52,966
see the dependencies between the metrics immediately. So you

452
00:28:52,988 --> 00:28:56,326
will see, for example, that the moment a

453
00:28:56,348 --> 00:28:59,626
cron job runs, a lot of things are

454
00:28:59,648 --> 00:29:03,142
affected. A lot of totally,

455
00:29:03,286 --> 00:29:06,502
seemingly independent, independent metrics

456
00:29:06,646 --> 00:29:09,770
get affected. So let's see them in action.

457
00:29:10,750 --> 00:29:14,506
Netdata trains 18 machine

458
00:29:14,538 --> 00:29:17,290
learning models for each metric.

459
00:29:17,450 --> 00:29:21,342
So if on a default Netdata you may have 3000

460
00:29:21,396 --> 00:29:25,970
4000 metrics on a server, for each of these 3000

461
00:29:26,040 --> 00:29:29,406
4000 metrics, it will train 18 machine

462
00:29:29,438 --> 00:29:33,042
learning models over time. Now these

463
00:29:33,096 --> 00:29:37,160
machine learning models generate anomalies, but the anomaly information

464
00:29:37,610 --> 00:29:41,554
is stored together with the samples. So every sample

465
00:29:41,602 --> 00:29:44,870
on disk has, okay, this is the value I collected.

466
00:29:45,210 --> 00:29:48,634
It was anomalous or not anomalous? A bit. It was anomalous or not

467
00:29:48,672 --> 00:29:52,150
anomalous. Then it calculates.

468
00:29:52,230 --> 00:29:56,154
The query engine can calculate the anomaly rate as a

469
00:29:56,192 --> 00:29:59,654
percentage. So when you view a time frame

470
00:29:59,702 --> 00:30:03,406
for a metric, it can tell you the anomaly rate of that

471
00:30:03,588 --> 00:30:05,840
metric during that time.

472
00:30:06,610 --> 00:30:09,806
It's a percentage, and it is the number of samples that

473
00:30:09,828 --> 00:30:13,506
were anomalous versus the total number of metrics. And it

474
00:30:13,528 --> 00:30:16,430
can also calculate host level anomaly score.

475
00:30:16,510 --> 00:30:20,462
So the host level anomaly score is when all the metrics

476
00:30:20,526 --> 00:30:24,254
get aligned as anomalies together that we were discussing

477
00:30:24,302 --> 00:30:28,534
before. Now the

478
00:30:28,572 --> 00:30:32,134
data query engine calculates the anomaly rates in

479
00:30:32,172 --> 00:30:34,886
one go. So this is another thing that we did.

480
00:30:34,988 --> 00:30:38,490
So with the moment you query charts

481
00:30:38,830 --> 00:30:43,158
for the samples, the chart that you want, et cetera,

482
00:30:43,334 --> 00:30:47,226
all the anomaly information, whatever anomaly information

483
00:30:47,328 --> 00:30:51,210
is there is visualized together. It's in the same output.

484
00:30:51,290 --> 00:30:55,098
One query does everything, both samples and anomalies.

485
00:30:55,274 --> 00:30:58,234
Now let's see, chart. This is a data chart.

486
00:30:58,362 --> 00:31:02,106
It looks like any chart, I think from any monitoring

487
00:31:02,138 --> 00:31:05,346
system, but there are a few differences. And let's see the

488
00:31:05,368 --> 00:31:08,706
differences. The first thing is that there is an

489
00:31:08,728 --> 00:31:12,178
anomaly rebound. Now this rebound shows

490
00:31:12,264 --> 00:31:15,810
the anomalies. How many

491
00:31:15,880 --> 00:31:18,750
samples were anomalous across time.

492
00:31:18,840 --> 00:31:22,726
So I don't know. This is for some time here. And you

493
00:31:22,748 --> 00:31:26,294
can see that at this moment there were anomalies. At this moment and

494
00:31:26,332 --> 00:31:30,170
at this moment there were anomalies. Now the Srecon thing

495
00:31:30,240 --> 00:31:33,718
is that we created this needle

496
00:31:33,814 --> 00:31:37,094
framework. Needle stands for nodes, instances,

497
00:31:37,142 --> 00:31:40,880
dimensions, and labels. Now look what this do.

498
00:31:41,330 --> 00:31:45,134
The moment you click nodes. So it

499
00:31:45,172 --> 00:31:49,246
clicked here. Nodes, you get this view. This view

500
00:31:49,348 --> 00:31:53,146
tells you the nodes the data are coming from.

501
00:31:53,268 --> 00:31:57,038
This is about transparency. So if you chart

502
00:31:57,054 --> 00:32:01,522
in the data and you immediately know which

503
00:32:01,576 --> 00:32:05,118
nodes contribute data to it, and as you will see, it's not just nodes,

504
00:32:05,134 --> 00:32:08,514
it's a lot more information. So the nodes that are contributing

505
00:32:08,562 --> 00:32:11,986
data to it, you can see here how many instances.

506
00:32:12,018 --> 00:32:15,206
So this is about applications, this is about 20.

507
00:32:15,308 --> 00:32:18,906
It comes from 18 nodes for a

508
00:32:18,928 --> 00:32:22,394
total of, you see number of

509
00:32:22,512 --> 00:32:28,646
metrics per node.

510
00:32:28,838 --> 00:32:32,122
You can see the volume. So this chart has a volume,

511
00:32:32,266 --> 00:32:35,662
has some volume in total.

512
00:32:35,796 --> 00:32:39,550
What's the contribution of each metric? Of each

513
00:32:39,620 --> 00:32:43,594
node? Sorry, to the total. So if you remove Bangalore

514
00:32:43,642 --> 00:32:46,766
from the chart, you are going to lose about 16% of the volume.

515
00:32:46,958 --> 00:32:50,382
And here is the anomaly rate. So widths

516
00:32:50,446 --> 00:32:53,838
of the nodes, of the metrics of the nodes, the anomaly rate of the metrics

517
00:32:53,854 --> 00:32:57,522
of the node. Of course we have mean average, maximum, et cetera

518
00:32:57,666 --> 00:33:00,886
for all metrics involved. And if you move on,

519
00:33:00,988 --> 00:33:05,270
the same happened for instances. So here that we have applications,

520
00:33:06,170 --> 00:33:09,670
you can see here that each application has two metrics

521
00:33:09,750 --> 00:33:14,090
and you can immediately see, okay, the SSH is anomalous

522
00:33:14,830 --> 00:33:18,234
on this server. And of

523
00:33:18,272 --> 00:33:22,090
course the same happens even for labels.

524
00:33:22,170 --> 00:33:25,742
So not only for label keys, but also

525
00:33:25,796 --> 00:33:29,358
for label values. So you can see again

526
00:33:29,444 --> 00:33:32,666
the volume, you can see the anomaly rate, minimum,

527
00:33:32,698 --> 00:33:35,140
average, maximum values for everything.

528
00:33:35,750 --> 00:33:39,630
Now the same information is available as a tooltip.

529
00:33:39,710 --> 00:33:42,994
So you can see this on the tooltip. You hover on a point

530
00:33:43,032 --> 00:33:46,246
on the chart, and together with the values that

531
00:33:46,268 --> 00:33:50,360
you normally see, you have the anomaly rate of

532
00:33:51,210 --> 00:33:54,934
that point, anomaly rate. And for

533
00:33:54,972 --> 00:33:58,266
each of the time series, of course, of the dimensions of the

534
00:33:58,288 --> 00:34:02,822
chart. Now, if we go back to the original chart

535
00:34:02,966 --> 00:34:06,282
that I show you, you have more control here.

536
00:34:06,416 --> 00:34:10,154
So you can change the aggregation across time. So if

537
00:34:10,192 --> 00:34:13,754
you zoom out the chart, it has to aggregate across

538
00:34:13,792 --> 00:34:17,006
time because your screen has 500 points. But behind the

539
00:34:17,028 --> 00:34:20,746
scenes in the database, if this per second, there are thousands and thousands

540
00:34:20,778 --> 00:34:23,858
of metrics. So you can change the aggregation across time.

541
00:34:23,944 --> 00:34:27,620
You can say here, minimum, maximum, so you can see,

542
00:34:28,390 --> 00:34:32,898
reveal the spikes or the dives, and you

543
00:34:32,904 --> 00:34:36,146
can change the group by and the aggregation.

544
00:34:36,338 --> 00:34:40,358
So you can pivot the chart. You can change

545
00:34:40,444 --> 00:34:43,686
the chart. It's like a cube. You see it

546
00:34:43,708 --> 00:34:46,520
from different angles. So let's continue.

547
00:34:48,110 --> 00:34:52,140
Netdata also has a scoring engine. A scoring engine

548
00:34:52,670 --> 00:34:56,186
allows Netdata to traverse the entire

549
00:34:56,368 --> 00:35:01,790
list of metrics on a server and score them based

550
00:35:01,860 --> 00:35:05,082
on anomaly rate or similarity. We have many algorithms

551
00:35:05,146 --> 00:35:08,458
there. Now we also have a metric

552
00:35:08,474 --> 00:35:12,750
correlation algorithm that tries to find similarity

553
00:35:13,970 --> 00:35:17,114
in changes. So you highlight

554
00:35:17,162 --> 00:35:20,606
a spike and you say, correlate this with anything else and it

555
00:35:20,628 --> 00:35:22,880
will find the dive. Because of the rate,

556
00:35:24,610 --> 00:35:28,454
the change is similar. The rate of change is similar. Now, how we use

557
00:35:28,492 --> 00:35:31,910
this, the first thing is that this energeta

558
00:35:32,250 --> 00:35:35,698
dashboard, it has one chart below the other and a menu

559
00:35:35,794 --> 00:35:38,738
where all the charts are segmented, as you can see.

560
00:35:38,844 --> 00:35:43,562
I don't see the number, but I think it's 500 charts or something

561
00:35:43,616 --> 00:35:46,870
like that there. So out of these charts,

562
00:35:47,030 --> 00:35:50,814
you press a button. These are all in sections. You press

563
00:35:50,852 --> 00:35:54,714
a button and the data will score them according to their anomaly

564
00:35:54,762 --> 00:35:58,410
rate to tell you in which sections.

565
00:35:58,570 --> 00:36:02,542
Which sections are anomalous and how much this

566
00:36:02,596 --> 00:36:06,238
allows you if you have a problem, for example, you just go to the Netdata

567
00:36:06,254 --> 00:36:10,162
dashboard that reflects the current, says, the last five minutes or the last 15 minutes.

568
00:36:10,296 --> 00:36:14,574
You press that button and you will immediately see which metrics

569
00:36:14,622 --> 00:36:18,278
across the entire dashboard are anomalous so that

570
00:36:18,364 --> 00:36:21,000
you can check what's happening, what's wrong.

571
00:36:23,130 --> 00:36:26,354
The next is the host anomaly rate. Now, for the host

572
00:36:26,402 --> 00:36:29,802
anomaly rate, what we do is that we

573
00:36:29,856 --> 00:36:33,318
calculate the percentage of metrics

574
00:36:33,494 --> 00:36:38,054
on a server that are anomalous concurrently.

575
00:36:38,182 --> 00:36:42,250
Concurrently. What we realize then is the following,

576
00:36:42,410 --> 00:36:45,870
that anomalies happen in clusters.

577
00:36:46,450 --> 00:36:50,250
So look at this, for example. These are servers.

578
00:36:50,330 --> 00:36:53,826
Every line is a different server, but you see

579
00:36:54,008 --> 00:36:58,020
that the anomalies happen close together.

580
00:36:58,390 --> 00:37:01,714
This is up to 10%. So 10% of

581
00:37:01,752 --> 00:37:05,734
the metrics of all the metrics collected on a server were

582
00:37:05,772 --> 00:37:09,254
anomalous at the same time. And as you see,

583
00:37:09,452 --> 00:37:13,910
for each server, it happened with a little delta here,

584
00:37:13,980 --> 00:37:17,654
it happened concurrently, so one server spiked to

585
00:37:17,692 --> 00:37:21,914
10%, but a lot other servers spiked to 5%.

586
00:37:22,112 --> 00:37:25,114
Now look what happens when you

587
00:37:25,152 --> 00:37:28,986
view this dashboard. What you can do is highlight an

588
00:37:29,008 --> 00:37:32,574
area. So here we have highlighted from here to there

589
00:37:32,692 --> 00:37:37,146
and the data, what it will do. It score the metrics.

590
00:37:37,258 --> 00:37:41,694
So it will traverse the entire all

591
00:37:41,732 --> 00:37:45,726
the metrics one by one. Score them for that little time frame,

592
00:37:45,838 --> 00:37:49,940
calculate the anomaly rate, and then provide a sorted list

593
00:37:50,310 --> 00:37:53,300
of what changed over.

594
00:37:55,510 --> 00:37:59,174
What is more important, what is more anomalous for that

595
00:37:59,212 --> 00:38:02,806
time frame. The whole point of this is to provide

596
00:38:02,908 --> 00:38:06,806
the AHA moment within the list, their top 2030

597
00:38:06,908 --> 00:38:10,954
items. So instead of speculating what could

598
00:38:10,992 --> 00:38:14,234
be wrong to have this issue there,

599
00:38:14,352 --> 00:38:17,674
Netdata tries to figure out this for you

600
00:38:17,712 --> 00:38:21,246
and gives you a list of the most anomalous things for that time frame so

601
00:38:21,268 --> 00:38:24,240
that your aha moment is there within that list.

602
00:38:24,770 --> 00:38:28,542
Now the highlights is

603
00:38:28,596 --> 00:38:32,922
that the data in ML is totally unsupervised,

604
00:38:33,066 --> 00:38:36,734
so you don't need to train it. It is trained

605
00:38:36,782 --> 00:38:40,222
for every metric, multiple models for every metric,

606
00:38:40,366 --> 00:38:43,778
and it learns the behavior of metrics for the last few days.

607
00:38:43,944 --> 00:38:46,360
So you just let the data run,

608
00:38:46,890 --> 00:38:50,326
you don't need to tell it what is good or what is

609
00:38:50,348 --> 00:38:53,890
bad, and the data will start automatically

610
00:38:53,970 --> 00:38:57,800
detect anomalies based

611
00:38:58,590 --> 00:39:01,580
on the behavior of metrics of the last two or three days.

612
00:39:03,150 --> 00:39:06,458
It is important to note that this

613
00:39:06,544 --> 00:39:10,330
is totally unsupervised. You don't need

614
00:39:10,400 --> 00:39:13,802
to do anything. Of course if an anomaly

615
00:39:13,866 --> 00:39:17,166
happens it will trigger it, but then after a while it

616
00:39:17,188 --> 00:39:20,000
will learn about it, so it will not trigger it again.

617
00:39:20,850 --> 00:39:24,226
But if it happens for the first time in the

618
00:39:24,248 --> 00:39:27,682
last few days, two days it will detect it

619
00:39:27,816 --> 00:39:31,874
and reveal it for you. The second is

620
00:39:31,912 --> 00:39:34,930
that the data can immediately,

621
00:39:35,830 --> 00:39:39,046
within minutes. So you install it and after ten or

622
00:39:39,068 --> 00:39:42,146
15 minutes it will start triggering anomalies.

623
00:39:42,178 --> 00:39:45,698
So it doesn't need to train all 18 models to detect anomalies.

624
00:39:45,794 --> 00:39:49,034
But as time passes it becomes better and better and better,

625
00:39:49,072 --> 00:39:52,394
so it eliminates noise. So even one model is

626
00:39:52,432 --> 00:39:56,410
enough to trigger anomalies. The second is it happens for

627
00:39:56,480 --> 00:40:00,506
all metrics. So every single metric, from database

628
00:40:00,538 --> 00:40:04,554
servers, web servers, disks, network interfaces,

629
00:40:04,682 --> 00:40:07,950
system metrics, every single metric

630
00:40:08,450 --> 00:40:10,990
gets this anomaly detection.

631
00:40:13,490 --> 00:40:16,814
The anomaly information is stored in the

632
00:40:16,852 --> 00:40:21,310
database. So you can query the anomaly of yesterday.

633
00:40:21,890 --> 00:40:25,634
Not based on today's models, on yesterday's models.

634
00:40:25,682 --> 00:40:29,910
So as the models were at the time the anomaly was triggered.

635
00:40:33,770 --> 00:40:38,190
There is a scoring engine that allows you to score metrics

636
00:40:38,290 --> 00:40:41,738
across the board. So you are looking for

637
00:40:41,824 --> 00:40:45,994
what is anomalous. Now, what is most anomalous now, what is most anomalous for

638
00:40:46,032 --> 00:40:49,974
that time frame? Or I want to find

639
00:40:50,112 --> 00:40:52,830
something that is similar to this.

640
00:40:52,980 --> 00:40:56,602
So all these queries are available with Netdata,

641
00:40:56,746 --> 00:41:00,734
and it has the host level anomaly score that allows you

642
00:41:00,772 --> 00:41:04,034
to see the strength and the

643
00:41:04,072 --> 00:41:07,134
spread of an anomaly across your systems,

644
00:41:07,182 --> 00:41:10,450
across each system, inside each system, but also

645
00:41:10,600 --> 00:41:15,234
across systems. So what

646
00:41:15,272 --> 00:41:18,534
we are next to do this solidity there, this works,

647
00:41:18,572 --> 00:41:22,246
you can try in the data, it's open source software. And actually

648
00:41:22,348 --> 00:41:25,846
it's amazing because you don't have to do anything, just install it,

649
00:41:25,948 --> 00:41:29,194
it will work for you. We are

650
00:41:29,232 --> 00:41:33,194
adding machine learning profiles, so we

651
00:41:33,232 --> 00:41:37,066
see users that are using machine learning in

652
00:41:37,088 --> 00:41:40,606
the data for different purposes. So some people want

653
00:41:40,628 --> 00:41:44,250
it for security, some people want it for troubleshooting,

654
00:41:44,410 --> 00:41:48,640
some people want it for learning

655
00:41:49,250 --> 00:41:52,346
special applications, training special applications,

656
00:41:52,378 --> 00:41:56,642
et cetera. So we are trying to make these,

657
00:41:56,696 --> 00:41:59,700
to create profiles that users can create,

658
00:42:00,310 --> 00:42:03,506
can create different settings for machine learning according to

659
00:42:03,528 --> 00:42:07,734
their needs. Of course, there are many settings now available,

660
00:42:07,932 --> 00:42:11,126
but they are applied to all metrics. Everything is

661
00:42:11,148 --> 00:42:15,014
the same. The second is that we

662
00:42:15,052 --> 00:42:19,018
want to segment this across time. So instead of learning

663
00:42:19,184 --> 00:42:22,362
the last two days and

664
00:42:22,416 --> 00:42:25,738
then detecting anomalies based on the total

665
00:42:25,824 --> 00:42:28,922
of the last two days, to learn Mondays, to learn

666
00:42:28,976 --> 00:42:32,590
Tuesdays, so detect anomalies based on

667
00:42:32,740 --> 00:42:36,026
Monday's models or Monday

668
00:42:36,138 --> 00:42:39,594
morning models. So this profiling

669
00:42:39,642 --> 00:42:43,582
will allow better to have a better control on many

670
00:42:43,636 --> 00:42:48,270
industries that the days are not exactly similar.

671
00:42:48,420 --> 00:42:52,766
So they have some big spikes on Wednesdays and the

672
00:42:52,788 --> 00:42:55,990
systems are totally idle on Tuesdays.

673
00:42:58,330 --> 00:43:01,766
That's it. So thank you very much.

674
00:43:01,868 --> 00:43:03,780
I hope you enjoyed it.

