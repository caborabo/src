1
00:00:27,280 --> 00:00:30,248
Hello everyone, welcome to cons 42,

2
00:00:30,296 --> 00:00:34,224
observability 2024 and thank you for taking time to join

3
00:00:34,264 --> 00:00:35,124
my session.

4
00:00:37,624 --> 00:00:41,284
I want to start today's session with a story.

5
00:00:42,984 --> 00:00:46,032
Let's say there is a bank xxx and it

6
00:00:46,048 --> 00:00:50,440
has released a newsletter stating this dear customers,

7
00:00:50,512 --> 00:00:54,636
we are happy to announce that you can now open savings account through your

8
00:00:54,780 --> 00:00:57,900
mobile banking. Place the request

9
00:00:57,932 --> 00:01:01,380
with a few clicks on your mobile app and

10
00:01:01,492 --> 00:01:04,144
get your account operational in 2 hours.

11
00:01:05,324 --> 00:01:09,948
And let's say that this solution was

12
00:01:09,996 --> 00:01:13,340
done by the bank using a very modern architecture,

13
00:01:13,412 --> 00:01:17,544
event driven architecture, a cloud native architecture.

14
00:01:18,364 --> 00:01:21,854
And after a few days of this product launch,

15
00:01:22,044 --> 00:01:25,858
a customer calls the customer service representative

16
00:01:26,026 --> 00:01:29,930
and states that I placed a request yesterday for savings account

17
00:01:30,002 --> 00:01:33,854
on mobile banking app but my account is not operational

18
00:01:34,234 --> 00:01:38,426
till now. The customer service representative logs

19
00:01:38,450 --> 00:01:42,306
a ticket to the mobile banking team. The mobile banking team

20
00:01:42,370 --> 00:01:46,386
takes a look at the backend systems and can see that the

21
00:01:46,410 --> 00:01:50,940
request is successfully placed. So she

22
00:01:50,972 --> 00:01:53,916
forwards the ticket to the core banking team.

23
00:01:54,060 --> 00:01:57,476
Now the core banking team looks at the system and

24
00:01:57,500 --> 00:02:00,704
says that I have not received any account opening request.

25
00:02:01,484 --> 00:02:05,064
So what happened to the account opening request?

26
00:02:05,364 --> 00:02:08,824
Or should I say, what happened to the account opening event?

27
00:02:10,804 --> 00:02:14,284
So the answer to this question is what is the premise

28
00:02:14,324 --> 00:02:18,492
of this session? So welcome to my session on observability

29
00:02:18,588 --> 00:02:22,212
for modern event driven applications. I'm Aurmila

30
00:02:22,228 --> 00:02:26,024
Raju. I'm a senior solution architect with Amazon Web Services.

31
00:02:26,404 --> 00:02:29,184
So let's get started and dive into this session.

32
00:02:29,804 --> 00:02:33,988
I want to do some basic level setting on what is event driven architecture.

33
00:02:34,156 --> 00:02:37,404
So please note the text in bold and underlined it says

34
00:02:37,444 --> 00:02:41,356
that this is an architectural style of building loosely

35
00:02:41,380 --> 00:02:44,616
coupled systems. And these loosely coupled

36
00:02:44,680 --> 00:02:48,344
systems talk to each other by emitting and responding

37
00:02:48,384 --> 00:02:51,496
to events. So what is an

38
00:02:51,520 --> 00:02:54,920
event? An event is a change in state or

39
00:02:54,952 --> 00:02:58,136
an update emitted by a producer.

40
00:02:58,240 --> 00:03:02,084
A producer can be any component within your application,

41
00:03:02,544 --> 00:03:06,444
and in this event consumers are interested in.

42
00:03:07,024 --> 00:03:10,912
So producers and consumers, which are like two components of your application,

43
00:03:11,008 --> 00:03:14,224
talk to each other through an event broker.

44
00:03:14,384 --> 00:03:18,280
So that's how the high level architecture

45
00:03:18,432 --> 00:03:21,324
of an event driven system comes about.

46
00:03:21,664 --> 00:03:25,840
So why do customers move towards event driven applications?

47
00:03:26,032 --> 00:03:29,204
Because it offers a lot of good things.

48
00:03:29,904 --> 00:03:33,688
And few of the highlights are speed and agility because

49
00:03:33,736 --> 00:03:37,240
the systems are loosely coupled, so each team can build their own

50
00:03:37,272 --> 00:03:41,364
component independently and get it to deployment. The next

51
00:03:41,404 --> 00:03:45,332
is resiliency, so the blast radius of failure is

52
00:03:45,428 --> 00:03:49,988
reduced because of the loose coupling between the systems,

53
00:03:50,116 --> 00:03:52,624
and each system fails independently,

54
00:03:53,884 --> 00:03:57,704
so there is no single point of failure. The next is scalability,

55
00:03:58,084 --> 00:04:02,024
so you are able to minimize any waiting time

56
00:04:02,404 --> 00:04:06,396
because of the asynchronous and parallel processing that we bring in

57
00:04:06,420 --> 00:04:10,184
event driven architecture. And lastly, but not the

58
00:04:10,264 --> 00:04:14,120
but it is the most important one which enables you to

59
00:04:14,152 --> 00:04:18,008
work backwards from your business requirements and your business process

60
00:04:18,096 --> 00:04:21,816
workflow. So this is an architectural style which

61
00:04:21,880 --> 00:04:25,232
brings your business and technology stakeholders together

62
00:04:25,408 --> 00:04:27,804
in building technology applications.

63
00:04:28,824 --> 00:04:32,256
So these good things will help you to

64
00:04:32,360 --> 00:04:35,284
meet your business objective in a very effective way.

65
00:04:35,784 --> 00:04:39,408
But it's important to understand that EDA is

66
00:04:39,456 --> 00:04:42,928
hard to get it right. There are various factors for it

67
00:04:43,056 --> 00:04:46,280
and key things are highlighted here,

68
00:04:46,432 --> 00:04:49,124
starting with eventual consistency.

69
00:04:49,904 --> 00:04:53,688
So what we mean by that is, due to loose

70
00:04:53,736 --> 00:04:57,872
coupled systems and asynchronous nature,

71
00:04:58,048 --> 00:05:01,404
components are not consistent at the same time.

72
00:05:01,584 --> 00:05:05,396
So your business process must be able to cope with that kind of

73
00:05:05,500 --> 00:05:08,744
delays. The next is end to end performance.

74
00:05:09,284 --> 00:05:12,668
If there is a performance bottleneck in one of the components,

75
00:05:12,796 --> 00:05:16,504
it is going to impact the end to end performance of your

76
00:05:17,084 --> 00:05:21,204
application. And third, is meeting business SLA's.

77
00:05:21,324 --> 00:05:26,020
So when we talked about the pros, we said that it

78
00:05:26,052 --> 00:05:28,864
helps you to work backwards from your business process.

79
00:05:29,794 --> 00:05:34,054
So that means you need to meet your business SLA's

80
00:05:34,554 --> 00:05:38,706
in this type of an architecture. So in this business process workflow,

81
00:05:38,770 --> 00:05:42,362
some steps might be real time, some might be real near real time,

82
00:05:42,458 --> 00:05:45,642
and some might be batch. So you must

83
00:05:45,698 --> 00:05:48,898
design in such a way that the SLA's of

84
00:05:48,946 --> 00:05:52,814
each step are met properly to get the EDA right.

85
00:05:53,554 --> 00:05:57,906
So how do we do this right? So there are many architectural

86
00:05:58,050 --> 00:06:01,514
design decisions that you need to make to get it right.

87
00:06:01,634 --> 00:06:05,370
And one of the key things to get this right is

88
00:06:05,442 --> 00:06:08,842
observability. So observing your

89
00:06:08,898 --> 00:06:11,894
system so that it's like if there is an event,

90
00:06:12,234 --> 00:06:16,186
as we started with, the story event is flowing between systems.

91
00:06:16,250 --> 00:06:19,602
So you need to know where the event is at and what time. And if

92
00:06:19,618 --> 00:06:23,590
there is an event that is failed, there must be proactive mechanism built

93
00:06:23,622 --> 00:06:27,398
into your architecture to recognize that and take appropriate

94
00:06:27,486 --> 00:06:30,806
action. So that's where observability plays a role.

95
00:06:30,990 --> 00:06:34,174
So what is observability? It's a measure of how

96
00:06:34,214 --> 00:06:38,354
well we can understand a system from the work it does.

97
00:06:39,214 --> 00:06:42,294
So it is about like collecting the right amount of

98
00:06:42,334 --> 00:06:45,582
data and gaining insights from it

99
00:06:45,678 --> 00:06:49,656
and taking proactive actions to make your application

100
00:06:49,830 --> 00:06:53,184
work better. So I want to

101
00:06:54,484 --> 00:06:58,476
demonstrate it with an example use case so where which

102
00:06:58,540 --> 00:07:02,220
relates back to our story of opening a savings account

103
00:07:02,292 --> 00:07:05,684
for an existing customer through a mobile banking app.

104
00:07:05,804 --> 00:07:09,268
So I want to show you a high level business process

105
00:07:09,356 --> 00:07:12,704
workflow. It can be

106
00:07:14,764 --> 00:07:18,182
any complex workflow but I just put here a

107
00:07:18,198 --> 00:07:21,750
very oversimplified version because we are

108
00:07:21,782 --> 00:07:25,062
going to use this just to see how the observability fits

109
00:07:25,118 --> 00:07:29,270
into the business process workflow.

110
00:07:29,422 --> 00:07:32,862
So let's say the customer logged into the mobile app

111
00:07:32,998 --> 00:07:37,394
and selects a savings product and checks the product eligibility,

112
00:07:37,694 --> 00:07:40,910
and then the request to open the savings

113
00:07:40,942 --> 00:07:44,680
account is placed. So once it is placed it

114
00:07:44,752 --> 00:07:48,248
goes to the core banking to get the account opened. And after

115
00:07:48,296 --> 00:07:52,112
the account is opened there could be like post account opening steps, like a

116
00:07:52,208 --> 00:07:56,844
monthly interest schedule getting updated. Or you send a mobile push notification

117
00:07:57,464 --> 00:08:01,644
to convey to the customer that the

118
00:08:02,104 --> 00:08:05,664
account is opened and account is operational. And you may send a welcome

119
00:08:05,704 --> 00:08:09,192
email, or you may send a survey or to

120
00:08:09,248 --> 00:08:13,708
know how the account opening journey has been. So usually these

121
00:08:13,836 --> 00:08:17,388
kind of steps or the events,

122
00:08:17,436 --> 00:08:21,028
so if you see the orange ones are all written in the past tense

123
00:08:21,076 --> 00:08:24,996
because events are usually written in the past tense and

124
00:08:25,100 --> 00:08:29,004
the gray boxes around it are the business domains

125
00:08:29,164 --> 00:08:32,804
from which these events originate. And events can flow

126
00:08:32,844 --> 00:08:36,612
between domain. So event driven architecture is primarily based

127
00:08:36,668 --> 00:08:41,395
on the domain driven design and event storming methodology.

128
00:08:41,539 --> 00:08:44,259
So I'm not going to dive into those concepts,

129
00:08:44,371 --> 00:08:48,115
but it's a good way of designing an EDA is

130
00:08:48,139 --> 00:08:52,059
this, that is, you start from your business process, identify the events

131
00:08:52,131 --> 00:08:56,051
in each of the business domain, and then design

132
00:08:56,107 --> 00:08:59,703
your technical architecture based on that.

133
00:09:00,003 --> 00:09:03,667
So let's say this is our business process workflow. And in

134
00:09:03,715 --> 00:09:07,430
accompanying this, that could be business SLA's because we talked about

135
00:09:07,462 --> 00:09:10,414
SLA's previously and for this example use case,

136
00:09:10,534 --> 00:09:13,702
you can have SLA's like something like, like the ones

137
00:09:13,718 --> 00:09:17,854
that I've highlighted here. Like open the account 24/7

138
00:09:17,894 --> 00:09:21,510
that is, customer can login into the mobile banking app anytime

139
00:09:21,622 --> 00:09:25,582
and place the request. And the product eligibility is

140
00:09:25,678 --> 00:09:29,198
done in real time. So it's always important to define what is real

141
00:09:29,246 --> 00:09:32,878
time. So here we are defining real time as like hundred milliseconds

142
00:09:33,006 --> 00:09:36,230
for the product eligibility to be done and the account opening

143
00:09:36,262 --> 00:09:39,950
request is placed. And also we are saying that the account should be operational

144
00:09:40,102 --> 00:09:44,334
in 2 hours of the request being placed and another SLA's

145
00:09:44,414 --> 00:09:47,486
in a similar way. So the point to

146
00:09:47,550 --> 00:09:50,422
note here is in this business process workflow,

147
00:09:50,558 --> 00:09:54,166
there are some synchronous real time steps and also

148
00:09:54,270 --> 00:09:57,742
some asynchronous near real time or

149
00:09:57,798 --> 00:10:01,338
batch steps. Yeah, and why

150
00:10:01,386 --> 00:10:04,210
it is hard to get it right is there could be a lot of things

151
00:10:04,242 --> 00:10:08,010
that can go wrong here. Examples are what happens if the

152
00:10:08,042 --> 00:10:12,174
product eligibility check takes more than 100 milliseconds, which is CSL

153
00:10:12,554 --> 00:10:15,214
what happens if account opening service is down?

154
00:10:15,554 --> 00:10:18,774
And what happens if mobile push notification fails?

155
00:10:19,434 --> 00:10:22,794
That's why we are saying that end to end observability is

156
00:10:22,834 --> 00:10:26,134
key for a successful edge. So you need to have

157
00:10:26,174 --> 00:10:30,110
visibility into each of the component that you

158
00:10:30,142 --> 00:10:33,766
have in your end to end architecture,

159
00:10:33,910 --> 00:10:37,470
and you should be able to do real time troubleshooting on the

160
00:10:37,542 --> 00:10:41,334
errors and issues that is occurring. And if you see in this

161
00:10:41,374 --> 00:10:45,246
picture, the benefits of observability has both

162
00:10:45,310 --> 00:10:48,566
operational benefits and also business benefits when you move towards

163
00:10:48,710 --> 00:10:51,998
the right. So in terms of business benefit, it is going to help

164
00:10:52,046 --> 00:10:55,598
you with your overall customer experience and also

165
00:10:55,646 --> 00:10:59,354
to meet your business objectives and business outcomes.

166
00:10:59,654 --> 00:11:02,274
So usually in a traditional monitoring,

167
00:11:02,814 --> 00:11:05,838
you need to do the monitoring across all of your layers,

168
00:11:05,966 --> 00:11:10,302
right from your storage or network layer up to your business layer.

169
00:11:10,438 --> 00:11:13,686
But when you do event driven architecture, and especially when you do it on

170
00:11:13,710 --> 00:11:17,390
AWS, you do it with many of the serverless services,

171
00:11:17,542 --> 00:11:21,304
like services like SQS, SNS, Amazon,

172
00:11:21,344 --> 00:11:26,008
Evenbridge, API, Gateway, Lambda. So these are the services

173
00:11:26,096 --> 00:11:29,672
that you usually use. And when they

174
00:11:29,688 --> 00:11:33,792
are serverless, these layers of

175
00:11:33,928 --> 00:11:37,800
monitoring is offloaded to AWS. So you will be able to focus

176
00:11:37,872 --> 00:11:41,248
on your business application and data

177
00:11:41,336 --> 00:11:42,884
observability alone.

178
00:11:45,104 --> 00:11:48,256
Now, going back to our example, so we so far we have been

179
00:11:48,280 --> 00:11:52,164
talking about only the business process and events. So I have given some

180
00:11:53,304 --> 00:11:56,928
services, put some services behind it to show how an

181
00:11:57,016 --> 00:12:00,368
architecture for such a solution would look like. So we

182
00:12:00,376 --> 00:12:03,920
are not going to rationalize on our design choices of why I've used

183
00:12:03,952 --> 00:12:08,664
Evenbridge here or why I've used sqs. Because in an event driven architecture

184
00:12:08,784 --> 00:12:12,544
there is no right or wrong answers. It's primarily based on what your

185
00:12:12,584 --> 00:12:16,782
requirements are and using the right architectural style

186
00:12:16,888 --> 00:12:20,322
and then choosing the appropriate services for that. So in

187
00:12:20,338 --> 00:12:24,618
this example, what we are doing is. So this top bit is a synchronous process

188
00:12:24,786 --> 00:12:28,442
where you do the product eligibility check through an

189
00:12:28,578 --> 00:12:32,834
API synchronous API call, which goes to the product DB and gives the

190
00:12:32,954 --> 00:12:36,666
request back. And once that is done, an event to open

191
00:12:36,730 --> 00:12:40,410
the account as a request is placed into Eventbridge

192
00:12:40,442 --> 00:12:43,840
as an event, and this event is put into an sqs queue

193
00:12:43,962 --> 00:12:47,156
and you have a core banking platform. And here I assume that it is going

194
00:12:47,180 --> 00:12:51,220
to be an on premise data center system which listens into

195
00:12:51,252 --> 00:12:54,628
this queue. So whenever there is a request, it picks it from there

196
00:12:54,676 --> 00:12:57,544
and then does the necessary process to open the account.

197
00:12:58,564 --> 00:13:01,676
And once the account is opened, an event is placed back into the

198
00:13:01,700 --> 00:13:05,804
event bridge and from there that event is of

199
00:13:05,844 --> 00:13:09,796
interest to various systems like the monthly interest schedule

200
00:13:09,900 --> 00:13:13,860
updating service or the customer comms with since mobile push notification and

201
00:13:13,892 --> 00:13:17,786
email, or the marketing team who is going to send the account

202
00:13:17,850 --> 00:13:21,554
opening survey. So this is just to show you how

203
00:13:21,594 --> 00:13:24,306
it is made up of synchronous and asynchronous step.

204
00:13:24,410 --> 00:13:28,090
And each step has got its own SLA's. So you need

205
00:13:28,122 --> 00:13:32,530
to monitor each of this component and achieve

206
00:13:32,562 --> 00:13:36,482
the observability as per your business SLA's.

207
00:13:36,618 --> 00:13:40,602
So how do we do that? Let's start looking at an observability

208
00:13:40,698 --> 00:13:43,904
maturity model for that. So maturity model

209
00:13:43,944 --> 00:13:47,704
helps customers evaluate where they are so that they know

210
00:13:47,744 --> 00:13:51,200
where they want to be and how to get there. So as

211
00:13:51,232 --> 00:13:54,808
they expand their workloads, the observability is expected to

212
00:13:54,856 --> 00:13:58,320
mature. So we will start from the foundational level, which is

213
00:13:58,352 --> 00:14:02,764
foundational monitoring and it relates to collecting of telemetry data.

214
00:14:03,144 --> 00:14:06,504
So what do we mean by telemetry data? Here we have three

215
00:14:06,544 --> 00:14:09,684
types and those are the actual three pillars of observability,

216
00:14:09,864 --> 00:14:13,812
which are metrics, logs and traces. So metrics is time

217
00:14:13,868 --> 00:14:17,436
series data calculated or measured at

218
00:14:17,460 --> 00:14:21,652
various time intervals. So it can be like API request rate

219
00:14:21,788 --> 00:14:24,924
or error rate, or a duration of a lambda function,

220
00:14:25,004 --> 00:14:28,948
etcetera. And logs or timestamp the records

221
00:14:28,996 --> 00:14:32,116
of discrete events. So those are

222
00:14:32,140 --> 00:14:36,262
like events that happen within your system or components of your application,

223
00:14:36,428 --> 00:14:40,698
such as a failure event or an error event or a state transformation.

224
00:14:40,866 --> 00:14:44,546
So those are examples of logs. And then we have got traces.

225
00:14:44,650 --> 00:14:48,258
A trace represents a single user journey across

226
00:14:48,346 --> 00:14:52,146
multiple components in your application. So usually it is very

227
00:14:52,210 --> 00:14:56,650
useful in case of microservices and API based architecture

228
00:14:56,762 --> 00:15:00,074
to see how the API request is being routed

229
00:15:00,114 --> 00:15:03,762
through various systems and the response coming

230
00:15:03,818 --> 00:15:06,986
back, you can trace that entire request.

231
00:15:07,130 --> 00:15:10,842
So these three form the pillars of observability. And when you do

232
00:15:10,898 --> 00:15:14,818
this observability using AWS native tools,

233
00:15:14,986 --> 00:15:18,094
Amazon Cloudwatch helps you to

234
00:15:18,434 --> 00:15:22,226
do logs and metrics, and AWS x ray helps

235
00:15:22,250 --> 00:15:25,658
you with traces. So let's go and look at each

236
00:15:25,706 --> 00:15:29,416
one of the observability pillar and

237
00:15:29,560 --> 00:15:33,184
what we will do is as and when we see each of the pillar.

238
00:15:33,304 --> 00:15:36,944
I'm going to go back to the example application design that we

239
00:15:36,984 --> 00:15:40,696
had and then relate what kind of observability examples

240
00:15:40,760 --> 00:15:44,684
that we can do for that application to provide you some context.

241
00:15:45,984 --> 00:15:50,284
So we'll start with viewing of standard metrics.

242
00:15:51,064 --> 00:15:54,424
So Cloudwatch has inbuilt metrics. So whenever

243
00:15:54,464 --> 00:15:58,836
you are integrating the AWS services with Cloudwatch automatically,

244
00:15:58,900 --> 00:16:02,184
there are, there are a set of metrics for each service which gets

245
00:16:03,644 --> 00:16:07,412
logged into Cloudwatch. So if

246
00:16:07,428 --> 00:16:11,316
you see, these are the serverless services that I have highlighted

247
00:16:11,420 --> 00:16:16,540
because they have been used in the example architecture that

248
00:16:16,572 --> 00:16:20,444
we just spoke about. But we can do the same with the

249
00:16:20,484 --> 00:16:23,968
other AWS services as well. So this

250
00:16:24,016 --> 00:16:28,256
inbuilt metrics with you must be able to meet like 70%

251
00:16:28,320 --> 00:16:32,448
of your observability needs just from the inbuilt

252
00:16:32,576 --> 00:16:35,808
metrics. So let's see some examples of

253
00:16:35,896 --> 00:16:39,368
what those key metrics can be. So for

254
00:16:39,416 --> 00:16:43,144
lambda, the invocation metric can be helpful to

255
00:16:43,184 --> 00:16:47,560
assess the amount of traffic and failures, if any, and performance

256
00:16:47,672 --> 00:16:51,520
metric can be on memory utilization and duration of

257
00:16:51,552 --> 00:16:54,958
execution execution. This relates to the cost of the function.

258
00:16:55,006 --> 00:16:58,966
Two like these parameters and then we have concurrency.

259
00:16:59,110 --> 00:17:02,694
Concurrency metric helps to assess the number

260
00:17:02,734 --> 00:17:06,478
of parallel invocations. So this metric can help to track

261
00:17:06,526 --> 00:17:10,478
the performance of application against existing

262
00:17:10,526 --> 00:17:14,014
concurrency limits and see if you need

263
00:17:14,054 --> 00:17:18,200
to increase the limits or secure like proficient concurrency

264
00:17:18,342 --> 00:17:20,824
as per the needs of your application.

265
00:17:22,364 --> 00:17:25,756
Similarly, for API gateway, I've highlighted

266
00:17:25,820 --> 00:17:29,020
a set of key metric. So the API

267
00:17:29,052 --> 00:17:33,228
gateway is the gateway for your microservices in modern application.

268
00:17:33,396 --> 00:17:37,700
So keeping track of like API call counts latency errors

269
00:17:37,812 --> 00:17:41,784
can be very helpful in measuring your business objectives.

270
00:17:43,984 --> 00:17:47,504
So when we talked about limits, right, though these

271
00:17:47,544 --> 00:17:51,312
are like serverless services and scales, inherently you

272
00:17:51,328 --> 00:17:55,072
need to be cautious about limits to avoid

273
00:17:55,168 --> 00:17:58,760
throttling. But in some cases throttling

274
00:17:58,832 --> 00:18:02,488
can be good too. For example, you can

275
00:18:02,536 --> 00:18:06,528
throttle the number of requests to API gateway to avoid

276
00:18:06,696 --> 00:18:10,622
security attacks and also setting like client limitations

277
00:18:10,808 --> 00:18:14,114
when there are like multiple clients accessing your API

278
00:18:14,154 --> 00:18:17,858
gateway. So it's important to analyze with what

279
00:18:17,906 --> 00:18:21,498
limits you are operating your application, whether they are the right

280
00:18:21,546 --> 00:18:25,450
limits or do you need any increase to

281
00:18:25,562 --> 00:18:28,730
make your application perform better. So the

282
00:18:28,762 --> 00:18:32,334
next service that we want to look at is Amazon Q service.

283
00:18:32,834 --> 00:18:36,654
So SQS is a pull based event broker.

284
00:18:36,754 --> 00:18:40,030
So what we mean by that is the consumer has to come

285
00:18:40,062 --> 00:18:43,846
and pull the messages. Until then, the messages or events are going to

286
00:18:43,870 --> 00:18:48,054
be within the queue. So metrics

287
00:18:48,094 --> 00:18:51,718
like approximate age of the oldest message. So if

288
00:18:51,726 --> 00:18:56,502
you're monitoring that, and if the age is increasing beyond

289
00:18:56,558 --> 00:18:59,806
a particular threshold, that means that the consumer is

290
00:18:59,870 --> 00:19:04,364
not keeping up with the speed of the amount

291
00:19:04,404 --> 00:19:07,732
of messages in the, in the queue. So that is something

292
00:19:07,788 --> 00:19:11,584
to keep track of and identify the issue.

293
00:19:13,084 --> 00:19:16,532
The next is Amazon Eventbridge. So in, even in

294
00:19:16,548 --> 00:19:19,660
our design we had these services. So I have highlighted

295
00:19:19,692 --> 00:19:24,384
some key metrics here, like the dead letter Q invocation.

296
00:19:24,684 --> 00:19:28,380
So I mentioned SQS is a pool based broker,

297
00:19:28,492 --> 00:19:31,900
whereas Amazon Eventbridge is a push based broker.

298
00:19:32,012 --> 00:19:36,532
That means the responsibility of doing

299
00:19:36,588 --> 00:19:40,548
retries and error handling lies with the broker itself.

300
00:19:40,716 --> 00:19:43,924
So let's say Eventbridge is trying to send

301
00:19:43,964 --> 00:19:48,508
an event to a target system and the target system is unavailable.

302
00:19:48,636 --> 00:19:52,228
It will do multiple retries as per the number of retries that

303
00:19:52,236 --> 00:19:56,112
is configured in Eventbridge. And even then if it is not able to

304
00:19:56,168 --> 00:20:00,776
reach the target, then it is going to write the information

305
00:20:00,920 --> 00:20:03,960
or event into a dead letter queue.

306
00:20:04,032 --> 00:20:08,032
So again, it will write into dead letter queue only if you have configured it.

307
00:20:08,168 --> 00:20:11,912
So if something is arriving in a queue, that means that is a failed event.

308
00:20:12,048 --> 00:20:15,216
So what is the business impact of a failed event? So you

309
00:20:15,240 --> 00:20:19,240
can configure your dead letter queue and the number of retries

310
00:20:19,392 --> 00:20:22,750
based on your business SLA's. So even

311
00:20:22,782 --> 00:20:27,318
in our example, you can have a scenario

312
00:20:27,446 --> 00:20:30,910
where the savings account opening request has come,

313
00:20:31,062 --> 00:20:35,350
but for some reason it has not been picked up by the account opening

314
00:20:35,382 --> 00:20:39,758
service. So that it could be a reason that the eventbridge didn't

315
00:20:39,886 --> 00:20:43,350
get to place the event into the queue at all. So if that is the

316
00:20:43,382 --> 00:20:46,414
case, then you can write that

317
00:20:46,494 --> 00:20:50,250
event into a dead letter queue and keep

318
00:20:50,282 --> 00:20:53,842
track of this metric and take some appropriate

319
00:20:53,938 --> 00:20:57,362
actions at the back of it. So those are the

320
00:20:57,378 --> 00:21:01,234
examples of standard metrics and how these metrics are written

321
00:21:01,274 --> 00:21:04,818
into or organized into the cloud watches through

322
00:21:04,866 --> 00:21:08,650
namespaces and dimensions. So namespaces consider

323
00:21:08,722 --> 00:21:13,622
it like a box or a container for your scope.

324
00:21:13,738 --> 00:21:17,454
So the scope can be your application. So in this case I just put

325
00:21:17,534 --> 00:21:21,142
savings account opening application is the namespace and within

326
00:21:21,198 --> 00:21:24,594
that you can have dimensions. So the dimensions can be

327
00:21:25,094 --> 00:21:29,158
the service name. For Lambda, it can be the check product eligibility services,

328
00:21:29,246 --> 00:21:32,998
the dimension within which you are tracking the metrics. Similarly for queue,

329
00:21:33,046 --> 00:21:36,662
the queue name can be a dimension for event bridge, the event bus

330
00:21:36,758 --> 00:21:40,102
can be a dimension within which you track

331
00:21:40,158 --> 00:21:43,696
the so these are for the standard metrics,

332
00:21:43,840 --> 00:21:47,800
but you can do custom dimensions and custom metrics

333
00:21:47,832 --> 00:21:51,216
as well, which is what we are going to see next.

334
00:21:51,360 --> 00:21:54,528
So I mentioned that 70% of your needs are going to be met

335
00:21:54,576 --> 00:21:58,176
with the standard metrics, but just the

336
00:21:58,200 --> 00:22:01,816
built in metrics may not be enough. So that could

337
00:22:01,840 --> 00:22:05,336
be criteria where or scenarios where you

338
00:22:05,360 --> 00:22:09,048
need to measure the application performance against your business goal,

339
00:22:09,216 --> 00:22:12,288
like the revenue or signups, the page views.

340
00:22:12,376 --> 00:22:15,520
So those are not your application level things that

341
00:22:15,552 --> 00:22:19,312
you need to track from your application.

342
00:22:19,368 --> 00:22:23,712
Business logic so what do we use in writing?

343
00:22:23,768 --> 00:22:27,112
The business logic in an event driven architecture is usually it's

344
00:22:27,128 --> 00:22:31,024
a lambda function, or it can be a container container

345
00:22:31,064 --> 00:22:34,864
service within which you're doing some business logic and

346
00:22:34,904 --> 00:22:38,230
you want the tracking to be done through the

347
00:22:38,262 --> 00:22:41,638
business logic. In that case you can instrument your

348
00:22:41,686 --> 00:22:45,846
code to create custom metrics and then send

349
00:22:45,870 --> 00:22:49,594
it into Cloudwatch. So this is something

350
00:22:50,614 --> 00:22:53,874
that most applications will need for better performance.

351
00:22:54,454 --> 00:22:58,222
So we will see how it is done in the coming

352
00:22:58,278 --> 00:23:02,468
slides. So now going back to our maturity model,

353
00:23:02,566 --> 00:23:06,064
so we have talked about the foundational monitoring and next

354
00:23:06,104 --> 00:23:09,632
we are moving towards doing telemetry analysis

355
00:23:09,768 --> 00:23:13,632
and insights. So we have got the data. How do you do insights?

356
00:23:13,768 --> 00:23:17,204
Primarily the lambda service, which is the key

357
00:23:17,504 --> 00:23:21,488
business logic service in an event driven architecture. How do you collect

358
00:23:21,536 --> 00:23:25,320
insights from that? So there is an out of the box feature in Cloudwatch for

359
00:23:25,352 --> 00:23:28,800
doing lambda insights. So when you enable

360
00:23:28,872 --> 00:23:32,244
that, you will be able to monitor, troubleshoot and optimize

361
00:23:32,284 --> 00:23:35,344
the performance of your AWS lambda functions.

362
00:23:36,444 --> 00:23:40,708
Some of the use cases where this might come in handy is identifying

363
00:23:40,796 --> 00:23:44,284
high cost functions, identifying memory leaks and

364
00:23:44,324 --> 00:23:47,660
identifying any performance changes whenever there is a new version of

365
00:23:47,692 --> 00:23:51,436
a lambda function that is being deployed, and also

366
00:23:51,500 --> 00:23:54,904
understanding the latency drivers in function.

367
00:23:55,044 --> 00:23:59,144
So latency drivers actually it's a very key concept because

368
00:23:59,264 --> 00:24:04,360
in a lambda execution time, there are various splits

369
00:24:04,432 --> 00:24:08,216
within that, like cold start time and there can

370
00:24:08,240 --> 00:24:11,728
be a bootstrapping time and then the actual execution

371
00:24:11,776 --> 00:24:15,536
time. Cold start is the time that AWS takes

372
00:24:15,600 --> 00:24:19,480
to provision and lambda instance

373
00:24:19,672 --> 00:24:23,680
and bootstrapping time is the time to get your dependencies and

374
00:24:23,712 --> 00:24:27,022
libraries loaded and then you have the actual execution

375
00:24:27,078 --> 00:24:31,194
time. So it's important to split the whole execution time

376
00:24:31,494 --> 00:24:34,590
to see how much is cold start, how much is the bootstrapping

377
00:24:34,622 --> 00:24:37,990
time and how much is execution time to see if there is

378
00:24:38,022 --> 00:24:41,230
any bottleneck. And there are various mechanism where in which

379
00:24:41,302 --> 00:24:43,714
each of these areas can be fine tuned.

380
00:24:45,654 --> 00:24:49,078
So the lambda insight is a dashboard within

381
00:24:49,126 --> 00:24:53,132
Cloudwatch. So these are automatic dashboards which Cloudwatch creates. There are

382
00:24:53,148 --> 00:24:57,092
two main types of dashboards. One is a multifunction dashboard,

383
00:24:57,228 --> 00:25:01,140
which provides an aggregated view across multiple lambda

384
00:25:01,172 --> 00:25:04,420
functions. So you can see the list of lambda functions in your account,

385
00:25:04,612 --> 00:25:07,956
how much of the code start, how much is the memory utilization and all that.

386
00:25:07,980 --> 00:25:12,144
So it looks something like this. The next is a single function dashboard.

387
00:25:12,604 --> 00:25:16,284
It helps to view a single lambda function and identify

388
00:25:16,444 --> 00:25:20,404
root causes for any issues. So this is a very useful

389
00:25:20,744 --> 00:25:24,144
feature. So recommend looking

390
00:25:24,184 --> 00:25:27,720
at it. So even in our architecture, if you want to go back to our

391
00:25:27,752 --> 00:25:31,512
example and see where it can be useful. So you, we had the

392
00:25:31,688 --> 00:25:34,312
product eligibility service as a lambda function.

393
00:25:34,448 --> 00:25:38,404
So if you calculate the whole duration of execution,

394
00:25:38,824 --> 00:25:43,240
that is going to directly impact your

395
00:25:43,432 --> 00:25:47,024
business SLA, which is like 100 milliseconds for the

396
00:25:47,144 --> 00:25:50,800
product eligibility check. So if the duration of the lambda

397
00:25:50,832 --> 00:25:54,284
itself is more than that, then that is something to be looked at.

398
00:25:55,584 --> 00:25:59,400
Okay, so now that was about metrics and also

399
00:25:59,592 --> 00:26:02,992
a quick overview of lambda insights. So now let's see

400
00:26:03,048 --> 00:26:07,032
what are the other things that you can do

401
00:26:07,168 --> 00:26:10,384
with the next pillar of observability, which is structured and

402
00:26:10,424 --> 00:26:14,764
centralized logging. So cloudwatch logs

403
00:26:16,504 --> 00:26:19,680
can be collected from various services. So the key

404
00:26:19,712 --> 00:26:23,040
services that I have highlighted is API Gateway and

405
00:26:23,152 --> 00:26:26,880
Lambda. So for API gateway there are two levels of logging,

406
00:26:26,952 --> 00:26:30,440
which is logging errors and logging information.

407
00:26:30,632 --> 00:26:34,032
So maybe in your lower environments you want to do both, but in your higher

408
00:26:34,088 --> 00:26:37,676
environments, a stable environment, you just want to track errors.

409
00:26:37,800 --> 00:26:41,772
Yeah. So it's up to the customer requirements.

410
00:26:41,948 --> 00:26:45,332
And also you can do custom metrics based on

411
00:26:45,388 --> 00:26:49,144
your logs. That is, you can filter a set of

412
00:26:49,844 --> 00:26:53,684
logs based on criteria. So example count of 400,

413
00:26:53,764 --> 00:26:57,428
403 errors, maybe a filter, and then you create,

414
00:26:57,516 --> 00:27:00,988
you create a custom metric saying like what was the count?

415
00:27:01,116 --> 00:27:05,028
So that can be a metric filter that you can create

416
00:27:05,156 --> 00:27:07,904
and add it into your custom dashboards.

417
00:27:09,084 --> 00:27:12,084
And next is lambda logging.

418
00:27:12,244 --> 00:27:15,820
So this is where lambda logging is going to help you

419
00:27:15,852 --> 00:27:19,572
to write custom metrics. So if you remember, we just talked about custom

420
00:27:19,628 --> 00:27:22,692
metrics. So this is the way you do it. So there are two ways you

421
00:27:22,708 --> 00:27:26,380
could do it, either through the put metric API or through the embedded

422
00:27:26,412 --> 00:27:31,154
metrics format. Put metric API is a synchronous API

423
00:27:31,194 --> 00:27:34,602
call. That means you're going to write the logs during

424
00:27:34,658 --> 00:27:37,762
the execution time of the lambda.

425
00:27:37,898 --> 00:27:41,938
So that is unnecessary add added overhead to your lambda execution

426
00:27:41,986 --> 00:27:45,694
time. So the recommended approach is to do it asynchronously.

427
00:27:46,634 --> 00:27:50,714
The great example is to do it via the embedded matrix format.

428
00:27:50,874 --> 00:27:53,770
So that will write,

429
00:27:53,882 --> 00:27:57,194
write the logs asynchronously or like outside of the execution

430
00:27:57,234 --> 00:28:00,922
of the lambda function. So what you

431
00:28:00,938 --> 00:28:04,858
can do is you can create your custom message of how bought

432
00:28:04,906 --> 00:28:08,854
or of what information you have to write into your logs

433
00:28:09,314 --> 00:28:12,514
and then put it into Cloudwatch.

434
00:28:12,634 --> 00:28:16,098
So in this, in a way you are bringing your custom metrics

435
00:28:16,146 --> 00:28:18,294
into Cloudwatch logs.

436
00:28:21,634 --> 00:28:25,218
So to give an example of Cloudwatch logging

437
00:28:25,266 --> 00:28:29,082
where it can be useful on how it relates to business

438
00:28:29,218 --> 00:28:33,042
SLA's is. So we said for API gateway we have

439
00:28:33,138 --> 00:28:36,634
error and information logs.

440
00:28:36,754 --> 00:28:40,978
So if it is an error event and if

441
00:28:40,986 --> 00:28:46,498
you remember there was a business SLA to ability to open account

442
00:28:46,586 --> 00:28:50,514
24/7 but if there are any errors that is happening at the

443
00:28:50,554 --> 00:28:54,048
product eligibility check, then it is impacting the business

444
00:28:54,176 --> 00:28:58,000
SLA so that is customer would not be able to open the or place the

445
00:28:58,032 --> 00:29:02,096
request if there is an error when doing the product eligibility

446
00:29:02,200 --> 00:29:06,256
check. So that is something to be avoided or remediated

447
00:29:06,320 --> 00:29:10,576
immediately. So now

448
00:29:10,720 --> 00:29:14,464
we have got all of the logs, let's say, and next step

449
00:29:14,504 --> 00:29:18,344
is to derive insights similar to the lambda insights, which was an inherent

450
00:29:18,384 --> 00:29:21,528
feature. If you want to do a similar querying of

451
00:29:21,576 --> 00:29:25,256
the logs and get some insights for

452
00:29:25,280 --> 00:29:28,960
the other metrics and log data that you have got,

453
00:29:29,112 --> 00:29:33,048
you have something called the querying of Cloudwatch logs

454
00:29:33,096 --> 00:29:37,200
insights. So there is a specific syntax

455
00:29:37,312 --> 00:29:40,736
to be used in order to do this. So here,

456
00:29:40,800 --> 00:29:44,918
if you want to do top hundred most expensive executions,

457
00:29:45,056 --> 00:29:48,386
then you select the fields, sort it by the

458
00:29:48,410 --> 00:29:52,386
builder duration in a descending order and you're limiting by 100 so that you get

459
00:29:52,410 --> 00:29:56,530
the top hundred records. Then you get the information,

460
00:29:56,642 --> 00:30:00,442
something like this. Another example is to get the last

461
00:30:00,498 --> 00:30:03,810
100 error messages. So again you are selecting

462
00:30:03,842 --> 00:30:07,978
the fields, putting the filter condition on this log level as err,

463
00:30:08,106 --> 00:30:12,362
sorting by timestamp in the descending order and limiting by 100 rows.

464
00:30:12,538 --> 00:30:16,042
So, but as you can see, writing this query and

465
00:30:16,058 --> 00:30:20,098
the syntax is a bit of a learning curve. So for to help customers

466
00:30:20,146 --> 00:30:23,762
you want to get started with this querying format. There is

467
00:30:23,778 --> 00:30:27,874
a new feature that has been announced which is called as a powered natural

468
00:30:27,914 --> 00:30:31,546
language query generation. So this is still in preview.

469
00:30:31,610 --> 00:30:34,714
As you, as you note, it's not generally available,

470
00:30:34,874 --> 00:30:39,264
but this is a great service and it is going to be very helpful where

471
00:30:39,304 --> 00:30:42,448
you, you can type in your query

472
00:30:42,496 --> 00:30:47,048
in natural language similar to what we had in the previous slide.

473
00:30:47,096 --> 00:30:51,264
That is if you type in as get the last hundred error messages

474
00:30:51,344 --> 00:30:54,624
in the API Gateway Cloudwatch log group,

475
00:30:54,744 --> 00:30:58,080
then it is going to create the query for you and then maybe you can

476
00:30:58,112 --> 00:31:01,352
fine tune it further and get your results.

477
00:31:01,488 --> 00:31:05,488
So this is one way of doing a powered

478
00:31:05,616 --> 00:31:09,836
insights from Cloudwatch. So that was on the intermediate

479
00:31:09,900 --> 00:31:13,188
monitoring or the analysis and insights.

480
00:31:13,316 --> 00:31:17,388
Now it's time to move to the last two stages, which is like advanced

481
00:31:17,516 --> 00:31:21,220
and proactive observability. How can you do that? How can

482
00:31:21,252 --> 00:31:25,644
you like proactively handle or

483
00:31:25,684 --> 00:31:29,228
find out errors and handle errors and see

484
00:31:29,276 --> 00:31:33,504
some, do some anomaly deduction and take appropriate actions?

485
00:31:33,944 --> 00:31:36,644
The first area is creating alerts.

486
00:31:37,464 --> 00:31:41,324
One way to do alerts is through cloudwatch alarms.

487
00:31:42,264 --> 00:31:46,048
How you can create an alarm is you can choose a specific metric

488
00:31:46,216 --> 00:31:50,256
and set a threshold on that metric. And then

489
00:31:50,320 --> 00:31:54,360
whenever that threshold is breached, this alarm will be raised.

490
00:31:54,512 --> 00:31:58,296
So by alarm, what we say is there will be a notification sent

491
00:31:58,360 --> 00:32:02,134
to a target. So the notification can be through an SNS,

492
00:32:02,174 --> 00:32:06,438
through an email to an appropriate operations

493
00:32:06,566 --> 00:32:10,030
team, or it can be an integration to

494
00:32:10,062 --> 00:32:13,114
an incident management system.

495
00:32:13,654 --> 00:32:17,462
So in our example, so if we say what can be an alert

496
00:32:17,598 --> 00:32:21,294
example is when the age of the message in the queue is growing

497
00:32:21,334 --> 00:32:24,806
beyond the business sla. We already looked at it. So if that

498
00:32:24,830 --> 00:32:28,644
is the case, then that means no one is picking up the account opening

499
00:32:28,684 --> 00:32:31,788
request. So that's something to be kind of concerned. Like before

500
00:32:31,836 --> 00:32:35,316
the business sla of 2 hours is reached, you need

501
00:32:35,340 --> 00:32:39,212
to get the account opened. So if no one is picking up the request,

502
00:32:39,348 --> 00:32:43,144
you need to like monitor, alert someone and get it corrected.

503
00:32:44,844 --> 00:32:49,476
The next way of doing some alarms is through CloudWatch anomaly deduction.

504
00:32:49,620 --> 00:32:53,554
So if you enable this feature within CloudWatch,

505
00:32:53,724 --> 00:32:57,434
CloudWatch is going to keep track of your metrics and patterns.

506
00:32:58,094 --> 00:33:01,590
Again, you will see that if you see in this graph,

507
00:33:01,702 --> 00:33:05,134
you're seeing like at what times or durations of

508
00:33:05,174 --> 00:33:08,142
your day or a week, that there is peaks.

509
00:33:08,198 --> 00:33:11,942
And when there is a lesser number of requests, when there is

510
00:33:11,958 --> 00:33:15,622
a peak, if there is a change in the regular pattern,

511
00:33:15,758 --> 00:33:18,114
it is going to alert as an alarm.

512
00:33:19,374 --> 00:33:23,278
So then maybe you will need a human in the loop step here

513
00:33:23,326 --> 00:33:26,934
to see if it isn't really an alarm situation or

514
00:33:28,194 --> 00:33:31,858
some increase in traffic which has caused the anomaly.

515
00:33:32,026 --> 00:33:36,970
So in our example, what could be an anomaly

516
00:33:37,002 --> 00:33:40,610
deduction scenario? So if there are any API requests.

517
00:33:40,642 --> 00:33:44,186
So you see, let's say from eight to five, you see a peak

518
00:33:44,370 --> 00:33:48,290
in account opening and after that it is less.

519
00:33:48,442 --> 00:33:51,946
And on some day, in middle of the night, there have been like

520
00:33:52,090 --> 00:33:55,672
hundreds and hundreds of account opening request. Then that is something

521
00:33:55,768 --> 00:33:58,936
of an anomaly and something to be looked at. It could be

522
00:33:58,960 --> 00:34:02,656
a security vulnerability pattern that someone is trying to attack the

523
00:34:02,720 --> 00:34:05,968
site with multiple requests. So by

524
00:34:06,056 --> 00:34:10,032
you might have to look at like putting a waf or preventing the

525
00:34:10,088 --> 00:34:13,552
distributed denial of service for your application.

526
00:34:13,728 --> 00:34:17,204
So in those cases, anomaly reduction will be very useful.

527
00:34:18,264 --> 00:34:22,606
And another feature of correlation

528
00:34:22,790 --> 00:34:26,686
is block patterns. So when do

529
00:34:26,710 --> 00:34:30,154
we. You would need to look at a pattern within logs.

530
00:34:30,614 --> 00:34:35,022
Usually that there are very large

531
00:34:35,078 --> 00:34:38,270
challenges with the log analysis. Some of the features are

532
00:34:38,302 --> 00:34:42,102
highlighted here, like there is too much of data because you are continuously getting

533
00:34:42,278 --> 00:34:46,006
logs from various components of a system. And if

534
00:34:46,030 --> 00:34:49,782
there is any change in the system, how it is changing, the amount of logs

535
00:34:49,878 --> 00:34:53,726
or the type of logs that you are creating will also change.

536
00:34:53,870 --> 00:34:57,566
So how do you proactively detect any unusual changes in

537
00:34:57,590 --> 00:35:01,314
your logs from the huge amount of logs that you have got?

538
00:35:01,654 --> 00:35:05,534
If there is a mechanism to do some pattern matching,

539
00:35:05,694 --> 00:35:09,806
to see that these are the various types of logs that I have got

540
00:35:09,950 --> 00:35:13,646
and that is the usual pattern of your application. And if there

541
00:35:13,670 --> 00:35:17,746
is any new pattern being recognized, please do let me know proactively

542
00:35:17,890 --> 00:35:21,698
so that I can go and check if that is something of concern or

543
00:35:21,786 --> 00:35:25,978
is it something due to a change in my application? An example

544
00:35:26,026 --> 00:35:30,194
could be. So this is an example of an API request in the

545
00:35:30,274 --> 00:35:33,610
API gateway. So the pattern here is, it is an

546
00:35:33,642 --> 00:35:36,842
information message followed by a timestamp and it

547
00:35:36,858 --> 00:35:39,894
says API request received followed by a customer id.

548
00:35:40,274 --> 00:35:44,426
So this is logged as a pattern. So pattern analysis in logging

549
00:35:44,450 --> 00:35:48,534
sites is a new feature that has been announced recently.

550
00:35:49,234 --> 00:35:52,906
So to log the various patterns. So whenever, for whenever

551
00:35:52,930 --> 00:35:56,354
there is a new application and you have enabled logging

552
00:35:56,394 --> 00:35:59,914
in components, this feature can be very useful in

553
00:35:59,954 --> 00:36:04,106
identifying the various patterns of logs that

554
00:36:04,130 --> 00:36:08,326
you have got in your application. And if there is any out of ordinary pattern,

555
00:36:08,450 --> 00:36:12,286
it's going to automatically highlight it to you so

556
00:36:12,310 --> 00:36:15,582
that you can be aware of it and see whether it is

557
00:36:15,598 --> 00:36:18,766
an anomaly or a

558
00:36:18,790 --> 00:36:22,594
real pattern that has emerged in your application.

559
00:36:24,294 --> 00:36:27,854
So we have covered logs and metrics

560
00:36:27,974 --> 00:36:32,394
and now we are going towards the tracing part. So tracing,

561
00:36:32,894 --> 00:36:36,458
as I mentioned, is through AWS X ray.

562
00:36:36,646 --> 00:36:39,906
And this service can help you to

563
00:36:39,930 --> 00:36:43,414
do end to end view of requests flowing through an application.

564
00:36:43,754 --> 00:36:47,146
So you can do it through Lambda and

565
00:36:47,170 --> 00:36:50,538
you can also do it on API gateway. And there are a

566
00:36:50,546 --> 00:36:53,930
few other services with which x ray

567
00:36:54,122 --> 00:36:57,898
integrates to bring those into your trays.

568
00:36:58,026 --> 00:37:01,914
For example Eventbridge. So there are limitations,

569
00:37:01,994 --> 00:37:05,938
but still you can do tracing with Eventbridge if you

570
00:37:05,946 --> 00:37:09,418
are instrumenting your code on the producer side.

571
00:37:09,546 --> 00:37:13,170
So on the producer side, if you instrument it and then send

572
00:37:13,202 --> 00:37:16,594
the event, the x ray header similar to how

573
00:37:16,634 --> 00:37:20,602
you see for API Gateway, you will see a tracing header that

574
00:37:20,618 --> 00:37:24,338
is being sent from the producer into Eventbridge, and Eventbridge can pass

575
00:37:24,386 --> 00:37:28,058
on that header to the target and the target can continue

576
00:37:28,146 --> 00:37:32,250
the tracing. So in this way you can bring the

577
00:37:32,282 --> 00:37:35,636
applications into your trace. And what

578
00:37:35,700 --> 00:37:39,340
xray creates when all of these traces from

579
00:37:39,372 --> 00:37:42,748
these applications are reported back to x ray is it

580
00:37:42,796 --> 00:37:46,364
creates a service map. The service map

581
00:37:46,484 --> 00:37:50,052
is nothing but a flow of the event

582
00:37:50,188 --> 00:37:53,580
or the request through various

583
00:37:53,652 --> 00:37:57,316
applications. So if we take our example, you can do

584
00:37:57,340 --> 00:38:00,672
a trace in this real time flow

585
00:38:00,828 --> 00:38:04,464
where the customer request is sent to API gateway and it is sent

586
00:38:04,504 --> 00:38:07,328
to Lambda and then to a dynamodb table.

587
00:38:07,496 --> 00:38:11,544
So you, if you remember, we had a business sla of doing

588
00:38:11,624 --> 00:38:14,984
this request in hundred milliseconds. So you can use

589
00:38:15,024 --> 00:38:18,280
x ray to see what is the end to end processing time is,

590
00:38:18,392 --> 00:38:21,416
and also you can see the split of time in each of the service,

591
00:38:21,480 --> 00:38:25,040
like how much latency is in IP gateway, how much is in

592
00:38:25,072 --> 00:38:28,316
lambda and how much is in lambda animal DB. And that will give

593
00:38:28,340 --> 00:38:31,796
you an idea of where fine tuning, if at all needed,

594
00:38:31,900 --> 00:38:35,604
has to be done. So for X ray you can

595
00:38:35,644 --> 00:38:39,508
do it on lambda console or through the API, Amazon API gateway

596
00:38:39,556 --> 00:38:42,764
console, or you can do it via infrastructure as

597
00:38:42,804 --> 00:38:46,224
code. So like a AWS Sam, you can do that as well.

598
00:38:47,604 --> 00:38:51,588
So that's a very quick overview of doing tracing using AWS

599
00:38:51,636 --> 00:38:55,140
x ray. And the last bit of service which

600
00:38:55,172 --> 00:38:58,804
can be very handy in terms of lambda that I wanted to highlight

601
00:38:58,924 --> 00:39:02,580
is the lambda power tools. We're not going to dive deep into that.

602
00:39:02,612 --> 00:39:06,300
I have added a resource link at the end of the slide

603
00:39:06,412 --> 00:39:09,860
to know more about it, but just on a high level,

604
00:39:09,932 --> 00:39:13,900
it is a developer toolkit or a very opinionated library

605
00:39:13,972 --> 00:39:17,116
that is created by AWS which helps you to

606
00:39:17,180 --> 00:39:20,476
implement observability best practices. That is, you can

607
00:39:20,500 --> 00:39:23,414
do logs, metrics and traces with very minimal code.

608
00:39:23,524 --> 00:39:26,810
So that's the main idea of it. So it's

609
00:39:26,842 --> 00:39:30,242
very useful, not just for observability, but for

610
00:39:30,298 --> 00:39:33,882
many other serverless best practices.

611
00:39:33,978 --> 00:39:37,666
So if you use the serverless links against the well architected framework,

612
00:39:37,810 --> 00:39:41,146
these are the various areas in which power tools are going to

613
00:39:41,210 --> 00:39:45,254
assist you. So it's worth mentioning. So that's why I've highlighted that.

614
00:39:46,994 --> 00:39:49,946
So now that we've seen all of the bits and pieces of what are the

615
00:39:49,970 --> 00:39:54,454
various things that you can do, imagine there is an observability

616
00:39:54,534 --> 00:39:58,550
team or all the application teams which is involved

617
00:39:58,622 --> 00:40:02,534
in building this end to end application. So if something

618
00:40:02,614 --> 00:40:06,074
goes wrong and if you want to troubleshoot,

619
00:40:07,374 --> 00:40:10,622
it's good to have all of the things in a single place. So you

620
00:40:10,638 --> 00:40:14,302
need not go to multiple places to find out where the issue was.

621
00:40:14,478 --> 00:40:18,018
So you need a single plane of glass in which

622
00:40:18,066 --> 00:40:21,602
you can see your logs, metrics, your alarms, your dashboards,

623
00:40:21,698 --> 00:40:26,002
on your traces. So that's why we have this service or feature

624
00:40:26,058 --> 00:40:29,538
in Cloudwatch called Cloudwatch service

625
00:40:29,626 --> 00:40:33,458
lens. So it is a single pane of glass which is going to help

626
00:40:33,506 --> 00:40:37,174
you to drill into your all of your

627
00:40:37,514 --> 00:40:40,978
observability telemetry data that we have discussed so far.

628
00:40:41,106 --> 00:40:44,866
So please take a look at it. It is going

629
00:40:44,890 --> 00:40:47,922
to be very useful. And lastly,

630
00:40:48,058 --> 00:40:51,858
I want to finish off with some best practices for observing event driven

631
00:40:51,906 --> 00:40:55,146
application. So these best practices is not

632
00:40:55,170 --> 00:40:59,090
just for EDA, but for any application where you want to get started.

633
00:40:59,162 --> 00:41:03,282
With the observability that is. So it is like an eight step process.

634
00:41:03,378 --> 00:41:06,334
We are going to like just whiz through that process.

635
00:41:07,834 --> 00:41:11,290
You can look into the resources for more details on this.

636
00:41:11,482 --> 00:41:15,546
First thing is observe what matters because as we discussed,

637
00:41:15,570 --> 00:41:18,794
there could be a huge amount of logs,

638
00:41:18,834 --> 00:41:22,546
metrics and traces that is generated from each of your component.

639
00:41:22,650 --> 00:41:25,530
So focus on what matters to your business,

640
00:41:25,722 --> 00:41:29,306
what matters to your customers. So that's why we keep going back

641
00:41:29,330 --> 00:41:32,930
to the business SLA's and work backwards from that to

642
00:41:32,962 --> 00:41:36,090
see what are the data that needs to be observed. And you need to

643
00:41:36,122 --> 00:41:39,974
measure your objectives against those SLA's

644
00:41:40,674 --> 00:41:43,906
so that you know what good looks like. Because we cannot be saying that.

645
00:41:43,930 --> 00:41:47,610
Yes, I might always be looking at the happy path. We need

646
00:41:47,642 --> 00:41:51,170
to see like this is what is the metric and I have achieved that and

647
00:41:51,202 --> 00:41:55,474
that's why my application is performing at its best. And identify

648
00:41:55,554 --> 00:41:59,466
the sources from which this telemetry data has to be taken.

649
00:41:59,610 --> 00:42:03,242
The plan ahead. So this is not an reactive monitoring,

650
00:42:03,338 --> 00:42:06,532
it's a proactive observability. So that is important to

651
00:42:06,548 --> 00:42:09,716
be kept in mind. The next is alerting strategy.

652
00:42:09,820 --> 00:42:13,308
So we discussed about the various types of alerts that can be

653
00:42:13,436 --> 00:42:16,724
created, but define the criteria because there are some

654
00:42:16,764 --> 00:42:20,764
alerts can be just warning, some can be critical where immediate action is

655
00:42:20,804 --> 00:42:24,724
needed. So define the criteria and also the appropriate actions

656
00:42:24,764 --> 00:42:27,908
for each of the alerts. The next is dashboards.

657
00:42:28,036 --> 00:42:32,116
So now we have all of the data. So you can create nice graphs

658
00:42:32,140 --> 00:42:35,540
and charts within Cloudwatch, but have

659
00:42:35,572 --> 00:42:39,372
some strategy of what data is going into each of

660
00:42:39,388 --> 00:42:42,740
the dashboard and who is going to look at it. So you can create like

661
00:42:42,772 --> 00:42:46,524
very high level dashboards, like customer experience or how your

662
00:42:46,684 --> 00:42:50,244
application is performing last week and how it is performing this week.

663
00:42:50,324 --> 00:42:53,572
So these kind of things for like cxo levels and maybe the

664
00:42:53,588 --> 00:42:57,424
head of levels and very low level dashboards going into the

665
00:42:57,924 --> 00:43:01,036
nitty gritty details of your application.

666
00:43:01,140 --> 00:43:05,044
Maybe for your platform engineering team. The next is

667
00:43:05,084 --> 00:43:08,372
tool selection. So choose the right tool for the job.

668
00:43:08,468 --> 00:43:12,484
So in this session we talked about AWS native tools for observability.

669
00:43:12,644 --> 00:43:16,156
But many of our customers who

670
00:43:16,220 --> 00:43:19,844
build ADA on AWS, but they still use third

671
00:43:19,884 --> 00:43:22,744
party tools or open source tools like open telemetry,

672
00:43:23,324 --> 00:43:27,622
which has the industry standard and supported by various vendor applications like

673
00:43:27,678 --> 00:43:31,014
Grafana and Prometheus. Those also can be good choices

674
00:43:31,054 --> 00:43:34,574
for your application. So it all depends on what is

675
00:43:34,614 --> 00:43:39,182
your need and then you pick the right features for it and

676
00:43:39,198 --> 00:43:42,654
then bringing it all together. Observability needs

677
00:43:42,694 --> 00:43:46,422
to be an internal process that everyone agrees on and

678
00:43:46,438 --> 00:43:49,030
it has to be part of the operational readiness.

679
00:43:49,222 --> 00:43:52,526
So if these are the necessary observability things to

680
00:43:52,550 --> 00:43:56,158
be in place for the application to go live. So that kind of one

681
00:43:56,206 --> 00:44:00,182
mindset and cultural change has to be there in your organization to

682
00:44:00,238 --> 00:44:03,630
mature your observability framework and

683
00:44:03,662 --> 00:44:07,710
finally iterate because this is not a one off process. As and

684
00:44:07,742 --> 00:44:11,270
when your application grows, your maturity model is going to grow

685
00:44:11,422 --> 00:44:14,782
and when there are like new features that is getting introduced

686
00:44:14,838 --> 00:44:18,198
into your application, your observability

687
00:44:18,286 --> 00:44:22,464
is also changing. So this has to be an iterative process

688
00:44:22,614 --> 00:44:26,204
and to be reviewed routinely. So that is my

689
00:44:26,244 --> 00:44:30,020
eight step process towards the best

690
00:44:30,052 --> 00:44:33,332
practices of observability. So use those best practices,

691
00:44:33,468 --> 00:44:36,932
overcome the challenges that you have in EDA and get

692
00:44:36,988 --> 00:44:40,300
your EDA right, because EDA is great and it is going to

693
00:44:40,332 --> 00:44:44,184
help you to deliver business outcomes very effectively.

694
00:44:44,764 --> 00:44:48,628
I added some further reading to know more about serverless observability

695
00:44:48,796 --> 00:44:52,364
and observability for modern applications and also a link to the lambda

696
00:44:52,404 --> 00:44:56,236
power tools that we talked about. So thank you so much for your time.

697
00:44:56,340 --> 00:45:00,036
I'm Umila Raju. Please feel free to connect with me on LinkedIn and

698
00:45:00,060 --> 00:45:02,364
I'm happy to take your questions as well. Thank you.

