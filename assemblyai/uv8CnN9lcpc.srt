1
00:00:20,730 --> 00:00:23,854
Hi everyone, I'm Bolu and I am

2
00:00:23,892 --> 00:00:27,446
an independent AI researcher. And today I'm going

3
00:00:27,468 --> 00:00:31,800
to talk about using Python for large language models research.

4
00:00:32,490 --> 00:00:36,310
Specifically, I'll be speaking on the insights Python library

5
00:00:36,810 --> 00:00:40,120
that enables mechanistic interpretability research.

6
00:00:40,650 --> 00:00:43,878
So a bit of a primer on this field

7
00:00:43,964 --> 00:00:47,750
of inquiry. What is mechanistic interpretability?

8
00:00:48,570 --> 00:00:52,442
Some things we can hopefully all agree on. First is that neural networks

9
00:00:52,506 --> 00:00:55,310
solve an increase in number of important tasks.

10
00:00:55,890 --> 00:01:00,158
And the second is that it would be at least interesting and probably important

11
00:01:00,324 --> 00:01:03,902
to understand how they do that is interesting in the sense

12
00:01:03,956 --> 00:01:07,298
of, if you feel any sense of curiosity, to basically

13
00:01:07,384 --> 00:01:11,060
look inside this whole world that is currently black box to most people

14
00:01:11,510 --> 00:01:15,246
just out of. Because these models arrive at solutions

15
00:01:15,278 --> 00:01:18,674
that no person could write a program for. So, out of curiosity,

16
00:01:18,722 --> 00:01:22,614
it'd be interesting to know what are the algorithms being implemented and

17
00:01:22,652 --> 00:01:26,600
hopefully describing them in a human understandable way,

18
00:01:26,970 --> 00:01:30,870
and important in a sense that any sufficiently powerful

19
00:01:30,950 --> 00:01:35,238
system that is being put in strategic places

20
00:01:35,334 --> 00:01:39,114
of great importance in society has

21
00:01:39,152 --> 00:01:42,254
to have a certain level of transparency and understanding before we

22
00:01:42,292 --> 00:01:45,774
as a society can trust it to be deployed. Any effort to

23
00:01:45,812 --> 00:01:49,422
understand how these models work will definitely continue

24
00:01:49,476 --> 00:01:52,254
to be increasingly important in the future. Now,

25
00:01:52,292 --> 00:01:55,946
mechanistic interpretability, or mechinterp, as I'll call it going

26
00:01:55,988 --> 00:01:59,698
forward, because as you can imagine, it's a bit of a mouthful, is a

27
00:01:59,704 --> 00:02:03,182
field of research that tackles this problem starting at a very granular

28
00:02:03,246 --> 00:02:06,566
level of the models. And what does that mean by

29
00:02:06,588 --> 00:02:10,466
granular level? The typical mechanistic interpretability

30
00:02:10,578 --> 00:02:14,342
result provides a mechanistic model that basically means

31
00:02:14,396 --> 00:02:18,242
a causal model describing how different discrete components

32
00:02:18,306 --> 00:02:21,974
in a very large model arrive at some

33
00:02:22,012 --> 00:02:25,754
observed behavior. So we have some observed behavior, and the question is, can you,

34
00:02:25,792 --> 00:02:29,194
through experimental processes, arrive at

35
00:02:29,232 --> 00:02:32,766
an explanation for how these observations come to

36
00:02:32,788 --> 00:02:37,050
be? That is the mechanistic approach to it. Again, this is identifying

37
00:02:37,130 --> 00:02:40,698
mechinterp in a much larger field of interpretability,

38
00:02:40,794 --> 00:02:44,366
which can have different flavors to it. But mechanistic

39
00:02:44,398 --> 00:02:47,966
interpretability is unique in taking this granular causal

40
00:02:47,998 --> 00:02:51,438
model of trying to drill as deep as possible and hoping

41
00:02:51,454 --> 00:02:54,814
to build on larger and larger abstractions,

42
00:02:54,862 --> 00:02:58,006
but starting from the very granular level. And for

43
00:02:58,028 --> 00:03:01,666
today's talk, we're going to be picking one item out of the mechinterp

44
00:03:01,698 --> 00:03:05,442
toolkit, which is that of causal interventions.

45
00:03:05,586 --> 00:03:09,850
So basically the idea is if we abstract the entire network

46
00:03:10,190 --> 00:03:13,786
to be a computational graph, that is, again, we forget that

47
00:03:13,808 --> 00:03:17,782
this is machine learning. Just imagine this has been any abstract computational

48
00:03:17,846 --> 00:03:21,340
graph, and the current state of

49
00:03:21,890 --> 00:03:25,326
not understanding simply means we don't know

50
00:03:25,428 --> 00:03:29,134
what computation each of the nodes are running and how they interact with each other.

51
00:03:29,252 --> 00:03:33,182
So from that perspective, if we're curious about knowing how

52
00:03:33,236 --> 00:03:36,434
one component that is, either it is an attention head,

53
00:03:36,552 --> 00:03:40,094
or an MLP

54
00:03:40,142 --> 00:03:43,310
layer, or a linear layer or an embedding unit, again, you don't have to worry

55
00:03:43,810 --> 00:03:46,914
about any of these means. You can just abstract these as being any node in

56
00:03:46,952 --> 00:03:50,950
some compute graph. But if you do, it'll help to paint a picture better.

57
00:03:51,100 --> 00:03:55,814
So if we're curious to know what any of these nodes contribute to

58
00:03:55,852 --> 00:03:58,860
start with, even knowing if they contribute anything to start with,

59
00:03:59,310 --> 00:04:03,420
one way of doing that is simply taking the node and

60
00:04:04,190 --> 00:04:07,754
observing some behavior that we find interesting

61
00:04:07,872 --> 00:04:11,402
and then changing the impute to that node to see if the downstream

62
00:04:11,466 --> 00:04:15,290
impact for the observed behavior is noteworthy.

63
00:04:15,370 --> 00:04:19,120
That is, if this node, in this example, the node d,

64
00:04:20,210 --> 00:04:23,466
is very vital to some observed behavior downstream.

65
00:04:23,578 --> 00:04:26,766
If we mess with it a bit, that's if we perturb the node,

66
00:04:26,878 --> 00:04:30,606
the observed result should change. That means, okay, this node

67
00:04:30,638 --> 00:04:34,126
is on the critical path from input to output for this observed

68
00:04:34,158 --> 00:04:37,522
behavior. Of course, we expect some part

69
00:04:37,576 --> 00:04:41,046
of the model to change if you mess with anything. But the whole point

70
00:04:41,068 --> 00:04:44,194
of this is that we have to first of all settle on some observed behavior,

71
00:04:44,322 --> 00:04:47,798
and then we tweak the value of some node of interest and

72
00:04:47,804 --> 00:04:51,586
then we observe downstream. If, however, it doesn't have any impact,

73
00:04:51,618 --> 00:04:54,858
then that means this node is not that important and then we

74
00:04:54,864 --> 00:04:58,550
can ignore it. But if it is, then we know that we can drill deeper.

75
00:04:58,710 --> 00:05:01,726
So I think in the rest of the course, I'm going to speak on a

76
00:05:01,748 --> 00:05:04,974
practical example in very recent research that

77
00:05:05,012 --> 00:05:09,214
uses this kind of intervention to try to understand how

78
00:05:09,252 --> 00:05:13,082
a model achieves some outcomes. The topic

79
00:05:13,146 --> 00:05:16,834
of interest today is that of function vectors. So this is a very

80
00:05:16,872 --> 00:05:19,362
recent paper, I think just published last year,

81
00:05:19,416 --> 00:05:23,410
October, from a group from Northeastern University,

82
00:05:24,710 --> 00:05:27,170
from Corey College of Computer Science.

83
00:05:27,670 --> 00:05:31,350
Basically, it is a mechanistic interoperability research effort

84
00:05:31,770 --> 00:05:35,346
that tries to observe some behavior in large language

85
00:05:35,378 --> 00:05:38,730
models, and that behavior is

86
00:05:38,880 --> 00:05:42,490
described thus. So the question is the hypothesis,

87
00:05:42,910 --> 00:05:46,282
is it possible to have some

88
00:05:46,336 --> 00:05:50,090
functional components of large language models?

89
00:05:50,430 --> 00:05:53,854
That is, we can all agree that if I gave,

90
00:05:53,972 --> 00:05:57,626
looking at the top left section here of a string

91
00:05:57,738 --> 00:06:01,114
of input, that is arrive column, depart small column,

92
00:06:01,162 --> 00:06:04,478
big common column, if I

93
00:06:04,484 --> 00:06:07,746
gave this input to something like, say, chad GPT, I think we can

94
00:06:07,768 --> 00:06:11,086
all agree that it will figure out, okay, this is a simple word and opposite

95
00:06:11,118 --> 00:06:13,906
game. That is the first example at the top.

96
00:06:14,008 --> 00:06:17,330
And the second example, I believe is converting to Spanish.

97
00:06:17,930 --> 00:06:21,254
I think we can all agree that something like chat, GBT and

98
00:06:21,292 --> 00:06:24,840
similar large language models are able to do such a thing.

99
00:06:25,290 --> 00:06:28,918
Is it possible for me to take some kernel

100
00:06:29,014 --> 00:06:32,570
of this function of opposite, say again, taking the first example,

101
00:06:32,720 --> 00:06:36,394
and transfer it to a completely different context and

102
00:06:36,432 --> 00:06:40,202
have that same behavior operate on

103
00:06:40,256 --> 00:06:43,806
a token in this new context? What that means is on

104
00:06:43,828 --> 00:06:47,358
the right you see the direction of the arrow. The example, the counterpart on the

105
00:06:47,364 --> 00:06:50,560
right, simply says the word fast means.

106
00:06:51,490 --> 00:06:55,578
Now, under the normal operation of a large language module

107
00:06:55,674 --> 00:06:59,438
model, trying to predict the next token, you can say something like the word fast

108
00:06:59,524 --> 00:07:03,466
means quick, or it means going quickly or any reasonable

109
00:07:03,498 --> 00:07:08,114
thing to follow. However, if this hypothesis

110
00:07:08,162 --> 00:07:11,366
of portability of functions, we should

111
00:07:11,388 --> 00:07:15,334
be able to move something from this context on the left

112
00:07:15,452 --> 00:07:18,906
that clearly is about word and opposite into a completely new

113
00:07:18,928 --> 00:07:22,694
context that has no conception of word and opposite

114
00:07:22,742 --> 00:07:26,426
as an objective and achieve the result of

115
00:07:26,608 --> 00:07:30,474
flipping the word fast into slow. I know

116
00:07:30,592 --> 00:07:34,046
it seems very almost crazy to

117
00:07:34,068 --> 00:07:37,674
expect this is true, but let's just assume this is the leading hypothesis.

118
00:07:37,802 --> 00:07:42,160
And of course we're going to discuss what exactly this thing will be exporting is.

119
00:07:42,790 --> 00:07:46,734
We see there the letter a average layer activation.

120
00:07:46,862 --> 00:07:50,866
What the hypothesis says is this thing in quote that we

121
00:07:50,968 --> 00:07:54,834
plan to port over is simply the

122
00:07:54,872 --> 00:07:58,034
average activation over a series of

123
00:07:58,152 --> 00:08:01,494
projects for a given task. Again, I'm going to break that down a bit.

124
00:08:01,532 --> 00:08:04,822
So again, let's say our task is simple or an opposite. So we have three

125
00:08:04,876 --> 00:08:07,906
different examples. Old young, vanish,

126
00:08:07,938 --> 00:08:11,226
appear, dark. Colin and I

127
00:08:11,248 --> 00:08:14,826
guess something like bright or dark and light will follow. And the

128
00:08:14,848 --> 00:08:18,394
second example, the same thing. Awake, sleep, future, past,

129
00:08:18,512 --> 00:08:21,662
joy. Colin at the very

130
00:08:21,716 --> 00:08:25,178
end of all these contexts, these like query

131
00:08:25,354 --> 00:08:28,846
inputs, the neural network is right on the

132
00:08:28,868 --> 00:08:32,858
verge of doing the thing called flip opposite the

133
00:08:32,884 --> 00:08:37,074
last thing I saw before my column. So the hypothesis is

134
00:08:37,112 --> 00:08:41,474
if we can take that activation state and in

135
00:08:41,512 --> 00:08:44,594
the section b you see there, simply add it

136
00:08:44,632 --> 00:08:47,338
to a completely unrelated context,

137
00:08:47,534 --> 00:08:50,966
would it be possible to observe the same behavior? Because again on

138
00:08:50,988 --> 00:08:54,534
the right we see fair simple. In the absence of this

139
00:08:54,572 --> 00:08:58,234
intervention, we have no reason to expect the model will

140
00:08:58,272 --> 00:09:02,202
say anything other than simple. Then something

141
00:09:02,256 --> 00:09:06,234
like simple, easy, or whatever the model finds appropriate to

142
00:09:06,272 --> 00:09:09,770
follow simple. But if indeed our intervention is important,

143
00:09:09,840 --> 00:09:13,066
we expect to observe something like simple colon

144
00:09:13,178 --> 00:09:16,458
complex or at the bottom there encode becomes decode

145
00:09:16,554 --> 00:09:19,790
just magically by intervening with this average

146
00:09:21,010 --> 00:09:25,018
activation state. Again, I would explain what we mean by activation state in the

147
00:09:25,044 --> 00:09:27,858
following line, but I hope you just get the general thesis of what this is

148
00:09:27,864 --> 00:09:31,182
meant to be. That is the question is, is there a portable

149
00:09:31,246 --> 00:09:35,054
component of operations

150
00:09:35,182 --> 00:09:38,498
and functions inside of neural networks

151
00:09:38,594 --> 00:09:42,038
and more specifically large language models? All right, so I

152
00:09:42,044 --> 00:09:45,862
guess to give a bit of shape what I mean

153
00:09:45,916 --> 00:09:49,034
by activation vector and what is being ported left and right.

154
00:09:49,152 --> 00:09:53,514
So here, this is just like a typical one layer example

155
00:09:53,632 --> 00:09:57,306
of an LLM decoder, only what

156
00:09:57,328 --> 00:10:01,130
we have here is at the very bottom, we input a token,

157
00:10:01,210 --> 00:10:04,522
a sequence of tokens, right? That is like the on colon,

158
00:10:04,586 --> 00:10:07,470
off wet column, dry old colon.

159
00:10:08,290 --> 00:10:11,982
And as we see, the expectation is as this

160
00:10:12,036 --> 00:10:16,020
input passes through subsequent layers in,

161
00:10:16,550 --> 00:10:20,274
there's one single set of vectors that are going to keep being updated and

162
00:10:20,312 --> 00:10:23,406
changed and added on. And again, due to some specifics

163
00:10:23,438 --> 00:10:26,674
to the neural network architectures, more specifically the

164
00:10:26,712 --> 00:10:30,434
skip connections, which I won't get too much into right now, each subsequent

165
00:10:30,482 --> 00:10:34,034
layer adds additional context that is literally just added

166
00:10:34,082 --> 00:10:36,822
on top the last. But in any way that's not really important for now.

167
00:10:36,876 --> 00:10:38,940
So let's just think of it. For example, again,

168
00:10:39,790 --> 00:10:44,090
looking at the journey of the column, the very last column,

169
00:10:44,590 --> 00:10:48,374
when it goes to the embedding layer, it has some vector

170
00:10:48,422 --> 00:10:51,854
that represents okay, cool. This is how the

171
00:10:51,892 --> 00:10:55,534
neural network's embedding layer represents the token of

172
00:10:55,572 --> 00:10:59,950
a column. And again, so we can kind of anthropomorphize,

173
00:11:00,370 --> 00:11:04,014
pretend it's like self aware almost to say

174
00:11:04,052 --> 00:11:08,846
I am a column. Because technically, if you took that embedding vector

175
00:11:08,958 --> 00:11:12,194
and you put it through the unembedding vector on the other

176
00:11:12,232 --> 00:11:15,602
side, it would come out as column is really likely to come,

177
00:11:15,656 --> 00:11:18,870
right? So we might as well just see this as the model

178
00:11:18,940 --> 00:11:22,838
being what information the model has for that position in

179
00:11:22,844 --> 00:11:26,566
the sequence. So somewhere between starting from I am

180
00:11:26,588 --> 00:11:29,766
a column in the beginning to the very end

181
00:11:29,948 --> 00:11:34,250
of the thing that follows me is the word new.

182
00:11:34,400 --> 00:11:38,314
The model has learned some interesting things, right? By definition, like how

183
00:11:38,352 --> 00:11:41,534
else would it know? Again, because it's still

184
00:11:41,572 --> 00:11:45,374
that same column vector that has been

185
00:11:45,412 --> 00:11:49,006
updated for the sequence position of the token column. So the

186
00:11:49,028 --> 00:11:52,286
conjecture here for the hypothesis of portable functions is

187
00:11:52,308 --> 00:11:56,046
that somewhere in between or containing that vector is information

188
00:11:56,148 --> 00:11:59,714
on amicolum, of course, which it had before. And it

189
00:11:59,752 --> 00:12:03,630
also has my next is new. That is, my next token

190
00:12:03,710 --> 00:12:06,786
is the word new, which again is just what

191
00:12:06,808 --> 00:12:10,582
we would observe from Chad GPT so the additional thing the hypothesis is saying

192
00:12:10,636 --> 00:12:14,742
is that, or is asking is that, is there a component that

193
00:12:14,796 --> 00:12:18,438
encodes the operation that it must do or the function it

194
00:12:18,444 --> 00:12:22,074
must do to arrive at new, perhaps before it came

195
00:12:22,112 --> 00:12:25,686
to the conclusion of the next is new, is there a component

196
00:12:25,718 --> 00:12:29,798
that says I am to do or am to call the function opposite

197
00:12:29,894 --> 00:12:33,566
on. Surely there must be of some sort, because how

198
00:12:33,588 --> 00:12:36,160
else would it know to come up with new.

199
00:12:36,530 --> 00:12:40,430
But the question is if there is linearity to this representation

200
00:12:40,850 --> 00:12:44,094
by linearity is just what allows us to do things like

201
00:12:44,132 --> 00:12:47,790
this? Literally take a thing, add it average,

202
00:12:47,870 --> 00:12:51,186
and add it somewhere else and have it do things right. This assumes a lot

203
00:12:51,208 --> 00:12:54,874
of linear behavior. So this is kind of the underlying

204
00:12:54,942 --> 00:12:58,546
implicit assumption that is guiding this hypothesis.

205
00:12:58,578 --> 00:13:02,438
To start with many of the different

206
00:13:02,524 --> 00:13:05,640
research inquiries leads to very interesting result.

207
00:13:06,170 --> 00:13:09,482
Often start with this assumption of can we assume there's lowlinearity? And again,

208
00:13:09,536 --> 00:13:13,354
due to details of the architecture of most

209
00:13:13,552 --> 00:13:16,570
transformer neural networks,

210
00:13:17,150 --> 00:13:20,810
there are reasons to expect there to be low linearity. But just to see it

211
00:13:20,960 --> 00:13:24,506
happen for real is always interesting. And I think this is the first time we're

212
00:13:24,538 --> 00:13:28,202
seeing it in the context of operations as again, just representations,

213
00:13:28,266 --> 00:13:31,120
which I think other research has demonstrated before,

214
00:13:31,570 --> 00:13:34,826
such as for example, the relationship between the word

215
00:13:34,868 --> 00:13:38,450
car and cars, that is, the relationship between a word

216
00:13:38,520 --> 00:13:42,302
and its plural. There's been some regularities observed

217
00:13:42,366 --> 00:13:45,826
in that regard. But this, however, is trying to take it a step further to

218
00:13:45,848 --> 00:13:48,962
say, okay, are there encodings also for functions?

219
00:13:49,106 --> 00:13:52,630
Okay, so we have a rough idea of what it means

220
00:13:52,780 --> 00:13:56,390
for what this h is. It's simply just some vector

221
00:13:56,730 --> 00:14:00,374
that at the very end of the network, right before it

222
00:14:00,412 --> 00:14:03,794
goes into the penultimate layer, or at the penultimate

223
00:14:03,842 --> 00:14:07,958
layer, we could run our model three different times

224
00:14:08,044 --> 00:14:11,598
and snatch that vector across, cross all of them, look at exactly what

225
00:14:11,684 --> 00:14:14,942
read literally what that vector is saying. Because again,

226
00:14:14,996 --> 00:14:18,570
the information on what is to come next is embedded

227
00:14:18,650 --> 00:14:21,982
in the colon token, right? It's the thing that is saying,

228
00:14:22,036 --> 00:14:25,550
okay, dark. So all the information

229
00:14:25,620 --> 00:14:28,994
for what is after dot, dot, dot is in colon. Cool. So we take

230
00:14:29,032 --> 00:14:32,820
that for different runs and we average it out and try to add.

231
00:14:33,190 --> 00:14:36,706
So that gives you an idea of just to draw

232
00:14:36,728 --> 00:14:38,440
a bit of a picture to it.

233
00:14:39,930 --> 00:14:42,998
And of course, this is just restating the same thing now that we have an

234
00:14:43,004 --> 00:14:46,598
idea of what h means and what that vector is.

235
00:14:46,684 --> 00:14:50,906
So for each of the different runs in

236
00:14:50,928 --> 00:14:54,742
a series of prompts that are basically doing the same task,

237
00:14:54,886 --> 00:14:58,374
if we literally took all the values of the vectors,

238
00:14:58,422 --> 00:15:02,442
averaged them in position, added, divided by this

239
00:15:02,576 --> 00:15:05,350
unified, averaged out mean vector,

240
00:15:05,510 --> 00:15:08,474
and we took it into a different environment, into a different context,

241
00:15:08,602 --> 00:15:12,160
and we literally just added it to something else.

242
00:15:12,690 --> 00:15:15,798
The question is, will we be able to get effects

243
00:15:15,834 --> 00:15:19,442
like seen below? That is, if we took. So I think here in the example

244
00:15:19,496 --> 00:15:23,214
you see the representation for encode, again, so encode

245
00:15:23,342 --> 00:15:26,694
column. So there you can see

246
00:15:26,732 --> 00:15:30,310
how we can presume that without this intervention at the end,

247
00:15:30,380 --> 00:15:33,766
after this token goes through the

248
00:15:33,788 --> 00:15:36,886
entire model, it might say something like the

249
00:15:36,908 --> 00:15:40,794
thing to come after the column is base 64, I guess, because maybe

250
00:15:40,912 --> 00:15:44,042
encoding and base 64 is something that shows up

251
00:15:44,096 --> 00:15:47,834
often, right? Remember, the base function

252
00:15:48,032 --> 00:15:51,306
of a large language model is just to predict

253
00:15:51,338 --> 00:15:55,194
the next most likely thing in human generated

254
00:15:55,242 --> 00:15:59,518
text. However, with the addition of our

255
00:15:59,684 --> 00:16:03,550
supposed, our hypothetical average out opposite

256
00:16:03,630 --> 00:16:07,774
function, would we be able to steer

257
00:16:07,822 --> 00:16:12,100
it towards saying something like, actually, instead of saying encode base 64,

258
00:16:12,790 --> 00:16:16,694
I all of a sudden feel the urge to say the opposite of

259
00:16:16,732 --> 00:16:20,166
encode and dev, say encode column decode. This is

260
00:16:20,188 --> 00:16:23,846
the hypothesis. So again, it would be super interesting and kind

261
00:16:23,868 --> 00:16:27,390
of weird if we can indeed prove

262
00:16:27,490 --> 00:16:32,554
this representation. For example, that just has 123456.

263
00:16:32,672 --> 00:16:37,382
Again, our vector just has about six different dimensions.

264
00:16:37,446 --> 00:16:41,290
Encoding it. Of course, as we know, actual large language models

265
00:16:41,370 --> 00:16:44,926
can be much bigger than this in

266
00:16:44,948 --> 00:16:49,022
the billions and billions of parameters. So how

267
00:16:49,076 --> 00:16:52,746
exactly do we plan to do this? Multiple runs

268
00:16:52,778 --> 00:16:55,966
and extracting values and averaging them and intervening and adding

269
00:16:55,998 --> 00:16:58,980
them in the real world, not for a toy model.

270
00:16:59,830 --> 00:17:03,886
And that brings us to our trusted interpretability libraries

271
00:17:03,918 --> 00:17:07,606
and packages. These are packages that are designed solely for

272
00:17:07,628 --> 00:17:10,994
this purpose of staring very deep

273
00:17:11,042 --> 00:17:14,182
into what large

274
00:17:14,236 --> 00:17:18,246
language models of different sizes are up to in

275
00:17:18,268 --> 00:17:20,860
a way that is practical to enable this kind of research.

276
00:17:21,390 --> 00:17:25,462
So we have an insight which is particularly popular

277
00:17:25,526 --> 00:17:29,580
for working with models on the larger side, and I'll discuss

278
00:17:30,030 --> 00:17:33,294
the details of its architecture that afford this

279
00:17:33,332 --> 00:17:36,190
kind of behavior. Then we have transformer lens,

280
00:17:37,010 --> 00:17:40,186
which is also a very great open source

281
00:17:40,218 --> 00:17:44,450
library for doing this. But for today's work, we're going to focus on insights.

282
00:17:45,110 --> 00:17:48,226
So what is it about insights that

283
00:17:48,248 --> 00:17:51,700
makes it work? What is the contact on insight? Where did it come from?

284
00:17:53,030 --> 00:17:56,886
From my understanding, the insights package came

285
00:17:57,068 --> 00:18:01,030
along with an effort called the

286
00:18:01,180 --> 00:18:05,014
NDIF initiative, which is

287
00:18:05,052 --> 00:18:07,430
basically a national deep inference facility.

288
00:18:07,770 --> 00:18:11,786
This is basically a compute cluster that is available to researchers for

289
00:18:11,808 --> 00:18:15,622
doing work that cannot afford the financial burden

290
00:18:15,686 --> 00:18:19,350
of actually running these very large models because they're very costly.

291
00:18:19,510 --> 00:18:23,406
Forget just training, even just running inference on them is quite expensive. So basically you

292
00:18:23,428 --> 00:18:27,214
have this remote cluster of

293
00:18:27,252 --> 00:18:30,874
compute that has been made available to researchers,

294
00:18:31,002 --> 00:18:35,700
and the insights package was basically made as

295
00:18:36,550 --> 00:18:40,206
a point, as an interface

296
00:18:40,238 --> 00:18:43,746
to this compute cluster. So the typical workflow, as is seen here

297
00:18:43,768 --> 00:18:48,822
in this schematic, is that you have the

298
00:18:48,956 --> 00:18:53,090
researcher working locally, basically writing interventions

299
00:18:53,170 --> 00:18:56,786
for how they want to run their experiments and intervene with networks,

300
00:18:56,818 --> 00:19:00,394
which we are going to see. And this is basically change into

301
00:19:00,432 --> 00:19:03,866
a compute graph, or more specifically an intervention graph, as like,

302
00:19:03,888 --> 00:19:07,498
this is how I want the running of this very large model

303
00:19:07,584 --> 00:19:11,070
to be tweaked.

304
00:19:11,570 --> 00:19:15,418
And this is then sent over the network into this cluster

305
00:19:15,514 --> 00:19:19,840
to say, okay, cool, please run this 70 billion model,

306
00:19:21,090 --> 00:19:24,186
70 billion parameter model that I definitely cannot run on

307
00:19:24,228 --> 00:19:27,714
my M one MacBook, but run it with

308
00:19:27,752 --> 00:19:31,314
these different interventions that make it look like that, make it no different from if

309
00:19:31,352 --> 00:19:35,398
I could actually run this locally. And as you

310
00:19:35,404 --> 00:19:38,642
can see, the thing between this boundary of the local environment

311
00:19:38,706 --> 00:19:43,154
to the NDIF infrastructure

312
00:19:43,202 --> 00:19:46,834
is simply this compute graph, and this compute graph is the output

313
00:19:46,882 --> 00:19:50,438
of the Nnsite library, and we'll

314
00:19:50,454 --> 00:19:53,862
see how it does. Cool. So that is the motivational

315
00:19:53,926 --> 00:19:57,098
setup for why an insight exists. It's basically a

316
00:19:57,104 --> 00:20:00,522
counterpart to the NDIF project, which is super interesting,

317
00:20:00,576 --> 00:20:05,722
by the way. Again, I think they just released their paper last November announcing

318
00:20:05,786 --> 00:20:09,646
the launch of the NDIF facility. It is live right now, I believe so,

319
00:20:09,668 --> 00:20:13,326
yeah. Really exciting project. I encourage anyone that's looking for

320
00:20:13,428 --> 00:20:16,560
computer resources, for inference in particular.

321
00:20:17,090 --> 00:20:19,906
And again, this has nothing to do with training. It's just like if you want

322
00:20:19,928 --> 00:20:23,282
to run a big model several times and do different

323
00:20:23,336 --> 00:20:27,014
interventions or read stuff from it to learn more, as we do

324
00:20:27,052 --> 00:20:31,654
with our hypothesis in question, then it

325
00:20:31,692 --> 00:20:35,426
works great. But of course, the library also offers

326
00:20:35,458 --> 00:20:39,100
the option of just running the if you happen to have

327
00:20:39,790 --> 00:20:42,730
several gigabytes of ram to spare.

328
00:20:43,550 --> 00:20:46,954
Okay, so let's jump into the code. What does

329
00:20:46,992 --> 00:20:51,070
it look like to do an intervention? By intervention, we just simply means

330
00:20:51,220 --> 00:20:56,346
anything that either writes or reads execution

331
00:20:56,458 --> 00:21:00,334
state of our model. That is,

332
00:21:00,372 --> 00:21:04,382
again, you have a model we put in a token sequence, and then

333
00:21:04,516 --> 00:21:07,986
stage after stage, the output of one stage is passed to

334
00:21:08,008 --> 00:21:10,834
the next, and that is added to the residual stream, which is just, again,

335
00:21:10,872 --> 00:21:15,038
think of it as like this ever accumulating output

336
00:21:15,134 --> 00:21:18,274
of each component in the model that eventually leads

337
00:21:18,322 --> 00:21:22,086
to a probability distribution or output that we

338
00:21:22,108 --> 00:21:26,614
observe. So if we ever want to poke into it either like

339
00:21:26,812 --> 00:21:30,486
use our binoculars or microscope to

340
00:21:30,508 --> 00:21:33,962
look in, that is one type of intervention, as you can see here on line

341
00:21:34,016 --> 00:21:37,338
five. Again, you can ignore the stuff above, I will explain that

342
00:21:37,344 --> 00:21:40,934
later. But just to dive straight into what exactly the interventions

343
00:21:40,982 --> 00:21:44,720
are, again, what are the things that make up these arrows of this

344
00:21:45,090 --> 00:21:49,150
intervention graph that is being sent over, which is the whole point of this package?

345
00:21:49,490 --> 00:21:53,126
Line five, we have something that is reading.

346
00:21:53,178 --> 00:21:56,866
So you see model layers, input is

347
00:21:56,888 --> 00:22:00,402
equal to something and we save it again, I'll explain why we're saving

348
00:22:00,456 --> 00:22:04,130
that. This giant model is not running on

349
00:22:04,200 --> 00:22:07,926
my colab notebook or my

350
00:22:07,948 --> 00:22:11,606
local environment, right? So it is interesting

351
00:22:11,708 --> 00:22:14,600
that I can indeed read what is happening on it.

352
00:22:14,970 --> 00:22:18,434
And on line six we have the opposite,

353
00:22:18,482 --> 00:22:21,942
which is me changing something in some other component

354
00:22:22,006 --> 00:22:25,530
of my model. So model the layers. So on layer eleven,

355
00:22:26,030 --> 00:22:29,866
on a component called the MLP, I want to change its output to

356
00:22:29,888 --> 00:22:33,082
become zero. And again, just to remind us what

357
00:22:33,136 --> 00:22:36,830
all this is for, how all this relates to our hypothesis,

358
00:22:37,570 --> 00:22:41,146
first we want to get the average of a bunch of runs for the task

359
00:22:41,178 --> 00:22:44,862
in question, which is the opposite. And then we want to append that

360
00:22:44,916 --> 00:22:48,686
average out value to some other examples

361
00:22:48,718 --> 00:22:52,126
that are in a different context called like the one shot or the zero shot

362
00:22:52,158 --> 00:22:55,790
examples. That is, we've not giving the model any idea what we're trying to achieve.

363
00:22:55,870 --> 00:22:59,426
We just wanted to feel the urge to do the thing

364
00:22:59,448 --> 00:23:02,374
we want it to do because we have added the vector. So literally the first

365
00:23:02,412 --> 00:23:06,226
thing of mean is literally just a read output that we want to run several

366
00:23:06,258 --> 00:23:10,262
times, read the average value of this vector, or read the value of this vector

367
00:23:10,326 --> 00:23:14,006
and then average out. And then six shows us changing

368
00:23:14,038 --> 00:23:17,354
the value of some component. Then we want to

369
00:23:17,392 --> 00:23:21,182
add this average value to the running of the

370
00:23:21,236 --> 00:23:24,606
different context and see what happens. Okay, cool. So this basically is

371
00:23:24,708 --> 00:23:29,214
the scaffolding for what we need for

372
00:23:29,252 --> 00:23:32,602
our project. But before we go into the code

373
00:23:32,676 --> 00:23:36,562
for our experiment and research

374
00:23:36,616 --> 00:23:40,050
in question, just to decode a bit,

375
00:23:40,200 --> 00:23:43,458
what exactly is happening here? So one thing is that

376
00:23:43,544 --> 00:23:47,070
the insights library loves Python contexts, which is one

377
00:23:47,080 --> 00:23:50,418
of the reasons why I guess Python

378
00:23:50,434 --> 00:23:54,118
might be a language of choice. But context managers are great in

379
00:23:54,124 --> 00:23:57,526
python, as we know, and they do take great advantage of

380
00:23:57,548 --> 00:24:01,834
them. And the general structure of it is

381
00:24:01,872 --> 00:24:05,562
that as we know, basically the code

382
00:24:05,616 --> 00:24:08,970
might look like the model is running locally. When I do things like save,

383
00:24:09,040 --> 00:24:12,654
I do edits and I do reads. But the whole point is that

384
00:24:12,772 --> 00:24:16,494
none of this, the model actually isn't running right

385
00:24:16,532 --> 00:24:20,234
now. But when the context closes, that's like when the code execution

386
00:24:20,282 --> 00:24:23,874
reaches the point where we exit the uppermost context, which is here

387
00:24:23,912 --> 00:24:27,426
is line three. Runner. All the edits we've made

388
00:24:27,528 --> 00:24:31,300
are, or in the course of running, of being

389
00:24:31,750 --> 00:24:35,330
intervention graph is updated with all the I o. That is,

390
00:24:35,480 --> 00:24:39,110
all the reading. And the writing we're doing to the model is basically

391
00:24:39,180 --> 00:24:42,754
just planned while in the context. And when the context

392
00:24:42,802 --> 00:24:46,662
is exited, this is then sent over,

393
00:24:46,716 --> 00:24:50,714
right? So the model does not run until the context of the highest level,

394
00:24:50,912 --> 00:24:55,050
which in case is the runner, which is a runner. Context is closed,

395
00:24:56,190 --> 00:24:59,562
and for context the

396
00:24:59,616 --> 00:25:03,902
invoker. Again, I would encourage anyone to read the documentation, but invoker is

397
00:25:04,036 --> 00:25:07,822
basically what

398
00:25:07,876 --> 00:25:10,942
does the writing for the graph right between

399
00:25:10,996 --> 00:25:14,418
the invoker and the runner. They are both coordinating. I think

400
00:25:14,424 --> 00:25:17,954
the runner definitely do some high level management, but one

401
00:25:17,992 --> 00:25:21,746
of the initialization inputs to

402
00:25:21,768 --> 00:25:24,914
the invoker implicitly is something called a tracer. And again

403
00:25:24,952 --> 00:25:28,378
you can think of the tracer as just being a new graph

404
00:25:28,414 --> 00:25:32,194
in question. As we're going to see. You can actually have construct multiple

405
00:25:32,242 --> 00:25:35,554
graphs inside of one runner,

406
00:25:35,602 --> 00:25:38,694
which we are going to see shortly. That is you can say like okay,

407
00:25:38,732 --> 00:25:42,026
I want to plan different experiments. And again, this fits perfectly for our

408
00:25:42,048 --> 00:25:45,820
use case, since first we want to run one set of operations that

409
00:25:46,350 --> 00:25:50,234
runs our task inputs and takes

410
00:25:50,272 --> 00:25:53,614
the average, and another set of operations that

411
00:25:53,652 --> 00:25:57,518
take that average and adds it to the state of

412
00:25:57,604 --> 00:26:01,246
the different context. Examples, again, that should have no

413
00:26:01,268 --> 00:26:05,086
idea about the task, and then see what happens when this average

414
00:26:05,118 --> 00:26:06,420
vector is added to it.

415
00:26:08,310 --> 00:26:12,002
So the runner is the high level context manager, and then each

416
00:26:12,136 --> 00:26:15,526
basic subgraph experiment that we want

417
00:26:15,548 --> 00:26:18,774
to run is contained in the

418
00:26:18,812 --> 00:26:22,054
invoker context. Interventions, every read

419
00:26:22,092 --> 00:26:26,438
and write intervention, all the iOS are what are the nodes in

420
00:26:26,604 --> 00:26:30,934
there of type tracing node, which again are

421
00:26:30,972 --> 00:26:34,778
what inform what our entire graph is made of to start with.

422
00:26:34,944 --> 00:26:37,642
And I said I was going to speak on why we need save. So again,

423
00:26:37,696 --> 00:26:41,942
remember that because this isn't running locally,

424
00:26:42,006 --> 00:26:45,582
we have to explicitly tell the model to save any

425
00:26:45,636 --> 00:26:49,550
value that we want to read outside of the context, because the standard

426
00:26:49,620 --> 00:26:53,226
behavior is when the context is exited, the model actually runs

427
00:26:53,258 --> 00:26:57,086
with all our interventions, but because these values are so

428
00:26:57,108 --> 00:27:00,578
large, we actually have to explicitly tell okay, please, I would like you to return

429
00:27:00,744 --> 00:27:04,558
several hundreds of thousands of vector values

430
00:27:04,574 --> 00:27:07,506
to me, because that is important. So that is the only reason why we can

431
00:27:07,528 --> 00:27:10,934
access hidden state outside context, otherwise we wouldn't need to.

432
00:27:11,052 --> 00:27:14,646
So perhaps this was just a temporary variable that

433
00:27:14,668 --> 00:27:18,280
we needed to use for our computation, which is fine

434
00:27:18,810 --> 00:27:22,646
if we have no intention to access it after the

435
00:27:22,668 --> 00:27:25,994
contact closes, we wouldn't put save. So this is only because we want to hold

436
00:27:26,032 --> 00:27:28,986
on to the value. So this is just one of the examples where we have

437
00:27:29,008 --> 00:27:32,890
to remind ourselves of the difference between running the model locally

438
00:27:32,970 --> 00:27:36,794
and just simply using, building an intervention

439
00:27:36,842 --> 00:27:40,480
graph for a remote resource that is going to run

440
00:27:41,090 --> 00:27:44,562
immediately we leave the context. And again,

441
00:27:44,616 --> 00:27:48,690
this is from documentation, basically showing how

442
00:27:48,760 --> 00:27:52,174
each. So here you can see that each line of intervention.

443
00:27:52,222 --> 00:27:55,566
So the first green arrow on the left blue

444
00:27:55,598 --> 00:27:59,362
box is a right. That is, we're setting some layers

445
00:27:59,426 --> 00:28:03,334
output to zero and the next is a read and

446
00:28:03,372 --> 00:28:06,646
the third is also a read. But you

447
00:28:06,668 --> 00:28:11,174
see here, we use the dot save because we do want this

448
00:28:11,212 --> 00:28:14,666
value to be sent over the network when the model isn't running. And you see

449
00:28:14,688 --> 00:28:18,410
the output of this is this intervention graph in the middle.

450
00:28:18,480 --> 00:28:22,326
And this is basically what is what is sent over the network in one direction.

451
00:28:22,438 --> 00:28:26,126
And then the result for things that we ask

452
00:28:26,148 --> 00:28:29,534
the model that we ask the graph to save

453
00:28:29,652 --> 00:28:32,880
are then sent back in the other direction when the execution is done.

454
00:28:33,730 --> 00:28:36,946
Again, just to remind ourselves on what we're trying to do now, we have an

455
00:28:36,968 --> 00:28:40,754
idea of what our library looks like and

456
00:28:40,792 --> 00:28:44,562
how we use it is that we want to pass

457
00:28:44,616 --> 00:28:47,400
in some context, we want to run it.

458
00:28:47,770 --> 00:28:51,638
Remember, we're only interested in what happens by the column, so we

459
00:28:51,644 --> 00:28:55,686
will be indexing to get only the

460
00:28:55,708 --> 00:28:58,886
vector at the very extreme end. Because the

461
00:28:58,908 --> 00:29:02,618
idea is that is the token that will

462
00:29:02,704 --> 00:29:05,802
contain information on what is to come next. Right? Again,

463
00:29:05,856 --> 00:29:09,834
just as a result of how transform architectures work is, the next

464
00:29:09,872 --> 00:29:13,358
prediction is containing the last token. To what end do we want to

465
00:29:13,364 --> 00:29:16,426
do that to this end? So we could do two sets of runs.

466
00:29:16,458 --> 00:29:20,058
The first run is to pass a bunch

467
00:29:20,074 --> 00:29:23,774
of examples doing the task we want. Again,

468
00:29:23,892 --> 00:29:27,794
this is exactly like how you would tell Chat GPT something

469
00:29:27,832 --> 00:29:31,154
like, I want you to give me words. And opposite, like this

470
00:29:31,192 --> 00:29:35,618
example, old, young, separated by Colin. Then it

471
00:29:35,624 --> 00:29:39,186
does this thing, right? So this is basically just like prompting

472
00:29:39,218 --> 00:29:43,062
it with the, this is similar

473
00:29:43,116 --> 00:29:46,294
to prompting with the format of code you want. But in this case we're actually

474
00:29:46,332 --> 00:29:50,370
just going to look at the very last token

475
00:29:50,450 --> 00:29:53,686
and then right before, when it's right on the verge

476
00:29:53,718 --> 00:29:56,774
of predicting, we just take a first, don't know the computation

477
00:29:56,902 --> 00:30:00,266
to know that, okay, this is a word and opposite game we're playing, and I

478
00:30:00,288 --> 00:30:03,706
am to predict the opposite of the last thing I saw before this token.

479
00:30:03,818 --> 00:30:07,246
So, right when it's supposedly figured all that out, we want

480
00:30:07,268 --> 00:30:10,446
to just snatch that vector and average a bunch of them

481
00:30:10,468 --> 00:30:13,794
out to get, hopefully a vector that represents in

482
00:30:13,832 --> 00:30:17,506
some pure form the very essence of the task that it

483
00:30:17,528 --> 00:30:21,250
has figured out, which is opposite function of previous

484
00:30:22,950 --> 00:30:26,094
experiment. That's the first part of the experiment. Then the second part of the experiment

485
00:30:26,142 --> 00:30:29,826
is to take this pure vector and then add it to a different context,

486
00:30:30,018 --> 00:30:33,542
a different series of examples that supposedly should have no idea

487
00:30:33,596 --> 00:30:37,830
what is going on, right? Because again, if you just told chat GBT encode column,

488
00:30:38,270 --> 00:30:41,914
it has no idea what you want is it can't read your mind

489
00:30:42,112 --> 00:30:45,354
yet. So this is

490
00:30:45,392 --> 00:30:49,402
called the zero shot intervention, which basically just means zero

491
00:30:49,456 --> 00:30:52,862
shot means zero examples of what you're looking

492
00:30:52,916 --> 00:30:57,006
for. Except now we're going to add this

493
00:30:57,108 --> 00:31:01,214
hopefully averaged out function has

494
00:31:01,252 --> 00:31:04,654
it just again feel compelled to do the thing that that

495
00:31:04,692 --> 00:31:08,018
vector was obtained from. Right, so how do

496
00:31:08,024 --> 00:31:11,058
we do the first part? That is the part where we just run a

497
00:31:11,064 --> 00:31:14,498
bunch of stuff and we extract the value

498
00:31:14,584 --> 00:31:17,960
at the very last column for all of them and average out.

499
00:31:18,890 --> 00:31:23,234
Cool. So again we have our trusted layout.

500
00:31:23,282 --> 00:31:27,206
Of course, first of all we have to determine what component we

501
00:31:27,228 --> 00:31:32,490
want to look at. Remember Mac, interp is all about having

502
00:31:32,640 --> 00:31:36,154
an observed and interesting observed behavior and trying to

503
00:31:36,272 --> 00:31:40,250
find the contribution of some discrete component, right? So in this case we

504
00:31:40,320 --> 00:31:44,000
narrow down by saying, okay, we want to look at layer eight again.

505
00:31:44,370 --> 00:31:48,238
In the actual experiment we run this for all the different layers, for all the

506
00:31:48,244 --> 00:31:52,478
different components of the model, and then we have like a plot of

507
00:31:52,644 --> 00:31:56,226
which of them happen to be most interesting. And then we drill down.

508
00:31:56,408 --> 00:32:00,046
But this is just showing an example of suppose we wanted to see what layer

509
00:32:00,078 --> 00:32:03,970
eight was doing as far as the task is concerned.

510
00:32:04,390 --> 00:32:07,814
Cool. So just imagine this done for a bunch of tens of

511
00:32:07,852 --> 00:32:11,538
other components. Cool. So we have a runner in Booger.

512
00:32:11,634 --> 00:32:15,080
Then here on line six we simply just

513
00:32:15,530 --> 00:32:18,966
do our trusted, as we would

514
00:32:18,988 --> 00:32:22,954
like to notice here that I don't do save because this

515
00:32:23,072 --> 00:32:26,682
value of hidden states, this variable is only needed

516
00:32:26,736 --> 00:32:30,186
for computation inside of my

517
00:32:30,208 --> 00:32:33,946
context, right? So I do not need to export

518
00:32:33,978 --> 00:32:38,766
this at this very point in time. I just simply need to take

519
00:32:38,788 --> 00:32:42,026
the variable, hold on to it, use it for other computation on len

520
00:32:42,058 --> 00:32:45,314
ten. And as you can see, line six

521
00:32:45,352 --> 00:32:49,314
is simply just the rightmost column. That is, sorry,

522
00:32:49,352 --> 00:32:53,346
beg your pardon, that is the

523
00:32:53,528 --> 00:32:57,026
on. Right. So between line six and seven we

524
00:32:57,048 --> 00:33:00,678
simply just take an example, we choose a layer and

525
00:33:00,764 --> 00:33:04,486
on line eight we say the sequence position should be, I think on

526
00:33:04,508 --> 00:33:07,478
line one, you see, we define that as minus one. So we just simply want

527
00:33:07,484 --> 00:33:10,886
to take the very last value, which is token. So again, all the dark gray

528
00:33:10,918 --> 00:33:14,326
bars is what line eight is holding

529
00:33:14,358 --> 00:33:18,230
onto. Then on line ten we simply just do the average.

530
00:33:18,310 --> 00:33:21,814
So we take that variable and we do the dot mean on the batch

531
00:33:21,862 --> 00:33:26,186
dimension, that's the 0th dimension. Again, that's the dimension of all the stacked examples

532
00:33:26,298 --> 00:33:28,766
on the right there. I think I just put like a clip out to show

533
00:33:28,788 --> 00:33:32,606
you what the vectors and

534
00:33:32,788 --> 00:33:35,854
matrixes will look like. So each of those stacks is just a batch dimension.

535
00:33:35,902 --> 00:33:38,882
So each of the examples of old, young,

536
00:33:38,936 --> 00:33:42,130
awake, asleep is represented by one of these slices.

537
00:33:42,550 --> 00:33:46,574
And we simply just want to average across that to get some hopefully

538
00:33:46,622 --> 00:33:50,610
pure vector that encodes the essence of opposites

539
00:33:51,370 --> 00:33:54,498
and that we want to save because that we want to send back. So it's

540
00:33:54,514 --> 00:33:57,398
kind of meant to be an efficient thing such that we don't want to send

541
00:33:57,564 --> 00:34:00,806
everything over the network, we don't want to send both the full,

542
00:34:00,908 --> 00:34:04,586
all the matrices. Thankfully we could decide to save everything and

543
00:34:04,608 --> 00:34:07,894
then compute locally. So again, this is just some of the considerations

544
00:34:08,022 --> 00:34:11,680
that you make when you remind yourself that actually there is

545
00:34:12,770 --> 00:34:15,502
some throughput cost and efficiency cost.

546
00:34:15,556 --> 00:34:19,230
So let's just do as much as we can on this

547
00:34:19,300 --> 00:34:22,550
environment and then just send down the most condensed

548
00:34:22,570 --> 00:34:26,434
version we want. Again, this should

549
00:34:26,472 --> 00:34:29,906
be similar to running using any remote resource or

550
00:34:29,928 --> 00:34:33,294
when you have trade offs between remote and local resources to contend

551
00:34:33,342 --> 00:34:36,626
with. Cool. So that is how we

552
00:34:36,648 --> 00:34:39,286
do the first part, that's how we get the averages. Literally this is all to

553
00:34:39,308 --> 00:34:42,598
do averages for one layer. And just imagine putting this in a for loop if

554
00:34:42,604 --> 00:34:45,846
you want to iterate over several layers. And for the

555
00:34:45,868 --> 00:34:51,222
second part, having possessed this average

556
00:34:51,286 --> 00:34:54,874
pure vector, which we called h, we want

557
00:34:54,912 --> 00:34:59,162
to then put h into our zero

558
00:34:59,216 --> 00:35:02,970
shot examples. Again, these are the examples that have no context on the task.

559
00:35:03,050 --> 00:35:06,506
They're just doing their own thing. They supposedly

560
00:35:06,538 --> 00:35:10,590
oblivious to the task we find interesting of opposites,

561
00:35:11,090 --> 00:35:14,478
but from nowhere they would just feel the urge to now just do opposites.

562
00:35:14,574 --> 00:35:18,542
Hopefully if we add this average vector

563
00:35:18,606 --> 00:35:21,922
state to them. And here is the example

564
00:35:21,976 --> 00:35:26,274
I mentioned where we're running two invoker

565
00:35:26,322 --> 00:35:30,246
contexts inside of the runner context. So basically the

566
00:35:30,268 --> 00:35:34,006
first is, again, we're trying to, as with

567
00:35:34,028 --> 00:35:37,270
any experiment, we have to have our control example

568
00:35:37,340 --> 00:35:39,962
or our reference or our baseline to say that,

569
00:35:40,016 --> 00:35:43,674
cool. Without adding this average vector, what does

570
00:35:43,712 --> 00:35:46,890
the model feel compelled to predict? So for simple,

571
00:35:46,960 --> 00:35:51,094
does it feel compelled to predict simple?

572
00:35:51,232 --> 00:35:55,214
Simpler? Maybe it just says cool. Simpler should be something to follow simple

573
00:35:55,332 --> 00:36:00,254
or given encode, does it feel compelled to predict base

574
00:36:00,292 --> 00:36:03,806
64? Or perhaps it does feel naturally compelled to decode,

575
00:36:03,838 --> 00:36:07,266
who knows? So the first run on line four and

576
00:36:07,288 --> 00:36:10,978
five is just again simply running the model and

577
00:36:11,144 --> 00:36:14,530
saving the output for the very last token.

578
00:36:15,510 --> 00:36:19,206
And the second is where we do the interesting stuff of running the

579
00:36:19,228 --> 00:36:22,994
model and basically intervening. So on line eleven we literally

580
00:36:23,042 --> 00:36:26,566
just plus equals two, which is identical to how

581
00:36:26,588 --> 00:36:29,606
we were taking the mean before. But again for this context we just do a

582
00:36:29,628 --> 00:36:33,450
plus equals two, add this value to the existing.

583
00:36:34,030 --> 00:36:37,466
And on line 13 we again just

584
00:36:37,488 --> 00:36:40,846
like line five save to see. Okay, cool. Now let's see what

585
00:36:40,868 --> 00:36:44,494
the predictions are and how similar they are, or to

586
00:36:44,532 --> 00:36:47,946
what extent this vector has changed

587
00:36:47,978 --> 00:36:51,120
the opinions of this model

588
00:36:51,730 --> 00:36:54,714
results. I mean,

589
00:36:54,772 --> 00:36:58,846
depending on your standards for impressive

590
00:36:58,878 --> 00:37:02,354
or not, this is what it looks like. This is what run one looks like.

591
00:37:02,392 --> 00:37:06,022
Just by doing that, we can see that. Indeed, on the third

592
00:37:06,076 --> 00:37:09,750
column here, adding that h vector does

593
00:37:09,820 --> 00:37:14,534
move the needle a bit does

594
00:37:14,572 --> 00:37:18,246
have the effect of the opposite function, right? So on

595
00:37:18,268 --> 00:37:21,686
the second column, we just see that the thing the model tries to do,

596
00:37:21,708 --> 00:37:25,782
if you tell the model, if you tell the model minimum column, it just repeats

597
00:37:25,846 --> 00:37:29,146
a lot of stuff, right? So it just says minimum is minimum, arrogant is

598
00:37:29,168 --> 00:37:34,622
arrogant, inside is inside. Although sometimes it does interesting things, like the

599
00:37:34,756 --> 00:37:38,174
fifth example from the bottom. If you say on, it says I.

600
00:37:38,292 --> 00:37:41,534
If you say answer, it says yes. Again,

601
00:37:41,572 --> 00:37:45,410
this is what the model feels compelled to just say if it has no context.

602
00:37:45,910 --> 00:37:50,110
But on the third column we see that in some examples

603
00:37:50,270 --> 00:37:54,254
we do manage to tilt its final judgment

604
00:37:54,302 --> 00:37:58,374
in a different direction. Now, I will mention though, that this

605
00:37:58,412 --> 00:38:01,394
is technically not where the paper stops.

606
00:38:01,522 --> 00:38:05,286
The paper decides to say beyond just averaging h.

607
00:38:05,468 --> 00:38:09,080
Remember, this is taking the value of

608
00:38:09,930 --> 00:38:13,740
the output of the entire layer. Remember, we just see layer eight.

609
00:38:14,110 --> 00:38:17,306
The paper takes it further by saying, okay, instead of just looking at layer eight,

610
00:38:17,408 --> 00:38:20,746
can we drill specifically into what component in layer

611
00:38:20,778 --> 00:38:24,590
eight is contributing? So, back to our reference architecture,

612
00:38:25,010 --> 00:38:28,314
neural network transformer

613
00:38:28,362 --> 00:38:31,994
component transformer block has different things. Our attention

614
00:38:32,042 --> 00:38:35,690
block, rather, excuse me, has the mass self attention,

615
00:38:35,770 --> 00:38:39,018
the feed forward, it has different things, layer norm,

616
00:38:39,124 --> 00:38:43,026
and they decide to drill into the contributions of

617
00:38:43,048 --> 00:38:47,266
the attention head. Again, the distinction isn't that important, but the experimental

618
00:38:47,298 --> 00:38:51,346
method is precisely the same. So they just find a way to drill into studying

619
00:38:51,378 --> 00:38:55,960
the contribution of the top x

620
00:38:56,650 --> 00:39:00,570
attention heads. And instead of averaging looking for the average of

621
00:39:00,640 --> 00:39:04,374
just all the components contributions, which again supposedly

622
00:39:04,422 --> 00:39:07,846
will have more noise, they basically tried to denoise by just narrowing

623
00:39:07,878 --> 00:39:11,770
in on a few, and with that the effect is way more obvious.

624
00:39:11,840 --> 00:39:14,960
But I don't include that for the purpose of this talk.

625
00:39:15,410 --> 00:39:18,686
And that was the talk. So if you are interested in

626
00:39:18,708 --> 00:39:22,266
looking at the NDIF project and Nn Insight Library

627
00:39:22,298 --> 00:39:26,078
as well, which is a companion, please view that site.

628
00:39:26,244 --> 00:39:28,990
And if you're interested in learning more on Mechinterp,

629
00:39:29,570 --> 00:39:33,118
many of the code snippets and

630
00:39:33,284 --> 00:39:36,774
basically concepts introduce you. They were introduced to myself

631
00:39:36,892 --> 00:39:41,142
on the platform called arena education. It is an awesome program.

632
00:39:41,276 --> 00:39:45,126
You should check it out. If you're interested in learning more on doing

633
00:39:45,228 --> 00:39:48,374
mechanistic interpretability. I hope you've had as much fun

634
00:39:48,412 --> 00:39:51,494
going through this as I have. And do enjoy the rest of

635
00:39:51,532 --> 00:39:52,130
the conference.

