1
00:00:20,560 --> 00:00:24,134
Hello everybody, it's Marko here. This is my first time as a speaker and I'm

2
00:00:24,174 --> 00:00:27,410
glad to be here at Conf 42 to bring some

3
00:00:27,482 --> 00:00:31,194
goodness in go. So let me introduce

4
00:00:31,234 --> 00:00:35,214
myself. So I am a software engineer at AIon.

5
00:00:36,354 --> 00:00:40,258
I'm a master degree student in AI at University of Pisa,

6
00:00:40,386 --> 00:00:43,602
and I'm into everything that is related to

7
00:00:43,658 --> 00:00:47,682
cloud, native and kubernetes. I also participated at

8
00:00:47,778 --> 00:00:51,250
closer 2023 Cloud conference and

9
00:00:51,442 --> 00:00:54,834
fortunately I won the best paper award and the

10
00:00:54,874 --> 00:00:58,250
work I proposed in that conference was something related

11
00:00:58,322 --> 00:01:02,266
to how to solve smells in a Kubernetes

12
00:01:02,330 --> 00:01:06,490
microservices environment. So let's

13
00:01:06,522 --> 00:01:10,506
go ahead and I'm going to introduce you some topics

14
00:01:10,690 --> 00:01:14,962
that you might already know, that more experts might already know. But still,

15
00:01:15,098 --> 00:01:19,494
I think it's still interesting to catch.

16
00:01:22,444 --> 00:01:27,660
As a first topic, I just want to go deep down through

17
00:01:27,692 --> 00:01:31,340
the go runtime scheduler and the memory model and

18
00:01:31,372 --> 00:01:35,164
how they are impacting the performance of Go applications and

19
00:01:35,244 --> 00:01:39,148
then how to measure your applications and

20
00:01:39,196 --> 00:01:43,124
find bottlenecks, if there are any, and then some best

21
00:01:43,164 --> 00:01:46,980
practices. But first things

22
00:01:47,052 --> 00:01:50,824
first. So ghost is a fast language, but why?

23
00:01:51,284 --> 00:01:54,996
What makes go stand out in the gorgeous

24
00:01:55,060 --> 00:01:59,372
realm of programming languages when it comes to performance? And to

25
00:01:59,468 --> 00:02:03,236
ask to that question, we need to look under the hood and

26
00:02:03,260 --> 00:02:07,100
examine two fundamental aspects of go, that is the memory model

27
00:02:07,212 --> 00:02:09,544
and the runtime shutter.

28
00:02:10,924 --> 00:02:14,962
So, but let's switch from go a

29
00:02:14,978 --> 00:02:17,818
little bit to Java. Okay,

30
00:02:17,986 --> 00:02:21,554
so some of you already know that Java

31
00:02:21,594 --> 00:02:24,134
uses native thread in us, okay,

32
00:02:24,634 --> 00:02:28,074
operating systems, right? So that means that every Java

33
00:02:28,114 --> 00:02:31,054
thread is mapped to one kernel thread.

34
00:02:31,634 --> 00:02:35,210
In this way, Java cannot determine which thread would occupy

35
00:02:35,242 --> 00:02:38,874
the core. And this is completely up to

36
00:02:38,914 --> 00:02:42,890
the OS shadware. So it's completely dependent on how

37
00:02:42,922 --> 00:02:46,400
many threads you have, right? So a problem

38
00:02:46,472 --> 00:02:50,724
could be, for example, if I am executing

39
00:02:51,864 --> 00:02:55,512
Java thread inside a certain OS

40
00:02:55,648 --> 00:02:59,656
kernel thread, I save the state and then the

41
00:02:59,680 --> 00:03:03,112
java thread is scheduled into another OS thread.

42
00:03:03,288 --> 00:03:06,632
Then I would suffer from context switching. And we

43
00:03:06,648 --> 00:03:10,648
will see an example. Let's do a

44
00:03:10,696 --> 00:03:14,358
Java example here. I have a function that does

45
00:03:14,406 --> 00:03:19,390
something as we, as we see for x

46
00:03:19,422 --> 00:03:22,750
times, and we have a

47
00:03:22,822 --> 00:03:26,154
number of threads that are executing this, this function,

48
00:03:26,814 --> 00:03:28,274
this number of times.

49
00:03:30,294 --> 00:03:34,846
Now if I run that code with

50
00:03:34,990 --> 00:03:37,834
100 thread rounded threads, okay,

51
00:03:38,184 --> 00:03:41,484
and with 1000 threads,

52
00:03:42,704 --> 00:03:46,552
we have different results, right? So you

53
00:03:46,568 --> 00:03:48,924
can see that in the picture.

54
00:03:49,824 --> 00:03:53,964
Basically when the number of threads is set to 100,

55
00:03:54,704 --> 00:03:58,192
about 51% of the cpu time is spent in

56
00:03:58,208 --> 00:04:02,008
the function. Do something. So our real function. But then

57
00:04:02,176 --> 00:04:04,764
when you increase the number of the tries to,

58
00:04:06,094 --> 00:04:09,630
to 1000, the cpu time spent

59
00:04:09,662 --> 00:04:13,606
for the actual function went down about

60
00:04:13,750 --> 00:04:18,062
27%. And all of these

61
00:04:18,118 --> 00:04:21,634
metrics basically are telling us that the cost of the thread,

62
00:04:22,334 --> 00:04:25,990
the Java trading model, suffer in nine concursive

63
00:04:26,022 --> 00:04:30,054
scenario. So in

64
00:04:30,094 --> 00:04:33,120
standard operating systems, the threads are scheduled after

65
00:04:33,152 --> 00:04:37,064
a certain amount of time. And when a

66
00:04:37,104 --> 00:04:40,432
timer, hardware timer, interrupts the processor,

67
00:04:40,568 --> 00:04:44,296
the OS kernel suspends the current executing thread,

68
00:04:44,400 --> 00:04:48,088
save its state in registries. So when it has

69
00:04:48,136 --> 00:04:51,056
to resume it, it doesn't lose anything,

70
00:04:51,240 --> 00:04:55,240
and then it finds among all the threads available for being executed

71
00:04:55,272 --> 00:04:58,716
the next one to one. And as I said,

72
00:04:58,860 --> 00:05:02,780
this is called the context switching process. And this

73
00:05:02,812 --> 00:05:06,944
process is kind of slow even, because as I said

74
00:05:07,484 --> 00:05:10,636
a couple of slides before, there could be like a

75
00:05:10,660 --> 00:05:14,100
catch miss. Okay, so this

76
00:05:14,132 --> 00:05:18,516
is the main reason why the

77
00:05:18,540 --> 00:05:21,540
co founders created the go runtime shadow.

78
00:05:21,612 --> 00:05:24,868
Okay, so go doesn't totally rely on the OS

79
00:05:24,916 --> 00:05:28,934
shader, but it has its own runtime shadower.

80
00:05:29,234 --> 00:05:32,962
And it uses a threading model called mn

81
00:05:33,098 --> 00:05:36,930
trading model, where basically m coroutines are

82
00:05:36,962 --> 00:05:41,014
being shadowed among N OS threads.

83
00:05:42,154 --> 00:05:45,194
So as we can see here, there are kind of,

84
00:05:45,234 --> 00:05:48,522
let's say interfaces between the coroutines

85
00:05:48,618 --> 00:05:51,882
and the actual kernel threads. So the kernel threads

86
00:05:51,898 --> 00:05:55,386
in this picture are the white triangles and the

87
00:05:55,410 --> 00:05:59,050
go teams are the one. The goutine can be

88
00:05:59,242 --> 00:06:02,698
green in this case, that means that is actually running

89
00:06:02,826 --> 00:06:07,250
if it's red is in the queue, but that

90
00:06:07,362 --> 00:06:10,570
yellow boxes act as an interface.

91
00:06:10,642 --> 00:06:13,874
And those yellow boxes are the context.

92
00:06:13,994 --> 00:06:17,810
Okay, they are fundamental. So once our

93
00:06:17,842 --> 00:06:21,692
context has run a goutine until a scheduling point,

94
00:06:21,748 --> 00:06:25,020
it pops a gootine of the queue,

95
00:06:25,212 --> 00:06:29,060
sets the stack and destruction pointer, and begins running

96
00:06:29,092 --> 00:06:33,556
the goroutine. And what

97
00:06:33,580 --> 00:06:37,052
can happen is that a yellow box, in this case a context,

98
00:06:37,188 --> 00:06:40,628
is running out of goroutines and automatically,

99
00:06:40,676 --> 00:06:45,064
without basically calling any kind of interrupts

100
00:06:45,484 --> 00:06:48,322
it automatically still work,

101
00:06:48,418 --> 00:06:52,334
still go teams from other contexts.

102
00:06:52,714 --> 00:06:56,482
And this makes sure that there is always work to

103
00:06:56,498 --> 00:06:59,850
do on each of the contexts, which in turn makes sure

104
00:06:59,882 --> 00:07:03,614
that all the threads are working at their maximum capacity.

105
00:07:04,474 --> 00:07:08,254
So taking advantage of this runtime shadow

106
00:07:09,474 --> 00:07:12,714
go was built upon a

107
00:07:12,754 --> 00:07:16,778
CSP model. CSP stands for communicating sequential

108
00:07:16,826 --> 00:07:20,594
processes, where basically we have goutines and these

109
00:07:20,634 --> 00:07:25,054
go teams can communicate each other via, via channel

110
00:07:25,634 --> 00:07:28,454
that can be buffered and buffered.

111
00:07:29,634 --> 00:07:33,466
But the cool thing is that we

112
00:07:33,490 --> 00:07:37,374
as a developer, we don't have to care about basically

113
00:07:38,194 --> 00:07:41,692
accessing shared data, so we

114
00:07:41,708 --> 00:07:45,844
don't have to set matrixes or semaphores

115
00:07:45,964 --> 00:07:49,884
or locks or queues and other

116
00:07:49,924 --> 00:07:52,784
things like that, sync variables and other things like that.

117
00:07:53,244 --> 00:07:57,020
So this is really, really cool. And go, this is something that

118
00:07:57,132 --> 00:08:00,684
I really appreciate because I love writing pipelines, for example.

119
00:08:00,804 --> 00:08:04,700
So for the pipelines, this kind of model is really,

120
00:08:04,852 --> 00:08:08,396
it's really cool even, because the way

121
00:08:08,540 --> 00:08:12,348
the two go teams, as they are not threads, are communicating, is really,

122
00:08:12,396 --> 00:08:15,636
really, really fast. Let's go back

123
00:08:15,780 --> 00:08:19,812
with Java, and this is a comparison. So we have the,

124
00:08:19,948 --> 00:08:23,804
this, this code here we are doing a slip of ten minutes

125
00:08:23,964 --> 00:08:27,904
and we, we're using 1000

126
00:08:29,004 --> 00:08:33,084
os net Java threads, right? And at the end of the execution

127
00:08:33,124 --> 00:08:36,836
we can see that there are actually 1000

128
00:08:36,900 --> 00:08:40,544
threads plus 18, and that 18

129
00:08:41,484 --> 00:08:45,424
is the number of threads occupied for handling the JVM.

130
00:08:46,564 --> 00:08:50,324
And the real power that we encounter

131
00:08:50,364 --> 00:08:53,740
when we execute in code is especially on the number of threads used,

132
00:08:53,812 --> 00:08:57,380
because the same code actually

133
00:08:57,492 --> 00:09:01,294
needs only two threads. And this is, this is really cool.

134
00:09:01,954 --> 00:09:05,146
We said that a couple of steps

135
00:09:05,170 --> 00:09:08,330
before that the Go teams saved their state, right?

136
00:09:08,482 --> 00:09:10,734
So Goat saved the Go team state.

137
00:09:11,594 --> 00:09:15,706
But let's do a comparison on how all

138
00:09:15,770 --> 00:09:19,494
threads are saving the state and

139
00:09:21,594 --> 00:09:25,138
how the Go team saved their state. So the host

140
00:09:25,186 --> 00:09:28,014
threads have a fixed style stack for saving the state.

141
00:09:28,924 --> 00:09:32,884
But this is kind of a problem because it

142
00:09:32,924 --> 00:09:36,212
is kind of a huge state waste of space. If we

143
00:09:36,228 --> 00:09:40,092
imagine that there is a single goutine, but at the same time

144
00:09:40,228 --> 00:09:43,812
can be too strict for hundreds of thousands of goutines that are

145
00:09:43,828 --> 00:09:47,228
being created, which is not a real event in a go based

146
00:09:47,276 --> 00:09:52,508
application. Therefore the

147
00:09:52,596 --> 00:09:56,344
go memory, memory model work in another way,

148
00:09:56,644 --> 00:09:59,740
maybe because we want to have a good amount

149
00:09:59,772 --> 00:10:03,500
of coroutines in contrast to the approach that we

150
00:10:03,532 --> 00:10:07,180
saw before, go creates very small stacks for each coroutine

151
00:10:07,332 --> 00:10:11,348
around two kb. And the surprising thing is that each of

152
00:10:11,396 --> 00:10:14,748
these stacks is growing and shrinking as needed.

153
00:10:14,876 --> 00:10:18,732
And this is possible because of the capability of these stacks to

154
00:10:18,748 --> 00:10:22,562
borrow memory from the heap. And the fact that these stacks

155
00:10:22,578 --> 00:10:26,162
are dynamic allows us to

156
00:10:26,218 --> 00:10:29,738
have better memory management. And as

157
00:10:29,786 --> 00:10:33,890
the memory hover is reduced when context switching happens,

158
00:10:34,082 --> 00:10:37,814
and that's because of the small states we have to

159
00:10:38,274 --> 00:10:42,138
save for each goutine. So this

160
00:10:42,186 --> 00:10:46,098
was like a sort of introductory topic

161
00:10:46,266 --> 00:10:49,986
for the new bio, for the

162
00:10:50,010 --> 00:10:53,614
new gophers. But once we,

163
00:10:55,074 --> 00:10:59,370
let's assume that you now know very well this concept,

164
00:10:59,562 --> 00:11:03,066
and you started take all

165
00:11:03,090 --> 00:11:07,014
the goodness off go for your applications.

166
00:11:07,714 --> 00:11:11,402
So now how

167
00:11:11,458 --> 00:11:16,000
can measure the performance of your go application applications

168
00:11:16,072 --> 00:11:17,564
in a systematic way?

169
00:11:18,704 --> 00:11:22,496
Before starting explaining how

170
00:11:22,520 --> 00:11:25,960
to benchmark the applications, I want to do

171
00:11:25,992 --> 00:11:29,960
some preconditions. So the preconditions that every time

172
00:11:29,992 --> 00:11:33,744
that we run our benchmarks, when you run

173
00:11:33,784 --> 00:11:37,204
benchmark you want to always keep the same environment.

174
00:11:37,584 --> 00:11:41,244
We don't want to get affected by the external environment.

175
00:11:41,814 --> 00:11:45,190
So another thing that is crucial

176
00:11:45,222 --> 00:11:48,574
to do is to isolate the code that is being benchmarked

177
00:11:48,614 --> 00:11:52,246
from the rest of the program. So how to write

178
00:11:52,270 --> 00:11:56,766
a benchmark? So this is a practical slide.

179
00:11:56,910 --> 00:12:00,198
So we create an underscore

180
00:12:00,246 --> 00:12:04,094
test go file where we put all the benchmark functions

181
00:12:04,174 --> 00:12:07,954
and the benchmark function as a specific

182
00:12:08,674 --> 00:12:13,850
signature. And we have to specify the

183
00:12:13,882 --> 00:12:17,774
b variable that has type testing b.

184
00:12:18,114 --> 00:12:21,978
And as we're gonna see right now, for example

185
00:12:22,106 --> 00:12:24,614
in this two, sorry,

186
00:12:26,954 --> 00:12:30,290
as we can see later on, that this b dot n represent

187
00:12:30,402 --> 00:12:33,866
the number of iterations that dynamically, the go,

188
00:12:33,930 --> 00:12:36,314
the go runtime.

189
00:12:36,934 --> 00:12:40,854
Decide for your function

190
00:12:40,894 --> 00:12:44,446
to run. Okay. So you don't have to

191
00:12:44,470 --> 00:12:48,834
touch that variable at all. Okay. So it's totally up to the

192
00:12:49,374 --> 00:12:53,566
go runtime to decide that how many times to iterate

193
00:12:53,670 --> 00:12:57,126
your benchmark. So let's

194
00:12:57,150 --> 00:13:00,434
take two functions. Here we have two pipelines.

195
00:13:00,754 --> 00:13:03,294
So the pipelines are,

196
00:13:04,274 --> 00:13:07,906
these two pipelines have the same structure. They are a producer that

197
00:13:07,930 --> 00:13:13,202
is producing some strings. Then there is like some

198
00:13:13,258 --> 00:13:16,658
stages where basically we take all the screens,

199
00:13:16,786 --> 00:13:20,974
the strings, we lower them, then we merge the results

200
00:13:22,714 --> 00:13:26,494
from the old coroutines and then we send them

201
00:13:27,114 --> 00:13:31,970
concurrently into the other stage, which basically takes

202
00:13:32,002 --> 00:13:35,994
the, take the string and

203
00:13:36,034 --> 00:13:39,362
capitalized, capitalize the first character and

204
00:13:39,378 --> 00:13:43,298
then merge all the result. Right, so they have the same structure

205
00:13:43,346 --> 00:13:46,530
here, but they are different under the hood. Right.

206
00:13:46,722 --> 00:13:50,374
So we want to test benchmark these functions

207
00:13:51,154 --> 00:13:56,404
and we now can do

208
00:13:56,444 --> 00:14:00,284
a smart thing since we, we want to use a tool

209
00:14:00,324 --> 00:14:04,164
called bench start. So bench start, what it does is to

210
00:14:04,324 --> 00:14:07,184
compare to benchmark results,

211
00:14:07,524 --> 00:14:10,504
okay? But in order to compare,

212
00:14:11,124 --> 00:14:14,900
to compare these benchmarks, the benchmark

213
00:14:14,932 --> 00:14:18,844
function must be the same. Okay, so a

214
00:14:18,884 --> 00:14:22,874
cool strategy could be to first create

215
00:14:23,534 --> 00:14:27,294
your benchmark, then call the function, for example,

216
00:14:27,334 --> 00:14:30,630
rampipeline one, you produce the benchmark as

217
00:14:30,662 --> 00:14:35,630
we do in the first, sorry, in the second image.

218
00:14:35,702 --> 00:14:38,830
So return the output as a

219
00:14:38,862 --> 00:14:42,174
before bench. And then we can go

220
00:14:42,214 --> 00:14:45,406
back to the same benchmark function and

221
00:14:45,470 --> 00:14:49,548
change the inner function. So instead of rank pipeline one, we're on pipeline

222
00:14:49,596 --> 00:14:52,984
two. Okay, so the result is in the first

223
00:14:53,484 --> 00:14:56,464
is in the number one code. Okay,

224
00:14:57,524 --> 00:14:59,504
so now we have two benchmarks.

225
00:15:01,164 --> 00:15:04,548
But before doing the bench start and comparing

226
00:15:04,596 --> 00:15:07,980
them, we can open one of them. For example,

227
00:15:08,052 --> 00:15:11,740
we can open the whatever

228
00:15:11,812 --> 00:15:13,968
the before dot match. Okay,

229
00:15:14,156 --> 00:15:17,048
so how to read a benchmark?

230
00:15:17,096 --> 00:15:20,896
So we have four values.

231
00:15:21,080 --> 00:15:25,064
And let me switch back, because an important thing

232
00:15:25,144 --> 00:15:28,936
is that we have to see those flags here. So the minus

233
00:15:29,000 --> 00:15:32,256
run equals to x means that basically what

234
00:15:32,280 --> 00:15:36,216
the benchmarking engine does is.

235
00:15:36,400 --> 00:15:40,168
Okay, I see the run pipeline function in the benchmark function. So I

236
00:15:40,176 --> 00:15:42,694
need to run all the tests related to that function,

237
00:15:43,114 --> 00:15:46,242
and I'm lazy, I didn't want to

238
00:15:46,258 --> 00:15:49,538
do that. So with the minus run equals to x,

239
00:15:49,586 --> 00:15:53,818
you avoid that. And also another important flag is the minus benchmam

240
00:15:53,906 --> 00:15:57,778
that allows you to keep track of the memory used for

241
00:15:57,906 --> 00:16:01,938
your functions. Now we can go ahead. So with

242
00:16:01,986 --> 00:16:05,642
that command we get this result.

243
00:16:05,738 --> 00:16:09,586
Okay, so we can see in the red circle that there

244
00:16:09,610 --> 00:16:13,554
is the number of iterations. So the b n that we measured

245
00:16:13,594 --> 00:16:18,210
before. And the

246
00:16:18,242 --> 00:16:22,354
blue circle is one of the most important because it

247
00:16:22,394 --> 00:16:26,082
explains to you on average how fast your progression

248
00:16:26,178 --> 00:16:29,850
is in terms of nanosecond per operation, the number

249
00:16:29,882 --> 00:16:33,026
of bytes per operation, and the number of allocs per

250
00:16:33,050 --> 00:16:36,490
operation. And the last two results came from

251
00:16:36,522 --> 00:16:40,308
the fact that we use the minus benchmam

252
00:16:40,386 --> 00:16:44,808
flag. So let's go ahead and use

253
00:16:44,856 --> 00:16:48,296
bench sort, which is a really great tool to compare to

254
00:16:48,480 --> 00:16:51,952
benchmarks. And as we can see,

255
00:16:52,088 --> 00:16:55,904
okay, so the before bench was running

256
00:16:55,944 --> 00:16:59,544
the run pipeline one, and the after bench was running

257
00:16:59,664 --> 00:17:03,376
run pipeline two. And we can see three rows.

258
00:17:03,400 --> 00:17:07,266
Okay, the first row is the speed, the second

259
00:17:07,330 --> 00:17:11,490
row is how much memory used your

260
00:17:11,522 --> 00:17:15,474
function, and the third row is how many allocations

261
00:17:15,594 --> 00:17:18,914
your function did during the benchmark.

262
00:17:19,074 --> 00:17:23,294
And we can see that the run pipeline too is faster by 40%.

263
00:17:24,514 --> 00:17:27,214
It wastes less memory,

264
00:17:28,474 --> 00:17:32,534
a lot like 40%, and it allocates

265
00:17:32,954 --> 00:17:36,736
less than the ram pipeline one

266
00:17:36,800 --> 00:17:39,644
does. So almost 86%.

267
00:17:40,304 --> 00:17:45,104
So this is kind of a systematic way to

268
00:17:45,224 --> 00:17:49,128
measure your applications. But now you

269
00:17:49,176 --> 00:17:53,056
maybe are wondering why the RAM pipeline one function is

270
00:17:53,160 --> 00:17:55,444
slow, since the structure is the same.

271
00:17:57,944 --> 00:18:00,524
So here comes profiling.

272
00:18:01,014 --> 00:18:04,414
Profiling is the process

273
00:18:04,574 --> 00:18:08,854
of keeping track of all the inner functions that your

274
00:18:08,894 --> 00:18:12,606
main function runs. And it allows you

275
00:18:12,630 --> 00:18:16,874
to track the cpu and the memory of all instructions.

276
00:18:17,974 --> 00:18:21,878
For doing that, we use a pprof. Pprof is a

277
00:18:21,926 --> 00:18:24,834
tool for visualize the profiling data.

278
00:18:25,414 --> 00:18:29,234
And it's available for free, of course, as a,

279
00:18:29,704 --> 00:18:33,024
as a go tool, and it's

280
00:18:33,064 --> 00:18:34,724
based on protocol buffers.

281
00:18:36,584 --> 00:18:39,880
So in order to exploit this

282
00:18:40,032 --> 00:18:44,004
pprof, now we want to add

283
00:18:44,384 --> 00:18:48,840
two more flags, which is the cpu profile

284
00:18:49,032 --> 00:18:53,824
and the memory profile. And we load the result of

285
00:18:53,864 --> 00:18:56,932
the profilings, one for the cpu

286
00:18:56,988 --> 00:19:01,148
and one for the memory, into two different prof

287
00:19:01,236 --> 00:19:05,004
files that are basically protobuf type files.

288
00:19:05,044 --> 00:19:08,596
Okay, what if we run the pprof? As we

289
00:19:08,620 --> 00:19:12,916
can see above the line above, there is go tool pprof cpu

290
00:19:13,060 --> 00:19:16,700
one. Prof. Now if we go

291
00:19:16,732 --> 00:19:20,348
back this like before, there is no

292
00:19:20,396 --> 00:19:24,574
one at the end of the minus bench flag. That means that

293
00:19:25,074 --> 00:19:30,194
all the benchmark functions that prefix

294
00:19:30,354 --> 00:19:34,410
starts. Okay? And that case I put basically

295
00:19:34,602 --> 00:19:38,674
the benchmark pipeline one and separated the benchmark

296
00:19:38,714 --> 00:19:41,954
pipeline two so that we can compare using the profiling,

297
00:19:42,034 --> 00:19:43,214
the two functions.

298
00:19:44,554 --> 00:19:49,054
Okay, so using this command,

299
00:19:50,694 --> 00:19:55,174
I'm inside the pprof and I can run the top 100 command.

300
00:19:55,254 --> 00:19:58,390
So the top 100 command gives

301
00:19:58,422 --> 00:20:02,634
me the most, the most 100 cpu expensive tasks.

302
00:20:03,014 --> 00:20:07,590
And I can see that, for example, there is a sleep and

303
00:20:07,662 --> 00:20:10,754
we have the strings dot lower function.

304
00:20:11,574 --> 00:20:15,310
Another important thing is that in the top list there

305
00:20:15,342 --> 00:20:18,604
are basically only stages

306
00:20:18,764 --> 00:20:22,732
that comes from the run pipeline one function

307
00:20:22,868 --> 00:20:26,084
and that, and we're not seeing any kind of function

308
00:20:26,124 --> 00:20:29,572
from run pipeline two. And that's because of course

309
00:20:29,668 --> 00:20:33,064
we saw before with the benchmark, that run pipeline one

310
00:20:33,404 --> 00:20:36,388
runs actually slower. Okay.

311
00:20:36,516 --> 00:20:40,556
Since this list is in

312
00:20:40,580 --> 00:20:44,208
descending order of how

313
00:20:44,256 --> 00:20:46,204
much time has been spent.

314
00:20:47,224 --> 00:20:51,192
So we are a little bit suspicious

315
00:20:51,248 --> 00:20:55,328
here. So, and instead

316
00:20:55,456 --> 00:20:59,124
if we scroll down, we can see our fast stages.

317
00:20:59,544 --> 00:21:03,800
So we are a bit suspicious here. So what we can do is

318
00:21:03,832 --> 00:21:07,848
to dive into the code of ramp API

319
00:21:07,896 --> 00:21:11,220
one. So we run again

320
00:21:11,412 --> 00:21:15,304
the, the pprof

321
00:21:16,324 --> 00:21:20,196
and we can use the list function,

322
00:21:20,260 --> 00:21:23,292
the list, sorry, the list method of pprof,

323
00:21:23,468 --> 00:21:28,308
where you can basically say which

324
00:21:28,356 --> 00:21:31,984
function you want to analyze. In this case, I want to analyze the first stage

325
00:21:32,884 --> 00:21:36,540
of the run pipeline one, which is transform to lower

326
00:21:36,612 --> 00:21:40,876
one. And if I go inside apart from the

327
00:21:40,900 --> 00:21:44,852
code above. Okay, we have to focus on the red box

328
00:21:45,028 --> 00:21:48,956
and seems that we are losing time here because we

329
00:21:48,980 --> 00:21:53,060
see that we are doing

330
00:21:53,092 --> 00:21:56,580
a loop where we are lowering

331
00:21:56,652 --> 00:21:59,424
chart by chart and then return the result.

332
00:22:00,784 --> 00:22:04,624
But this is kind of a problem because we know already that

333
00:22:04,664 --> 00:22:10,200
there is a function that is

334
00:22:10,232 --> 00:22:13,964
coming from the strings module that is

335
00:22:14,744 --> 00:22:17,640
lowering our strings in a faster way.

336
00:22:17,792 --> 00:22:21,416
Okay, so let's

337
00:22:21,440 --> 00:22:25,360
analyze the faster one. So as we can see, we have the

338
00:22:25,392 --> 00:22:28,672
strings lower function that is coming for the module and

339
00:22:28,688 --> 00:22:31,964
it takes 26 seconds. Okay,

340
00:22:34,064 --> 00:22:38,208
so overall we saved almost 5 seconds,

341
00:22:38,296 --> 00:22:41,592
you know, because here we have to

342
00:22:41,608 --> 00:22:44,720
take into account this, this amount of time.

343
00:22:44,752 --> 00:22:48,400
So 70 seconds. But we always, we also have to take

344
00:22:48,432 --> 00:22:51,672
into account the time that has been spent to send the data to

345
00:22:51,688 --> 00:22:55,158
your channel. That is almost 13 seconds.

346
00:22:55,326 --> 00:22:58,694
Okay, so here's why there's the difference.

347
00:22:58,734 --> 00:23:01,446
So it's 13 plus 17,

348
00:23:01,590 --> 00:23:05,174
okay. Rather than 26

349
00:23:05,214 --> 00:23:08,878
overall. Okay. We found out why our

350
00:23:08,926 --> 00:23:12,766
function is lower, okay. But we

351
00:23:12,870 --> 00:23:16,314
now analyze the cpu profile.

352
00:23:16,614 --> 00:23:20,006
Okay. But we also produced a memory profile. And as

353
00:23:20,030 --> 00:23:24,110
you can see, for example, you is can that in the slower

354
00:23:24,182 --> 00:23:29,502
function we have a waste of memory and we

355
00:23:29,518 --> 00:23:33,194
have enormous quantity of allocations.

356
00:23:33,574 --> 00:23:37,314
So of course

357
00:23:38,054 --> 00:23:41,774
this is what I wanted to show you. So the

358
00:23:41,854 --> 00:23:45,838
systematic way to find

359
00:23:45,886 --> 00:23:49,156
out how, where your program is low,

360
00:23:49,260 --> 00:23:52,584
where your program is fast. Okay, so,

361
00:23:54,604 --> 00:23:58,716
but before concluding, a suggestion

362
00:23:58,780 --> 00:24:02,156
is that I want to like that to be

363
00:24:02,180 --> 00:24:05,380
completely accurate. Any benchmark should be

364
00:24:05,412 --> 00:24:09,044
careful to avoid any kind of optimization optimizations

365
00:24:09,084 --> 00:24:12,348
that the compiler does. For example,

366
00:24:12,436 --> 00:24:16,232
if we, if we don't save

367
00:24:16,288 --> 00:24:20,048
the result of the ram pipeline, sometimes it happens that the

368
00:24:20,136 --> 00:24:24,004
compiler eliminates the function under the test and

369
00:24:25,624 --> 00:24:29,320
somehow it's artificially lower the runtime of the

370
00:24:29,352 --> 00:24:31,004
benchmark and we don't want that.

371
00:24:37,224 --> 00:24:41,084
When utilizing the profiler, it's important to consider that

372
00:24:42,494 --> 00:24:45,862
it samples both cpu usage and memory at

373
00:24:45,918 --> 00:24:50,198
specified frequency. Okay. However, this sampling

374
00:24:50,286 --> 00:24:53,514
may not always be 100% representative,

375
00:24:53,814 --> 00:24:56,686
particularly if the sample time is very low.

376
00:24:56,830 --> 00:25:00,078
So to announce accuracy,

377
00:25:00,246 --> 00:25:03,654
I recommend increasing the bench time

378
00:25:03,694 --> 00:25:07,310
parameter accordingly.

379
00:25:07,342 --> 00:25:11,050
Okay, so in this way

380
00:25:11,202 --> 00:25:14,410
you allow the profiler to collect more samples over an

381
00:25:14,442 --> 00:25:17,674
extended period and then we can obtain more

382
00:25:17,714 --> 00:25:20,854
precise insights into how our application performance.

383
00:25:21,594 --> 00:25:25,170
So of course try to design your

384
00:25:25,202 --> 00:25:28,534
applications as a pipeline of coroutines.

385
00:25:30,194 --> 00:25:33,734
Always keep track of the memory usage as it can use,

386
00:25:34,834 --> 00:25:38,826
actually can cause garbage collection to run and

387
00:25:38,850 --> 00:25:41,946
therefore that means that we potentially wasting time.

388
00:25:42,130 --> 00:25:45,986
And of course, as I said, as in the precondition,

389
00:25:46,050 --> 00:25:49,914
try to execute benchmarks on a stable machine without

390
00:25:50,034 --> 00:25:53,334
having spikes during the test.

391
00:25:53,714 --> 00:25:57,922
So here I left some study references

392
00:25:58,018 --> 00:26:01,026
that I really found that interesting.

393
00:26:01,090 --> 00:26:04,734
So how gouting works, how the memory

394
00:26:04,774 --> 00:26:08,022
model works and the shadowing works, and how you

395
00:26:08,038 --> 00:26:13,246
can use benchmark and profiling to improve

396
00:26:13,430 --> 00:26:14,914
your functions performance.

397
00:26:16,174 --> 00:26:20,286
So that's it. Thank you for

398
00:26:20,310 --> 00:26:23,874
your attention. Hopefully this was

399
00:26:25,414 --> 00:26:28,490
an important way,

400
00:26:28,682 --> 00:26:32,186
an important moment for you to learn something more

401
00:26:32,290 --> 00:26:36,106
about go. So I'm really

402
00:26:36,290 --> 00:26:40,026
excited to hear you in the comments and so that I can

403
00:26:40,050 --> 00:26:43,554
improve my speaking skills as this is

404
00:26:43,594 --> 00:26:47,522
my first time speaking in public. So really speaking

405
00:26:47,578 --> 00:26:50,698
with the bottom of my heart, really, thanks.

406
00:26:50,786 --> 00:26:54,914
And I hope that this is the first of

407
00:26:55,214 --> 00:27:00,894
an infinite number of speakings.

408
00:27:00,934 --> 00:27:04,358
Okay, so thank you guys

409
00:27:04,526 --> 00:27:05,534
and have a great day.

