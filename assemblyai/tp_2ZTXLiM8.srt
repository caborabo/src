1
00:00:27,294 --> 00:00:31,126
Hi everyone, I am Pravar Agrawal and I welcome you all to

2
00:00:31,150 --> 00:00:34,958
my session on debugging cluster level issues as on call

3
00:00:35,006 --> 00:00:38,394
SRE. Let us quickly take a look at

4
00:00:38,694 --> 00:00:42,114
today's agenda. So we start off

5
00:00:42,414 --> 00:00:45,990
by getting to know about reliability

6
00:00:46,062 --> 00:00:50,102
engineering in much briefer way and

7
00:00:50,278 --> 00:00:53,698
also about the role of an on call engineer, what it is really

8
00:00:53,746 --> 00:00:57,370
about. We will also try to find some commonly

9
00:00:57,402 --> 00:01:00,894
occurring cluster level issues inside Kubernetes

10
00:01:01,394 --> 00:01:04,746
ecosystem. We will try to

11
00:01:04,770 --> 00:01:08,906
understand different approaches to debugging such

12
00:01:08,970 --> 00:01:13,254
issues also with the help of different types of automation.

13
00:01:13,874 --> 00:01:17,034
And if you are a beginner, then what should you

14
00:01:17,114 --> 00:01:20,246
be really looking for in the role of an on

15
00:01:20,270 --> 00:01:23,470
call engineer? A little bit about me

16
00:01:23,582 --> 00:01:27,022
I work as a senior engineer at IBM for the

17
00:01:27,038 --> 00:01:30,670
IBM Cloud Kubernetes service and these are

18
00:01:30,702 --> 00:01:33,830
different handles to stay

19
00:01:33,862 --> 00:01:37,062
in touch with me on Twitter,

20
00:01:37,118 --> 00:01:40,234
LinkedIn and Slack. Okay,

21
00:01:40,654 --> 00:01:44,654
so with an introduction to site availability engineering,

22
00:01:44,774 --> 00:01:48,590
I assume that most of us so

23
00:01:48,622 --> 00:01:52,086
far has been hearing this word a

24
00:01:52,110 --> 00:01:56,354
lot of times and have a lot of awareness around it as well.

25
00:01:57,534 --> 00:02:00,814
But to still understand it is more of an

26
00:02:00,854 --> 00:02:04,910
approach or a methodology to like

27
00:02:04,942 --> 00:02:08,726
for the it operations to use different tools in solving different set

28
00:02:08,750 --> 00:02:12,406
of problems and even automate different operation

29
00:02:12,470 --> 00:02:15,514
level tasks. It's a practice

30
00:02:15,674 --> 00:02:18,946
to ensure the our

31
00:02:19,010 --> 00:02:23,194
systems are reliable and scalable and

32
00:02:23,354 --> 00:02:27,962
the SRE folks also try to maintain a balance between releasing

33
00:02:28,058 --> 00:02:32,098
different features or new features and at

34
00:02:32,106 --> 00:02:35,194
the meantime also ensuring reliability for

35
00:02:35,234 --> 00:02:39,298
their customers as well. Also, there is a word

36
00:02:39,466 --> 00:02:42,542
we often hear along with SRE which is

37
00:02:42,558 --> 00:02:45,994
the on call and it basically means for

38
00:02:46,454 --> 00:02:50,438
set of period of time, you are available to respond

39
00:02:50,566 --> 00:02:53,918
to production level issues or incidents with

40
00:02:53,966 --> 00:02:58,286
urgency. And different

41
00:02:58,350 --> 00:03:01,878
companies, basically they have their own implementation of this on

42
00:03:01,926 --> 00:03:05,486
call process. But everyone's main

43
00:03:05,510 --> 00:03:08,638
aim is to ensure the availability of our

44
00:03:08,686 --> 00:03:12,554
production and the support to our production 24/7 by

45
00:03:12,594 --> 00:03:15,874
managing different incidents. And there are few rules

46
00:03:15,954 --> 00:03:20,034
around it. So they could be starting

47
00:03:20,074 --> 00:03:23,266
off with acknowledging and verifying the alert, trying to analyze

48
00:03:23,290 --> 00:03:25,334
the impact caused by that issue,

49
00:03:26,154 --> 00:03:29,914
then communicating different teams involved

50
00:03:29,954 --> 00:03:33,178
through proper channels, whichever is

51
00:03:33,346 --> 00:03:36,970
used in your organization, and also

52
00:03:37,122 --> 00:03:40,834
eventually applying a corrective action or solid fix

53
00:03:40,914 --> 00:03:44,984
to that issue. There are many tools

54
00:03:45,924 --> 00:03:50,064
which have been popular in last couple of years for incident management.

55
00:03:51,004 --> 00:03:53,172
There is pagerduty, Jira,

56
00:03:53,348 --> 00:03:57,028
Obsgeny and ServiceNow, but there are other tools,

57
00:03:57,196 --> 00:04:01,060
these are more popular. So this diagram is basically

58
00:04:01,092 --> 00:04:05,184
a representation of SRE and the on call ecosystem.

59
00:04:05,524 --> 00:04:07,944
And it involves different phases,

60
00:04:09,124 --> 00:04:10,864
starting from the deployment,

61
00:04:11,684 --> 00:04:15,644
troubleshooting, receiving the alerts, then the

62
00:04:15,724 --> 00:04:20,468
escalation metrics which is involved and tracking

63
00:04:20,596 --> 00:04:23,264
everything with the help of proper ticketing system.

64
00:04:24,204 --> 00:04:27,500
Moving on to different cluster issues which

65
00:04:27,532 --> 00:04:30,860
are very common and we often see those. So we

66
00:04:30,892 --> 00:04:35,090
are taking, or we are taking an example of an

67
00:04:35,122 --> 00:04:38,970
environment which comprise of a single or

68
00:04:39,042 --> 00:04:41,734
multiple kubernetes clusters running in production.

69
00:04:42,154 --> 00:04:46,106
So there could be issues related

70
00:04:46,170 --> 00:04:50,906
to the services running on to node and it's not always possible

71
00:04:50,970 --> 00:04:54,294
to do a manually SSH and try to debug those.

72
00:04:54,954 --> 00:04:58,914
We will take a look at how we can basically automate

73
00:04:58,954 --> 00:05:03,062
this process and how we can utilize

74
00:05:03,118 --> 00:05:06,286
or debug these sort of issues in the next slide.

75
00:05:06,470 --> 00:05:10,030
Then there could be issues related to multiple pods getting stuck

76
00:05:10,062 --> 00:05:13,950
in pending or terminating state, which is a very common early

77
00:05:14,022 --> 00:05:17,086
occurring issue. So this

78
00:05:17,270 --> 00:05:20,846
mainly happens when there are the like for

79
00:05:20,870 --> 00:05:24,246
the pending state, when the schedule ability

80
00:05:24,310 --> 00:05:27,992
is, is very low for uh,

81
00:05:28,088 --> 00:05:31,480
the pod going on to different nodes during its life

82
00:05:31,512 --> 00:05:35,160
cycle and terminating mainly when the application is

83
00:05:35,192 --> 00:05:38,080
lost, uh is not responding.

84
00:05:38,112 --> 00:05:42,136
Basically when you issue a delete command of

85
00:05:42,160 --> 00:05:46,084
the application resource inside the cluster, then there are also

86
00:05:46,944 --> 00:05:49,696
uh errors related to the API endpoints.

87
00:05:49,800 --> 00:05:53,760
Going down or responding is slow. So these API endpoints

88
00:05:53,792 --> 00:05:57,332
could be related to the application or

89
00:05:57,348 --> 00:06:00,548
at the application level or something

90
00:06:00,636 --> 00:06:04,604
related to Kubernetes core components as well. Then the unavailability

91
00:06:04,684 --> 00:06:08,596
of HCD pods. Also when

92
00:06:08,620 --> 00:06:12,196
you want to issue different reloads of various

93
00:06:12,260 --> 00:06:15,500
worker nodes manually doing

94
00:06:15,532 --> 00:06:18,980
it is not always an option. Then there

95
00:06:19,012 --> 00:06:22,818
could also be issues where your discs are getting full

96
00:06:22,906 --> 00:06:26,930
or maybe reaching the 90% capacity and you often get alerted for

97
00:06:26,962 --> 00:06:30,762
it and also your health checks are failing or maybe readiness

98
00:06:30,818 --> 00:06:33,978
or liveness probes are failing. So how we can

99
00:06:34,026 --> 00:06:37,454
approach towards debugging these sort of issues.

100
00:06:38,234 --> 00:06:41,666
So there is no such golden rule to do it, but there

101
00:06:41,690 --> 00:06:44,254
are different right ways to do it.

102
00:06:44,954 --> 00:06:48,352
We will start off by analyzing the error message which we

103
00:06:48,368 --> 00:06:51,616
have received and try to see if the

104
00:06:51,640 --> 00:06:56,280
low, if the blast radius can be lowered or it is like

105
00:06:56,312 --> 00:06:59,856
how many services or how many components it is impacting

106
00:06:59,880 --> 00:07:03,536
at the backend. So lower the blast radius is better it

107
00:07:03,560 --> 00:07:07,644
is for us to debug and issuing the corrective action.

108
00:07:08,184 --> 00:07:12,184
Then we also might want to utilize our monitoring

109
00:07:12,224 --> 00:07:15,750
tools like Prometheus log dnssdig properly

110
00:07:15,942 --> 00:07:19,374
to take a look at the last recorded state of the application

111
00:07:19,534 --> 00:07:23,182
with the help of different metrics which were pushed till the time it

112
00:07:23,198 --> 00:07:26,806
was running fine and also different logs which were pushed,

113
00:07:26,950 --> 00:07:30,430
maybe the application level log as well as the cluster

114
00:07:30,462 --> 00:07:33,774
level logs to understand more the current

115
00:07:33,814 --> 00:07:37,382
state and the previous state of the and if

116
00:07:37,398 --> 00:07:40,782
it is kubernetes related, then we can try to get the access to the

117
00:07:40,798 --> 00:07:44,520
cluster as soon as possible and try to find out

118
00:07:44,632 --> 00:07:48,376
different master components or other important components are

119
00:07:48,400 --> 00:07:51,976
functioning properly or not. If they're deployed as pods,

120
00:07:52,040 --> 00:07:55,824
we can try to get the logs of the pods. Otherwise, if those are running

121
00:07:55,864 --> 00:08:00,204
as system D services, we should try to check

122
00:08:01,544 --> 00:08:04,712
the service level logs and if it's

123
00:08:04,728 --> 00:08:08,336
a widely impacting issue, we should try to isolate that service by

124
00:08:08,400 --> 00:08:11,910
restricting its usage. Of course we are going to need the

125
00:08:11,942 --> 00:08:15,114
automation to our help

126
00:08:15,494 --> 00:08:19,206
and why we basically need it so that we can reduce the

127
00:08:19,230 --> 00:08:22,510
time which we need to respond and get

128
00:08:22,542 --> 00:08:26,434
our infrastructure statistics. At the earliest.

129
00:08:29,174 --> 00:08:33,006
We basically automate or we implement the automation to

130
00:08:33,030 --> 00:08:36,974
get the cluster level statistics, or maybe run some real time

131
00:08:37,014 --> 00:08:40,440
commands with maybe

132
00:08:40,512 --> 00:08:43,936
a single line which is not always possible. If you're

133
00:08:43,960 --> 00:08:48,280
trying to get the access of the cluster and then trying

134
00:08:48,312 --> 00:08:52,192
to run those commands, and also to handle node level reboots

135
00:08:52,248 --> 00:08:55,964
or restarts of different core services, and also to query

136
00:08:56,344 --> 00:09:00,084
historical data or maybe try to analyze different patterns

137
00:09:00,584 --> 00:09:04,818
which you could be seeing related to one issue or different issues right then

138
00:09:04,866 --> 00:09:09,210
also preparing the cleanup jobs to clean

139
00:09:09,242 --> 00:09:12,934
up the resources so that you always don't

140
00:09:13,434 --> 00:09:15,894
face the resource crunch whenever it is needed,

141
00:09:16,434 --> 00:09:19,774
and different types of automation which we might want to implement.

142
00:09:20,194 --> 00:09:23,810
So we start off by writing meaningful

143
00:09:23,842 --> 00:09:27,666
and well documented run books for the on call folks which can direct

144
00:09:27,730 --> 00:09:32,014
them to different points

145
00:09:32,094 --> 00:09:35,318
available to that particular issue which has been reported,

146
00:09:35,406 --> 00:09:39,550
right? So these, these points could be directions

147
00:09:39,622 --> 00:09:42,990
related to different automations which are linked to it,

148
00:09:43,022 --> 00:09:46,742
which the on call SRE can utilize. It could be a Jenkins

149
00:09:46,798 --> 00:09:50,574
job, or maybe some GitHub issue

150
00:09:50,614 --> 00:09:54,974
which was also reported in a similar way. So the

151
00:09:55,014 --> 00:09:58,490
runbooks are really necessary in this

152
00:09:58,522 --> 00:10:02,134
case, and implementation of bots

153
00:10:03,474 --> 00:10:07,214
which can also take care of the chat ops basically. So these bots

154
00:10:07,514 --> 00:10:10,738
are the main resource for running on our

155
00:10:10,786 --> 00:10:14,694
communication platforms like slack teams or Metromost,

156
00:10:15,034 --> 00:10:18,730
and these can help in gathering real time cluster statistics,

157
00:10:18,802 --> 00:10:23,142
maybe issue some commands or even the Kubectl commands,

158
00:10:23,338 --> 00:10:26,590
and also issue reboots and reloads of

159
00:10:26,622 --> 00:10:29,790
our worker nodes if authorized

160
00:10:29,822 --> 00:10:32,974
to do so. And one of the example

161
00:10:33,014 --> 00:10:36,678
of bot is bot cube, which is a very popular

162
00:10:36,726 --> 00:10:40,750
tool and has an extensive usage and connectivity to

163
00:10:40,782 --> 00:10:44,574
different platforms available, and it's an open source

164
00:10:44,614 --> 00:10:48,166
tool, then we can also deploy

165
00:10:48,230 --> 00:10:52,044
some custom scripts running as kubernetes resources inside our

166
00:10:52,084 --> 00:10:55,300
cluster in the form of daemon set

167
00:10:55,452 --> 00:10:59,220
or sidecar containers like running inside sidecar

168
00:10:59,252 --> 00:11:03,044
containers or maybe custom resources. And these

169
00:11:03,084 --> 00:11:06,852
will basically invoke whenever they face a certain situation

170
00:11:07,028 --> 00:11:10,284
so one example could be if my disk is getting full,

171
00:11:10,324 --> 00:11:14,344
then I can invoke a script which can clean up my disk and

172
00:11:15,504 --> 00:11:18,164
maybe try to shuffle some resources from it.

173
00:11:19,264 --> 00:11:22,744
Then of course we cannot escape implementing

174
00:11:22,784 --> 00:11:26,124
or taking the help of resources available

175
00:11:26,664 --> 00:11:30,344
with the AI based capabilities

176
00:11:30,504 --> 00:11:34,192
nowadays. So these could be some analyzers to gather

177
00:11:34,328 --> 00:11:37,768
more detailed information for the cluster. We should

178
00:11:37,816 --> 00:11:41,672
also be looking at integrating our existing monitoring solutions or

179
00:11:41,688 --> 00:11:45,888
the tools to extend their capabilities, like curating

180
00:11:45,936 --> 00:11:49,008
some custom dashboards which can give us a holistic as well

181
00:11:49,016 --> 00:11:52,964
as a detailed view of what is basically happening inside the cluster

182
00:11:53,504 --> 00:11:58,800
irrespective of what tool you are using. But nowadays

183
00:11:58,832 --> 00:12:02,216
the tools, they basically have a

184
00:12:02,240 --> 00:12:06,084
numerous support of different type of dashboards which we can utilize.

185
00:12:06,744 --> 00:12:09,784
So moving on to the last part,

186
00:12:10,564 --> 00:12:13,264
for someone who's starting afresh in this field,

187
00:12:14,084 --> 00:12:17,636
basically more the exposure you have, more season

188
00:12:17,700 --> 00:12:21,476
you will get in handling different situations, and it's always essential

189
00:12:21,540 --> 00:12:25,532
to get the broader understanding of the entire architecture

190
00:12:25,588 --> 00:12:29,464
and the infrastructure so that you will be able to link

191
00:12:29,844 --> 00:12:33,344
to different components whenever there's a situation.

192
00:12:35,034 --> 00:12:38,418
Also keep your runbooks handy to deal with different

193
00:12:38,466 --> 00:12:41,730
issues, errors or warnings, and also try

194
00:12:41,762 --> 00:12:45,346
to analyze the historical or recent occurrence of

195
00:12:45,410 --> 00:12:49,250
such similar issues which might have caused the outages

196
00:12:49,322 --> 00:12:53,002
in real in recent past. These could

197
00:12:53,098 --> 00:12:56,534
prove as a different like as a very good learning point.

198
00:12:57,754 --> 00:13:01,394
Lastly, I would like to say if the alert

199
00:13:01,774 --> 00:13:05,006
auto resolves, it doesn't always mean that there

200
00:13:05,030 --> 00:13:09,154
is nothing wrong, so it's better to look anyways,

201
00:13:10,454 --> 00:13:12,974
that was all from my side. Thank you.

