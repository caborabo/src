1
00:00:27,294 --> 00:00:31,014
Today I'll be talking about zero instrumentation observability

2
00:00:31,134 --> 00:00:35,022
based on EBBF. Let's start from the very beginning.

3
00:00:35,158 --> 00:00:39,830
What is observability? Miserability is

4
00:00:39,862 --> 00:00:42,714
being able to answer questions about the system.

5
00:00:43,134 --> 00:00:46,630
How is the system performing right now?

6
00:00:46,822 --> 00:00:50,734
How its current performance compares to

7
00:00:50,774 --> 00:00:54,398
the past period? Why are some

8
00:00:54,446 --> 00:00:58,438
requests failing? Or why are certain requests

9
00:00:58,566 --> 00:01:02,270
taken longer than expected? In other words,

10
00:01:02,382 --> 00:01:06,034
why system performs slower than before?

11
00:01:06,774 --> 00:01:11,406
A while ago, systems web applications were

12
00:01:11,590 --> 00:01:14,074
much more simpler than today.

13
00:01:14,414 --> 00:01:18,006
Usually it was a dedicated nod based

14
00:01:18,110 --> 00:01:21,470
applications, making few replicas

15
00:01:21,502 --> 00:01:25,056
of applications. Dedicated database,

16
00:01:25,160 --> 00:01:28,284
pretty simple. If something goes wrong,

17
00:01:28,704 --> 00:01:32,536
you just will need to analyze logs of the

18
00:01:32,560 --> 00:01:35,896
application, maybe metrics on

19
00:01:35,920 --> 00:01:39,016
the application work, database metrics

20
00:01:39,080 --> 00:01:42,304
and logs. That's it. Pretty simple.

21
00:01:42,464 --> 00:01:45,824
Now systems are getting more and more

22
00:01:45,864 --> 00:01:49,524
complex microservice architectures. A lot of

23
00:01:49,624 --> 00:01:53,476
databases, which in a typical application

24
00:01:53,540 --> 00:01:57,012
this day can contain hundreds or even thousands

25
00:01:57,028 --> 00:02:00,276
of services which are run on

26
00:02:00,300 --> 00:02:03,964
a kubernetes cluster. Nodes are

27
00:02:04,084 --> 00:02:06,916
dynamic, which can appear and disappear,

28
00:02:07,100 --> 00:02:10,820
autoscaler spot mods or something

29
00:02:10,892 --> 00:02:13,988
like that. While troubleshooting such a

30
00:02:14,036 --> 00:02:18,142
system, we need to follow the system topology

31
00:02:18,238 --> 00:02:21,286
from service, from one service to another,

32
00:02:21,470 --> 00:02:25,470
to database to infrastructure level, database level

33
00:02:25,582 --> 00:02:29,406
network. A lot of stuff should be analyzed

34
00:02:29,550 --> 00:02:33,118
to identify the cause. Let's discuss

35
00:02:33,246 --> 00:02:36,934
the steps. How to integrate make a

36
00:02:36,974 --> 00:02:40,926
system observable what does it mean? The first step is

37
00:02:41,030 --> 00:02:44,702
collecting telemetric data such as metrics,

38
00:02:44,758 --> 00:02:48,358
logs, traces and force telemetry signal.

39
00:02:48,406 --> 00:02:51,594
These days, continuous profiling.

40
00:02:52,014 --> 00:02:56,406
If we do that manually, we should instrument

41
00:02:56,510 --> 00:03:00,422
every application with some observability framework such as open

42
00:03:00,478 --> 00:03:03,662
to magnetree. This approach is good,

43
00:03:03,718 --> 00:03:07,274
but it requires time of your team.

44
00:03:07,574 --> 00:03:10,862
It can be time consuming because of

45
00:03:11,038 --> 00:03:14,590
we have a lot of services and the

46
00:03:14,622 --> 00:03:17,774
most like one. One of the additional

47
00:03:17,814 --> 00:03:21,710
disadvantage that you cannot achieve 100% coverage

48
00:03:21,782 --> 00:03:25,342
of such a system because usually we have a

49
00:03:25,358 --> 00:03:28,670
lot of third party services or legacy services,

50
00:03:28,822 --> 00:03:32,798
we cannot modify their code. That means we cannot integrate

51
00:03:32,846 --> 00:03:35,622
open telemetry into such applications.

52
00:03:35,798 --> 00:03:40,074
Then we need to store somewhere this date.

53
00:03:40,414 --> 00:03:44,774
Usually it can be now open source databases

54
00:03:44,894 --> 00:03:47,990
like click House or Grafana storages,

55
00:03:48,062 --> 00:03:52,526
which elementary data, or it can be some commercial

56
00:03:52,590 --> 00:03:55,750
services such as the baby bug, Neuralik and so on.

57
00:03:55,782 --> 00:03:58,830
A lot of from my perspective,

58
00:03:58,942 --> 00:04:03,162
the most challenging part of this is learn

59
00:04:03,218 --> 00:04:06,418
how to extract insights from all this

60
00:04:06,466 --> 00:04:09,794
data. Because we have 100gb or

61
00:04:09,834 --> 00:04:13,514
terabytes of logs, we have thousands or

62
00:04:13,554 --> 00:04:17,882
millions of metrics, and we have a

63
00:04:17,898 --> 00:04:21,266
lot of traces, how to know item and how to turn

64
00:04:21,330 --> 00:04:24,938
them into insights. What's happening with my systems

65
00:04:25,066 --> 00:04:28,970
right now and how to understand the root cause

66
00:04:29,002 --> 00:04:32,074
of an issue using all this data.

67
00:04:32,454 --> 00:04:36,782
Our system is pretty complex and we

68
00:04:36,838 --> 00:04:40,486
need to gather a lot of parameters

69
00:04:40,550 --> 00:04:44,154
or metrics or telemetry of

70
00:04:44,574 --> 00:04:48,534
each subsystem to be able to troubleshoot

71
00:04:48,574 --> 00:04:52,366
our application. And let's talk about what

72
00:04:52,430 --> 00:04:56,494
exactly we want to know about each application

73
00:04:56,574 --> 00:04:59,234
in our system. First of all,

74
00:04:59,394 --> 00:05:02,906
we need to understand how the particular

75
00:05:03,010 --> 00:05:07,426
application performs right now it's called slis

76
00:05:07,610 --> 00:05:11,594
service level indicators. Usually it means we

77
00:05:11,714 --> 00:05:15,738
want to know how many request system is processing

78
00:05:15,786 --> 00:05:19,682
right now, success rate or errors rate

79
00:05:19,858 --> 00:05:23,594
and letters of each request. Or it can

80
00:05:23,634 --> 00:05:27,672
be a histogram or something like that. Any aggregation

81
00:05:27,728 --> 00:05:31,512
of latency will work. The second one, if we have

82
00:05:31,568 --> 00:05:35,712
a distributed system, we should understand how each component

83
00:05:35,848 --> 00:05:38,936
communicates with other services or databases.

84
00:05:39,120 --> 00:05:42,760
It means we need to know each outbound request

85
00:05:42,832 --> 00:05:46,112
to an external or internal service. We know

86
00:05:46,168 --> 00:05:49,296
how many requests is performing right now,

87
00:05:49,440 --> 00:05:53,314
success rate and weapons. We need to understand

88
00:05:53,434 --> 00:05:56,666
everything about resource consumption because

89
00:05:56,850 --> 00:06:00,010
any service can degrade

90
00:06:00,082 --> 00:06:04,274
or perform slower than usual. If there will be

91
00:06:04,394 --> 00:06:08,074
a lack of cpu time, for example, or the application

92
00:06:08,154 --> 00:06:11,614
instance can be restarted because of out of memory,

93
00:06:11,914 --> 00:06:15,254
or if you run database,

94
00:06:15,594 --> 00:06:18,844
it's really sensitive to disk performance.

95
00:06:19,944 --> 00:06:24,168
Next, our components usually communicate using network,

96
00:06:24,336 --> 00:06:27,840
so we should understand how network is

97
00:06:27,872 --> 00:06:31,912
performing right now. So it can be some

98
00:06:32,008 --> 00:06:35,696
sort of network issues like connectivity loss

99
00:06:35,760 --> 00:06:38,904
between some nodes or availability zones or

100
00:06:38,944 --> 00:06:42,464
regions. If you're our system, in many regions that

101
00:06:42,504 --> 00:06:46,306
can be a packet clause or some

102
00:06:46,490 --> 00:06:49,914
network calls can proceed longer than usual because of

103
00:06:49,954 --> 00:06:52,854
delays at the network level.

104
00:06:53,194 --> 00:06:56,986
Then we should be able to explain

105
00:06:57,170 --> 00:07:00,930
why an application is limited cpu time.

106
00:07:01,082 --> 00:07:04,874
It can be the reason behind this can be like

107
00:07:04,954 --> 00:07:08,922
being out of capacity of cpu on the node,

108
00:07:09,098 --> 00:07:12,738
or a node can fail or something like that.

109
00:07:12,906 --> 00:07:16,898
Then the next class of possible failure

110
00:07:16,946 --> 00:07:20,058
scenarios is something related to

111
00:07:20,186 --> 00:07:23,226
application runtime, such as GVI runtime,

112
00:07:23,330 --> 00:07:26,746
dotnet runtime, or database internals.

113
00:07:26,890 --> 00:07:30,706
So we need to collect a lot of metrics related to the

114
00:07:30,730 --> 00:07:34,714
application runtime, such as garbage collector metrics,

115
00:07:34,794 --> 00:07:38,594
the state thread pools or connection pools

116
00:07:38,634 --> 00:07:42,390
or logs, and so on. This telematching

117
00:07:42,462 --> 00:07:45,726
will allow us to explain why,

118
00:07:45,870 --> 00:07:49,982
for example, a Java application stop a handling

119
00:07:50,038 --> 00:07:53,394
crash for a while because of GC activity.

120
00:07:53,694 --> 00:07:57,342
Then if we run our application in

121
00:07:57,358 --> 00:08:01,334
a Kubernetes cluster, we need to gather orchestrator

122
00:08:01,414 --> 00:08:05,350
related matrix. Some of our application instances can

123
00:08:05,382 --> 00:08:08,394
be in an unscheduled state.

124
00:08:09,054 --> 00:08:12,622
An orchestrator cannot place the application

125
00:08:12,678 --> 00:08:16,086
instance to the node because of out of capacity or

126
00:08:16,110 --> 00:08:20,462
something like that. Then to be able to investigate

127
00:08:20,598 --> 00:08:23,734
unknown issues like application specific issues,

128
00:08:23,814 --> 00:08:26,910
we need to gather application logs.

129
00:08:27,062 --> 00:08:31,606
And the last one I would say is collecting profiling

130
00:08:31,670 --> 00:08:35,242
data to be able to explain why

131
00:08:35,378 --> 00:08:38,594
the particular application consume more,

132
00:08:38,674 --> 00:08:43,014
for example cpu time than before. And knowing

133
00:08:43,834 --> 00:08:47,946
only CPU usage is not enough to understand the

134
00:08:47,970 --> 00:08:51,554
reason behind this. And I

135
00:08:51,594 --> 00:08:55,370
see that there is two approaches

136
00:08:55,442 --> 00:08:58,762
together. To match data. You can do it manually,

137
00:08:58,898 --> 00:09:03,310
or you can use some sort of automatic instrumentation

138
00:09:03,482 --> 00:09:07,334
using EBPF. The advantage

139
00:09:07,414 --> 00:09:12,074
of automated instrumentation is that you

140
00:09:12,374 --> 00:09:15,806
don't need to change anything

141
00:09:15,870 --> 00:09:19,574
in your applications. You just need to initially install some

142
00:09:19,614 --> 00:09:23,478
agent and that's it. You will have all

143
00:09:23,566 --> 00:09:27,190
telemetry data with some limitations.

144
00:09:27,262 --> 00:09:31,506
But we discuss it. And also automated

145
00:09:31,570 --> 00:09:35,594
automatic instrumentations allows you to avoid blind spots.

146
00:09:35,754 --> 00:09:39,586
It means since you don't need to change code of

147
00:09:39,610 --> 00:09:43,506
your applications, you can instrument even legacy services

148
00:09:43,650 --> 00:09:47,874
or services that you don't

149
00:09:47,914 --> 00:09:51,202
have access to to their code. It allows you

150
00:09:51,258 --> 00:09:54,534
to cover all your,

151
00:09:55,374 --> 00:09:58,114
but not on the most critical services.

152
00:09:58,774 --> 00:10:02,474
But I've seen a lot of cases where companies

153
00:10:02,814 --> 00:10:06,310
started to add instrumentation to their applications,

154
00:10:06,502 --> 00:10:09,754
starting from the most critical services.

155
00:10:10,294 --> 00:10:13,822
And in fact for a while, after a while

156
00:10:13,998 --> 00:10:17,566
they have only a small part of

157
00:10:17,630 --> 00:10:21,114
their system, instrumented telemetry data.

158
00:10:21,424 --> 00:10:25,124
And the last aspect is that

159
00:10:26,224 --> 00:10:30,008
manual instrumentation of your services. It's not a one

160
00:10:30,056 --> 00:10:33,604
time project. It is a continual, continuous process

161
00:10:33,904 --> 00:10:37,264
because you need, if you want to

162
00:10:37,304 --> 00:10:40,044
add a new service into your system,

163
00:10:40,344 --> 00:10:45,304
you will need to be sure that this

164
00:10:45,344 --> 00:10:48,960
service is also instrumented, resulting to damages, decay,

165
00:10:49,032 --> 00:10:53,086
or something like that. Let's go deeper into what is

166
00:10:53,190 --> 00:10:57,070
EBPF and how it can be used to

167
00:10:57,182 --> 00:11:00,566
gather to invention. EBPS is a

168
00:11:00,590 --> 00:11:04,174
feature of Linux kernel. Linux kernel allows

169
00:11:04,214 --> 00:11:07,782
you to run your own small programs in

170
00:11:07,798 --> 00:11:11,998
the kernel space, and such programs will

171
00:11:12,046 --> 00:11:16,254
be called some kernel function code

172
00:11:16,334 --> 00:11:19,596
or some user lan function call.

173
00:11:19,740 --> 00:11:23,124
PF itself doesn't gather any data,

174
00:11:23,284 --> 00:11:26,524
it's just a way to instrument a

175
00:11:26,604 --> 00:11:30,588
system, but you have to write your own program and

176
00:11:30,636 --> 00:11:33,908
run it in the kernel space. It's pretty

177
00:11:33,956 --> 00:11:37,372
complicated task because kernel has a

178
00:11:37,388 --> 00:11:41,836
lot of limitations that applied to such programs.

179
00:11:42,020 --> 00:11:46,766
In simple terms how we use EBPF the

180
00:11:46,790 --> 00:11:50,638
first way is to attach our program to any

181
00:11:50,806 --> 00:11:53,874
kernel function call, such as

182
00:11:54,454 --> 00:11:57,354
for example open or VertCP connection.

183
00:11:57,854 --> 00:12:01,462
But this approach is not the best

184
00:12:01,518 --> 00:12:05,074
option because of possible compatibilities.

185
00:12:05,894 --> 00:12:09,998
Because different kernel versions can have

186
00:12:10,166 --> 00:12:14,060
different function calls, the function can be renamed,

187
00:12:14,132 --> 00:12:17,844
deleted and so on. So you need to support many

188
00:12:17,964 --> 00:12:21,052
variants of them. The second one,

189
00:12:21,108 --> 00:12:25,004
the kernel team tried to provide

190
00:12:25,084 --> 00:12:28,572
a selfdirect stable API. To solve the problem.

191
00:12:28,708 --> 00:12:31,740
They inserted a set of trace points,

192
00:12:31,852 --> 00:12:35,668
trace points in the statically defined places

193
00:12:35,716 --> 00:12:39,108
in the kernel code and your EBPS

194
00:12:39,196 --> 00:12:43,274
program can attach to at this point, and there

195
00:12:43,314 --> 00:12:46,906
are some guarantees that arguments of such

196
00:12:46,970 --> 00:12:50,842
trace points will not be changed over time. For example

197
00:12:50,898 --> 00:12:54,874
ecaroot we mostly we try to use trace points as

198
00:12:54,994 --> 00:12:58,618
and not use gpro. The third one option is

199
00:12:58,706 --> 00:13:04,154
UCL is a way to call

200
00:13:04,234 --> 00:13:08,754
your EBP program when some user

201
00:13:08,794 --> 00:13:12,408
went function is called, for example you

202
00:13:12,456 --> 00:13:16,728
have some binary file, this Golang program

203
00:13:16,816 --> 00:13:20,404
for example, and you want to attach

204
00:13:20,984 --> 00:13:24,360
your program to calling crypto

205
00:13:24,392 --> 00:13:27,496
library for example. And you can do these

206
00:13:27,560 --> 00:13:31,800
a pros and two things

207
00:13:31,952 --> 00:13:35,472
I want to mention here. The first one is maps.

208
00:13:35,648 --> 00:13:39,184
Your EBPF program can store

209
00:13:39,224 --> 00:13:43,704
some data in the kernel space. Using EBPF maps.

210
00:13:43,824 --> 00:13:47,584
You can keep some state between your

211
00:13:47,624 --> 00:13:51,408
program calls. Usually it's used to

212
00:13:51,576 --> 00:13:55,256
cache some data to capture some data from one call

213
00:13:55,320 --> 00:13:58,688
and then use it from next program.

214
00:13:58,776 --> 00:14:02,352
For example, you open HTTP connection and the first

215
00:14:02,408 --> 00:14:06,280
call you store a file descriptor in

216
00:14:06,312 --> 00:14:09,656
some map by process h

217
00:14:09,720 --> 00:14:13,664
for example. And then when connection goes to the

218
00:14:13,784 --> 00:14:17,160
established state you can get data

219
00:14:17,232 --> 00:14:20,960
from the map using process id

220
00:14:21,032 --> 00:14:24,444
for example, or other id like socketing.

221
00:14:25,104 --> 00:14:28,764
And the second one is turfmaps.

222
00:14:29,184 --> 00:14:33,152
It's a way to share some data between

223
00:14:33,248 --> 00:14:36,498
the kernel space and the user space program.

224
00:14:36,696 --> 00:14:40,150
Looks like a circle boot

225
00:14:40,182 --> 00:14:43,598
buffer and it allows for example

226
00:14:43,726 --> 00:14:47,462
store some data in the kernel space and read this

227
00:14:47,518 --> 00:14:50,934
data from user space program. That's the

228
00:14:50,974 --> 00:14:54,582
main way to exchange data between user

229
00:14:54,638 --> 00:14:58,634
and kernel space. But it's

230
00:15:00,334 --> 00:15:03,636
low level entities of EBPF

231
00:15:03,830 --> 00:15:07,648
and I believe that you don't need

232
00:15:07,736 --> 00:15:11,168
to write BPF program because

233
00:15:11,296 --> 00:15:14,568
there are a lot of ready made and

234
00:15:14,656 --> 00:15:18,000
cartoon. But I think it's

235
00:15:18,032 --> 00:15:22,040
good to know how it works. But you don't necessarily need

236
00:15:22,112 --> 00:15:25,964
to write your own EBPF programs. Now I'm telling

237
00:15:26,544 --> 00:15:30,512
you about how current uses EBPF

238
00:15:30,568 --> 00:15:34,090
together. Two dimension data we have our own

239
00:15:34,202 --> 00:15:37,554
node agent. Node agent is a Golang tool,

240
00:15:37,714 --> 00:15:40,850
open source distributed under Apache

241
00:15:40,882 --> 00:15:43,734
to no two to zero license.

242
00:15:44,034 --> 00:15:47,530
It's an open source agent that should

243
00:15:47,562 --> 00:15:50,174
be installed on every node in your cluster.

244
00:15:50,474 --> 00:15:53,810
Discovers processes containers running on the

245
00:15:53,842 --> 00:15:57,014
node, it discovers their logs.

246
00:15:57,354 --> 00:16:02,060
We can analyze logs for repeated patterns.

247
00:16:02,212 --> 00:16:06,004
It tracks all CPA communications between processes,

248
00:16:06,084 --> 00:16:09,876
processes containers, and it also

249
00:16:09,980 --> 00:16:13,944
captures application level protocols data and

250
00:16:14,484 --> 00:16:17,984
can expose metrics and traces

251
00:16:18,284 --> 00:16:20,344
without any instrumentation.

252
00:16:22,124 --> 00:16:25,532
It supports the most popular protocols

253
00:16:25,668 --> 00:16:29,144
such as HTTP postgres, JPC,

254
00:16:29,714 --> 00:16:32,454
Redis, Cassandra and so on.

255
00:16:33,034 --> 00:16:36,738
And let's talk about how the

256
00:16:36,826 --> 00:16:40,170
agent uses CBPF. As I need

257
00:16:40,242 --> 00:16:44,346
mentioned before, we try to use trace points for example,

258
00:16:44,410 --> 00:16:47,690
to be able to discover new

259
00:16:47,722 --> 00:16:52,106
processes in the system we call

260
00:16:52,170 --> 00:16:55,754
of trace point testing tasks. It allows us to know

261
00:16:55,834 --> 00:16:59,284
that in real time that a new

262
00:16:59,324 --> 00:17:02,676
process has been started and we know

263
00:17:02,740 --> 00:17:06,524
it's process id. And then the user

264
00:17:06,564 --> 00:17:10,932
space part of the engine can resolve all metadata

265
00:17:11,028 --> 00:17:13,664
like cgroup or variant process,

266
00:17:14,564 --> 00:17:18,316
understand the name of container labels and

267
00:17:18,340 --> 00:17:22,884
so on. The second one is recapture events

268
00:17:22,964 --> 00:17:26,768
where some process is marked

269
00:17:26,816 --> 00:17:30,776
as a victim of the computer. We need the

270
00:17:30,880 --> 00:17:34,224
data to understand the reasons why the

271
00:17:34,264 --> 00:17:37,448
particular process has been terminated.

272
00:17:37,616 --> 00:17:41,192
And on this call, on this event,

273
00:17:41,328 --> 00:17:45,096
we just put the flag into kernel space,

274
00:17:45,160 --> 00:17:49,456
map that process with process id 123

275
00:17:49,640 --> 00:17:52,722
marked as a victim and will be terminated. So,

276
00:17:52,888 --> 00:17:56,526
and on the next call when process is terminated,

277
00:17:56,670 --> 00:18:00,166
we can get this flag from the map and

278
00:18:00,270 --> 00:18:04,238
mark the event that goes to userland.

279
00:18:04,406 --> 00:18:08,710
That reason of the termination of this process

280
00:18:08,862 --> 00:18:12,234
is it has been terminated by the open.

281
00:18:12,854 --> 00:18:16,870
And then we also check openings to be

282
00:18:16,902 --> 00:18:20,606
able to discover containers log where

283
00:18:20,710 --> 00:18:23,614
container or process stores its log.

284
00:18:23,734 --> 00:18:26,878
And also we want to understand

285
00:18:27,046 --> 00:18:30,742
the actual storage partition. A process

286
00:18:30,838 --> 00:18:34,038
communicates with then few trace

287
00:18:34,086 --> 00:18:37,678
points that we use to track TCP

288
00:18:37,726 --> 00:18:41,846
connections. The first one is a syscall connect

289
00:18:42,030 --> 00:18:45,494
where a process wants to open a new TCP

290
00:18:45,534 --> 00:18:48,930
connection. We call this system call. And we

291
00:18:49,042 --> 00:18:52,714
see that process id 123 opens

292
00:18:52,794 --> 00:18:56,294
GCP connections to some destination

293
00:18:57,754 --> 00:19:01,962
IP and then using socks set

294
00:19:02,018 --> 00:19:06,130
state. At this point we can track the

295
00:19:06,242 --> 00:19:09,854
proticity handshake and we can see

296
00:19:10,314 --> 00:19:14,194
if this connection was successfully

297
00:19:14,234 --> 00:19:17,928
established or failed. We can track the

298
00:19:17,976 --> 00:19:21,604
errors of establishing TCP connections. And also

299
00:19:21,944 --> 00:19:26,064
we can, we can track all successfully established

300
00:19:26,104 --> 00:19:29,960
TCP connections. And this last one

301
00:19:30,072 --> 00:19:33,576
is we track TCP redrawing

302
00:19:33,640 --> 00:19:37,336
in as associated with all connections in our

303
00:19:37,400 --> 00:19:41,444
system. It's really helpful to understand

304
00:19:41,784 --> 00:19:46,042
that service a communicates with service

305
00:19:46,138 --> 00:19:50,050
p slower than usual because of pagan laws. For example,

306
00:19:50,122 --> 00:19:54,890
we need to retransmit our TCP segments and

307
00:19:55,042 --> 00:19:58,734
it brings additional latency into

308
00:19:59,314 --> 00:20:03,174
communication process. And the last one is

309
00:20:04,274 --> 00:20:07,614
trace point that allows us to capture

310
00:20:08,034 --> 00:20:12,376
application level communications. The agent tracks

311
00:20:12,480 --> 00:20:16,312
that. Some process writes something

312
00:20:16,408 --> 00:20:20,160
to a file descriptor, usually it's

313
00:20:20,352 --> 00:20:23,896
socket, and we can capture a

314
00:20:23,920 --> 00:20:27,952
preload of such communications and

315
00:20:28,088 --> 00:20:31,912
we can parse, we can detect and parse application level

316
00:20:31,968 --> 00:20:35,848
protocols within such connections. We have two phase

317
00:20:35,976 --> 00:20:39,296
application level protocol parsing. The first one is

318
00:20:39,440 --> 00:20:43,480
performed in the kernel space to like high

319
00:20:43,512 --> 00:20:46,872
performance protocol detection. But EBPF

320
00:20:46,928 --> 00:20:50,176
program is super limited in the,

321
00:20:50,320 --> 00:20:53,848
you know, ability to

322
00:20:53,896 --> 00:20:57,280
analyze something because it's like

323
00:20:57,472 --> 00:21:00,800
complexity. EBPF validator check the complexity of

324
00:21:00,832 --> 00:21:04,656
each program and for example cannot use loops

325
00:21:04,760 --> 00:21:08,296
in your EBPF programs. And if you want

326
00:21:08,360 --> 00:21:12,094
to parse protocol like for example to read,

327
00:21:12,224 --> 00:21:15,370
reload or to extract

328
00:21:15,482 --> 00:21:19,130
URL of request or status of the

329
00:21:19,162 --> 00:21:22,874
response. We use a user space protocol

330
00:21:22,914 --> 00:21:26,834
parsing and we are not limited in

331
00:21:26,954 --> 00:21:30,546
our complexity of the program, so we

332
00:21:30,570 --> 00:21:33,858
can implement any logic. And the last

333
00:21:33,946 --> 00:21:38,482
one is about encryption, because these

334
00:21:38,538 --> 00:21:42,222
days usually even communications inside

335
00:21:42,278 --> 00:21:46,206
your cluster should be encrypted by compliance requirement

336
00:21:46,270 --> 00:21:50,214
or something like that. And we would somehow

337
00:21:50,294 --> 00:21:54,262
deal with that. There are two primary approaches

338
00:21:54,358 --> 00:21:58,478
to capture within encrypted TCP

339
00:21:58,526 --> 00:22:02,478
connections. The obvious way is to read the

340
00:22:02,526 --> 00:22:06,394
data before, before the encryption or

341
00:22:06,854 --> 00:22:10,246
after decryption, where you receive

342
00:22:10,310 --> 00:22:14,314
some response. In our agent we support two

343
00:22:14,934 --> 00:22:18,634
ways to encryption to

344
00:22:19,454 --> 00:22:22,830
two types of programs. The first one is programs

345
00:22:22,862 --> 00:22:26,934
that use OpensSl library. It's for example

346
00:22:27,014 --> 00:22:30,714
or Python pro applications

347
00:22:31,094 --> 00:22:33,874
or other interpreted languages.

348
00:22:34,354 --> 00:22:38,026
And the second one we use a special

349
00:22:38,170 --> 00:22:42,210
new probe for capture go and crypto

350
00:22:42,242 --> 00:22:46,322
library calls to capture data before the

351
00:22:46,338 --> 00:22:49,866
encryption and it allows us to have

352
00:22:49,930 --> 00:22:53,514
pretty good coverage. So we can say that

353
00:22:53,554 --> 00:22:56,890
the agent 90% of data

354
00:22:57,002 --> 00:23:00,374
even consequently, and few

355
00:23:00,414 --> 00:23:03,982
words about performance impact because at first glance

356
00:23:04,078 --> 00:23:07,582
it looks like a super crazy idea,

357
00:23:07,678 --> 00:23:11,102
allows users to run their programs in the kernel space

358
00:23:11,278 --> 00:23:14,222
because kernel should be,

359
00:23:14,358 --> 00:23:17,958
should perform like a low latency. And it's

360
00:23:18,006 --> 00:23:21,614
crazy idea if we have custom programs in the kernel

361
00:23:21,654 --> 00:23:26,002
space. But in fact there are few

362
00:23:26,138 --> 00:23:29,746
guarantees from the kernel that allows us to

363
00:23:29,930 --> 00:23:33,722
run our programs without any, without having

364
00:23:33,778 --> 00:23:37,482
signals it can perform a sync. The first

365
00:23:37,538 --> 00:23:41,738
one is a validator and the kernel that validates

366
00:23:41,866 --> 00:23:45,634
every problem before it runs. And it

367
00:23:45,674 --> 00:23:49,538
must have a finite complexity, you cannot use

368
00:23:49,586 --> 00:23:52,774
loops and you cannot operate this

369
00:23:53,104 --> 00:23:55,728
huge amount of memory and so on.

370
00:23:55,816 --> 00:23:59,184
And it's some sort of, it's super tricky to

371
00:23:59,224 --> 00:24:02,760
write your EBPF program that can be successfully

372
00:24:02,792 --> 00:24:06,664
validated by the validator. And the second,

373
00:24:06,784 --> 00:24:10,968
the second thing that branch is that perform

374
00:24:11,056 --> 00:24:14,328
of your EBPF program

375
00:24:14,496 --> 00:24:18,620
or your user space programs will

376
00:24:18,652 --> 00:24:22,796
not affect the system, is that communication between a kernel

377
00:24:22,900 --> 00:24:26,340
space and user space is occurred in

378
00:24:26,452 --> 00:24:29,144
using limited buffer.

379
00:24:29,444 --> 00:24:32,884
And if program user lamp program will

380
00:24:32,924 --> 00:24:36,892
not receive some data because of a performance degradation

381
00:24:36,948 --> 00:24:40,420
or something like that, we just lose some data and

382
00:24:40,532 --> 00:24:43,828
it will not bring some locking into

383
00:24:43,876 --> 00:24:47,860
kernel space. And from my perspective it's

384
00:24:47,892 --> 00:24:51,788
pretty fair. And for observability purposes

385
00:24:51,876 --> 00:24:55,180
it's a nice, because I think it's more

386
00:24:55,212 --> 00:24:58,556
important to be sure that our system do

387
00:24:58,580 --> 00:25:02,024
not be impacted by our observability tool.

388
00:25:02,484 --> 00:25:05,732
But in worst case we just lose some telemetry

389
00:25:05,788 --> 00:25:09,548
data, we lose some traces or some

390
00:25:09,596 --> 00:25:12,624
metrics, but it's not a critical thing.

391
00:25:12,974 --> 00:25:16,822
As a result we can instrument all

392
00:25:16,878 --> 00:25:20,926
our distributed system and understand how

393
00:25:21,030 --> 00:25:24,434
components of such a system communicates with each other,

394
00:25:24,814 --> 00:25:28,798
latency inside, latency between any particular

395
00:25:28,926 --> 00:25:32,314
services, status of tcp,

396
00:25:33,094 --> 00:25:36,838
connections between them and so on. So we

397
00:25:36,886 --> 00:25:41,444
can have that for example front end communicators services

398
00:25:41,524 --> 00:25:45,884
such as card catalog and so on. We can see latency,

399
00:25:45,964 --> 00:25:48,344
we can see number of requests,

400
00:25:48,964 --> 00:25:52,564
you will have all the all this service

401
00:25:52,644 --> 00:25:56,092
map just in few minutes after the installation.

402
00:25:56,148 --> 00:25:59,744
You don't need to change code of your application,

403
00:26:00,044 --> 00:26:03,260
integrate open telemetry, SDK and so on.

404
00:26:03,412 --> 00:26:06,760
And it's pretty useful for if you

405
00:26:06,792 --> 00:26:10,664
want to understand how a system perform right now,

406
00:26:10,824 --> 00:26:14,856
you cannot wait for few weeks when

407
00:26:14,920 --> 00:26:19,040
you're development team at some integration.

408
00:26:19,232 --> 00:26:22,400
And we have telemetry

409
00:26:22,472 --> 00:26:25,400
data, pretty granular of telemetry data.

410
00:26:25,512 --> 00:26:29,744
It means we can track connections not between applications,

411
00:26:29,864 --> 00:26:33,488
but between any application instance with

412
00:26:33,576 --> 00:26:37,480
their peers. So we can

413
00:26:37,552 --> 00:26:41,776
see communications as with

414
00:26:41,800 --> 00:26:45,656
the optimization we need. For example, we can see how

415
00:26:45,840 --> 00:26:49,208
a few postgres instances communicate with each other.

416
00:26:49,256 --> 00:26:52,504
This is the primary instance and the two replicas

417
00:26:52,584 --> 00:26:55,904
are connected to it for a cocation.

418
00:26:56,064 --> 00:27:00,328
And we know about each of the connections,

419
00:27:00,416 --> 00:27:03,928
we know the number of requests, number of errors,

420
00:27:04,016 --> 00:27:08,324
latency of application level requests,

421
00:27:08,744 --> 00:27:12,872
we know how network performs between any

422
00:27:12,928 --> 00:27:16,284
instances, we know network round trip time,

423
00:27:16,824 --> 00:27:20,576
and we also know connection level metrics

424
00:27:20,720 --> 00:27:23,984
such as number of connections, number of field

425
00:27:24,064 --> 00:27:28,016
connections, number of TCP transmissions.

426
00:27:28,160 --> 00:27:31,832
It's pretty useful because in distributed

427
00:27:31,888 --> 00:27:35,400
systems it's

428
00:27:35,432 --> 00:27:39,248
hard to understand what's happening right now, and it's hard

429
00:27:39,336 --> 00:27:43,088
to current topology of services.

430
00:27:43,216 --> 00:27:47,432
You should know the nodes where your application

431
00:27:47,528 --> 00:27:51,444
runs. And to check the connectivity between two services,

432
00:27:51,824 --> 00:27:55,804
you need to perform many applications to understand the topology,

433
00:27:56,124 --> 00:28:00,116
then goes to some pod to perform ping

434
00:28:00,260 --> 00:28:04,068
or something like that. And here we have geometry

435
00:28:04,116 --> 00:28:07,524
data that already understands

436
00:28:07,564 --> 00:28:10,836
that an application can migrate between nodes

437
00:28:10,940 --> 00:28:14,604
and something like that. And it reflects the current

438
00:28:14,724 --> 00:28:17,944
picture of your system, current topology of your system.

439
00:28:18,444 --> 00:28:21,684
And also the edge provides not only metrics,

440
00:28:21,724 --> 00:28:24,904
it also provides APF based traces.

441
00:28:25,224 --> 00:28:28,416
So we can see that we have,

442
00:28:28,560 --> 00:28:31,804
for example, application calls a database,

443
00:28:32,104 --> 00:28:36,000
and we can see that some of

444
00:28:36,072 --> 00:28:39,608
queries perform slower than others. And we can

445
00:28:39,656 --> 00:28:43,440
drill down and see particular requests

446
00:28:43,632 --> 00:28:46,404
and useful while troubleshooting.

447
00:28:46,984 --> 00:28:50,684
But EVPF tracing has some limitations.

448
00:28:51,814 --> 00:28:54,870
Most from my perspective. Limitation is that

449
00:28:54,942 --> 00:28:58,446
EBP and BBF traces are not actually

450
00:28:58,510 --> 00:29:01,726
traces, it's individual spans.

451
00:29:01,870 --> 00:29:05,478
Because when instruments and applications are open

452
00:29:05,526 --> 00:29:09,678
to images decay, it can originate some tracing

453
00:29:09,726 --> 00:29:13,302
id and propagate it to other

454
00:29:13,358 --> 00:29:18,060
services. As a result, you will have a

455
00:29:18,092 --> 00:29:21,504
single trace that contains all the requests

456
00:29:22,524 --> 00:29:25,144
from all services.

457
00:29:27,004 --> 00:29:30,508
In case of EBPF, we don't have a tracing

458
00:29:30,556 --> 00:29:34,220
id so we only

459
00:29:34,292 --> 00:29:38,116
can check the particular queries and we cannot

460
00:29:38,260 --> 00:29:41,836
connect them to show you the whole trace.

461
00:29:41,940 --> 00:29:45,888
And yeah there is some tool open

462
00:29:45,936 --> 00:29:49,848
source tool that tried to solve that problem

463
00:29:50,016 --> 00:29:53,880
and they use approach where they

464
00:29:54,072 --> 00:29:57,152
capture the request, then modify them,

465
00:29:57,208 --> 00:30:00,704
inserting for example the JCG header and

466
00:30:00,744 --> 00:30:04,604
then send it. But from my perspective it's

467
00:30:05,224 --> 00:30:09,332
not a good idea because I believe

468
00:30:09,428 --> 00:30:12,984
that observability tools should observe.

469
00:30:14,764 --> 00:30:18,588
They shouldn't change your data, they shouldn't modify

470
00:30:18,676 --> 00:30:22,708
by law and understanding this, we supported

471
00:30:22,756 --> 00:30:25,868
both methods of instrumentation in Karoot.

472
00:30:25,996 --> 00:30:30,132
We support open telemetry generated traces and we

473
00:30:30,188 --> 00:30:33,860
also support EBBF based traces for systems,

474
00:30:33,972 --> 00:30:37,764
for example for legacy services and so on.

475
00:30:39,424 --> 00:30:43,032
After the installation you will have VTF based

476
00:30:43,088 --> 00:30:46,720
tracing, but then you can enrich your telemetry data

477
00:30:46,792 --> 00:30:50,312
with open generator instrumentation.

478
00:30:50,488 --> 00:30:54,844
It's the best way to have all this data and

479
00:30:56,144 --> 00:31:00,248
it allows you to extend your

480
00:31:00,376 --> 00:31:03,724
visibility of a system with MBPF.

481
00:31:05,274 --> 00:31:09,538
And few words about another

482
00:31:09,706 --> 00:31:13,922
another way to gather telemetry data using EBPF

483
00:31:13,978 --> 00:31:17,602
it's EBPF based profiling. What is profiling?

484
00:31:17,658 --> 00:31:21,414
Profiling allows you to explain any

485
00:31:22,674 --> 00:31:26,138
it allows you to answer the question what this

486
00:31:26,186 --> 00:31:29,266
particular application did at

487
00:31:29,290 --> 00:31:32,784
the time x which code were executed

488
00:31:32,864 --> 00:31:36,328
and which code consumed more

489
00:31:36,376 --> 00:31:40,608
cpu the most extract most cpu consuming

490
00:31:40,696 --> 00:31:44,024
part of your application. MPR based approach

491
00:31:44,144 --> 00:31:48,244
doesn't require integrating some continuous profiling

492
00:31:48,904 --> 00:31:52,088
tools into your application doesn't

493
00:31:52,136 --> 00:31:55,800
require redeployment of your application. You just need

494
00:31:55,832 --> 00:31:59,176
to install the agent and it

495
00:31:59,200 --> 00:32:02,564
will gather providing data and storing it

496
00:32:02,864 --> 00:32:06,152
with some storage. In case of carrot we store

497
00:32:06,328 --> 00:32:09,964
all the telemetry data exception metrics in qcause.

498
00:32:10,424 --> 00:32:13,824
Here we have an example of how to

499
00:32:13,864 --> 00:32:18,536
use profiling data to understand to

500
00:32:18,560 --> 00:32:22,284
explain cpu consumption we can use

501
00:32:22,624 --> 00:32:26,016
cpu usage chart and select some area and see

502
00:32:26,080 --> 00:32:29,444
the flame graph work reflecting what part

503
00:32:29,484 --> 00:32:32,996
of your code consumed more cpu.

504
00:32:33,140 --> 00:32:36,804
In this case it's example of coordinates component

505
00:32:36,844 --> 00:32:40,884
of kubernetes and we can see the particular

506
00:32:41,044 --> 00:32:44,396
function calls here. The wider frame and on flay

507
00:32:44,420 --> 00:32:48,212
graphs means corresponding code consumed

508
00:32:48,348 --> 00:32:52,196
more cpu's and the smaller one. And also

509
00:32:52,260 --> 00:32:56,476
we can easily compare cpu usage. We think some

510
00:32:56,580 --> 00:33:00,108
cpu spike be the previous period and

511
00:33:00,156 --> 00:33:03,860
we can highlight it's not a significant spike but

512
00:33:03,892 --> 00:33:07,300
in this case we can see that the service

513
00:33:07,372 --> 00:33:10,104
running more DM's queries than before.

514
00:33:10,764 --> 00:33:14,356
And here notes on how code works.

515
00:33:14,420 --> 00:33:17,544
You install the agent to your nodes.

516
00:33:18,284 --> 00:33:21,568
The agents gathers gather telemetry

517
00:33:21,616 --> 00:33:25,680
data about all containers running on this particular

518
00:33:25,792 --> 00:33:28,912
node, their communication. They gather their

519
00:33:29,088 --> 00:33:33,224
logs and trace profiles and send them

520
00:33:33,304 --> 00:33:37,120
to carot which stores matching the

521
00:33:37,152 --> 00:33:40,656
parameters and other telemetry signals in

522
00:33:40,680 --> 00:33:44,408
decals. Also, if you instrument your applications

523
00:33:44,496 --> 00:33:48,042
with Opentelemergy, you use curves

524
00:33:48,138 --> 00:33:51,754
endpoints that demands ole

525
00:33:51,794 --> 00:33:55,314
mature protocol, and you can store

526
00:33:55,394 --> 00:33:58,050
telemetry data in the same storages.

527
00:33:58,242 --> 00:34:01,458
It's open source, every component is

528
00:34:01,506 --> 00:34:05,010
open source, and you can use it. Or you can

529
00:34:05,042 --> 00:34:09,514
use core cloud to offload your storing

530
00:34:09,554 --> 00:34:13,218
telemetry data. In the end, let's just

531
00:34:13,306 --> 00:34:17,064
scrape it up of the target. EDP is awesome

532
00:34:17,144 --> 00:34:21,232
and it allows us to gather a lot of telemetry

533
00:34:21,288 --> 00:34:25,044
data without the need to instrument your code,

534
00:34:25,464 --> 00:34:29,444
and we can be sure that performance impact

535
00:34:29,824 --> 00:34:33,336
on your application is negligible. On our side,

536
00:34:33,400 --> 00:34:37,560
we have a page with benchmarks, so we

537
00:34:37,672 --> 00:34:40,864
understand principles, but we

538
00:34:40,944 --> 00:34:44,747
decided to to validate that museum

539
00:34:44,795 --> 00:34:49,059
benchmark. If you want to gain visibility

540
00:34:49,171 --> 00:34:53,043
into your system just in few minutes after installation,

541
00:34:53,203 --> 00:34:56,419
just install code. You can reach out to me on

542
00:34:56,451 --> 00:34:59,323
LinkedIn Twitter. Thank you for your time.

