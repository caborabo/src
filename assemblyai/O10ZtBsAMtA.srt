1
00:00:20,650 --> 00:00:24,026
So welcome everyone. Today we'll be talking about how we can leverage the Apache

2
00:00:24,058 --> 00:00:27,446
flight, Python and influxdB. We'll also talk about

3
00:00:27,468 --> 00:00:31,702
how we can just leverage the influxDB Python v three client and

4
00:00:31,836 --> 00:00:34,982
any aeroflight SQL client as well. So we'll go over all

5
00:00:35,036 --> 00:00:38,326
three. So just a little bit about me for those of you who

6
00:00:38,348 --> 00:00:42,342
don't know what a developer advocate is, a developer advocate is someone who represents

7
00:00:42,406 --> 00:00:46,154
the community to the company and the company to the community. And I do that

8
00:00:46,192 --> 00:00:49,866
through webinars like this, also creating pocs or

9
00:00:49,888 --> 00:00:53,546
demos that showcase code examples of how to use a variety

10
00:00:53,578 --> 00:00:57,086
of technologies together so that you can understand how to

11
00:00:57,108 --> 00:01:00,174
leverage something like influxdB. Yeah,

12
00:01:00,212 --> 00:01:04,422
and I work for Influxdata, and Influxdata is the creator of influxdB.

13
00:01:04,506 --> 00:01:07,474
So if you enjoy any of this content today, or you want to learn more

14
00:01:07,512 --> 00:01:10,882
about Apache Arrow, about the Apache ecosystem in general,

15
00:01:11,016 --> 00:01:14,786
about influxdB, or perhaps have access

16
00:01:14,888 --> 00:01:18,242
to any questions about time series or data

17
00:01:18,296 --> 00:01:21,538
science in the time series space, I encourage you to reach out on LinkedIn.

18
00:01:21,634 --> 00:01:25,110
Please connect with me there. I'd love to learn about what you're doing

19
00:01:25,180 --> 00:01:29,026
and any questions that you have. But for today's agenda,

20
00:01:29,058 --> 00:01:33,094
we're going to start with a quick introduction to influxdB and time series.

21
00:01:33,222 --> 00:01:37,046
Because influxdb is a time series database and you can't understand what influxdB

22
00:01:37,078 --> 00:01:40,006
is without understanding what time series data is.

23
00:01:40,128 --> 00:01:44,026
Then I'm going to talk about InfluxdB's commitment to the open data architecture,

24
00:01:44,138 --> 00:01:47,760
along with the FDAP stack or the

25
00:01:48,130 --> 00:01:52,474
Aeroflight data fusion, Apache Arrow and Parquet

26
00:01:52,522 --> 00:01:56,334
data Stack. And then I'll talk about leveraging the aeroflight

27
00:01:56,382 --> 00:01:59,650
client and the influxdb v three python client.

28
00:02:00,230 --> 00:02:03,662
And then last but not least, I'll share some projects that leveraging

29
00:02:03,726 --> 00:02:07,186
all of these tools. So let's get right into it.

30
00:02:07,208 --> 00:02:10,760
Let's talk about the introduction to influxDB and time series data.

31
00:02:11,210 --> 00:02:14,342
So time series data is any data

32
00:02:14,396 --> 00:02:18,534
that really has a timestamp associated with it. So the earliest or most

33
00:02:18,572 --> 00:02:21,674
simple example is probably stock market data.

34
00:02:21,792 --> 00:02:25,786
And the unique thing about time series data is that the single value of

35
00:02:25,808 --> 00:02:29,660
it is not really that interesting. So single stock value, for example,

36
00:02:30,030 --> 00:02:33,306
doesn't have a lot of meaning. But what's really important is the

37
00:02:33,328 --> 00:02:36,574
trend of the stock over time. And that's what's really interesting, because that

38
00:02:36,612 --> 00:02:39,966
tells us whether or not we should be buying or selling based on a particular

39
00:02:40,068 --> 00:02:43,474
trend. And so this is true for time series data, any time

40
00:02:43,512 --> 00:02:47,026
series data, which is that the sequence of data points that are

41
00:02:47,048 --> 00:02:50,498
usually consisting of successive measurements from a

42
00:02:50,504 --> 00:02:54,414
data source over time is what really defines

43
00:02:54,462 --> 00:02:57,794
it. And this is true whether or not we're looking at SoC

44
00:02:57,842 --> 00:03:01,126
data, or we're looking at industrial IoT data or application

45
00:03:01,228 --> 00:03:05,400
monitoring data. And so that's time series data.

46
00:03:06,010 --> 00:03:09,094
And when we think of time series data, we really think of it existing within

47
00:03:09,132 --> 00:03:12,666
two categories. So, the first is metrics. And metrics are time

48
00:03:12,688 --> 00:03:16,486
series data that are predictable, and they occur at the same time interval.

49
00:03:16,598 --> 00:03:20,410
So, if we were monitoring, let's say, a machine on

50
00:03:20,480 --> 00:03:24,138
some plant floor, maybe we're

51
00:03:24,154 --> 00:03:27,434
looking at the vibration reading per second from an accelerator,

52
00:03:27,482 --> 00:03:31,194
let's say, and we're pulling that at a regular interval.

53
00:03:31,242 --> 00:03:34,538
So that would be a metric. But you can have event data too,

54
00:03:34,564 --> 00:03:37,614
as a type of time series data. And events are unpredictable,

55
00:03:37,742 --> 00:03:41,186
so we can't derive when an event will occur, but we

56
00:03:41,208 --> 00:03:44,580
can still store events, and we can also,

57
00:03:45,110 --> 00:03:48,558
through aggregation, convert any event data into a

58
00:03:48,584 --> 00:03:51,906
metric. So, for example, we could count the number of events

59
00:03:51,938 --> 00:03:55,298
that occur per day. Maybe event in this machine

60
00:03:55,394 --> 00:03:58,390
use case is when we have some sort of machine fault,

61
00:03:59,710 --> 00:04:03,034
and we can record how many faults happen per day. And then this

62
00:04:03,072 --> 00:04:06,614
way, we're converting our event data into a metric

63
00:04:06,662 --> 00:04:10,198
where we're getting a count on a regular

64
00:04:10,374 --> 00:04:13,866
basis. So what is a time series

65
00:04:13,898 --> 00:04:17,178
database? Well, time series database has four components,

66
00:04:17,354 --> 00:04:20,926
or really pillars that comprise it. The first is that it

67
00:04:20,948 --> 00:04:23,838
must be able to handle time series data,

68
00:04:24,004 --> 00:04:27,742
maybe, pretty obviously. So, every data point in a time series database

69
00:04:27,806 --> 00:04:31,374
is associated with a timestamp. And you should be able to query

70
00:04:31,422 --> 00:04:34,500
this data in a time ordered fashion, because, as we mentioned,

71
00:04:35,030 --> 00:04:38,610
time series data is really meaningless unless it's in that time ordered fashion.

72
00:04:38,770 --> 00:04:42,614
Additionally, it has really high throughput. High, excuse me, really high

73
00:04:42,652 --> 00:04:45,190
write throughput. So, in mace cases,

74
00:04:45,930 --> 00:04:49,734
we're talking about really high volumes of batch data or real time streams

75
00:04:49,782 --> 00:04:53,414
from multiple endpoints. So we could think of like thousands

76
00:04:53,462 --> 00:04:56,822
of sensors that are writing pressure,

77
00:04:56,886 --> 00:04:58,570
concentration, temperature,

78
00:04:59,890 --> 00:05:03,760
light, et cetera data. Or we might even be thinking about

79
00:05:05,890 --> 00:05:09,006
potentially also just one sensor, or a

80
00:05:09,028 --> 00:05:12,880
few sensors that have really high throughput. Like when we think about

81
00:05:13,190 --> 00:05:16,910
an industrial vibration sensor.

82
00:05:17,070 --> 00:05:20,882
That type of sensor is typically writing around 10,000

83
00:05:20,936 --> 00:05:22,100
points per second.

84
00:05:23,830 --> 00:05:27,238
Another kind of pillar of time series data is that we

85
00:05:27,244 --> 00:05:31,430
need to be able to handle really efficient queries over time ranges,

86
00:05:32,010 --> 00:05:35,158
and we also need to be able to perform aggregation over time.

87
00:05:35,244 --> 00:05:38,986
Things like averages, sum min, max, with a

88
00:05:39,008 --> 00:05:41,770
variety of different granularities, whether that's seconds, minutes,

89
00:05:41,840 --> 00:05:45,146
hours, days, or nanoseconds. And then last but

90
00:05:45,168 --> 00:05:49,046
not least, your time series database should be both scalable

91
00:05:49,078 --> 00:05:52,366
and performant. So you need to be able

92
00:05:52,388 --> 00:05:56,346
to design something and scale it horizontally in order to handle

93
00:05:56,378 --> 00:06:01,358
the increased load that is often associated or

94
00:06:01,524 --> 00:06:04,366
often across distributed clusters of machines,

95
00:06:04,558 --> 00:06:07,874
and also to be able to handle the really high write

96
00:06:07,912 --> 00:06:11,698
throughput of and query requirements of

97
00:06:11,784 --> 00:06:15,494
the time series use cases, which really are often

98
00:06:15,612 --> 00:06:18,470
really high dimensionality and high volume use cases.

99
00:06:19,130 --> 00:06:22,690
So let's talk about influxdb specifically. So influxdb

100
00:06:22,850 --> 00:06:26,354
three and influxdb in general is a time series database

101
00:06:26,402 --> 00:06:29,674
platform, and it is built

102
00:06:29,792 --> 00:06:32,170
on Apache Arrow,

103
00:06:32,830 --> 00:06:36,394
arrow, flight data fusion, and parquet. And I'll talk

104
00:06:36,432 --> 00:06:39,786
in detail about what all these are. But one thing that is at our

105
00:06:39,808 --> 00:06:43,610
core is that we believe in open architecture and open data format,

106
00:06:43,690 --> 00:06:46,640
and we believe that these technologies really enable this.

107
00:06:49,090 --> 00:06:52,706
But before I go into those technologies, I just wanted to also highlight some of

108
00:06:52,728 --> 00:06:56,274
our customers, just to give some context around how influxDB is used.

109
00:06:56,392 --> 00:07:00,386
So we're primarily used in IoT monitoring, but also

110
00:07:00,568 --> 00:07:03,890
in application monitoring and software monitoring.

111
00:07:04,390 --> 00:07:08,210
So for some examples, Wayfair uses us for application monitoring.

112
00:07:08,370 --> 00:07:12,150
Tesla uses us to monitor all of their batteries,

113
00:07:13,210 --> 00:07:17,078
their wall batteries, b box, which is not a

114
00:07:17,084 --> 00:07:20,806
logo that's up here, but a company that I think is really cool develops

115
00:07:20,838 --> 00:07:24,010
and manufactures products to provide affordable, clean solar

116
00:07:24,080 --> 00:07:29,740
energy to off grid communities in developing countries. And we

117
00:07:30,290 --> 00:07:33,966
help them monitor all their solar panels. We have companies

118
00:07:34,068 --> 00:07:37,374
that are doing indoor agriculture that are using us,

119
00:07:37,572 --> 00:07:40,602
community members that are monitoring endangered

120
00:07:40,666 --> 00:07:43,966
birds from also community members that are

121
00:07:43,988 --> 00:07:47,886
monitoring their barbecue at home. So there's so many different use cases.

122
00:07:47,918 --> 00:07:51,300
But the one thread is that it all involves time series data.

123
00:07:52,150 --> 00:07:55,894
And just to kind of highlight just the high throughput requirements that

124
00:07:55,932 --> 00:07:59,080
time series requires and that influxDB can provide.

125
00:07:59,930 --> 00:08:03,766
So the latest benchmark for our latest version of

126
00:08:03,788 --> 00:08:08,086
influxdb v three, which is built upon data fusion,

127
00:08:08,198 --> 00:08:11,050
Apache Arrow, arrow and parquet.

128
00:08:11,390 --> 00:08:15,050
If we had a dimensionality of 160,000,

129
00:08:15,120 --> 00:08:19,900
we are able to ingest around

130
00:08:20,750 --> 00:08:24,046
329,000,000 rows per hour,

131
00:08:24,148 --> 00:08:27,326
or 4 million values per

132
00:08:27,348 --> 00:08:30,506
second. So that's really what we're looking at when we're

133
00:08:30,618 --> 00:08:34,226
thinking about the high volume use cases that time

134
00:08:34,248 --> 00:08:36,660
series datasets like influxDB can provide.

135
00:08:37,510 --> 00:08:42,258
So let's talk about this commitment to open data architecture with

136
00:08:42,424 --> 00:08:45,454
Apache Arrow, Apache Arrow, flight data fusion,

137
00:08:45,502 --> 00:08:49,026
and Parquet. So what we mean by this is just the ability

138
00:08:49,138 --> 00:08:52,866
to have good interoperability with a bunch of other tools

139
00:08:52,898 --> 00:08:56,774
and really easily transport data to and from other sources so

140
00:08:56,812 --> 00:09:00,774
that people have the ability to develop their architecture

141
00:09:00,822 --> 00:09:04,762
with the tools that are most aligned with the project. And the problem

142
00:09:04,816 --> 00:09:08,106
that they're trying to solve. And the

143
00:09:08,128 --> 00:09:11,678
way that this stack enables influxdb to do this is

144
00:09:11,844 --> 00:09:15,390
that essentially Apache Arrow

145
00:09:15,730 --> 00:09:19,706
is an in columnar

146
00:09:19,738 --> 00:09:23,886
memory format for defining data. And Apache

147
00:09:23,918 --> 00:09:28,094
Arrow flight is a way to transport ledge data sets like arrow over network

148
00:09:28,142 --> 00:09:32,580
interface. Parquet influxdb uses Parquet as the

149
00:09:33,750 --> 00:09:37,346
durable file format on disk. It's also columnar,

150
00:09:37,458 --> 00:09:41,186
and data fusion is the query execution framework that allows

151
00:09:41,298 --> 00:09:45,250
developers to query influxDB v three in both SQL and influxdb.

152
00:09:45,330 --> 00:09:50,146
InfluxDB happens to be a SQL like language

153
00:09:50,178 --> 00:09:53,638
that's specific to influxdB, but essentially what it allows

154
00:09:53,734 --> 00:09:57,334
developers to do is ingest, store and analyze all types of time series

155
00:09:57,382 --> 00:10:01,030
data, handle this at a really high speed and high volume,

156
00:10:01,110 --> 00:10:04,698
and then also have the ability to have increase interoperability with a bunch

157
00:10:04,714 --> 00:10:07,370
of tools, and be part of the Apache ecosystem.

158
00:10:07,530 --> 00:10:11,450
Which means that when you contribute to these upstream projects,

159
00:10:11,530 --> 00:10:14,882
then so many other tools benefit from it. And any other tools that are also

160
00:10:14,936 --> 00:10:18,306
leveraging these technologies means that you can have a

161
00:10:18,328 --> 00:10:22,194
standard for basically transporting these

162
00:10:22,232 --> 00:10:25,290
data sets to and from all of these different tools.

163
00:10:25,390 --> 00:10:29,074
So we think of things like Dremio leverages,

164
00:10:29,122 --> 00:10:31,510
Apache Arrow and Apache Arrow flight.

165
00:10:32,650 --> 00:10:36,898
A whole bunch of machine learning tools leverage things like Parquet,

166
00:10:37,074 --> 00:10:39,882
I know h 20 does, I know Google,

167
00:10:39,936 --> 00:10:44,170
Bigquery does, I believe Cassandra.

168
00:10:44,510 --> 00:10:48,038
So so many different tools that you can use. If you can extract parquet

169
00:10:48,054 --> 00:10:52,190
files from influxdb and then quickly leverage them in other use cases and other tools,

170
00:10:52,690 --> 00:10:56,526
you just have access to them

171
00:10:56,548 --> 00:10:58,720
and then you can build the architecture as you need.

172
00:10:59,570 --> 00:11:03,082
The other thing that these tools enable for influxDB specifically,

173
00:11:03,226 --> 00:11:06,446
for example, is it allows you to have schema on

174
00:11:06,468 --> 00:11:09,986
write. So schema on write means you don't have to define your schema beforehand so

175
00:11:10,008 --> 00:11:13,518
you can modify the schema as you go. Like we said, it allows

176
00:11:13,534 --> 00:11:17,506
you to query and write millions of rows per second. Since we're

177
00:11:17,538 --> 00:11:21,234
on the bleeding edge of throughput

178
00:11:21,282 --> 00:11:24,854
through the use of this columnar store and also

179
00:11:24,892 --> 00:11:28,746
influxdB is a database purpose built for handling time

180
00:11:28,768 --> 00:11:30,650
series data at a massive scale.

181
00:11:32,030 --> 00:11:35,482
And yeah, that's pretty much that.

182
00:11:35,536 --> 00:11:38,950
So what I wanted to do is take a step back and kind of highlight

183
00:11:39,030 --> 00:11:42,346
why leveraging arrow, Apache Arrow specifically

184
00:11:42,378 --> 00:11:47,294
is so important, because it is that way that we are defining our

185
00:11:47,332 --> 00:11:51,226
data in memory in this columnar format. So the reason why columnar

186
00:11:51,258 --> 00:11:55,038
data works so well specifically for influxdB and helps us achieve

187
00:11:55,054 --> 00:11:58,802
this throughput is that imagine that we had this data

188
00:11:58,856 --> 00:12:02,654
that we were writing to influxdb. This just happens to be the ingest format

189
00:12:02,702 --> 00:12:06,118
for influxdB. Well,

190
00:12:06,204 --> 00:12:09,602
we might have different measurements, which are essentially

191
00:12:09,666 --> 00:12:13,110
tables, and we want to write different fields with a timestamp and

192
00:12:13,180 --> 00:12:16,566
some metadata. So this

193
00:12:16,588 --> 00:12:20,266
is maybe what a table would look like once we wrote this data.

194
00:12:20,368 --> 00:12:24,186
And you can imagine that if we wanted to query this

195
00:12:24,208 --> 00:12:27,242
data for like a max value for our field one,

196
00:12:27,296 --> 00:12:31,550
let's say if we store our data in a row format,

197
00:12:32,610 --> 00:12:36,094
we would run into some problems, which is that

198
00:12:36,212 --> 00:12:39,882
we would have to iterate through every single row,

199
00:12:39,946 --> 00:12:43,662
essentially, and every single column

200
00:12:43,726 --> 00:12:47,358
in order to try and find the max value associated

201
00:12:47,454 --> 00:12:50,626
with one column. But if you store things in

202
00:12:50,648 --> 00:12:52,420
a columnar fashion instead,

203
00:12:54,230 --> 00:12:57,606
then it looks like this. In other words, the data would be

204
00:12:57,628 --> 00:13:01,286
stored like the formatted block. And the other thing to

205
00:13:01,308 --> 00:13:04,502
note too is that neighboring values are the same data

206
00:13:04,556 --> 00:13:07,730
type and oftentimes also the same value

207
00:13:07,820 --> 00:13:11,658
itself. This is especially true for time series data when you

208
00:13:11,664 --> 00:13:15,478
are monitoring things like your environment, where you're

209
00:13:15,654 --> 00:13:18,854
maybe measuring the temperature at a minute interval.

210
00:13:18,902 --> 00:13:22,458
Oftentimes the temperature in a specific environment stays the same for periods

211
00:13:22,474 --> 00:13:26,074
of time. So what does this mean? This provides opportunity for really cheap

212
00:13:26,122 --> 00:13:29,546
compression, which enables these really high cardinality,

213
00:13:29,578 --> 00:13:33,578
high volume use cases that we're talking about. This also enables

214
00:13:33,674 --> 00:13:37,250
faster scan writes by using SMD instructions.

215
00:13:38,470 --> 00:13:41,778
So depending on how your data is stored, you may only have

216
00:13:41,784 --> 00:13:45,394
to look at the first column of data to find the max value

217
00:13:45,432 --> 00:13:49,158
of a particular field. Contrast to that row Arrington storage, where you

218
00:13:49,164 --> 00:13:52,566
have to look at every field,

219
00:13:52,668 --> 00:13:56,262
every tag, and every time set in order to find the max field

220
00:13:56,316 --> 00:14:00,138
value for that one column. So that's just a little sidebar of

221
00:14:00,304 --> 00:14:04,282
why columnar storage specifically suits things

222
00:14:04,336 --> 00:14:08,054
like influxDb and why Arrow is really purpose

223
00:14:08,102 --> 00:14:12,134
built for influxdB. So now let's talk about actually

224
00:14:12,272 --> 00:14:15,854
leveraging the aeroflight client with influxDB and also

225
00:14:15,892 --> 00:14:19,626
the Python client library. So this is what the aeroflight client looks like, the Python

226
00:14:19,658 --> 00:14:23,226
Arrow flight client library. To install it, you do something like PiP,

227
00:14:23,258 --> 00:14:26,874
install Pyro, and you can query a database

228
00:14:26,922 --> 00:14:30,366
like influxdb, three, V three, or any other database that leverages

229
00:14:30,398 --> 00:14:33,794
aeroflight. And what you would do is you'd first create a

230
00:14:33,832 --> 00:14:37,746
flight client by passing in the URL and instantiate

231
00:14:37,778 --> 00:14:40,998
after you've imported the library. And then you write a

232
00:14:41,004 --> 00:14:44,406
JSOn that has the necessary information for running the

233
00:14:44,428 --> 00:14:48,694
query on a specific platform, including the namespace name,

234
00:14:48,812 --> 00:14:52,860
the SQL query that you want to use, and the query type.

235
00:14:53,790 --> 00:14:57,766
So notice here how we are specifying the SQL query. So if you're querying

236
00:14:57,798 --> 00:15:01,478
influxdB exclusively, you could easily switch this between SQL

237
00:15:01,494 --> 00:15:05,066
and influxDB, for example, because you can query influxdb

238
00:15:05,098 --> 00:15:08,926
with both. And so this makes aeroflight a

239
00:15:08,948 --> 00:15:13,650
more convenient tool for querying influxDB specifically or any other database,

240
00:15:14,470 --> 00:15:17,362
just because you have that option to specify the query type.

241
00:15:17,496 --> 00:15:21,490
And this boilerplate is basically query is database agnostic.

242
00:15:23,590 --> 00:15:26,802
And this is what it would look like to use the arrow flight SQL client.

243
00:15:26,866 --> 00:15:30,306
So the arrow flight SQL client basically just wraps

244
00:15:30,418 --> 00:15:36,102
the arrow flight client and the

245
00:15:36,156 --> 00:15:39,986
protocol here is pretty similar. We just instantiate a flight SQL client that's

246
00:15:40,018 --> 00:15:43,142
configured for a particular database. Then we execute

247
00:15:43,206 --> 00:15:46,534
a query to retrieve the flight info. We extract a token

248
00:15:46,582 --> 00:15:50,642
for retrieving the data, use a ticket to request an arrow data stream,

249
00:15:50,726 --> 00:15:54,106
and in this example, we are returning the flight

250
00:15:54,138 --> 00:15:57,486
stream reader for the streaming results. We're reading all the data to a

251
00:15:57,508 --> 00:16:00,670
pirate arrow table and printing that table.

252
00:16:02,210 --> 00:16:05,902
So another important thing to note though too is that about the flight SQL client

253
00:16:05,966 --> 00:16:09,074
specifically is that it returns a stream of data.

254
00:16:09,272 --> 00:16:13,138
The return of the streams of data differ

255
00:16:13,224 --> 00:16:17,026
a good amount between different languages. So that's just something to keep in

256
00:16:17,048 --> 00:16:21,094
mind. It can be a little bit harder to use and a little bit less

257
00:16:21,132 --> 00:16:24,694
flexible, especially if you want to query a variety of different

258
00:16:24,732 --> 00:16:28,358
databases and you want to use kind of the same boilerplate in your

259
00:16:28,364 --> 00:16:31,702
Python script to query any data

260
00:16:31,756 --> 00:16:34,150
stores that leverage aeroflight.

261
00:16:36,430 --> 00:16:39,606
And then this is what it looks like to use the influxdb v three Python

262
00:16:39,638 --> 00:16:43,310
client library. So this basically just wraps the arrow flight client.

263
00:16:43,810 --> 00:16:46,766
And we would first import our library like we would do before.

264
00:16:46,868 --> 00:16:51,274
Then we initialize our client for influxdb. That includes providing the

265
00:16:51,322 --> 00:16:55,166
iD, the URL that influxdB

266
00:16:55,278 --> 00:16:58,894
is being run on the database

267
00:16:58,942 --> 00:17:02,434
name that we are querying data from. We initialize that

268
00:17:02,472 --> 00:17:06,278
client, we provide a SQL query, and then we

269
00:17:06,364 --> 00:17:09,874
actually return our query results. And in the client

270
00:17:09,922 --> 00:17:13,926
method or the query method, you can specify if you

271
00:17:13,948 --> 00:17:17,570
want to use SQL or influxdb. You can also specify

272
00:17:17,650 --> 00:17:21,020
the mode that you want to return your data back in. And that could be

273
00:17:21,790 --> 00:17:25,674
supports both polars and pandas. So you can return a polars or pandas data

274
00:17:25,712 --> 00:17:29,030
frame directly and that just increases interoperability

275
00:17:29,110 --> 00:17:32,542
with a bunch of python libraries that you can use for

276
00:17:32,596 --> 00:17:35,470
things like anomaly detection and forecasting.

277
00:17:37,970 --> 00:17:41,840
So before I go, I want to talk about some projects that leverage the

278
00:17:42,390 --> 00:17:43,810
aeroflight client.

279
00:17:46,070 --> 00:17:49,234
So the first one is that

280
00:17:49,272 --> 00:17:52,674
involve using Grafana and influxdb. So one thing we

281
00:17:52,712 --> 00:17:56,438
did was to contribute this plugin to Grafana, which is

282
00:17:56,444 --> 00:17:59,986
the flight SQL plugin. And so you can use the flight SQL

283
00:18:00,018 --> 00:18:04,050
connector to connect any data store that leverages aeroflight

284
00:18:04,130 --> 00:18:09,434
to or

285
00:18:09,472 --> 00:18:12,618
that leverages arrow to Grafana. So this is kind of

286
00:18:12,624 --> 00:18:16,646
what we mean by our commitment to open data architecture. By contributing

287
00:18:16,678 --> 00:18:20,334
this plugin, we provide this benefit to a number

288
00:18:20,372 --> 00:18:23,310
of different open source tools that leverage aeroflight.

289
00:18:24,930 --> 00:18:28,302
And I want to encourage you too to visit the

290
00:18:28,356 --> 00:18:31,150
influx community organization on GitHub.

291
00:18:32,130 --> 00:18:36,274
That organization just has a bunch of different POCs and demos for

292
00:18:36,312 --> 00:18:40,194
how to use influxdb with a variety of different tools. So one

293
00:18:40,232 --> 00:18:43,842
repo in specific is the Grafana Quickstart, which shows

294
00:18:43,896 --> 00:18:47,334
you how to use that flight SQL plugin, but also how to build

295
00:18:47,372 --> 00:18:50,722
dashboards and query influxdb using SQL,

296
00:18:50,786 --> 00:18:55,714
and also how to leverage the influxdb Grafana

297
00:18:55,762 --> 00:18:59,930
v three plugin too, which is specific to just influxdb.

298
00:19:00,590 --> 00:19:04,406
Another fun project is influxdb that uses influxdb

299
00:19:04,438 --> 00:19:07,866
and mage. So Mage is an open source data pipelining tool for

300
00:19:07,888 --> 00:19:11,518
transforming and integrating data. In essence, you can think

301
00:19:11,524 --> 00:19:14,794
of it as an open source alternative to Apache Airflow,

302
00:19:14,922 --> 00:19:19,194
and it contains a UI that simplifies the ETL creation

303
00:19:19,242 --> 00:19:22,538
process, and it has documentation on

304
00:19:22,564 --> 00:19:25,422
how to deploy these pipelines on AWS,

305
00:19:25,486 --> 00:19:28,514
azure, digitalocean, and GCP. I think they

306
00:19:28,552 --> 00:19:32,142
provide both terraform and helm charts so you can leverage

307
00:19:32,206 --> 00:19:35,830
those. And one specific demo

308
00:19:35,900 --> 00:19:39,030
that we have in the influx community organization

309
00:19:39,370 --> 00:19:43,078
is this one on anomaly detection, where we have

310
00:19:43,244 --> 00:19:47,206
some machines generating some different data

311
00:19:47,388 --> 00:19:51,366
and we use half space trees to identify anomalous behavior

312
00:19:51,398 --> 00:19:55,258
in the machine data and actually allow you to use a

313
00:19:55,264 --> 00:19:59,210
little interface to generate the anomalies in real time

314
00:19:59,280 --> 00:20:02,814
and also get alerts on those

315
00:20:02,852 --> 00:20:06,110
anomalies. So I encourage you to try it yourself

316
00:20:06,180 --> 00:20:11,002
here. Another really cool solution

317
00:20:11,066 --> 00:20:14,686
or project is one that uses quicks. So quicks is similar to

318
00:20:14,708 --> 00:20:18,526
mage in that it's also a solution for building and deploying monitoring

319
00:20:18,558 --> 00:20:22,066
event streaming applications. But the cool thing is that it uses Kafka under

320
00:20:22,088 --> 00:20:25,954
the hood and allows you to control all the elements

321
00:20:26,002 --> 00:20:30,034
of your pipeline with just Python. So it's specifically

322
00:20:30,082 --> 00:20:33,798
designed for processing time series data,

323
00:20:33,964 --> 00:20:37,106
and it comes both in cloud and on prem offerings.

324
00:20:37,218 --> 00:20:41,798
And it also offers a UI that simplifies this ETL processing

325
00:20:41,974 --> 00:20:45,130
building to make that much simpler.

326
00:20:46,270 --> 00:20:49,562
But the really cool thing about it is that it really is

327
00:20:49,616 --> 00:20:53,786
specifically built to handle event streaming and leverages

328
00:20:53,818 --> 00:20:56,794
Kafka so that you don't have to be a domain expert in how to leverage

329
00:20:56,842 --> 00:21:00,650
it. And this particular demo also leverages hive MQ,

330
00:21:00,810 --> 00:21:04,098
which is an MQTT broker. So we get data from a

331
00:21:04,104 --> 00:21:07,534
lot of different machines using MQTT, pass them into HiveMQ

332
00:21:07,582 --> 00:21:11,186
that broker, then centralize all that data in quicks and

333
00:21:11,288 --> 00:21:15,098
perform anomaly detection there. And also quicks

334
00:21:15,134 --> 00:21:18,534
has built in integration with hugging face, which is basically like

335
00:21:18,732 --> 00:21:22,982
sort of a GitHub on steroids for data

336
00:21:23,036 --> 00:21:26,294
scientists where they can publish any

337
00:21:26,332 --> 00:21:30,006
of their algorithms or anomaly

338
00:21:30,038 --> 00:21:34,890
detection or forecasting tools

339
00:21:35,310 --> 00:21:38,874
and algorithms there. And so yeah,

340
00:21:38,912 --> 00:21:42,346
Quix does all of the anomaly detection there after

341
00:21:42,448 --> 00:21:46,346
pulling data out of influxDB, where all

342
00:21:46,368 --> 00:21:50,266
of the MQTT data is written directly to influxDB and performs this anomaly

343
00:21:50,298 --> 00:21:55,086
detection and then also creates

344
00:21:55,118 --> 00:21:59,566
alerts. And in this example, we use auto

345
00:21:59,598 --> 00:22:03,358
encoder instead, which is a type of neural

346
00:22:03,374 --> 00:22:08,470
network that is unsupervised.

347
00:22:10,970 --> 00:22:14,390
And yeah, this is essentially what the architecture for this project looks like.

348
00:22:14,460 --> 00:22:18,410
We have all of these machines that are generating and these

349
00:22:18,480 --> 00:22:23,062
robot arms in this example are also generating

350
00:22:23,126 --> 00:22:28,646
data, pushing that data to hive

351
00:22:28,678 --> 00:22:32,122
MQ with MQTT, and then we use MQTT

352
00:22:32,186 --> 00:22:35,466
clients to write the data to influxdb.

353
00:22:35,578 --> 00:22:38,670
We take advantage of quicks to

354
00:22:38,740 --> 00:22:42,046
actually perform the querying and apply the machine learning model which

355
00:22:42,068 --> 00:22:45,546
is stored in hugging face. And then we use Grafana

356
00:22:45,578 --> 00:22:49,086
to visualize all the data. So the cool thing about this solution architecture

357
00:22:49,118 --> 00:22:53,054
is that while we're only performing this on three types of machines, it could easily

358
00:22:53,102 --> 00:22:56,754
be scaled out for a real world use

359
00:22:56,792 --> 00:22:59,750
case. And so yeah, in general,

360
00:22:59,820 --> 00:23:04,114
I recommend that you check out this demo, but also a variety of other demos

361
00:23:04,162 --> 00:23:07,862
that exist at influx community. If you want to learn more about

362
00:23:07,996 --> 00:23:11,878
how we're leveraging Apache Aero data fusion,

363
00:23:12,054 --> 00:23:15,974
parquet, and aeroflight to increase

364
00:23:16,022 --> 00:23:19,178
interoperability with other tools and provide solutions like some of

365
00:23:19,184 --> 00:23:22,518
the projects that I mentioned here today. Last but not least,

366
00:23:22,544 --> 00:23:25,966
I want to encourage you to join the influxDB community. You can get started with

367
00:23:25,988 --> 00:23:29,934
influxDb by visiting influxdata.com. You can

368
00:23:29,972 --> 00:23:33,578
also check out our documentation. InfluxDB University is a resource

369
00:23:33,674 --> 00:23:37,246
that offers free and live training on a variety of

370
00:23:37,268 --> 00:23:41,102
different topics, including some of the projects that I mentioned

371
00:23:41,156 --> 00:23:44,718
today. And also please join our community. You can join our

372
00:23:44,724 --> 00:23:47,718
community slack at influxcommunity, slack.com,

373
00:23:47,884 --> 00:23:51,206
or also our forums as well,

374
00:23:51,228 --> 00:23:54,486
our discourse forums. So it's entirely up to you what you

375
00:23:54,508 --> 00:23:56,100
want to use. Thank you so much.

