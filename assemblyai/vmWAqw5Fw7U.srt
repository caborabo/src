1
00:00:23,400 --> 00:00:27,232
We are gonna be basically going through an introduction to genaid

2
00:00:27,278 --> 00:00:31,004
and what it is, along with applications of Gen II across

3
00:00:31,084 --> 00:00:35,124
different facets of life. Then we will try to understand different

4
00:00:35,204 --> 00:00:39,052
key security risks in gen AI models. Then after

5
00:00:39,108 --> 00:00:42,788
that, we will go through real world case studies and how

6
00:00:42,836 --> 00:00:46,524
Gen AI is used in governance, risk management and

7
00:00:46,564 --> 00:00:50,020
compliance. We will then conclude by understanding the

8
00:00:50,052 --> 00:00:53,276
ethical and legal considerations of AI as a

9
00:00:53,300 --> 00:00:56,594
technology. So let's start by understanding what

10
00:00:56,634 --> 00:01:01,266
generative AI is. So, unlike traditional AI

11
00:01:01,330 --> 00:01:05,250
systems, which primarily analyze data to make decisions,

12
00:01:05,362 --> 00:01:08,738
like suggesting what movie you might enjoy next

13
00:01:08,826 --> 00:01:12,442
based on your viewing history, generative AI goes a step

14
00:01:12,498 --> 00:01:16,090
further. It uses what it has learned from different

15
00:01:16,282 --> 00:01:20,334
large amounts of data to create new, original content.

16
00:01:20,514 --> 00:01:24,102
This could be anything from a piece of music to a realistic

17
00:01:24,158 --> 00:01:27,926
image of a person who doesn't exist. So imagine

18
00:01:27,990 --> 00:01:31,830
a painter who learns by studying thousands of paintings,

19
00:01:31,982 --> 00:01:35,198
and then he or she starts creating unique art.

20
00:01:35,326 --> 00:01:39,078
That's similar to how generative AI works. It uses

21
00:01:39,126 --> 00:01:43,262
techniques based on advanced algorithms, specifically something

22
00:01:43,358 --> 00:01:47,326
called neural networks. So these neural networks are designed

23
00:01:47,350 --> 00:01:50,000
to mimic the way human brains operate.

24
00:01:50,182 --> 00:01:53,504
Another important technique involves what we call

25
00:01:53,844 --> 00:01:57,224
generative adversarial networks, or gans,

26
00:01:58,644 --> 00:02:02,508
where two neural networks contest with each other to

27
00:02:02,556 --> 00:02:05,844
improve the final output. Now, think of how

28
00:02:05,884 --> 00:02:09,996
this technology can be applied across different sectors.

29
00:02:10,140 --> 00:02:13,708
In healthcare, generative AI can be used to analyze

30
00:02:13,756 --> 00:02:18,102
data from thousands of patients to propose customized treatments,

31
00:02:18,198 --> 00:02:20,942
or simulate how drugs might work.

32
00:02:21,118 --> 00:02:24,814
Consider this scenario where there's sudden epidemic.

33
00:02:24,974 --> 00:02:29,646
So in the absence of lab rats,

34
00:02:29,750 --> 00:02:33,206
or maybe lab test rats that may be used,

35
00:02:33,350 --> 00:02:37,030
this software can be used to minimize the go to market

36
00:02:37,102 --> 00:02:40,750
time of a specific drug. This might be very helpful in

37
00:02:40,782 --> 00:02:43,896
saving thousands or millions of lives.

38
00:02:44,080 --> 00:02:47,776
Now let's consider the application of genai in media.

39
00:02:47,920 --> 00:02:51,888
In media, genai can be automatically used to generate new

40
00:02:51,936 --> 00:02:55,104
content for games or special effects in movies,

41
00:02:55,264 --> 00:02:59,120
thereby drastically reducing the time and cost involved

42
00:02:59,152 --> 00:03:02,872
in creative processes. Just think, the amount of time

43
00:03:02,928 --> 00:03:07,004
and people that are involved in creating an animation movie.

44
00:03:07,144 --> 00:03:10,300
So with Genai, these processes will

45
00:03:10,332 --> 00:03:14,404
be more efficient and more streamlined, and it will also

46
00:03:14,484 --> 00:03:18,292
help enhance and bring movies more closer

47
00:03:18,348 --> 00:03:21,964
to the fans who are expecting movies to be

48
00:03:22,084 --> 00:03:25,748
released at a faster scale. But why is security so

49
00:03:25,796 --> 00:03:29,468
crucial in Genai? So, as we depend more on AI

50
00:03:29,516 --> 00:03:33,500
to make decisions or create content, ensuring these systems

51
00:03:33,532 --> 00:03:37,094
are not only effective, but also secure and unbiased

52
00:03:37,134 --> 00:03:40,846
becomes paramount. If the training data is flawed,

53
00:03:40,950 --> 00:03:45,070
say it's biased against certain groups, those biases

54
00:03:45,142 --> 00:03:48,510
will definitely be reflected in the AI's output.

55
00:03:48,662 --> 00:03:52,190
Ensuring security in AI systems means maintaining

56
00:03:52,222 --> 00:03:55,514
the integrity and fairness of what AI creates,

57
00:03:56,214 --> 00:03:59,358
which is essential for gaining and keeping

58
00:03:59,406 --> 00:04:03,074
public trust. With that, let's go to different security

59
00:04:03,194 --> 00:04:06,378
attacks in Genai. First off, let's start

60
00:04:06,426 --> 00:04:09,890
with data poisoning attacks in Genai. Now, what is

61
00:04:09,922 --> 00:04:13,314
jet data poisoning attacked in Genai? So these kind

62
00:04:13,354 --> 00:04:17,434
of attacks basically target the training data of AI models,

63
00:04:17,594 --> 00:04:21,154
subtly inserting or manipulating information to affect

64
00:04:21,194 --> 00:04:25,122
the model's learning process, thereby causing it to make

65
00:04:25,178 --> 00:04:28,394
errors or bias judgments post deployment.

66
00:04:28,554 --> 00:04:32,626
So the mechanics for data poisoning attack would

67
00:04:32,650 --> 00:04:36,218
be by introducing corrupted, misleading, or specifically

68
00:04:36,386 --> 00:04:40,050
design data into a training dataset, attackers can

69
00:04:40,082 --> 00:04:43,722
skew an AI models output, making it unreliable or

70
00:04:43,778 --> 00:04:47,754
incorrect. For example, altering a street sign image

71
00:04:47,834 --> 00:04:51,426
used in training an autonomous vehicle's AI could cause the

72
00:04:51,450 --> 00:04:54,824
vehicle to misrecognize stop signs, as in signs.

73
00:04:54,994 --> 00:04:58,492
What would be the impacts and examples of data

74
00:04:58,548 --> 00:05:01,676
poisoning attacks? The consequences of such attacks would

75
00:05:01,700 --> 00:05:05,268
be definitely severe, especially in applications where

76
00:05:05,316 --> 00:05:08,884
like finance, where predictive models could determine credit

77
00:05:08,924 --> 00:05:12,516
worthiness, or in healthcare, where diagnostic accuracy

78
00:05:12,580 --> 00:05:17,144
is critical. We previously went through a traffic self learning

79
00:05:17,924 --> 00:05:21,444
traffic slash traffic example where self learning cars

80
00:05:21,484 --> 00:05:25,152
use the data to, to train

81
00:05:25,208 --> 00:05:28,928
themselves. The stakes here are much more higher because human life

82
00:05:28,976 --> 00:05:32,744
is involved. But what are the prevention and mitigation techniques for

83
00:05:32,784 --> 00:05:36,608
such an attack? So, solutions include robust

84
00:05:36,656 --> 00:05:40,408
data validation at the start before training them, making sure

85
00:05:40,496 --> 00:05:44,344
we use only trusted data sources to train the AI model,

86
00:05:44,464 --> 00:05:47,928
as well as implementing anomaly detection during

87
00:05:47,976 --> 00:05:51,168
the training phase to identify and mitigate suspicious

88
00:05:51,216 --> 00:05:54,832
data patterns. The next type of attack we are

89
00:05:54,848 --> 00:05:57,864
going to be looking at is called as model inversion attack.

90
00:05:58,024 --> 00:06:01,624
So what is a model inversion attack? So these kind of attacks

91
00:06:01,664 --> 00:06:05,328
basically focus on exploiting a model to relieve

92
00:06:05,376 --> 00:06:08,720
sensitive information about retraining data. So this is

93
00:06:08,752 --> 00:06:12,464
particularly concerning where models handle private data like personal

94
00:06:12,544 --> 00:06:16,336
identifiers or medical records. So the mechanics

95
00:06:16,400 --> 00:06:20,160
for such an attack. So by querying such an AI system with

96
00:06:20,192 --> 00:06:23,514
grafted inputs, attackers can reconstruct or

97
00:06:23,554 --> 00:06:27,306
infer data about individuals that were used in the model's

98
00:06:27,330 --> 00:06:31,034
training dataset, thereby effectively inverting the model's

99
00:06:31,074 --> 00:06:34,738
output to reveal the private information. By knowing

100
00:06:34,786 --> 00:06:38,458
certain private information, it's possible for the attacker

101
00:06:38,586 --> 00:06:42,482
to decipher the relationship and connections between different pieces

102
00:06:42,538 --> 00:06:46,466
of information. It doesn't take long to recognize

103
00:06:46,530 --> 00:06:50,652
most of our information is on social media. Just getting any

104
00:06:50,708 --> 00:06:54,364
one person identifying information can be dangerous

105
00:06:54,444 --> 00:06:58,588
and can be exposing the personal information of

106
00:06:58,756 --> 00:07:02,876
people. So what would be in the impact of such an attack, such as

107
00:07:02,900 --> 00:07:06,260
model inversion attack? So the impact of such an exam,

108
00:07:06,412 --> 00:07:09,724
such an attack would be in healthcare, for example,

109
00:07:09,844 --> 00:07:13,380
reconstructing patients faces from a model trained on

110
00:07:13,412 --> 00:07:17,312
medical images can be very dangerous

111
00:07:17,448 --> 00:07:21,016
because this could either misclassify or

112
00:07:21,160 --> 00:07:24,816
misreconstruct the patient's face, and thereby allowing

113
00:07:24,840 --> 00:07:28,304
the doctor to wrongly diagnose a patient for disease that they were not

114
00:07:28,344 --> 00:07:31,808
meant to imagine. If somebody is being diagnosed

115
00:07:31,856 --> 00:07:34,984
with another person's disease diagnosis,

116
00:07:35,104 --> 00:07:39,112
which could be life threatening for the specific person, and it has the

117
00:07:39,128 --> 00:07:42,642
ability to cause. So what would be

118
00:07:42,658 --> 00:07:46,154
the techniques for prevention and mitigation here? So,

119
00:07:46,194 --> 00:07:49,922
in this case, the techniques used to counter these attacks include limiting

120
00:07:49,978 --> 00:07:53,450
the amount of information the models output, as well as

121
00:07:53,522 --> 00:07:56,898
applying different privacy methods to obscure

122
00:07:56,946 --> 00:07:59,970
training data. The next type of attack we're going to be

123
00:08:00,002 --> 00:08:03,314
going through are called adversarial attacks in Genai.

124
00:08:03,474 --> 00:08:07,186
In adversarial attacks, they involve manipulating the input

125
00:08:07,250 --> 00:08:10,354
to the AI systems in a certain way that induce the model

126
00:08:10,394 --> 00:08:13,748
to make errors. So these attacks are often difficult

127
00:08:13,796 --> 00:08:18,224
to predict as they exploit the model's inherent weaknesses.

128
00:08:18,684 --> 00:08:21,868
So the mechanics for such an attack would be they

129
00:08:21,916 --> 00:08:25,064
are small, calculated changes to the input data,

130
00:08:25,684 --> 00:08:29,948
such that adding noise to image files

131
00:08:29,996 --> 00:08:33,748
that are impossible, imperceptible to the humans can

132
00:08:33,796 --> 00:08:38,104
lead to AI misclassifying these images or making erroneous

133
00:08:38,824 --> 00:08:42,208
decisions. An example here would be subtly

134
00:08:42,296 --> 00:08:46,232
ordering images that could trick a facial recognition system into

135
00:08:46,288 --> 00:08:50,136
misclassifying individuals, compromising security systems.

136
00:08:50,280 --> 00:08:54,504
Imagine what could happen if a facial recognition algorithm could

137
00:08:54,584 --> 00:08:57,816
classify an innocent person as a criminal and the

138
00:08:57,840 --> 00:09:01,976
criminal as an innocent person. This could have life changing consequences

139
00:09:02,080 --> 00:09:05,900
in both lives of both people. So what would be the prevention

140
00:09:05,932 --> 00:09:10,068
of such attacks? So, defenses for such attacks include adversarial

141
00:09:10,116 --> 00:09:14,164
training, where models are trained with adversarial examples to

142
00:09:14,204 --> 00:09:18,996
learn to withstand them, and implementing input validation to

143
00:09:19,020 --> 00:09:22,484
detect and filter out manipulated inputs. The last

144
00:09:22,524 --> 00:09:26,652
type of attack we're going to be going through is called backdoor attacks.

145
00:09:26,748 --> 00:09:30,662
So in backdoor attacks, basically what happens is

146
00:09:30,828 --> 00:09:34,178
when attackers introduce malicious

147
00:09:34,306 --> 00:09:38,154
functionality in a secret embedded manner during

148
00:09:38,234 --> 00:09:41,946
AI training phase, which later becomes active in response

149
00:09:41,970 --> 00:09:45,458
to a specific input trigger. The attacker

150
00:09:45,506 --> 00:09:49,554
basically introduces a trigger during the training phase that causes

151
00:09:49,594 --> 00:09:53,426
the model to act in a predefined malicious way when the model

152
00:09:53,490 --> 00:09:57,118
encounters the trigger during the operation. An example

153
00:09:57,166 --> 00:10:00,414
of a backdoor attack would be a surveillance system that could

154
00:10:00,454 --> 00:10:04,222
make it ignore specific individuals, effectively creating a

155
00:10:04,238 --> 00:10:07,534
security loophole. So the prevention and mitigation of

156
00:10:07,574 --> 00:10:11,286
such attacks that include rigorous inspection of training

157
00:10:11,350 --> 00:10:15,182
data, continuous monitoring of model behavior, and applying

158
00:10:15,238 --> 00:10:18,838
techniques to detect anomalies and models response. Now,

159
00:10:18,886 --> 00:10:22,274
let's go through different case studies where Gen A.

160
00:10:24,864 --> 00:10:28,368
So in case study one, we have a deepfake

161
00:10:28,496 --> 00:10:32,360
misuse. So the situation here is somebody is

162
00:10:32,392 --> 00:10:36,008
using deepfake technology to create realistically yet

163
00:10:36,136 --> 00:10:39,600
fabricated videos of public figures making contribution

164
00:10:39,672 --> 00:10:43,408
statements. In this case, it's a

165
00:10:43,456 --> 00:10:46,848
news anchor who belongs to a popular news channel

166
00:10:46,896 --> 00:10:50,860
that was created using AI. The impact of such deepfakes

167
00:10:50,932 --> 00:10:54,868
can spread, is that it can spread misinformation, it can influence

168
00:10:54,916 --> 00:10:57,904
public opinion and cause political instability.

169
00:10:58,204 --> 00:11:01,460
Along with this, it basically undermines the trust the

170
00:11:01,492 --> 00:11:05,396
public basically has in the media. So the mitigation of

171
00:11:05,420 --> 00:11:09,144
these attacks would be to carefully understand and analyze

172
00:11:09,844 --> 00:11:13,684
minute details in the videos. If in deepweight

173
00:11:13,724 --> 00:11:16,760
videos, its possible possible that the sound

174
00:11:16,832 --> 00:11:20,656
from the lips as well as the video is not always in sync.

175
00:11:20,760 --> 00:11:24,680
So these are very minute details one would not notice unless keenly

176
00:11:24,712 --> 00:11:28,824
observed. The development and application of deepwake detection tools

177
00:11:28,864 --> 00:11:32,656
is also helpful, and public awareness is basically crucial

178
00:11:32,720 --> 00:11:36,376
in understanding these types of attacks. In the second type

179
00:11:36,400 --> 00:11:40,374
of case study here is identity driven theft,

180
00:11:40,504 --> 00:11:44,202
where somebody is stealing someone's identity to perform a

181
00:11:44,218 --> 00:11:48,162
malicious task. So criminals use AI voice

182
00:11:48,218 --> 00:11:51,954
synthesis tools, in this case to impersonate a CEO,

183
00:11:52,034 --> 00:11:55,818
basically directing a subordinate to transfer

184
00:11:55,866 --> 00:11:59,834
funds to an unauthorized account. This leads to a big and

185
00:11:59,954 --> 00:12:03,170
huge financial loss, as well as reputational hit.

186
00:12:03,322 --> 00:12:07,322
This case study basically shows how criminals can use voice

187
00:12:07,378 --> 00:12:10,794
synthesizing a software to commit fraudulent activities,

188
00:12:10,874 --> 00:12:14,654
thereby exploiting the trust in human voice authentication.

189
00:12:14,954 --> 00:12:18,654
This case study shows the importance of different

190
00:12:19,314 --> 00:12:22,802
multiple factors of authentication that are needed. So some

191
00:12:22,858 --> 00:12:26,490
examples are maybe biometrics fingerprints,

192
00:12:26,522 --> 00:12:30,242
which are part of biometrics ubikeys, as well

193
00:12:30,258 --> 00:12:33,226
as two factor authentication techniques like phones,

194
00:12:33,330 --> 00:12:37,554
etcetera. So the need for setting up government governance frameworks

195
00:12:37,674 --> 00:12:42,426
for this kind of AI authentication system is

196
00:12:42,490 --> 00:12:46,274
particularly important in this case, so that it's easy

197
00:12:46,314 --> 00:12:49,730
to audit internal processes and prevent such type of fraud,

198
00:12:49,842 --> 00:12:53,386
and making sure that one or the organization itself is

199
00:12:53,410 --> 00:12:56,346
adherent to different regulatory standards.

200
00:12:56,530 --> 00:12:59,858
So the last case study here is AI manipulation.

201
00:13:00,026 --> 00:13:03,914
So in this case, an AI system was basically that used for

202
00:13:03,954 --> 00:13:07,354
trading was manipulated by poisoning the data which

203
00:13:07,394 --> 00:13:10,738
it used for training. So basically

204
00:13:10,826 --> 00:13:15,010
this caused the AI algorithm to make unpopular trades

205
00:13:15,122 --> 00:13:19,114
where the cyber criminals profited by making a hefty profit

206
00:13:19,234 --> 00:13:23,490
and the organization that was using the trading algorithm faced

207
00:13:23,522 --> 00:13:26,914
an irrecoverable loss. So the problem with these

208
00:13:26,954 --> 00:13:30,770
kind of attacks is it's because these are minor changes to the

209
00:13:30,802 --> 00:13:34,616
training data, it's very difficult to analyze and

210
00:13:34,640 --> 00:13:38,192
detect these algorithms. This shows the need of continuous

211
00:13:38,248 --> 00:13:41,832
auditing and detection that are needed along with anomaly detection

212
00:13:41,928 --> 00:13:45,832
systems in order to maintain the integrity and reliability of

213
00:13:45,888 --> 00:13:49,824
AI systems such as trading algorithms. So with

214
00:13:49,864 --> 00:13:53,784
that being said, let's go to application of GenaI in

215
00:13:53,824 --> 00:13:57,814
governance risk and compliance. So, in governance,

216
00:13:57,984 --> 00:14:01,450
generative AI is transforming how corporate governance

217
00:14:01,522 --> 00:14:05,362
is admins stirred by automating and optimizing decision making

218
00:14:05,418 --> 00:14:09,414
processes, thereby ensuring transparent and accountable

219
00:14:10,194 --> 00:14:13,858
governance practices. And a prime application example

220
00:14:13,906 --> 00:14:17,186
of this would be AI tools increasingly being used to

221
00:14:17,210 --> 00:14:21,178
analyze vast amounts of corporate data to identify

222
00:14:21,266 --> 00:14:25,152
different trends, forecast potential governance issues,

223
00:14:25,298 --> 00:14:28,484
and provide data driven insights to board members and

224
00:14:28,524 --> 00:14:32,060
executives. For instance, AI can automate the

225
00:14:32,092 --> 00:14:35,260
monitoring of compliance with corporate policies and

226
00:14:35,292 --> 00:14:39,308
regulatory requirements, alerting the management to potential non

227
00:14:39,356 --> 00:14:43,084
compliance and governance failures. The benefits of

228
00:14:43,124 --> 00:14:46,892
using genre in governments include enhancement of corporate

229
00:14:46,948 --> 00:14:50,684
efficiency and effectiveness. Basically, AI allows

230
00:14:50,724 --> 00:14:54,594
real time decision making with a higher degree of accuracy and less

231
00:14:54,634 --> 00:14:57,946
bias than traditional methods, provided the data

232
00:14:58,010 --> 00:15:01,410
that was used in training is itself fair.

233
00:15:01,562 --> 00:15:05,698
So it also supports a dynamic governance environment where strategic

234
00:15:05,746 --> 00:15:10,050
decisions are informed by up to the minute data analysis,

235
00:15:10,202 --> 00:15:14,186
enhancing responsiveness to market or internal changes.

236
00:15:14,370 --> 00:15:17,930
Let's go to the application of genai and risk management.

237
00:15:18,122 --> 00:15:21,930
So, in risk management, Genai is used to predict and

238
00:15:21,962 --> 00:15:25,534
mitigate potential risks before they materialize,

239
00:15:26,194 --> 00:15:29,602
using advanced analytical technologies to model risk

240
00:15:29,658 --> 00:15:33,522
scenarios and their potential impacts. The applications and

241
00:15:33,578 --> 00:15:37,642
examples of GenAi in risk management include being

242
00:15:37,698 --> 00:15:41,130
AI systems that are being adept at identifying different

243
00:15:41,202 --> 00:15:44,440
patterns and anomalies that may indicate risk,

244
00:15:44,562 --> 00:15:49,028
such as fraudulent activities or cybersecurity threats.

245
00:15:49,116 --> 00:15:52,984
For example, let's consider a financial services organization.

246
00:15:53,284 --> 00:15:56,852
A algorithms can predict credit risk by analyzing

247
00:15:56,908 --> 00:16:00,228
transaction patterns on customer behavior or more

248
00:16:00,276 --> 00:16:03,836
accurately than traditional models. The same can be

249
00:16:03,860 --> 00:16:07,812
said of banks. Banks use a algorithms,

250
00:16:07,948 --> 00:16:12,210
machine learning algorithms to basically identify the

251
00:16:12,242 --> 00:16:15,866
transactions we make and if that transaction passes

252
00:16:15,930 --> 00:16:20,122
above a certain threshold, they basically think that as fraudulent activity

253
00:16:20,218 --> 00:16:23,826
and many banks have the potential to block such activity.

254
00:16:24,010 --> 00:16:27,694
The benefits of using genai in risk management include

255
00:16:28,714 --> 00:16:32,986
allowing companies to anticipate and mitigate risks more effectively,

256
00:16:33,130 --> 00:16:36,514
thereby reducing the costs associated with losses and

257
00:16:36,554 --> 00:16:39,794
insurance. By enabling predictive risk management,

258
00:16:40,174 --> 00:16:44,158
organizations can allocate resources more efficiently and

259
00:16:44,206 --> 00:16:48,630
improve their overall risk posture. So lastly, let's look at GenaI

260
00:16:48,702 --> 00:16:52,638
in compliance. As for us visage of Gen

261
00:16:52,686 --> 00:16:56,262
AI in compliance, which is one of the key areas where AI can have

262
00:16:56,278 --> 00:16:59,966
a significant impact, especially regulated sectors

263
00:17:00,070 --> 00:17:02,902
like finance, healthcare and pharmaceuticals,

264
00:17:02,958 --> 00:17:06,610
etcetera. So AI systems here basically help

265
00:17:06,762 --> 00:17:10,306
ensure that compliance is maintained

266
00:17:10,370 --> 00:17:14,514
by monitoring regulations and automatically implementing changes.

267
00:17:14,674 --> 00:17:18,522
Application and example of GenAi in compliance include

268
00:17:18,618 --> 00:17:22,658
AI tools being automatically tracking and analyzing

269
00:17:22,706 --> 00:17:27,058
changes in regulations to help companies adjust their operations

270
00:17:27,146 --> 00:17:30,404
accordingly. In healthcare sector, for example,

271
00:17:30,554 --> 00:17:33,888
AI systems ensure patient data handling

272
00:17:33,976 --> 00:17:37,840
complies with the HIPAA or HIPAA regulations

273
00:17:37,952 --> 00:17:41,392
by automatically encrypting data and controlling access

274
00:17:41,488 --> 00:17:45,044
based on user roles, the benefits

275
00:17:45,584 --> 00:17:48,392
of use of AI in compliance.

276
00:17:48,528 --> 00:17:52,048
It basically reduces the likelihood of human error

277
00:17:52,176 --> 00:17:56,040
and the risk of non compliance, which could otherwise lead to hefty

278
00:17:56,072 --> 00:18:00,146
fines and legal issues. It also streamlines the documentation

279
00:18:00,210 --> 00:18:03,842
process, making audits more straightforward and

280
00:18:03,898 --> 00:18:07,186
less frequent. With that being said,

281
00:18:07,290 --> 00:18:10,842
let's go to risk management in AI. So as far as risk

282
00:18:10,898 --> 00:18:14,562
management in AI is concerned, AI is a very crucial aspect

283
00:18:14,618 --> 00:18:18,498
for safe and effective use. So effective

284
00:18:18,546 --> 00:18:21,850
safe use of AI begins with identifying what can

285
00:18:21,922 --> 00:18:25,510
initially go wrong, understanding the potential threats like

286
00:18:25,542 --> 00:18:29,670
data poisoning or adversarial attacks, as well as performing

287
00:18:29,862 --> 00:18:33,574
regular security assessments such as vulnerability assessments,

288
00:18:33,734 --> 00:18:36,990
threat modeling, helping anticipate and prepare

289
00:18:37,022 --> 00:18:40,414
for these kind of threats. So as long as security is

290
00:18:40,454 --> 00:18:44,518
included early in the face of development of a Gen AI model,

291
00:18:44,646 --> 00:18:48,334
it's safe to say that that model is likely to be more secure

292
00:18:48,374 --> 00:18:52,134
than models that try to incorporate security as an afterthought.

293
00:18:52,214 --> 00:18:55,494
Once the model has been trained, this likely

294
00:18:55,574 --> 00:18:58,822
has the potential to face

295
00:18:58,878 --> 00:19:02,750
different kind of attacks that we earlier saw. So employing tools

296
00:19:02,782 --> 00:19:06,854
like real time monitoring systems can alert to unusual AI

297
00:19:06,894 --> 00:19:11,222
behavior that might indicate a security issue. So encryption

298
00:19:11,358 --> 00:19:14,554
is a security technology here that provides data,

299
00:19:16,234 --> 00:19:20,202
that provides data by encrypting it with a strong algorithm,

300
00:19:20,258 --> 00:19:23,906
making sure only people who are able to access the

301
00:19:23,970 --> 00:19:28,134
specific data and interact with it will be able to interact it.

302
00:19:28,674 --> 00:19:31,954
So keeping AI systems and their components up to date

303
00:19:31,994 --> 00:19:35,170
with the latest security patches is also of paramount

304
00:19:35,242 --> 00:19:39,034
importance. Regular updates help protect against newly

305
00:19:39,074 --> 00:19:43,278
discovered vulnerabilities. Adversarial training basically teaches

306
00:19:43,326 --> 00:19:47,286
AI systems to recognize and understand disruptive inputs,

307
00:19:47,390 --> 00:19:49,114
enhancing their robustness.

308
00:19:50,734 --> 00:19:54,598
So utilizing AI security platforms to continuously

309
00:19:54,646 --> 00:19:57,830
monitor real time attacks is of paramount importance.

310
00:19:57,982 --> 00:20:02,166
Here we have different example security controls such as encryption

311
00:20:02,230 --> 00:20:06,078
access controls that we would be basically talking about and

312
00:20:06,126 --> 00:20:10,014
incident response plan is also of paramount importance,

313
00:20:10,314 --> 00:20:14,082
just because one needs to be ready to understand what would be

314
00:20:14,098 --> 00:20:17,506
the path one would take if a specific attack happens.

315
00:20:17,610 --> 00:20:21,394
So as we saw, incorporating adversarial examples

316
00:20:21,474 --> 00:20:25,338
during training is really helpful in helping the AI model

317
00:20:25,386 --> 00:20:29,090
distinguish between real pattern of data and then

318
00:20:29,242 --> 00:20:32,530
attack pattern of attack,

319
00:20:32,682 --> 00:20:35,934
basically where cybercriminals try to infiltrate the data.

320
00:20:36,254 --> 00:20:39,954
Let's go to ethical considerations for AI.

321
00:20:40,694 --> 00:20:44,246
So the rapid adoption of AI technologies brings

322
00:20:44,270 --> 00:20:46,886
to light numerous ethical considerations.

323
00:20:47,070 --> 00:20:51,006
Ethical AI basically involves ensuring that these systems

324
00:20:51,070 --> 00:20:54,550
operate in a manner that reflects societal values,

325
00:20:54,622 --> 00:20:58,430
protects individual rights, and promotes fairness and

326
00:20:58,462 --> 00:21:02,278
justice. Some of the key concerns for ethical considerations

327
00:21:02,366 --> 00:21:05,824
include bias and fairness. So AI systems

328
00:21:05,864 --> 00:21:09,240
can perpetuate or even amplify biases present

329
00:21:09,312 --> 00:21:12,664
in their training data, which lead to unfair outcomes

330
00:21:12,704 --> 00:21:16,304
that can discriminate against certain groups. For example,

331
00:21:16,384 --> 00:21:20,360
facial recognition technologies have shown lower accuracy rates

332
00:21:20,392 --> 00:21:24,184
for women and people of color, raising significant concerns

333
00:21:24,224 --> 00:21:27,856
about fairness and equality. The next point would be

334
00:21:27,920 --> 00:21:31,812
transparency and accountability. This is a growing demand for AI

335
00:21:31,868 --> 00:21:35,476
systems to be transparent about how decisions

336
00:21:35,500 --> 00:21:38,636
are made, what kind of data are being used to train them,

337
00:21:38,700 --> 00:21:42,708
especially during the training phase. So ensuring

338
00:21:42,796 --> 00:21:45,852
accountability when things go wrong is important,

339
00:21:46,028 --> 00:21:49,676
which requires clear guidelines on who is responsible. Is it going

340
00:21:49,700 --> 00:21:52,788
to be the developers, the users, or the AI

341
00:21:52,836 --> 00:21:56,308
itself? Privacy is also an important key concern

342
00:21:56,396 --> 00:21:59,678
here. AI systems often require vast amounts of

343
00:21:59,726 --> 00:22:02,994
data, which can include sensitive personal information.

344
00:22:03,374 --> 00:22:06,926
Ensuring that this data is handled security is of paramount

345
00:22:06,990 --> 00:22:10,142
importance so that privacy is maintained as

346
00:22:10,158 --> 00:22:13,622
a fundamental ethical requirement. Some of the strategies

347
00:22:13,678 --> 00:22:17,654
for addressing ethical concerns include implementing

348
00:22:17,774 --> 00:22:21,270
robust data handling and processing protocols to make

349
00:22:21,302 --> 00:22:25,026
sure there is fairness and reduce bias in the

350
00:22:25,050 --> 00:22:28,882
data that's used for training. The AI systems being

351
00:22:28,938 --> 00:22:32,410
developed they need to be developed with explainable

352
00:22:32,482 --> 00:22:35,706
AI features that allow users to understand and

353
00:22:35,730 --> 00:22:39,250
test the decisions made by AI. Basically, this is a way

354
00:22:39,282 --> 00:22:42,930
of saying the user should be able to understand how AI makes the

355
00:22:42,962 --> 00:22:46,826
decision so that it's not biased against certain

356
00:22:46,890 --> 00:22:50,192
people of color, women, or any kind of gender, etcetera.

357
00:22:50,298 --> 00:22:54,316
So regular ethical audits of AI systems are also helpful here

358
00:22:54,420 --> 00:22:58,264
as they help adhere to ethical standards throughout their life cycle.

359
00:22:58,644 --> 00:23:02,524
Let's go to the next section of the presentation, which is legal

360
00:23:02,564 --> 00:23:06,052
considerations for AI. So,

361
00:23:06,228 --> 00:23:09,972
as AI technologies become more integral to business operations

362
00:23:10,068 --> 00:23:14,196
and daily life, they also encounter a complex web of legal

363
00:23:14,260 --> 00:23:17,716
challenges. These involve compliance with existing laws

364
00:23:17,740 --> 00:23:21,230
and regulations and development of new laws to

365
00:23:21,262 --> 00:23:24,574
address emerging issues. Some example frameworks

366
00:23:24,614 --> 00:23:28,238
include a GDPR general data protection regulation,

367
00:23:28,406 --> 00:23:32,126
which basically imposes strict guidelines on data privacy

368
00:23:32,190 --> 00:23:35,998
and security, including AI systems that process personal

369
00:23:36,086 --> 00:23:39,790
data of EU citizens, European Union citizens.

370
00:23:39,982 --> 00:23:43,550
The other regulation we're going to be talking about is California

371
00:23:43,622 --> 00:23:47,312
Consumer Privacy act, which is similar to GDPR,

372
00:23:47,488 --> 00:23:51,352
but it provides consumers with specific rights regarding their personal

373
00:23:51,448 --> 00:23:55,008
data used by AI systems.

374
00:23:55,136 --> 00:23:59,360
This applies to the state of California only biometric

375
00:23:59,432 --> 00:24:03,200
Information Privacy act. This regulation

376
00:24:03,312 --> 00:24:06,928
basically regulates the collection and storage of biometric data,

377
00:24:07,056 --> 00:24:11,032
which is crucial for AI systems that use special recognition and other

378
00:24:11,088 --> 00:24:14,930
biometrics technologies. Some of the key challenges

379
00:24:15,082 --> 00:24:19,098
faced for legal consideration include intellectual

380
00:24:19,146 --> 00:24:22,738
property. Determining the ownership of AI generated

381
00:24:22,826 --> 00:24:26,810
content or invention pose significant challenges.

382
00:24:26,962 --> 00:24:30,098
Traditional laws here are not well suited to address whether

383
00:24:30,146 --> 00:24:33,090
an AI can be a copyright or a patent holder.

384
00:24:33,242 --> 00:24:36,654
Traditional laws were defined for publications.

385
00:24:37,034 --> 00:24:40,466
AI itself cannot be defined as a publication or

386
00:24:40,490 --> 00:24:44,128
a magazine or anything else because it's being generated

387
00:24:44,176 --> 00:24:47,424
by a software. And as technologies evolve,

388
00:24:47,544 --> 00:24:51,016
these kind of laws and regulations need to evolve to make

389
00:24:51,040 --> 00:24:56,284
sure they are being adapted to the evolving technology liability.

390
00:24:56,944 --> 00:25:00,544
As we, as we saw

391
00:25:00,584 --> 00:25:04,336
before, when AI systems cause harm, whether physically,

392
00:25:04,440 --> 00:25:07,924
financially or emotionally, determining the liability

393
00:25:08,344 --> 00:25:11,852
of a specific system can be complex. For instance,

394
00:25:11,948 --> 00:25:14,892
if an autonomous vehicle is involved in accidents,

395
00:25:15,068 --> 00:25:18,716
questions arise about whether the manufacturer, the software

396
00:25:18,780 --> 00:25:22,364
developer who provides the navigation system and the

397
00:25:22,524 --> 00:25:26,164
understanding, the self learning, understanding self driving system,

398
00:25:26,284 --> 00:25:29,780
or the vehicle owner itself should be held responsible for such an

399
00:25:29,812 --> 00:25:33,916
accident. The next part would be regulatory compliance.

400
00:25:34,100 --> 00:25:37,728
Different countries may have different regulations regarding AI,

401
00:25:37,836 --> 00:25:40,640
such as GDPR in Europe, as we saw,

402
00:25:40,752 --> 00:25:44,840
which imposes strict rules on data protection and privacy.

403
00:25:45,032 --> 00:25:48,656
AI systems operating across borders must navigate these kind

404
00:25:48,680 --> 00:25:51,840
of different legal landscapes. There is a

405
00:25:51,872 --> 00:25:55,456
new AI act that recently came that has been developed

406
00:25:55,480 --> 00:25:59,760
by EU. So as these regulations keep evolving

407
00:25:59,872 --> 00:26:03,788
to meet the legal evolving technology, it's a

408
00:26:03,816 --> 00:26:07,500
up to the organizations to make sure those that are

409
00:26:07,532 --> 00:26:11,076
operating across different borders must navigate and adhere to

410
00:26:11,100 --> 00:26:15,224
these legal landscapes. Some of the strategies for navigating

411
00:26:15,844 --> 00:26:19,548
legal challenges include engaging with legal experts

412
00:26:19,676 --> 00:26:23,292
to make sure that AI applications comply with relevant laws

413
00:26:23,308 --> 00:26:26,924
and regulations, participating in industry groups and

414
00:26:26,964 --> 00:26:30,596
standards organization to help shape the laws and

415
00:26:30,620 --> 00:26:34,774
regulations itself that are fair and practical, as well as implementing

416
00:26:34,814 --> 00:26:38,838
rigorous testing and compliance checks within the development process

417
00:26:39,006 --> 00:26:42,634
to mitigate potential legal issues before they arise.

418
00:26:44,054 --> 00:26:48,086
With that being said, let us look at some of the challenges currently facing

419
00:26:48,150 --> 00:26:51,494
AI systems. So with AI systems,

420
00:26:51,574 --> 00:26:54,782
if AI systems are being compliant with different regular

421
00:26:54,838 --> 00:26:57,742
frameworks, they are still being challenged.

422
00:26:57,918 --> 00:27:01,264
Challenges faced by organizations,

423
00:27:01,414 --> 00:27:04,972
basically that perform out of the border transactions.

424
00:27:05,108 --> 00:27:09,116
Some of these challenges include regular audits, which is either at

425
00:27:09,140 --> 00:27:13,444
a state level federal or a globe federal level, or a central

426
00:27:13,524 --> 00:27:17,092
level. Data management strategies. So as

427
00:27:17,148 --> 00:27:20,884
different nations have different regulations,

428
00:27:20,924 --> 00:27:24,764
data management becomes very important. Some regulations

429
00:27:24,804 --> 00:27:28,546
require data to be stored in state or within the country,

430
00:27:28,700 --> 00:27:33,270
or need certain regulations to be followed before they are being accessed

431
00:27:33,302 --> 00:27:37,374
outside the country. So, cross functional compliance

432
00:27:37,414 --> 00:27:41,470
teams, some of the challenges here include making sure

433
00:27:41,542 --> 00:27:45,190
working with cross functional compliance teams to make sure

434
00:27:45,262 --> 00:27:49,046
these organizations that operate across borders

435
00:27:49,110 --> 00:27:53,222
are adhering to different regulations frameworks,

436
00:27:53,318 --> 00:27:57,072
making sure that these organizations are evolving their

437
00:27:57,128 --> 00:28:00,720
systems to meet such regulations to keep in

438
00:28:00,832 --> 00:28:04,184
touch with what is needed for these technologies

439
00:28:04,224 --> 00:28:07,576
to stay safe and compliant. The last part is

440
00:28:07,600 --> 00:28:11,336
going to be continuous education and awareness. Of course, as technology

441
00:28:11,440 --> 00:28:15,256
keeps evolving, there's going to be the challenge of continuous education

442
00:28:15,360 --> 00:28:18,720
and awareness. So it's up to these organizations to devise

443
00:28:18,752 --> 00:28:22,488
a training plan, work with their different partners,

444
00:28:22,576 --> 00:28:26,634
and employees to make sure each and every one of them is

445
00:28:27,814 --> 00:28:31,954
up for the different evolution of technologies.

446
00:28:32,254 --> 00:28:36,014
With that being said, let's conclude this session by

447
00:28:36,054 --> 00:28:40,062
talking about what by summarizing what we saw

448
00:28:40,198 --> 00:28:44,594
and what we definitely how AI systems itself

449
00:28:45,814 --> 00:28:49,206
are beneficial to humans. So, to conclude, the security

450
00:28:49,310 --> 00:28:53,070
of generative AI is not a technical issue, but rather a broad

451
00:28:53,142 --> 00:28:56,912
concern that impacts public trust and ethical deployment

452
00:28:56,968 --> 00:29:00,288
of technology. One must be proactive,

453
00:29:00,376 --> 00:29:04,488
embedding security at every stage of AI development and

454
00:29:04,536 --> 00:29:08,528
deployment. So security needs to be deployed,

455
00:29:08,656 --> 00:29:12,164
considered as early as possible, rather than afterthought.

456
00:29:13,584 --> 00:29:17,224
The commitment to robust security measures must evolve as new threats

457
00:29:17,264 --> 00:29:21,384
emerge. This requires vigilance, continuous innovation,

458
00:29:21,464 --> 00:29:24,880
and the commitment to ethical practices. By doing so,

459
00:29:24,952 --> 00:29:28,880
one can ensure generative AI technologies are not only powerful and

460
00:29:28,912 --> 00:29:31,912
effective, but also safe and trustworthy.

461
00:29:32,048 --> 00:29:35,480
Let's take forward the understanding that security generative

462
00:29:35,552 --> 00:29:39,284
AI is integral to leveraging its full potential.

463
00:29:40,424 --> 00:29:44,424
We need to work collaboratively across industries and disciplines

464
00:29:44,544 --> 00:29:48,150
to uphold the highest standards of security and ethics,

465
00:29:48,302 --> 00:29:52,046
ensuring that it benefits the entire society without

466
00:29:52,110 --> 00:29:55,574
compromising the safety or integrity of the public. Thank you.

