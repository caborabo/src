1
00:00:21,280 --> 00:00:24,694
Last year, like a lot of people, I spent part of a weekend

2
00:00:24,774 --> 00:00:28,188
making a little demo for a retrieval augmented generation,

3
00:00:28,326 --> 00:00:32,352
or rag chatbot. This is a chatbot that uses an LLM

4
00:00:32,408 --> 00:00:35,448
to answer questions about a specific set of documents,

5
00:00:35,576 --> 00:00:39,448
where those documents aren't necessarily in the LLM's training data.

6
00:00:39,576 --> 00:00:43,208
So the user asks something, your tool searches to find related

7
00:00:43,296 --> 00:00:46,704
text in your documents, and then it passes those chunks of

8
00:00:46,744 --> 00:00:50,564
text to an LLM to use in answering the question.

9
00:00:51,264 --> 00:00:54,832
This was my weekend demo project. It used lang chain,

10
00:00:54,928 --> 00:00:58,224
it used radio. It was probably 80% code

11
00:00:58,264 --> 00:01:01,784
I took from someone else's collab notebook. But my code was a little

12
00:01:01,824 --> 00:01:05,568
cleaner and better organized, at least. But the actual chatbot

13
00:01:05,616 --> 00:01:09,696
guys was terrible. It didn't work. It was supposed to

14
00:01:09,720 --> 00:01:13,488
answer questions about shiny for Python, which is a python library

15
00:01:13,536 --> 00:01:17,256
for building dashboards that was too new to have its documentation in

16
00:01:17,280 --> 00:01:21,064
GPT four. But my chatbot was making things up. It was getting

17
00:01:21,104 --> 00:01:24,340
stuff wrong, and just generally it wasn't usable.

18
00:01:24,512 --> 00:01:27,940
And I could tell you it wasn't usable from, you know, trying to use

19
00:01:27,972 --> 00:01:31,716
it, but I couldn't tell you how not usable it was. I couldn't

20
00:01:31,780 --> 00:01:36,344
rate its lack of usability because I didn't have an evaluation strategy.

21
00:01:36,724 --> 00:01:40,116
Then I made another one, this time to answer questions about the

22
00:01:40,140 --> 00:01:43,508
California traffic code. I scraped the data from the website.

23
00:01:43,636 --> 00:01:47,020
I fought with it a little to get it parsed appropriately. And then

24
00:01:47,052 --> 00:01:50,956
this one was with streamlit, and someone else built a nice interface for it,

25
00:01:51,100 --> 00:01:55,204
and this one was somewhat better. But when we were trying to make decisions about

26
00:01:55,244 --> 00:01:58,932
things like what open source model should we use as our underlying

27
00:01:58,988 --> 00:02:02,588
LLM, I still didn't have an assessment strategy for figuring

28
00:02:02,636 --> 00:02:06,116
this out. How were we supposed to tell if it was working when we

29
00:02:06,140 --> 00:02:09,484
demoed it out to potential clients? Was the best we could do to just ask

30
00:02:09,524 --> 00:02:12,988
some questions and hope that the questions the client asked was something it could

31
00:02:13,036 --> 00:02:16,692
answer. But now I've been working on LLM model evals

32
00:02:16,748 --> 00:02:20,284
for a while, and I spent just a lot of time with evaluating

33
00:02:20,364 --> 00:02:23,692
LLM output in different ways. And so I have at least the beginning

34
00:02:23,708 --> 00:02:26,924
of a strategy for how to evaluate your rag chatbot

35
00:02:26,964 --> 00:02:30,340
or other generative AI tool like that you're building

36
00:02:30,372 --> 00:02:34,180
for customers, or maybe internally to do a specific thing.

37
00:02:34,372 --> 00:02:36,464
And I'm going to tell you about that today.

38
00:02:37,164 --> 00:02:39,892
First, why are we automating testing?

39
00:02:40,068 --> 00:02:43,808
Well, look, why do we ever automate testing? We automate

40
00:02:43,856 --> 00:02:47,160
testing because you're going to break things, and you want to find out that you

41
00:02:47,192 --> 00:02:50,736
broke it when you push your code to a branch that isn't your main branch,

42
00:02:50,840 --> 00:02:54,404
and before you merge that code in and your product goes boom.

43
00:02:54,824 --> 00:02:58,416
We automate testing because we're human and therefore we're fallible.

44
00:02:58,600 --> 00:03:02,312
But in the context of your LLM tools specifically, we also

45
00:03:02,368 --> 00:03:05,600
automate testing because there are choices you're going to make about your

46
00:03:05,632 --> 00:03:09,064
tool, and you want to have quick feedback about how it works,

47
00:03:09,104 --> 00:03:13,636
or could work. For instance, like I mentioned, if you're trying to decide what underlying

48
00:03:13,700 --> 00:03:16,756
LLM to use, or what broad system prop to use,

49
00:03:16,780 --> 00:03:20,412
if that's relevant for you when you make or consider changes,

50
00:03:20,508 --> 00:03:24,300
you want to know how they affect performance. And the more your tool

51
00:03:24,332 --> 00:03:27,532
is doing, like with any kind of software, the less feasible it

52
00:03:27,548 --> 00:03:31,228
becomes to test things manually. What I was doing with my California

53
00:03:31,276 --> 00:03:34,948
traffic code chatbot, which was having a series of questions,

54
00:03:35,076 --> 00:03:39,026
running them through multiple models, and then looking at the responses myself,

55
00:03:39,210 --> 00:03:42,294
it's not the worst, but it's not the greatest either.

56
00:03:43,074 --> 00:03:46,666
Let's talk next about how to automate testing broadly.

57
00:03:46,730 --> 00:03:49,970
We test to make sure that our tools are doing what we want them to

58
00:03:50,002 --> 00:03:53,642
do. So what do you want your tool to do?

59
00:03:53,778 --> 00:03:57,282
What are some questions you want it to be able to answer? What does a

60
00:03:57,298 --> 00:04:00,106
good answer look like? What does a bad answer look like?

61
00:04:00,170 --> 00:04:03,406
Now test that it's doing that easy,

62
00:04:03,470 --> 00:04:07,166
right? Just test that it's doing what you want it to be doing. We do

63
00:04:07,190 --> 00:04:10,750
that all the time for machine learning problems generally, and NLP

64
00:04:10,822 --> 00:04:14,054
specifically. But actually, okay, this is not that easy.

65
00:04:14,094 --> 00:04:17,942
It's actually pretty hard. Why is it hard? It's hard because

66
00:04:17,998 --> 00:04:21,422
text is high dimensional data. It's complicated, it has a

67
00:04:21,438 --> 00:04:25,166
lot of features. And with generative AI, like with a chatbot,

68
00:04:25,270 --> 00:04:28,542
we're not talking about a classification model where the result is

69
00:04:28,598 --> 00:04:31,590
pass or fail, spam or not spam.

70
00:04:31,782 --> 00:04:35,470
Now, as a digression, you can also use large language models

71
00:04:35,542 --> 00:04:38,982
to build classification tools or do entity extraction,

72
00:04:39,118 --> 00:04:42,174
and they're really good at them. And that's my favorite use

73
00:04:42,214 --> 00:04:45,758
case for llms, in part because you can evaluate them super

74
00:04:45,846 --> 00:04:49,134
easily, the same way we've always evaluated these

75
00:04:49,174 --> 00:04:53,158
kinds of tools, by comparing the model output with our ground truth

76
00:04:53,206 --> 00:04:56,598
labeled data. So if you have the opportunity to do that

77
00:04:56,646 --> 00:04:59,728
instead. Instead, oh my gosh, of do that,

78
00:04:59,896 --> 00:05:03,016
evaluate it with a confusion matrix, call it

79
00:05:03,040 --> 00:05:07,104
a day, but everyone wants a chatbot,

80
00:05:07,144 --> 00:05:10,264
so that's what we're talking about. And in the case of chatbots,

81
00:05:10,344 --> 00:05:14,312
we're asking a question and getting a whole sentence back, or a paragraph.

82
00:05:14,448 --> 00:05:18,072
How can we assess that? Well, we have a few options

83
00:05:18,168 --> 00:05:21,400
which I'll go through, but first I want to note that the

84
00:05:21,432 --> 00:05:25,424
purpose of this kind of testing is not to comprehensively test everything

85
00:05:25,504 --> 00:05:29,144
someone might ask your tool about for accuracy. If you were going

86
00:05:29,184 --> 00:05:32,608
to generate every possible question someone could ask of your tool,

87
00:05:32,736 --> 00:05:36,008
as well as criteria for evaluating answers that are

88
00:05:36,056 --> 00:05:39,344
specific to that question, then you wouldn't need a generative tool,

89
00:05:39,424 --> 00:05:43,524
you would need a frequently asked questions page, and then some search functionality.

90
00:05:44,104 --> 00:05:47,464
So the purpose of this instead is to select some of the types of

91
00:05:47,504 --> 00:05:50,844
questions you want your tool to be able to answer and then test

92
00:05:50,884 --> 00:05:54,868
the content of those. So first option string

93
00:05:54,916 --> 00:05:58,788
matching we've got some choices here. We can look for an exact

94
00:05:58,876 --> 00:06:02,948
match, like the answer to an exact, the answer to be an exact sentence,

95
00:06:03,076 --> 00:06:06,796
or to contain a particular substring, like if we ask it for the

96
00:06:06,820 --> 00:06:10,740
capital of France is Paris. Somewhere in that response

97
00:06:10,892 --> 00:06:13,892
we can use regular expressions or regex if there's a

98
00:06:13,908 --> 00:06:18,024
pattern we want, like if we want a particular substring,

99
00:06:18,144 --> 00:06:21,624
but only if it's a standalone word, not part of a bigger

100
00:06:21,664 --> 00:06:25,224
word. We can measure edit distance, or how syntactically

101
00:06:25,304 --> 00:06:29,048
close two pieces of text are, like how many characters we

102
00:06:29,056 --> 00:06:32,324
have to flip to get from one string to another.

103
00:06:32,664 --> 00:06:36,576
Or we can do a variation of that exact matching where we want to

104
00:06:36,600 --> 00:06:40,204
find a list of keywords rather than just one.

105
00:06:41,104 --> 00:06:44,682
Here's an example. Here we have a little unit test where we do

106
00:06:44,698 --> 00:06:48,650
a call to our tools API. We passed a question, we get

107
00:06:48,682 --> 00:06:52,546
back a response, and then we test to see if there's something formatted

108
00:06:52,610 --> 00:06:56,258
like an email address in it. So ship it.

109
00:06:56,346 --> 00:07:00,002
Does that look good? Is this a good way of evaluating high dimensional

110
00:07:00,058 --> 00:07:02,734
text data to see if it's got the answer we want?

111
00:07:03,234 --> 00:07:07,146
Nope, this isn't great. There's a lot we just can't do in terms

112
00:07:07,170 --> 00:07:10,240
of text evaluation with string matching. Maybe there

113
00:07:10,272 --> 00:07:14,168
are some test cases you can write like, okay, if you want very short

114
00:07:14,256 --> 00:07:17,800
factual answers, you can do this, but in general,

115
00:07:17,952 --> 00:07:21,752
don't ship it. Next, we have semantic similarity.

116
00:07:21,888 --> 00:07:25,656
With semantic similarity we can test how close one string

117
00:07:25,720 --> 00:07:29,584
is to another string in a way that takes into account both synonyms and

118
00:07:29,624 --> 00:07:32,776
context. There are a lot of small models. We can

119
00:07:32,800 --> 00:07:36,382
project our text into 760 dimensional space, which is

120
00:07:36,398 --> 00:07:40,070
actually a major reduction in dimensionality, and then we can take the distance

121
00:07:40,142 --> 00:07:43,606
between those two strings. So the thing, the response we got from

122
00:07:43,630 --> 00:07:46,874
our model versus the response we wanted to get from it.

123
00:07:47,694 --> 00:07:51,422
Here's an example. This isn't exactly real, but basically you download

124
00:07:51,438 --> 00:07:55,454
a model that's going to do your tokenizing. So that'll break your text up.

125
00:07:55,574 --> 00:07:59,398
You hit your tool API with a particular prompt to get a response.

126
00:07:59,526 --> 00:08:02,716
You project that response into your n dimensional space in your

127
00:08:02,740 --> 00:08:05,956
model, and you project the target text that you wanted your tool to

128
00:08:05,980 --> 00:08:09,804
say into that space as well. And then you compare the two. Your text

129
00:08:09,844 --> 00:08:13,156
then uses a threshold for similarity for how close

130
00:08:13,220 --> 00:08:16,404
they were, and then tells you if it passed or not, or if the two

131
00:08:16,444 --> 00:08:19,876
texts were sufficiently close using that model.

132
00:08:20,060 --> 00:08:23,460
So ship it again. I don't want to say never,

133
00:08:23,532 --> 00:08:27,560
but there's a lot of nuance. You're not necessarily going to capture its similarity,

134
00:08:27,692 --> 00:08:31,520
and that's especially as your text responses get longer. Something can be important

135
00:08:31,712 --> 00:08:35,232
and you can miss it. Okay, so finally we

136
00:08:35,248 --> 00:08:38,928
have LLM led evaluations. That's where you write a

137
00:08:38,976 --> 00:08:42,568
specific test for what you're looking for, and you let your LLM or

138
00:08:42,616 --> 00:08:45,872
an LLM do your evaluation for you.

139
00:08:46,008 --> 00:08:49,384
And this doesn't need to be the large language model you're using

140
00:08:49,504 --> 00:08:53,128
behind your tool. For instance, maybe you use an open source

141
00:08:53,176 --> 00:08:56,892
tool or smaller LLM to power your actual chatbot,

142
00:08:56,948 --> 00:09:00,644
say, to save money. You might still want to use GPT four

143
00:09:00,764 --> 00:09:04,500
for your test cases because it's still going to be pennies or less to

144
00:09:04,532 --> 00:09:08,476
run them each time. So what does this look like?

145
00:09:08,540 --> 00:09:11,700
It looks like whatever you want it to. Here's one I used

146
00:09:11,732 --> 00:09:15,020
to evaluate text closeness, so this would be how close is the

147
00:09:15,052 --> 00:09:18,660
text that tool output to the text you wanted to see? And this gives

148
00:09:18,692 --> 00:09:22,572
back an answer on a scale of one to ten. Here's another one.

149
00:09:22,668 --> 00:09:26,428
You can write an actual grading rubric for each of your tests.

150
00:09:26,556 --> 00:09:30,084
This is a grading rubric for a set of instructions where it passes

151
00:09:30,124 --> 00:09:34,060
if it contains all seven steps and it fails otherwise.

152
00:09:34,212 --> 00:09:37,780
I'm using a package called marvin AI, which I highly recommend,

153
00:09:37,892 --> 00:09:41,812
and that makes getting precise, structured outputs from OpenAI

154
00:09:41,908 --> 00:09:45,620
models really as easy as writing this rubric. You can also write

155
00:09:45,652 --> 00:09:48,812
rubrics which return scores instead of pass fail, and then

156
00:09:48,828 --> 00:09:52,596
you can set a threshold for your test passing. For instance, you could make

157
00:09:52,620 --> 00:09:55,924
this pass if it returned four, five, or six

158
00:09:55,964 --> 00:09:59,540
or seven of the tasks and failed otherwise.

159
00:09:59,692 --> 00:10:03,300
Again, this is a level of detail which you can't get using

160
00:10:03,372 --> 00:10:07,020
string matching or semantic similarity. I'm doing

161
00:10:07,052 --> 00:10:10,156
some other work involving LLM led evals, and so I'll

162
00:10:10,180 --> 00:10:14,396
show you another example of how we can get pretty complicated with these there's

163
00:10:14,420 --> 00:10:18,036
this logic problem about transporting a wolf, a cabbage, and a

164
00:10:18,060 --> 00:10:21,580
goat, but you can only bring one at once. And the goat can't be left

165
00:10:21,612 --> 00:10:25,356
alone with either the fox or the cabbage because something will

166
00:10:25,380 --> 00:10:28,772
get eaten if you swap out those names, the goat,

167
00:10:28,828 --> 00:10:33,116
cabbage, and fox for other things, some of the LLMs get confused

168
00:10:33,220 --> 00:10:36,860
and can't answer accurately on screen. This is a

169
00:10:36,892 --> 00:10:40,092
rubric for using an LLM to evaluate text to see

170
00:10:40,148 --> 00:10:43,596
did it pass or fail. The question and what it's possible

171
00:10:43,660 --> 00:10:47,332
to do here is write a rubric that works for both the five step

172
00:10:47,388 --> 00:10:51,228
answer and the seven step answer, the difference being the seven

173
00:10:51,276 --> 00:10:55,076
step one is two steps where you're traveling or teleporting alone without

174
00:10:55,140 --> 00:10:58,324
an object. And the LLM, if you pass it as a rubric,

175
00:10:58,364 --> 00:11:01,900
is capable enough to accurately rate each answer as passing

176
00:11:01,932 --> 00:11:05,204
or failing. You can do the same thing with a select

177
00:11:05,244 --> 00:11:08,844
set of questions you want your generative tools to be able to answer.

178
00:11:08,964 --> 00:11:12,324
You can write the question, you can write the rubric, and you can

179
00:11:12,364 --> 00:11:15,636
play with different llms, system prompts, etcetera,

180
00:11:15,740 --> 00:11:19,340
any parameter your tool uses to see how it changes the

181
00:11:19,372 --> 00:11:23,684
rate of accuracy. It's not a substitute for doing user testing,

182
00:11:23,804 --> 00:11:27,516
but it can complement it. But you're also

183
00:11:27,580 --> 00:11:31,108
going to want more flexible testing. That is, testing that doesn't

184
00:11:31,156 --> 00:11:34,436
rely on having specific rubrics and therefore can be

185
00:11:34,460 --> 00:11:38,104
done on the fly with new questions. And there are also

186
00:11:38,184 --> 00:11:42,088
tools for that. For instance, you can use LLM

187
00:11:42,176 --> 00:11:45,408
led evals to see if your rag chatbot is using

188
00:11:45,456 --> 00:11:48,728
your documents to answer the questions versus if it's making

189
00:11:48,776 --> 00:11:52,240
things up. That is, when we're asking the LLM, in this

190
00:11:52,272 --> 00:11:55,640
case is was the answer that my tool gave only

191
00:11:55,712 --> 00:11:59,480
based on information that was contained in the context that I

192
00:11:59,512 --> 00:12:02,284
passed to it. And then we give it both the context,

193
00:12:02,384 --> 00:12:06,092
so the document chunks, and we give it the answer that the tool

194
00:12:06,148 --> 00:12:09,780
actually gave. That's really useful. And you can run it on

195
00:12:09,812 --> 00:12:13,508
your log file, you can even run it on real time as a step

196
00:12:13,556 --> 00:12:17,060
in your tool, and then you don't give a response to your user which

197
00:12:17,092 --> 00:12:20,332
didn't pass this test. You can also use LLM

198
00:12:20,388 --> 00:12:24,164
led evals to assess completeness. That is, did the response

199
00:12:24,244 --> 00:12:27,740
fully answer the question. Now, neither of

200
00:12:27,772 --> 00:12:31,592
these will necessarily get you accuracy. For accuracy, you need

201
00:12:31,608 --> 00:12:35,216
to, you know, define what accuracy means, and that's individual to

202
00:12:35,240 --> 00:12:38,712
each question. But these can still get you a lot. And again,

203
00:12:38,768 --> 00:12:41,584
the strength of these is you can run them on any question,

204
00:12:41,704 --> 00:12:45,672
even in real time. I got these from Athena AI,

205
00:12:45,848 --> 00:12:48,888
which is a startup in this space, but there are other

206
00:12:48,936 --> 00:12:52,648
companies in the space as well, doing novel and cool things with

207
00:12:52,696 --> 00:12:56,512
monitoring llms in production. I do think you can roll your own

208
00:12:56,568 --> 00:12:59,960
on a lot of these for getting your own evals, but if you don't want

209
00:12:59,992 --> 00:13:03,192
to, you don't have to. So ship it.

210
00:13:03,328 --> 00:13:06,840
Yeah, totally ship it. Write some tests and treat your

211
00:13:06,872 --> 00:13:10,472
LLM like real software, because it is real software.

212
00:13:10,648 --> 00:13:14,176
Before I go, I want to again mention real quick two products that

213
00:13:14,200 --> 00:13:17,504
I'm in no way affiliated with but that I think are doing cool stuff.

214
00:13:17,664 --> 00:13:21,552
So Marvin AI is a python library where you can very quickly

215
00:13:21,608 --> 00:13:25,312
write evaluation rubrics to do classification or score,

216
00:13:25,488 --> 00:13:29,344
and it'll manage both the API interactions and also will

217
00:13:29,384 --> 00:13:33,032
transform your rubrics into full prompts to then send to

218
00:13:33,048 --> 00:13:36,520
the OpenAI models. And then Athena AI,

219
00:13:36,632 --> 00:13:40,160
that's Athena with an I in the middle, is doing some really cool things with

220
00:13:40,192 --> 00:13:43,432
LLM led evals, including specifically for rag

221
00:13:43,488 --> 00:13:46,816
applications. This is me on LinkedIn. Please get in

222
00:13:46,840 --> 00:13:50,312
touch if you're interested. And here's my substack. I wrote something on

223
00:13:50,328 --> 00:13:53,802
this specific specifically, but I'm also writing about LLM evals

224
00:13:53,858 --> 00:13:56,890
in general, red teaming, other data science topics,

225
00:13:56,962 --> 00:13:58,594
et cetera. Thank you for coming.

