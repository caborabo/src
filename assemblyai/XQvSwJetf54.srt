1
00:00:27,400 --> 00:00:30,696
Hello, good morning, good afternoon,

2
00:00:30,800 --> 00:00:32,724
good evening, wherever you are.

3
00:00:33,784 --> 00:00:38,164
Welcome to CON 42, observability 2024,

4
00:00:38,704 --> 00:00:41,004
the online conference about observability.

5
00:00:42,384 --> 00:00:46,280
My name is Swapnil and I am going to talk a bit about observability

6
00:00:46,352 --> 00:00:49,600
today. I would like to thank the

7
00:00:49,632 --> 00:00:53,704
organizers of this conference for including this session and specifically

8
00:00:53,744 --> 00:00:56,844
including it into the monitorings section,

9
00:00:57,424 --> 00:01:01,004
because that is the primary motivation of this session,

10
00:01:01,824 --> 00:01:06,324
to understand what is the difference between monitoring

11
00:01:06,704 --> 00:01:10,164
the current state of observability and what it can be.

12
00:01:12,344 --> 00:01:14,204
So what is monitoring?

13
00:01:15,824 --> 00:01:19,936
We have been doing monitoring over the years for our applications

14
00:01:20,040 --> 00:01:23,804
and infrastructure that

15
00:01:23,844 --> 00:01:27,980
can be getting the heartbeat or the logs

16
00:01:28,132 --> 00:01:31,572
of that system, but it was limited

17
00:01:31,628 --> 00:01:32,384
to that.

18
00:01:34,084 --> 00:01:37,524
Then why do we need observability when we have monitoring

19
00:01:37,644 --> 00:01:39,584
when we get the data that we need?

20
00:01:40,124 --> 00:01:43,540
Right. So before we dive further into

21
00:01:43,572 --> 00:01:47,588
this session, let's have a look at what observability is by definition,

22
00:01:47,716 --> 00:01:51,476
and then we will see different aspects

23
00:01:51,500 --> 00:01:55,492
of it. So observability

24
00:01:55,628 --> 00:01:59,196
by definition is defined as the ability to

25
00:01:59,220 --> 00:02:02,828
determine a system's internal state by examining its

26
00:02:02,876 --> 00:02:06,860
outputs. So what

27
00:02:06,892 --> 00:02:10,924
are these outputs? In the technical term, these outputs

28
00:02:10,964 --> 00:02:13,864
are termed as signals.

29
00:02:16,304 --> 00:02:20,736
They are classified primarily into these four types,

30
00:02:20,800 --> 00:02:23,564
as we see matrix events, log entries.

31
00:02:25,264 --> 00:02:28,592
This data can be captured by using different agents.

32
00:02:28,768 --> 00:02:32,224
They provide that data and a mechanism where you can collect

33
00:02:32,264 --> 00:02:36,336
that. So this collection is

34
00:02:36,360 --> 00:02:39,872
stored into a time series or similar database where you can see

35
00:02:39,968 --> 00:02:43,648
the value of these signals

36
00:02:43,736 --> 00:02:47,440
at different time intervals and then kind of make decision based

37
00:02:47,472 --> 00:02:51,192
on that. So these

38
00:02:51,328 --> 00:02:55,524
signals are then aggregated into the database

39
00:02:56,224 --> 00:03:02,168
right on the time series level. And first

40
00:03:02,216 --> 00:03:06,284
thing that people want to do based on this data is visualization.

41
00:03:08,304 --> 00:03:11,928
You might have seen Grafana hall, right. So where

42
00:03:11,976 --> 00:03:16,392
we have different ways where you can visualize the data by

43
00:03:16,448 --> 00:03:20,048
creating graphs, panels or different

44
00:03:20,216 --> 00:03:23,644
visualizations. In addition to this,

45
00:03:24,064 --> 00:03:27,904
the important part about observability as using this

46
00:03:27,944 --> 00:03:32,152
data and alert the user for any abnormal

47
00:03:32,208 --> 00:03:35,516
behavior of the system based on thresholds

48
00:03:35,540 --> 00:03:37,184
or based on some events.

49
00:03:38,964 --> 00:03:43,196
So this is the primary observability that we see

50
00:03:43,260 --> 00:03:44,544
at most places.

51
00:03:46,444 --> 00:03:50,244
But this has a number of

52
00:03:50,284 --> 00:03:53,544
challenges because of the amount of data that we get.

53
00:03:55,244 --> 00:03:58,744
It is not only the amount of data, but the

54
00:03:59,344 --> 00:04:03,296
quality of the data and how we

55
00:04:03,320 --> 00:04:04,924
can use the data bandwidth.

56
00:04:06,224 --> 00:04:09,680
So let's have a look at some the challenges

57
00:04:09,872 --> 00:04:11,324
related to observability.

58
00:04:14,064 --> 00:04:18,644
So first challenge is the increasing complexity of software systems.

59
00:04:19,864 --> 00:04:23,368
So what happened initially was people use

60
00:04:23,416 --> 00:04:27,330
the existing monitoring setup and made a

61
00:04:27,402 --> 00:04:30,802
few changes and they thought they

62
00:04:30,818 --> 00:04:34,694
are ready for observability, but it is not.

63
00:04:35,114 --> 00:04:40,094
Why? Because the observability stack

64
00:04:40,434 --> 00:04:44,094
needs to understand the increasing complexity of your systems.

65
00:04:44,674 --> 00:04:49,194
So in today's distributed systems,

66
00:04:49,314 --> 00:04:52,980
there are a number of applications, infrastructure components

67
00:04:53,012 --> 00:04:56,820
are added every day. So the

68
00:04:56,852 --> 00:05:00,348
observatory stack needs to understand the

69
00:05:00,516 --> 00:05:04,004
new components that are being added, what is the data that they are

70
00:05:04,044 --> 00:05:07,584
getting about these components, and how to

71
00:05:08,484 --> 00:05:12,436
provide similar visualization

72
00:05:12,580 --> 00:05:16,436
alerting mechanism to the end user with

73
00:05:16,460 --> 00:05:20,578
the same inquest software system. So that complexity

74
00:05:20,626 --> 00:05:24,778
needs to be understood by the system. And we'll see why this complexity makes a

75
00:05:24,906 --> 00:05:26,294
statement about this.

76
00:05:27,474 --> 00:05:30,946
Because with cloud native technologies, there are a

77
00:05:30,970 --> 00:05:33,374
number of components that are added every day.

78
00:05:34,514 --> 00:05:37,866
They can be installed with a click of a button or with

79
00:05:37,930 --> 00:05:41,914
some motivation using DevOps. And they

80
00:05:41,954 --> 00:05:46,108
start in just creating a number of metrics and logs and sending

81
00:05:46,236 --> 00:05:49,748
them to the monitoring system or the observability system that you are

82
00:05:49,876 --> 00:05:53,228
deployed. So it

83
00:05:53,276 --> 00:05:57,332
kind of provides a lot of data that cannot,

84
00:05:57,508 --> 00:06:00,424
if not understood by the system, is of no use,

85
00:06:01,004 --> 00:06:04,644
right? In addition to just receiving

86
00:06:04,684 --> 00:06:08,620
the data, the expectation of the end user is to

87
00:06:08,732 --> 00:06:12,424
have the end to end visibility into the complete stack,

88
00:06:14,084 --> 00:06:17,780
not only in terms of errors, but every activity

89
00:06:17,812 --> 00:06:21,404
that you do on the system needs

90
00:06:21,444 --> 00:06:25,356
to be tracked. So you should be able to track each event from

91
00:06:25,420 --> 00:06:29,004
the user to the backend system, or from backend

92
00:06:29,044 --> 00:06:32,140
system to the user who the data flows and

93
00:06:32,212 --> 00:06:35,932
where it goes, at which point of the system it touches. You need

94
00:06:35,948 --> 00:06:36,824
to know that.

95
00:06:39,984 --> 00:06:43,480
And one of the primary drivers for observability

96
00:06:43,632 --> 00:06:46,324
beyond monitoring is the cost.

97
00:06:48,344 --> 00:06:51,784
The cost of downtime can be very high

98
00:06:51,904 --> 00:06:55,520
for both end user as well as the product

99
00:06:55,592 --> 00:06:59,872
of the product teams if the

100
00:06:59,888 --> 00:07:03,436
downtime is not understood. So that

101
00:07:03,460 --> 00:07:07,244
was the primary motivation for having this observatory system beyond mounting that can

102
00:07:07,284 --> 00:07:11,704
give additional information and insights and actions

103
00:07:12,244 --> 00:07:15,864
for the data that we consume from the mounting systems.

104
00:07:16,964 --> 00:07:20,604
In addition to that, in the recent years, what we have seen is the

105
00:07:20,644 --> 00:07:24,424
cost of observability. Platform itself can be very high

106
00:07:25,684 --> 00:07:29,564
if the user does not know what data they are ingesting,

107
00:07:30,064 --> 00:07:33,400
how much data they are ingesting, how they are processing

108
00:07:33,432 --> 00:07:36,608
the data, for creating the dashboards and alerts,

109
00:07:36,776 --> 00:07:40,736
then the running of objective platform can be very costly

110
00:07:40,760 --> 00:07:44,072
to system and it can bring a very big hole

111
00:07:44,088 --> 00:07:45,164
into a pocket.

112
00:07:46,824 --> 00:07:50,472
So that is another challenge that people are facing. And why do

113
00:07:50,488 --> 00:07:53,616
we need objective platform? We are good with what

114
00:07:53,640 --> 00:07:57,512
we have with monitoring and we will have people who can basically

115
00:07:57,568 --> 00:08:00,244
look at this information and take actions.

116
00:08:02,344 --> 00:08:06,216
And the additional driver that we see recently is the security part of

117
00:08:06,240 --> 00:08:10,352
it. Security has always been

118
00:08:10,448 --> 00:08:14,176
key to all the enterprise and product

119
00:08:14,240 --> 00:08:17,808
deployments, but with

120
00:08:17,936 --> 00:08:21,480
the adoption of cloud native technologies. All the

121
00:08:21,552 --> 00:08:25,454
data that is sitting on cloud security brings

122
00:08:25,494 --> 00:08:29,246
another aspect to it. So you need to know

123
00:08:29,350 --> 00:08:33,034
where my data is, how my data is flowing,

124
00:08:33,494 --> 00:08:37,434
if the data is flowing correctly or not, is it security compliant

125
00:08:38,014 --> 00:08:41,470
so that all data, you can get

126
00:08:41,502 --> 00:08:45,158
the information from the laws or matrix

127
00:08:45,206 --> 00:08:49,238
related to that. But you need to have a mechanism to get

128
00:08:49,286 --> 00:08:52,554
alerted for the scenarios that affect security.

129
00:08:53,334 --> 00:08:56,462
For example, my transit data is

130
00:08:56,478 --> 00:09:00,190
secured with TL's. Where is my Cert going to

131
00:09:00,222 --> 00:09:03,446
expire? Who is going to renew the Cert if that

132
00:09:03,470 --> 00:09:06,590
is expired? Right? So just one

133
00:09:06,622 --> 00:09:09,754
use case, right? And there are many similar to this.

134
00:09:10,334 --> 00:09:13,954
And in addition to security, there are compliance activities.

135
00:09:14,734 --> 00:09:18,626
So many enterprise system

136
00:09:18,690 --> 00:09:23,014
as well as financial systems require you to be compliant to certain things.

137
00:09:23,554 --> 00:09:25,894
And this involves a number of steps.

138
00:09:26,714 --> 00:09:30,934
Each step is equally important and can bring down the compliance efforts.

139
00:09:31,434 --> 00:09:34,938
So you need to have the ability to get

140
00:09:34,986 --> 00:09:39,374
the compliance requirement as part of observability,

141
00:09:40,034 --> 00:09:43,250
see what data you are ingesting and make

142
00:09:43,322 --> 00:09:46,334
decisions out of that, or make alerts based on that.

143
00:09:47,074 --> 00:09:50,170
So the current systems are not completely

144
00:09:50,202 --> 00:09:53,530
capable of doing that. So that is a challenge. So you

145
00:09:53,562 --> 00:09:57,178
need specific personnel to

146
00:09:57,306 --> 00:10:01,614
look into this, do the changes and then they beyond that.

147
00:10:03,034 --> 00:10:04,974
So these are the current challenges.

148
00:10:06,594 --> 00:10:10,308
And to overcome these challenges, what we

149
00:10:10,356 --> 00:10:13,704
are proposing is observability 2.0.

150
00:10:15,044 --> 00:10:18,436
What is it we have seen, we are

151
00:10:18,500 --> 00:10:22,036
getting different signals from different

152
00:10:22,140 --> 00:10:26,564
agents. Each agent

153
00:10:26,724 --> 00:10:30,404
that is sending the data is sent into

154
00:10:30,444 --> 00:10:34,108
a different format for the data

155
00:10:34,196 --> 00:10:37,164
metadata and everything that they are sending to the system.

156
00:10:39,864 --> 00:10:43,976
If the format is different, it becomes very difficult

157
00:10:44,120 --> 00:10:48,136
to manage the data and then showcase in

158
00:10:48,160 --> 00:10:50,884
the same user interface.

159
00:10:51,744 --> 00:10:54,964
For example, Grafana has done a very

160
00:10:55,424 --> 00:10:59,604
good job of aggregating data from different data sources.

161
00:11:00,944 --> 00:11:04,516
So you can configure different data sources, consume the data

162
00:11:04,620 --> 00:11:08,460
and then show it on the visualization

163
00:11:08,532 --> 00:11:12,024
panel. But if you want to

164
00:11:12,724 --> 00:11:17,820
use the

165
00:11:17,932 --> 00:11:21,676
data from different sources into single panel or single UI

166
00:11:21,700 --> 00:11:25,524
frame, it becomes a bit difficult and sometimes not manageable

167
00:11:25,604 --> 00:11:29,704
to the user, primarily because

168
00:11:30,024 --> 00:11:34,044
the information that you get from the sources is different.

169
00:11:35,504 --> 00:11:39,040
So what we need is a unified database wherein whatever

170
00:11:39,112 --> 00:11:42,672
data that you are sending from any agent, so we

171
00:11:42,688 --> 00:11:46,684
are basically calling it as poly agent. Any agent that you are sending the data,

172
00:11:47,064 --> 00:11:51,016
consume the data and store into a very similar format inside

173
00:11:51,040 --> 00:11:54,248
the database. So that unified database

174
00:11:54,336 --> 00:11:57,964
adds a different value to the objective platform

175
00:11:58,044 --> 00:12:02,124
in terms of setting the context of the information

176
00:12:02,204 --> 00:12:05,316
that is received, correlating it

177
00:12:05,380 --> 00:12:08,924
and enriching it. So what is the context? So for example,

178
00:12:09,004 --> 00:12:11,504
for every logline that I am receiving,

179
00:12:12,244 --> 00:12:17,024
I need to know where it is coming from, what is the application?

180
00:12:18,244 --> 00:12:21,764
If you are deploying it into kubernetes, what is the Kubernetes

181
00:12:21,804 --> 00:12:25,534
cluster name? What is the pod, what is the namespace?

182
00:12:26,274 --> 00:12:29,778
If you're sending it from host, what is the host

183
00:12:29,826 --> 00:12:33,174
ip address? Or what is the host name? What is the

184
00:12:34,394 --> 00:12:37,698
cloud? If you're using AWS, what is the region from

185
00:12:37,746 --> 00:12:41,586
where the host is situated? All this is the context about

186
00:12:41,650 --> 00:12:44,946
that particular log line and that needs to be stored so

187
00:12:44,970 --> 00:12:48,810
that you can correlate it with additional

188
00:12:48,842 --> 00:12:52,662
services. So based on this data,

189
00:12:52,798 --> 00:12:56,670
you can basically see what

190
00:12:56,702 --> 00:13:00,394
all services are failing into a particular AWS zone.

191
00:13:00,814 --> 00:13:03,954
If I want to see that, I need to have this data stored.

192
00:13:04,534 --> 00:13:08,654
And sometimes what happens is the raw data does not have this amount

193
00:13:08,694 --> 00:13:12,486
of information. The application is just sending the

194
00:13:12,670 --> 00:13:16,150
data to the monitoring platform or the observability platform,

195
00:13:16,302 --> 00:13:19,634
but it doesn't bother sending all the details related to that

196
00:13:20,254 --> 00:13:23,670
application. So then it becomes the job of

197
00:13:23,702 --> 00:13:27,134
the agents to enrich the data, or job

198
00:13:27,174 --> 00:13:30,750
of the objective platform to scrape that additional information

199
00:13:30,862 --> 00:13:34,634
and enrich the data so that you have all the data with the context

200
00:13:34,974 --> 00:13:39,222
for you to be correlating

201
00:13:39,318 --> 00:13:41,194
or when you are querying it.

202
00:13:43,354 --> 00:13:47,426
What this helps is setting up dynamic and intelligent

203
00:13:47,490 --> 00:13:50,994
alerting. What is dynamic and intelligent

204
00:13:51,034 --> 00:13:55,994
alerting? So based

205
00:13:56,034 --> 00:13:59,494
on the correlated information,

206
00:13:59,874 --> 00:14:03,170
you can set the alert that looks at the

207
00:14:03,202 --> 00:14:06,506
data and then makes a decision rather than some defined

208
00:14:06,570 --> 00:14:10,078
threshold behind the scenes. So the threshold

209
00:14:10,126 --> 00:14:13,230
can be dynamic or it can

210
00:14:13,262 --> 00:14:17,142
be based on some dynamic activity. For example, your power detection has

211
00:14:17,198 --> 00:14:20,934
gone from a certain percentage

212
00:14:21,014 --> 00:14:24,758
plus 20% in matter of five minutes. Then it will create an alert

213
00:14:24,806 --> 00:14:28,430
because it is basically mounting the current of

214
00:14:28,462 --> 00:14:31,794
that port. Same goes for host,

215
00:14:32,374 --> 00:14:36,270
and that adds the intelligence into the system

216
00:14:36,382 --> 00:14:39,954
to basically just change the threshold

217
00:14:40,334 --> 00:14:42,114
at any point of time.

218
00:14:44,854 --> 00:14:48,822
And all this data can be fed into a

219
00:14:48,838 --> 00:14:52,846
machine learning algorithm to find out patterns and

220
00:14:52,950 --> 00:14:55,554
creating actions out of this.

221
00:14:56,294 --> 00:14:59,834
So we'll see some examples of this actions in the

222
00:15:00,484 --> 00:15:05,132
next slides. But this is the primary motivation

223
00:15:05,188 --> 00:15:09,064
behind obsolete zero and how we are trying to build it into a platform.

224
00:15:11,684 --> 00:15:15,428
The most important part in all this is that we

225
00:15:15,476 --> 00:15:19,644
should not go into the same space where we were while

226
00:15:19,684 --> 00:15:23,372
trying to solve the problem. So we are

227
00:15:23,428 --> 00:15:28,528
making sure that all the data

228
00:15:28,576 --> 00:15:34,344
that we are consuming, that we are utilizing and putting

229
00:15:34,384 --> 00:15:37,444
into the product is using the open standards.

230
00:15:38,464 --> 00:15:42,240
So any new system or existing system that

231
00:15:42,272 --> 00:15:45,736
is using the open source standards for this should be able to use the

232
00:15:45,760 --> 00:15:49,248
data if you want to migrate from the current system.

233
00:15:49,296 --> 00:15:57,500
So that should be the case for observability how

234
00:15:57,532 --> 00:15:58,904
we can achieve this.

235
00:16:00,884 --> 00:16:04,380
So it's a combined effort. So it's not only that the operations

236
00:16:04,452 --> 00:16:08,892
teams deploy the stack and the

237
00:16:08,948 --> 00:16:12,868
development team starts using it for

238
00:16:12,916 --> 00:16:16,924
using observatory zero, the development practices needs to

239
00:16:16,964 --> 00:16:21,002
change. So what we call it as observability

240
00:16:21,098 --> 00:16:24,562
driven development, that needs to come from

241
00:16:24,618 --> 00:16:28,482
development as well. All the new applications,

242
00:16:28,618 --> 00:16:31,946
all the new infrastructure that we are adding needs to be designed

243
00:16:31,970 --> 00:16:35,178
for observability. It needs to send the

244
00:16:35,306 --> 00:16:39,082
required signals with the correct data to

245
00:16:39,098 --> 00:16:42,722
the platform, so that it can help you find

246
00:16:42,778 --> 00:16:46,428
the issues easily. So we

247
00:16:46,476 --> 00:16:49,932
will leverage the existing monitoring data. But at the same time we

248
00:16:49,948 --> 00:16:53,524
are expecting a collaboration between development and operations team

249
00:16:53,604 --> 00:16:56,916
to basically set the

250
00:16:56,980 --> 00:17:00,516
infrastructure correctly and the development teams to have the

251
00:17:00,540 --> 00:17:04,572
right side of instrumentation that is sending the required

252
00:17:04,628 --> 00:17:06,944
data to the observatory platform.

253
00:17:08,004 --> 00:17:11,798
And as always, this is a continuous

254
00:17:11,846 --> 00:17:15,342
improvement into the system. So you might not get everything in

255
00:17:15,358 --> 00:17:18,686
the first go, but you will evolve as you go and you basically

256
00:17:18,750 --> 00:17:22,390
add additional information both from the instrumentation side and from

257
00:17:22,422 --> 00:17:24,274
the infrastructure components.

258
00:17:27,374 --> 00:17:31,254
So this has helped into faster

259
00:17:31,374 --> 00:17:34,726
issue resolutions for us, because we are able

260
00:17:34,790 --> 00:17:38,014
to correlate the data and come to a conclusion. Okay,

261
00:17:38,054 --> 00:17:42,834
this is the application that is being

262
00:17:43,414 --> 00:17:47,710
getting the errors, and this is only into this AWS region,

263
00:17:47,782 --> 00:17:51,990
on this node, into this particular port because of the correlation

264
00:17:52,062 --> 00:17:56,102
that you could find out. This helps the

265
00:17:56,278 --> 00:17:59,434
system performance as well as the resilience of the system,

266
00:18:00,374 --> 00:18:04,034
and it will help you with the collaboration activities as well.

267
00:18:06,144 --> 00:18:09,224
So if we do this, this basically helps you into

268
00:18:09,264 --> 00:18:12,672
the automation part of it, where we saw you

269
00:18:12,688 --> 00:18:16,072
can have machine learning and AI on

270
00:18:16,128 --> 00:18:19,284
the objective platform. So what is that?

271
00:18:20,264 --> 00:18:22,684
So with the data that we have,

272
00:18:23,384 --> 00:18:26,720
we enhance the root cause analysis for

273
00:18:26,792 --> 00:18:31,024
any errors or failures that we see into the observed system.

274
00:18:33,044 --> 00:18:36,396
So we get a lot of additional details

275
00:18:36,460 --> 00:18:39,900
and correlation to the other services, so that we can track

276
00:18:40,012 --> 00:18:43,324
the entire failure from

277
00:18:43,364 --> 00:18:44,504
one service to another.

278
00:18:46,484 --> 00:18:50,012
This also helps into setting up alerts

279
00:18:50,068 --> 00:18:54,396
with anomaly. So you find the services

280
00:18:54,540 --> 00:18:58,010
that are showing these patterns and then

281
00:18:58,042 --> 00:19:01,762
you alert based on that. So you can include

282
00:19:01,858 --> 00:19:05,754
the algorithms as part of your observability system that are analyzing

283
00:19:05,794 --> 00:19:09,442
this data and then giving the output that can be used

284
00:19:09,498 --> 00:19:11,734
by the alerting system.

285
00:19:13,474 --> 00:19:16,842
You can help this to optimize the system performance, so that you

286
00:19:16,858 --> 00:19:20,370
can tune the system regularly based on what kind

287
00:19:20,402 --> 00:19:26,040
of data that you see into the now dashboards,

288
00:19:26,192 --> 00:19:30,304
and as we have seen, the increasing alerting

289
00:19:30,344 --> 00:19:34,880
part of it. So you can create the intelligent alerts for changes

290
00:19:35,072 --> 00:19:38,592
or forecast on average. So there is

291
00:19:38,608 --> 00:19:42,360
an interesting use case that we have achieved with this and that is very important.

292
00:19:42,432 --> 00:19:46,192
So you can do a predictive maintenance of your system or

293
00:19:46,248 --> 00:19:50,344
infrastructure based on the data that you consume and

294
00:19:50,764 --> 00:19:54,604
c. Right. So most

295
00:19:54,644 --> 00:19:57,584
of us are deploying the applications on kubernetes.

296
00:19:58,244 --> 00:20:02,384
If you are using stateful sets, you mandatory need a volume

297
00:20:03,044 --> 00:20:05,224
where you store the state data.

298
00:20:06,604 --> 00:20:10,700
And it is very possible that the data volume

299
00:20:10,772 --> 00:20:14,588
might not be sufficient for the amount of data that you ingest

300
00:20:14,756 --> 00:20:19,544
over the period of time. So what

301
00:20:20,844 --> 00:20:24,724
you need to do if the data is exceeding to the volume

302
00:20:24,764 --> 00:20:28,396
that you need to manually or maybe sometimes with automation,

303
00:20:28,500 --> 00:20:30,944
increase the size of the volume.

304
00:20:31,844 --> 00:20:35,264
That is not something that is being taken care of by the

305
00:20:35,604 --> 00:20:37,104
cloud till now.

306
00:20:38,364 --> 00:20:42,482
So what we have done is we have applied the

307
00:20:42,628 --> 00:20:46,598
forecast algorithm to the data

308
00:20:46,646 --> 00:20:50,270
that we receive for volume sizes and

309
00:20:50,302 --> 00:20:53,998
we forecast that this is the timeframe

310
00:20:54,046 --> 00:20:57,634
when the volume will be filled. So what will be the volumes

311
00:20:58,134 --> 00:21:01,394
utilization in next five days? Next three days, right.

312
00:21:01,894 --> 00:21:05,526
And based on that you can do the predictive maintenance. You can go ahead and

313
00:21:05,670 --> 00:21:09,674
increase the volume size beforehand so that you don't turn into the errors.

314
00:21:11,284 --> 00:21:15,612
So this is, this kind of reactive maintenance is possible into observability system

315
00:21:15,708 --> 00:21:19,884
when you have the data that is not only correct but that is corrected

316
00:21:19,924 --> 00:21:23,504
and that can be put to the ML.

317
00:21:24,964 --> 00:21:28,412
So that helps you with capacity planning. So similar to what the use case that

318
00:21:28,428 --> 00:21:32,544
I mentioned. So you can increase not only the volumes but the number of nodes,

319
00:21:32,884 --> 00:21:36,256
number of replicas of the pods, number of hosts that are

320
00:21:36,280 --> 00:21:39,872
needed beyond the auto scaling part that is

321
00:21:39,968 --> 00:21:44,284
given by the cloud provider or orchestrator platforms.

322
00:21:45,304 --> 00:21:49,096
You can define dynamic thresholds that you have seen already. So the threshold

323
00:21:49,160 --> 00:21:52,336
basically is automatically updated using the change

324
00:21:52,400 --> 00:21:55,912
algorithm. So based on the current utilization of

325
00:21:55,928 --> 00:21:59,680
the system, it will see for next 20% in the next five

326
00:21:59,712 --> 00:22:02,644
days. And this means it will basically throw an alert.

327
00:22:03,154 --> 00:22:06,534
Okay, this is not behaving correctly. Please have a look at that.

328
00:22:07,434 --> 00:22:11,786
In addition to this, just by we

329
00:22:11,810 --> 00:22:15,914
have the alerting capabilities which can create the incident

330
00:22:15,994 --> 00:22:19,618
response to the off streams for

331
00:22:19,666 --> 00:22:22,970
critical alerts. So it will automatically send the

332
00:22:23,002 --> 00:22:26,602
alert to things like pagerduty or create Jira tickets and they

333
00:22:26,618 --> 00:22:27,814
can have a look at that.

334
00:22:29,054 --> 00:22:32,270
And in addition to the capacity planning,

335
00:22:32,302 --> 00:22:34,754
you can do proactive workforce management as well.

336
00:22:36,174 --> 00:22:40,294
So this is basically a

337
00:22:40,374 --> 00:22:44,214
set of things that are intended to

338
00:22:44,254 --> 00:22:46,114
be in an observable platform.

339
00:22:48,614 --> 00:22:51,634
The current observable platforms that we see,

340
00:22:52,414 --> 00:22:56,354
they have few of these things in pieces, but not all.

341
00:22:57,134 --> 00:23:00,702
And the most important thing that we are looking at in object

342
00:23:00,758 --> 00:23:04,702
2.0 is open standards. So you

343
00:23:04,718 --> 00:23:08,926
should be able to work with things like promises hotel and

344
00:23:09,030 --> 00:23:12,954
blockchain for all your objective operations.

345
00:23:13,734 --> 00:23:17,514
Right. So this is

346
00:23:17,854 --> 00:23:20,114
mostly what I had in this session.

347
00:23:23,434 --> 00:23:26,826
Just a quick word about the observation that I

348
00:23:26,850 --> 00:23:31,210
represent. So we are a small observer

349
00:23:31,282 --> 00:23:35,146
startup cloudflues. We have

350
00:23:35,210 --> 00:23:39,074
a product and you can basically have look at that

351
00:23:39,154 --> 00:23:42,854
from our download page, or you can even play about it on the

352
00:23:43,194 --> 00:23:45,814
playground that we can see in the links.

353
00:23:46,874 --> 00:23:50,458
And yeah, that's pretty much that I

354
00:23:50,466 --> 00:23:51,074
had from my side.

