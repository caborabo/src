1
00:00:27,554 --> 00:00:30,850
Hello. Welcome to my session on deep learning for protein

2
00:00:30,882 --> 00:00:35,490
structure prediction. My name is Yaroslav. Let's get started who

3
00:00:35,522 --> 00:00:39,530
I am. I spent four years of my life doing machine

4
00:00:39,562 --> 00:00:43,454
learning research and machine learning related software development.

5
00:00:43,794 --> 00:00:47,402
I have a master's degree in computational biology and I also

6
00:00:47,458 --> 00:00:51,554
worked for a year on antibody structure prediction,

7
00:00:51,674 --> 00:00:55,232
machine learning for drug discovery in a biopharmaceutical

8
00:00:55,288 --> 00:00:59,768
company. Today I want to talk about proteins,

9
00:00:59,856 --> 00:01:03,240
why we need proteins and how they can

10
00:01:03,272 --> 00:01:05,124
help solve different problems.

11
00:01:06,504 --> 00:01:10,968
I want to talk about evolutionary information and information

12
00:01:11,056 --> 00:01:14,844
that we can use to get

13
00:01:15,624 --> 00:01:20,536
more data about proteins and predictions,

14
00:01:20,550 --> 00:01:24,636
their behavior and structure. I want to talk about structure

15
00:01:24,700 --> 00:01:28,224
prediction methods and a little bit of history behind that.

16
00:01:29,164 --> 00:01:32,740
And we will discuss different kinds of structure

17
00:01:32,772 --> 00:01:36,516
prediction methods, physics based, statistical and

18
00:01:36,620 --> 00:01:40,504
finally deep learning. So first let's

19
00:01:41,644 --> 00:01:45,704
talk about what proteins are. So proteins are

20
00:01:46,414 --> 00:01:51,086
composed of amino acids, and amino acids are the

21
00:01:51,110 --> 00:01:55,594
building blocks of proteins. They are often

22
00:01:56,854 --> 00:02:00,314
shown as letters or those green

23
00:02:01,174 --> 00:02:04,766
boxes. Here are the same things as letters on

24
00:02:04,790 --> 00:02:08,918
the image below. And proteins are

25
00:02:08,966 --> 00:02:11,994
essentially chains of amino acids.

26
00:02:12,884 --> 00:02:16,620
And proteins have structure,

27
00:02:16,732 --> 00:02:20,484
different kinds of structure. First kind of structure

28
00:02:20,564 --> 00:02:24,144
is the amino acid chain, the sequence

29
00:02:24,484 --> 00:02:28,104
of amino acids that defines the protein.

30
00:02:28,844 --> 00:02:32,572
Secondary structure is a structure that

31
00:02:32,708 --> 00:02:36,036
local parts of protein intake.

32
00:02:36,180 --> 00:02:40,336
They can have spirals, or they

33
00:02:40,360 --> 00:02:44,084
can align on each other in different ways.

34
00:02:44,384 --> 00:02:48,000
And the third way we can describe the

35
00:02:48,032 --> 00:02:52,120
structure of a protein is its ternary structure. So it's

36
00:02:52,152 --> 00:02:55,824
the whole 3d structure of

37
00:02:55,864 --> 00:02:58,952
one chain of protein. And finally,

38
00:02:59,008 --> 00:03:02,464
we can describe the structure of a complex

39
00:03:02,504 --> 00:03:06,884
of proteins, so multiple chains interacting together.

40
00:03:07,524 --> 00:03:11,544
And why do we need to describe this

41
00:03:12,444 --> 00:03:15,544
structure, and why do we need to predict it and know it?

42
00:03:16,724 --> 00:03:20,236
So for different kinds of things,

43
00:03:20,300 --> 00:03:24,428
we may want to estimate

44
00:03:24,596 --> 00:03:28,364
the protein function based on its

45
00:03:28,404 --> 00:03:31,844
amino acid sequence, because we don't know the structure. And the structure

46
00:03:31,884 --> 00:03:35,318
is very hard to obtain through

47
00:03:35,366 --> 00:03:39,030
experimental methods. It's much easier to obtain the

48
00:03:39,182 --> 00:03:42,914
amino acid sequence. But to

49
00:03:43,454 --> 00:03:47,206
run some experiments on computers with the

50
00:03:47,310 --> 00:03:50,834
actual 3d structure, we need to first obtain it.

51
00:03:51,294 --> 00:03:54,782
And we may use this information about the structure

52
00:03:54,838 --> 00:03:58,874
to try to understand the protein function,

53
00:03:59,864 --> 00:04:04,724
try to understand how we can modify the protein and what

54
00:04:06,544 --> 00:04:10,004
use cases there are for that particular protein.

55
00:04:10,424 --> 00:04:13,752
So a couple of examples why we

56
00:04:13,768 --> 00:04:18,592
need proteins. First. One is plastic

57
00:04:18,728 --> 00:04:21,724
degeneration. So we can have a bacteria,

58
00:04:22,264 --> 00:04:26,868
genetically modified bacteria, that produces protein

59
00:04:26,956 --> 00:04:31,344
that acts as an enzyme, so it will speed up

60
00:04:31,804 --> 00:04:35,836
breakdown of plastic, and it

61
00:04:35,860 --> 00:04:40,344
can help us get rid of waste in

62
00:04:40,644 --> 00:04:44,916
different kinds of. The other thing

63
00:04:44,980 --> 00:04:48,620
that we can use antibodies for is vaccines and drugs.

64
00:04:48,692 --> 00:04:52,270
So for example, on the right there

65
00:04:52,302 --> 00:04:56,034
is a coronavirus displayed. And it has proteins

66
00:04:57,214 --> 00:05:00,878
going out of the shell. And those are the spike proteins

67
00:05:00,926 --> 00:05:04,606
that the virus uses to enter the

68
00:05:04,630 --> 00:05:08,894
cell. And those are proteins that our

69
00:05:09,014 --> 00:05:12,398
immune system reacts to. And it produces

70
00:05:12,446 --> 00:05:16,286
antibodies, which are also proteins that can bind to

71
00:05:16,430 --> 00:05:20,906
the spike proteins of coronavirus. And they

72
00:05:20,930 --> 00:05:24,654
can alert your immune system to

73
00:05:24,994 --> 00:05:29,414
destroy the virus. How can we

74
00:05:30,074 --> 00:05:33,134
get more information about protein,

75
00:05:33,634 --> 00:05:35,014
a specific protein,

76
00:05:38,794 --> 00:05:42,994
without any other experimental

77
00:05:43,034 --> 00:05:46,402
methods? On that particular protein we

78
00:05:46,418 --> 00:05:49,414
can have a look at similar proteins. And,

79
00:05:50,964 --> 00:05:54,544
and the idea is that if

80
00:05:54,924 --> 00:05:58,292
we have a similar protein, we have a similar structure or maybe a similar

81
00:05:58,388 --> 00:06:02,380
function. If there are some changes

82
00:06:02,452 --> 00:06:06,132
to protein sequence

83
00:06:06,228 --> 00:06:09,548
in some position, maybe there would be a change

84
00:06:09,636 --> 00:06:13,380
in another position which can be far away

85
00:06:13,532 --> 00:06:17,164
in a sequence, but it is actually close

86
00:06:17,324 --> 00:06:21,220
in 3d space. So if

87
00:06:21,252 --> 00:06:24,484
say one position changes its charge,

88
00:06:24,564 --> 00:06:27,596
so the other position has to change charge as well to preserve

89
00:06:27,620 --> 00:06:31,308
the structure. So we can

90
00:06:31,436 --> 00:06:35,900
have a look in a database and find similar proteins,

91
00:06:36,092 --> 00:06:40,500
align them together. So we have kind

92
00:06:40,532 --> 00:06:43,872
of the same structural positions on

93
00:06:43,888 --> 00:06:47,680
top of each other. And that can help us to

94
00:06:47,752 --> 00:06:51,808
get information about how variable is this

95
00:06:51,936 --> 00:06:56,240
concrete position or which positions it

96
00:06:56,352 --> 00:06:58,924
interacts with. Okay,

97
00:07:00,184 --> 00:07:03,656
so structure prediction methods can

98
00:07:03,680 --> 00:07:07,072
be different. And first of all, we have a

99
00:07:07,208 --> 00:07:10,700
protein folding on the upper left

100
00:07:10,732 --> 00:07:14,020
corner. And this is like the natural protein

101
00:07:14,052 --> 00:07:17,188
folding. It's really, really accurate.

102
00:07:17,276 --> 00:07:20,772
And it doesn't need to obtain a

103
00:07:20,788 --> 00:07:24,604
lot of sequence information or information from multiple sequence

104
00:07:24,684 --> 00:07:28,236
alignment. It only has to do

105
00:07:28,260 --> 00:07:32,860
its natural job. But if

106
00:07:32,892 --> 00:07:36,260
we are talking about protein structure prediction methods, they can

107
00:07:36,292 --> 00:07:39,866
be classified by the amount of

108
00:07:39,930 --> 00:07:44,170
information they use and their accuracy.

109
00:07:44,322 --> 00:07:48,394
So on the bottom left, we have physics based methods.

110
00:07:48,554 --> 00:07:53,626
They are not really accurate and they

111
00:07:53,770 --> 00:07:56,850
need a lot of compute to actually produce

112
00:07:56,882 --> 00:08:01,274
the result. So the next thing is methods

113
00:08:01,314 --> 00:08:05,070
using PSSM. PSSM is derived from

114
00:08:05,102 --> 00:08:08,174
multiple sequence alignment, and it is

115
00:08:08,214 --> 00:08:11,958
kind of a statistic about each

116
00:08:12,006 --> 00:08:15,274
and every position of multiple sequence alignment.

117
00:08:15,894 --> 00:08:19,554
Second order methods use coevolution information,

118
00:08:20,014 --> 00:08:24,646
so they will encode information about pair

119
00:08:24,750 --> 00:08:28,270
interactions in multiple

120
00:08:28,302 --> 00:08:31,704
sequence alignment, and use different

121
00:08:31,744 --> 00:08:35,524
kinds of methods produce the result.

122
00:08:36,704 --> 00:08:40,592
And finally, full multiple sequence alignment methods.

123
00:08:40,768 --> 00:08:44,496
We'll use full multiple sequence alignment and

124
00:08:44,560 --> 00:08:48,632
we'll use deep learning to process the

125
00:08:48,648 --> 00:08:52,044
whole data available from multiple sequence alignment.

126
00:08:52,704 --> 00:08:56,416
And for some classes

127
00:08:56,440 --> 00:09:00,714
of proteins which are not

128
00:09:00,834 --> 00:09:05,770
that, for which that

129
00:09:05,842 --> 00:09:08,914
information from multiple sequence alignment is

130
00:09:08,954 --> 00:09:13,574
not really useful. For example, for highly variable proteins

131
00:09:14,274 --> 00:09:18,058
such as antibodies, multiple sequence alignment

132
00:09:18,186 --> 00:09:22,374
can not be that useful to,

133
00:09:24,144 --> 00:09:28,048
to get more information about the protein. And the

134
00:09:28,176 --> 00:09:32,288
other thing is that end to end, deep learning methods are usually

135
00:09:32,336 --> 00:09:36,016
faster than physics based methods. And we will talk about

136
00:09:36,080 --> 00:09:39,848
why in a moment. So on average,

137
00:09:39,936 --> 00:09:43,124
if you have more information, you have more

138
00:09:44,304 --> 00:09:48,016
accuracy. Using different kinds

139
00:09:48,040 --> 00:09:52,524
of information, you get higher on prediction accuracy.

140
00:09:54,024 --> 00:09:58,016
So why is it difficult to get

141
00:09:58,040 --> 00:10:02,284
a result with physics based methods, and why do they have to

142
00:10:03,104 --> 00:10:08,364
use a lot of compute? That's because problem

143
00:10:09,024 --> 00:10:12,432
with a lot of particles interacting.

144
00:10:12,488 --> 00:10:16,284
So if you have even three particles

145
00:10:16,694 --> 00:10:20,794
interacting with each other, and you know the forces acting on them,

146
00:10:21,614 --> 00:10:25,926
that system cannot be solved in a closed form solution.

147
00:10:26,070 --> 00:10:29,754
And any changes to initial state can

148
00:10:30,814 --> 00:10:34,350
change your end state very drastically,

149
00:10:34,502 --> 00:10:37,678
because that's a chaotic system. And the only

150
00:10:37,726 --> 00:10:42,138
thing, the only method we have for solving that problem is

151
00:10:42,326 --> 00:10:45,454
iteration methods, which require a lot of compute.

152
00:10:46,114 --> 00:10:49,614
So, molecular dynamics

153
00:10:49,914 --> 00:10:53,770
methods use simulation, step by step simulation,

154
00:10:53,922 --> 00:10:58,946
and high performance compute systems to see

155
00:10:59,010 --> 00:11:03,690
how a protein folds and how the

156
00:11:03,722 --> 00:11:10,238
parts of the protein move under forces acting

157
00:11:10,286 --> 00:11:13,434
on the protein from inside and from outside.

158
00:11:13,814 --> 00:11:17,518
So those methods usually use

159
00:11:17,646 --> 00:11:21,502
some really, really expensive hardware,

160
00:11:21,598 --> 00:11:25,494
such as supercomputers. But they also

161
00:11:25,534 --> 00:11:29,278
have benefits, such as trajectory analysis

162
00:11:29,406 --> 00:11:33,074
can be performed on the whole

163
00:11:33,414 --> 00:11:37,184
simulation. So you can know the

164
00:11:37,264 --> 00:11:41,084
dynamic behavior of a protein in

165
00:11:41,424 --> 00:11:45,728
some cases. So those methods

166
00:11:45,856 --> 00:11:49,920
work with forces, and there are many different forces acting on

167
00:11:50,112 --> 00:11:53,256
particles in the protein, and some

168
00:11:53,280 --> 00:11:56,324
of them are described here on the on the right.

169
00:11:57,144 --> 00:12:01,616
And those forces are potential

170
00:12:01,680 --> 00:12:05,888
forces, which means they don't depend on particles

171
00:12:05,936 --> 00:12:10,752
velocity, they only depend on particles coordinates

172
00:12:10,808 --> 00:12:13,284
and properties. So,

173
00:12:14,264 --> 00:12:18,084
with the

174
00:12:18,384 --> 00:12:21,084
methods using physics based simulation,

175
00:12:23,064 --> 00:12:26,644
we are struggling to obtain a good first representation,

176
00:12:27,104 --> 00:12:31,102
because to achieve a low energy state, we have

177
00:12:31,118 --> 00:12:34,750
to spend a lot of iterations. So maybe we can do something

178
00:12:34,822 --> 00:12:38,874
and achieve a good first

179
00:12:39,254 --> 00:12:43,062
structure, and then take it from there to speed up

180
00:12:43,078 --> 00:12:47,454
the whole process. And for that, we can use homology modeling.

181
00:12:47,534 --> 00:12:51,510
Homology modeling is based on the same idea as multiple

182
00:12:51,542 --> 00:12:55,194
sequence alignment, that similar sequences have have similar structures.

183
00:12:55,274 --> 00:12:58,914
And if you have a database with structures and their

184
00:12:58,954 --> 00:13:02,930
sequences, you can look for similar sequences to the sequence

185
00:13:02,962 --> 00:13:07,330
you want to fold, and you can find fragments of

186
00:13:07,402 --> 00:13:10,690
that, of similar sequences, and you can

187
00:13:10,722 --> 00:13:14,226
combine them together to create the

188
00:13:14,250 --> 00:13:18,530
first model, and then you can evaluate

189
00:13:18,682 --> 00:13:22,134
multiple such models, or you can fine tune those

190
00:13:22,174 --> 00:13:24,714
models using physics based methods.

191
00:13:25,294 --> 00:13:28,434
The other problem with physics based methods is that

192
00:13:28,854 --> 00:13:33,670
we don't know how

193
00:13:33,822 --> 00:13:39,350
likely this current position is for that

194
00:13:39,422 --> 00:13:43,190
molecule to be in. So if we have a lot of statistics about

195
00:13:43,262 --> 00:13:46,878
which positions we observe in

196
00:13:47,046 --> 00:13:52,078
real proteins, then we can use this information to try

197
00:13:52,126 --> 00:13:56,790
to kind of forbid

198
00:13:56,862 --> 00:14:00,434
some states in a molecular dynamics process.

199
00:14:02,254 --> 00:14:06,630
If we know that this position is unlikely, we will apply forces

200
00:14:06,742 --> 00:14:10,486
to bring the molecule out of this

201
00:14:10,590 --> 00:14:14,374
position, because we assume that this is an optimization dead

202
00:14:14,414 --> 00:14:17,850
end. But for that we need

203
00:14:17,882 --> 00:14:22,410
to know the likelihood of different positions

204
00:14:22,602 --> 00:14:26,370
in the molecular structure. So if we use

205
00:14:26,482 --> 00:14:30,014
statistics, we just get a lot of data and estimate

206
00:14:30,314 --> 00:14:32,214
likelihood of every position.

207
00:14:33,434 --> 00:14:37,338
But it only works on a

208
00:14:37,386 --> 00:14:41,658
specific protein families, because statistics

209
00:14:41,706 --> 00:14:45,654
in one family can be different from statistics from another family.

210
00:14:46,314 --> 00:14:49,442
And that's where deep learning comes

211
00:14:49,498 --> 00:14:53,082
in. What we can do is we can estimate

212
00:14:53,138 --> 00:14:55,898
that position likelihood using machine learning.

213
00:14:55,946 --> 00:14:59,370
And that is what a model

214
00:14:59,402 --> 00:15:02,834
called alphafold one tried to do. So it

215
00:15:02,874 --> 00:15:06,534
tried to predict the likelihood of different positions

216
00:15:08,184 --> 00:15:12,488
for pairs of atoms. So that matrix

217
00:15:12,656 --> 00:15:17,288
in the middle, it can be treated as distribution

218
00:15:17,376 --> 00:15:20,800
over distances between the atoms. And you can see

219
00:15:20,952 --> 00:15:25,664
the diagonal has green

220
00:15:25,824 --> 00:15:29,424
color to it. That means that those atoms are close together,

221
00:15:29,584 --> 00:15:32,948
but some of the other atoms are close together as well,

222
00:15:33,056 --> 00:15:36,824
and they are not adjacent in the sequence.

223
00:15:37,764 --> 00:15:42,304
And to produce this distribution,

224
00:15:43,124 --> 00:15:47,204
we can use sequence and MSA features, which we

225
00:15:47,244 --> 00:15:50,892
can encode like a picture in

226
00:15:50,908 --> 00:15:54,812
a 2d space. And each

227
00:15:54,988 --> 00:15:58,652
position will tell us how

228
00:15:58,748 --> 00:16:02,154
those two atoms, on those two

229
00:16:02,194 --> 00:16:06,066
amino acids, on two different positions, I and

230
00:16:06,090 --> 00:16:10,098
j, interact together. And then finally, when we

231
00:16:10,226 --> 00:16:14,346
produce this, this kind of likelihood map,

232
00:16:14,490 --> 00:16:21,202
we can use physics based methods to fold

233
00:16:21,258 --> 00:16:25,010
the protein really quickly, because we

234
00:16:25,162 --> 00:16:28,078
know which positions it likely to take,

235
00:16:28,166 --> 00:16:32,070
and it really speeds up

236
00:16:32,102 --> 00:16:34,994
the whole physics based process.

237
00:16:36,494 --> 00:16:40,718
The next evolutionary step in protein prediction

238
00:16:40,766 --> 00:16:44,758
is alphafold two model, which uses multiple sequence

239
00:16:44,806 --> 00:16:48,486
alignment directly. And what

240
00:16:48,510 --> 00:16:52,110
it does is it

241
00:16:52,142 --> 00:16:55,658
produces the whole structure end to end with machine learning,

242
00:16:55,746 --> 00:16:59,994
without using any physics based iterational methods, which is

243
00:17:00,034 --> 00:17:04,522
a lot faster. So it can be divided into three steps.

244
00:17:04,578 --> 00:17:07,814
First step is obtaining an input.

245
00:17:08,914 --> 00:17:12,362
Using an input sequence, you can find a

246
00:17:12,378 --> 00:17:16,282
lot of similar sequences to produce an MSA,

247
00:17:16,458 --> 00:17:19,614
and you can also find their structure.

248
00:17:20,394 --> 00:17:23,806
As in homology modeling, you can find templates for

249
00:17:23,930 --> 00:17:27,478
your protein, pieces of other known

250
00:17:27,526 --> 00:17:30,874
structures that are likely similar to yours.

251
00:17:31,334 --> 00:17:34,662
After that, there is deep learning magic happening,

252
00:17:34,718 --> 00:17:40,502
and in the middle, we just encode

253
00:17:40,598 --> 00:17:43,998
the information that we got into the model.

254
00:17:44,166 --> 00:17:47,694
And the final step is structure prediction.

255
00:17:47,854 --> 00:17:51,918
So for that model, a new

256
00:17:52,086 --> 00:17:55,454
kind of structure prediction model was

257
00:17:55,494 --> 00:17:58,926
created, which would predict

258
00:17:58,990 --> 00:18:02,674
and update angles and distances between

259
00:18:03,334 --> 00:18:08,686
amino acids to produce the final result. End to end worked

260
00:18:08,710 --> 00:18:12,278
with geometrical features to get the final result,

261
00:18:12,366 --> 00:18:15,806
which also can be fine

262
00:18:15,830 --> 00:18:19,684
tuned with physics based methods, because sometimes this

263
00:18:19,724 --> 00:18:24,340
result will not be locally accurate because

264
00:18:24,492 --> 00:18:28,044
the model doesn't know physics. And a few iterations of physics based

265
00:18:28,084 --> 00:18:31,396
methods can kind of relax the model and

266
00:18:31,540 --> 00:18:34,584
push some atoms away or

267
00:18:35,124 --> 00:18:39,504
bring them together so the whole structure looks more natural.

268
00:18:40,044 --> 00:18:44,464
The other method for encoding protein information

269
00:18:45,444 --> 00:18:48,908
using a lot of data is language models.

270
00:18:49,036 --> 00:18:52,876
So proteins consist of different amino

271
00:18:52,900 --> 00:18:56,624
acids, just like text consists out of words.

272
00:18:56,924 --> 00:19:00,868
And we can use similar techniques from text

273
00:19:00,916 --> 00:19:04,556
processing and language processing to encode

274
00:19:04,580 --> 00:19:08,932
a lot of sequences into

275
00:19:09,028 --> 00:19:13,044
a large language model. And then we can use this large

276
00:19:13,084 --> 00:19:17,404
language model to encode our input sequence into

277
00:19:18,944 --> 00:19:21,704
some representation from that representation.

278
00:19:21,824 --> 00:19:25,704
Using the same idea as alpha Fold two,

279
00:19:25,744 --> 00:19:29,216
we can predict geometrical

280
00:19:29,240 --> 00:19:32,120
features for the structure,

281
00:19:32,272 --> 00:19:36,444
and we can predict the structure end to end using

282
00:19:37,024 --> 00:19:40,840
a lot more data for language model

283
00:19:40,872 --> 00:19:44,872
per training, protein language model per training. And then we

284
00:19:44,888 --> 00:19:48,448
can use a smaller model to predict the geometric features.

285
00:19:48,616 --> 00:19:52,304
And the same way as before, we can

286
00:19:52,464 --> 00:19:55,736
use refinement steps to

287
00:19:55,920 --> 00:20:00,724
fine tune the model using physics based methods and

288
00:20:01,224 --> 00:20:05,256
final model that was only released

289
00:20:05,400 --> 00:20:09,536
this month. Alphafold three expands

290
00:20:09,560 --> 00:20:14,058
on this idea of alphafold

291
00:20:14,106 --> 00:20:17,514
two template using using templates,

292
00:20:17,594 --> 00:20:21,214
using multiple sequence alignment, and using

293
00:20:21,914 --> 00:20:26,574
other things that bind to proteins to

294
00:20:27,274 --> 00:20:30,730
get the better result in protein structure prediction. So this

295
00:20:30,762 --> 00:20:33,962
model can not only work on

296
00:20:34,018 --> 00:20:37,494
proteins, but it was changed a little bit. So it can

297
00:20:38,594 --> 00:20:42,274
get other information in from things

298
00:20:42,314 --> 00:20:46,546
that proteins bind to or interact with

299
00:20:46,690 --> 00:20:50,810
that are non proteins and

300
00:20:50,882 --> 00:20:54,546
come from different origins, for example protein DNA interactions or

301
00:20:54,570 --> 00:20:58,274
something like that. So essentially it can be

302
00:20:58,354 --> 00:21:02,746
split into three different stages as well. First is

303
00:21:02,930 --> 00:21:07,202
input input building.

304
00:21:07,338 --> 00:21:11,090
The second is deep learning processing. And the

305
00:21:11,122 --> 00:21:14,946
third one was updated too. So it

306
00:21:14,970 --> 00:21:19,042
can predict not only proteins but other

307
00:21:19,218 --> 00:21:22,378
molecular structures too, such as DNA.

308
00:21:22,506 --> 00:21:26,694
And in this model, they used diffusion

309
00:21:27,474 --> 00:21:31,878
module to produce protein

310
00:21:31,926 --> 00:21:35,702
structure and other molecular

311
00:21:35,758 --> 00:21:39,514
structures from noise, similar to

312
00:21:39,974 --> 00:21:43,314
generative AI for images and videos.

313
00:21:43,814 --> 00:21:47,554
And we can see that many of

314
00:21:48,614 --> 00:21:52,742
technologies that are used in image

315
00:21:52,798 --> 00:21:57,924
and text processing, such as diffusion models,

316
00:21:59,704 --> 00:22:01,204
large language models,

317
00:22:02,904 --> 00:22:06,840
transformers and convolutional models,

318
00:22:07,032 --> 00:22:11,600
they all trickled down into biology. And people

319
00:22:11,672 --> 00:22:15,232
found ways to use this technology for biological

320
00:22:15,288 --> 00:22:18,720
applications, which are kind of far from

321
00:22:18,872 --> 00:22:22,136
image processing and also far

322
00:22:22,240 --> 00:22:25,102
from language processing a little bit.

323
00:22:25,248 --> 00:22:27,774
But anyway,

324
00:22:28,434 --> 00:22:33,378
people find new ways to use technologies,

325
00:22:33,426 --> 00:22:37,418
not only in the spaces where they

326
00:22:37,506 --> 00:22:41,054
were created, but also in biology and many other

327
00:22:41,714 --> 00:22:46,034
applications. So today you

328
00:22:46,114 --> 00:22:49,202
learn about physics based methods,

329
00:22:49,378 --> 00:22:53,534
statistical methods, and deep learning methods for protein structure prediction.

330
00:22:54,334 --> 00:22:57,914
You learned that physics based methods require a lot of compute,

331
00:22:58,214 --> 00:23:01,758
and there is a lot of research

332
00:23:01,886 --> 00:23:06,034
on how to speed them up. There are

333
00:23:06,614 --> 00:23:10,262
heuristics to speed up physics based methods

334
00:23:10,438 --> 00:23:14,438
such as statistical potentials and other

335
00:23:14,486 --> 00:23:18,694
statistical tricks to speed

336
00:23:18,734 --> 00:23:22,024
up the protein folding.

337
00:23:22,924 --> 00:23:26,580
The next logical step is to replace statistics with deep

338
00:23:26,612 --> 00:23:30,436
learning and kind of

339
00:23:30,500 --> 00:23:34,380
automate statistical feature recovery from

340
00:23:34,492 --> 00:23:36,544
data using deep learning.

341
00:23:39,044 --> 00:23:43,812
And the problem with that is that getting more data

342
00:23:43,908 --> 00:23:48,018
into a machine learning model, a single model or multiple models is

343
00:23:48,146 --> 00:23:52,146
challenging. And as the time goes, more and

344
00:23:52,170 --> 00:23:55,978
more methods can unify

345
00:23:56,026 --> 00:23:59,522
information from multiple sources to encode it together

346
00:23:59,618 --> 00:24:02,854
and get a better result. For protein prediction,

347
00:24:03,834 --> 00:24:07,010
you learned that end to end methods allow to

348
00:24:07,042 --> 00:24:11,666
use deep learning for every step of structure prediction except

349
00:24:11,690 --> 00:24:14,644
of obtaining the input.

350
00:24:15,664 --> 00:24:19,520
But those kinds of

351
00:24:19,552 --> 00:24:23,040
methods are end to end. Methods are really important because they can

352
00:24:23,072 --> 00:24:27,604
save a lot of time, because they have really

353
00:24:28,024 --> 00:24:31,112
good properties for parallelization, and they

354
00:24:31,128 --> 00:24:34,880
can be run on efficient hardware, and they

355
00:24:34,912 --> 00:24:38,856
don't require as much compute or as hard of a compute

356
00:24:38,960 --> 00:24:42,804
as iterations in physics based methods.

357
00:24:43,424 --> 00:24:46,800
Physics based methods are not dead

358
00:24:46,952 --> 00:24:50,344
still, so there are still use cases where

359
00:24:50,384 --> 00:24:53,648
you can only use physics based methods if you

360
00:24:53,696 --> 00:24:57,320
want to achieve good performance and accuracy. For example,

361
00:24:57,352 --> 00:25:00,936
if you want to analyze trajectories, or if

362
00:25:00,960 --> 00:25:04,376
you want to refine other structures that were

363
00:25:04,400 --> 00:25:08,616
produced by deep learning models without without really

364
00:25:08,800 --> 00:25:12,240
knowing the physics of it. So they are

365
00:25:12,272 --> 00:25:16,592
still useful for post processing and other applications

366
00:25:16,768 --> 00:25:20,728
where accuracy is really important. But they

367
00:25:20,776 --> 00:25:24,456
use a lot of compute new deep learning methods

368
00:25:24,600 --> 00:25:28,800
such as transformers, diffusion models, and convolutional

369
00:25:28,832 --> 00:25:32,524
networks. They trickle down into biology and

370
00:25:33,214 --> 00:25:36,950
with time I hope we can see more

371
00:25:37,022 --> 00:25:40,510
methods appear in text processing and

372
00:25:40,542 --> 00:25:44,554
image processing that can be applied to biology

373
00:25:45,254 --> 00:25:48,766
and structure prediction. Thank you for joining

374
00:25:48,790 --> 00:25:52,314
me. If you have any questions you can

375
00:25:53,094 --> 00:25:56,374
leave me a message on LinkedIn and I'll be happy to answer them.

