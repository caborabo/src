1
00:00:27,634 --> 00:00:30,650
Welcome to my presentation about affordable machine

2
00:00:30,682 --> 00:00:34,882
learning platform where data is becoming the new oil.

3
00:00:35,058 --> 00:00:39,146
The ability to harness this data through machine learning and

4
00:00:39,210 --> 00:00:43,290
artificial intelligence is no longer a luxury reserved

5
00:00:43,322 --> 00:00:47,298
for large corporations. It has become a necessity

6
00:00:47,386 --> 00:00:51,130
for bases of all size, researchers and

7
00:00:51,202 --> 00:00:54,950
individual developers. However, the journey to

8
00:00:54,982 --> 00:00:57,834
effective machine learning can often seem expensive.

9
00:00:58,414 --> 00:01:02,134
High cost, complex infrastructures and the need

10
00:01:02,174 --> 00:01:06,326
for specialized hardware can create significant barriers

11
00:01:06,390 --> 00:01:10,526
to entry. Many small business startups and

12
00:01:10,590 --> 00:01:14,034
individual developers find themselves wondering,

13
00:01:14,774 --> 00:01:18,214
how can we leverage the power of machine learning without

14
00:01:18,294 --> 00:01:21,490
breaking the bank. This is where furthermore,

15
00:01:21,522 --> 00:01:25,690
machine learning platforms come into play. These platforms

16
00:01:25,842 --> 00:01:29,254
democratize success to machine learning capabilities,

17
00:01:29,674 --> 00:01:33,562
providing cost effective, scalable and user friendly

18
00:01:33,618 --> 00:01:37,814
solutions to build, deploy and manage machine learning models.

19
00:01:38,474 --> 00:01:40,814
They are designed to lower the barriers,

20
00:01:41,154 --> 00:01:45,414
enabling you to turn your data into actionable insights

21
00:01:45,844 --> 00:01:47,664
regardless of your budget.

22
00:01:48,444 --> 00:01:51,476
Today, Vivi will explore the landscape

23
00:01:51,580 --> 00:01:55,476
of a football machine learning platform, understand their

24
00:01:55,540 --> 00:01:59,860
key features and learn how they can be utilized to maximize

25
00:02:00,012 --> 00:02:03,344
efficiency, minimizing costs.

26
00:02:03,844 --> 00:02:06,664
Thank you for joining us. Let's get started.

27
00:02:07,364 --> 00:02:10,652
Before we dive into the details, let me

28
00:02:10,668 --> 00:02:14,270
walk through today's agenda. We will cover the

29
00:02:14,302 --> 00:02:17,654
following topics. We will start with

30
00:02:17,694 --> 00:02:21,702
an overview of what an affordable machine learning platform is

31
00:02:21,878 --> 00:02:25,914
and variety essential in todays data driven world.

32
00:02:26,574 --> 00:02:29,870
We will identify the key stakeholders and user

33
00:02:29,902 --> 00:02:33,718
groups who benefit the most from affordable machine learning

34
00:02:33,766 --> 00:02:37,988
platform. Then we will break down the essential components

35
00:02:38,126 --> 00:02:41,712
of a machine learning platform, explaining which

36
00:02:41,768 --> 00:02:45,644
parts are necessary for furthermore machine learning platforms.

37
00:02:46,224 --> 00:02:50,280
We will dive deeper into the technical aspects

38
00:02:50,432 --> 00:02:55,496
that make an affordable machine learning platform efficient

39
00:02:55,600 --> 00:02:58,776
and powerful. Let's begin with the first

40
00:02:58,840 --> 00:03:02,644
topic, what's an affordable machine learning platform?

41
00:03:03,344 --> 00:03:06,790
To start, let's define what a machine learning platform

42
00:03:06,862 --> 00:03:11,150
is. A machine learning platform is a comprehensive

43
00:03:11,262 --> 00:03:15,038
environment that provides a necessary tool,

44
00:03:15,166 --> 00:03:18,398
frameworks and infrastructure to develop,

45
00:03:18,486 --> 00:03:22,154
team, deploy and manage machine learning models.

46
00:03:22,774 --> 00:03:26,606
It streamlines the entire machine learning workflow from data

47
00:03:26,670 --> 00:03:29,950
preprocessing and model building to

48
00:03:29,982 --> 00:03:33,478
deploy. An affordable machine learning platform

49
00:03:33,646 --> 00:03:37,086
is generally designed to work effectively with a

50
00:03:37,110 --> 00:03:41,222
single or few GPU's, focusing on the importance

51
00:03:41,278 --> 00:03:45,262
of resource sharing to ensure the cost efficiency and

52
00:03:45,318 --> 00:03:48,710
broad accessibility. So the key

53
00:03:48,742 --> 00:03:52,434
point of affordability is GPU sharing.

54
00:03:53,014 --> 00:03:56,790
Who needs this machine learning platform? Before answering

55
00:03:56,862 --> 00:04:00,868
this question, I am going to discuss why sharing GPU's is

56
00:04:00,916 --> 00:04:03,984
crucial in creating an affordable machine learning platform.

57
00:04:04,484 --> 00:04:08,276
First, lets talk about the cost. High performance

58
00:04:08,380 --> 00:04:12,924
gpu's which are necessary for running complex machine learning tasks

59
00:04:13,044 --> 00:04:16,836
come with a press tag. For many startups,

60
00:04:16,980 --> 00:04:19,884
small business and individual researchers,

61
00:04:20,044 --> 00:04:23,344
this cost can be a significant barrier to entry.

62
00:04:24,064 --> 00:04:27,400
Consider that GPU's are often idle out of

63
00:04:27,512 --> 00:04:30,840
regular working hours. In many organizations,

64
00:04:30,992 --> 00:04:35,168
these expensive resources sit on youth after workday

65
00:04:35,256 --> 00:04:38,124
and leading to inefficiency.

66
00:04:38,584 --> 00:04:42,136
Most applications have CPU and IO work in

67
00:04:42,160 --> 00:04:45,824
between. Launching GPU kernel the GPU utilization

68
00:04:45,944 --> 00:04:49,784
of a deep learning model running solely on the GPU

69
00:04:49,824 --> 00:04:53,564
is most of the time much less 100%.

70
00:04:53,934 --> 00:04:57,574
It means even during working hours there can

71
00:04:57,614 --> 00:05:01,194
be periods when CPU's are not fully utilized.

72
00:05:01,974 --> 00:05:04,634
GPU's are getting powerful each year.

73
00:05:05,014 --> 00:05:08,254
Experimenting with a new model allows and

74
00:05:08,374 --> 00:05:12,382
sometimes even requests when to use smaller

75
00:05:12,518 --> 00:05:15,822
hyperparameters, making the model use much less

76
00:05:15,878 --> 00:05:19,070
GPU memory than normally. Such tasks

77
00:05:19,142 --> 00:05:22,754
lead to underutilization and inefficiency.

78
00:05:23,484 --> 00:05:26,860
Typically, GPU's are the most expensive part of

79
00:05:26,932 --> 00:05:30,396
machinery PI four and also the component that

80
00:05:30,460 --> 00:05:33,304
mostly affects platform utilization.

81
00:05:34,164 --> 00:05:37,412
Increasing GPU utilization is key

82
00:05:37,468 --> 00:05:41,744
to reducing costs and building an thermal machine learning platform.

83
00:05:42,524 --> 00:05:46,124
About who needs this platform first lets

84
00:05:46,164 --> 00:05:48,904
talk about startups and small basics.

85
00:05:49,354 --> 00:05:53,210
The key for startups and small businesses is to

86
00:05:53,242 --> 00:05:56,426
respond flexible to market

87
00:05:56,490 --> 00:06:00,722
demands at a low cost. When building a machine learning platform,

88
00:06:00,818 --> 00:06:03,658
they shouldnt invest heavily from the start.

89
00:06:03,826 --> 00:06:07,178
Instead they should look for cost effective solutions

90
00:06:07,346 --> 00:06:10,654
that can scale as their needs grow.

91
00:06:11,194 --> 00:06:14,994
Nice educational institutions, including universities

92
00:06:15,114 --> 00:06:18,848
and research labs, need to provide students with

93
00:06:18,896 --> 00:06:22,360
hands on experience in completing end to end machine

94
00:06:22,392 --> 00:06:25,760
learning tasks. While modern GPU's might

95
00:06:25,792 --> 00:06:29,504
be overkill for education and thoughtful machine learning platform

96
00:06:29,584 --> 00:06:33,884
can offer practical waste powder to industry practices

97
00:06:34,184 --> 00:06:38,272
without the excessive. This approach enables

98
00:06:38,328 --> 00:06:42,312
students to gain valuable skills and experience while

99
00:06:42,368 --> 00:06:46,334
helping institutions manage their budgets effectively.

100
00:06:47,114 --> 00:06:50,322
Nonprofit organizations also work on tight

101
00:06:50,378 --> 00:06:53,978
budgets and need to maximize their impact

102
00:06:54,146 --> 00:06:57,194
with limited resources. They can

103
00:06:57,234 --> 00:07:00,698
use machine learning to analyze data, optimize operations,

104
00:07:00,826 --> 00:07:04,054
and drive their missions more effectively.

105
00:07:04,714 --> 00:07:08,914
An affordable machine learning platform provides them with the necessary

106
00:07:09,074 --> 00:07:12,948
computational power without diverting too much of their

107
00:07:12,996 --> 00:07:16,344
funds from their primary objectives.

108
00:07:17,724 --> 00:07:20,892
Freelancers and consultants in

109
00:07:20,908 --> 00:07:25,684
the field of data science and machine learning independently

110
00:07:25,844 --> 00:07:29,988
or in smart teams. Access to an affordable machine learning platform

111
00:07:30,156 --> 00:07:34,372
allows them to offer competitive services and solutions

112
00:07:34,548 --> 00:07:38,502
to their clients without the need to invest heavily

113
00:07:38,668 --> 00:07:42,098
in expensive hardware. It can help them

114
00:07:42,146 --> 00:07:45,554
maintain flexibility and scalability in their

115
00:07:45,594 --> 00:07:48,922
operations. In summary, an affordable

116
00:07:48,978 --> 00:07:52,818
machine learning platform can significantly benefit various

117
00:07:52,906 --> 00:07:56,354
groups. Before introducing the affordable machine learning

118
00:07:56,394 --> 00:08:00,346
platform, I would like to first introduce what components

119
00:08:00,450 --> 00:08:03,894
a typical machine learning platform should consist of.

120
00:08:04,494 --> 00:08:08,294
Here I used a simplified diagram. Through this

121
00:08:08,334 --> 00:08:12,182
diagram, we can see that a typical machine learning platform

122
00:08:12,278 --> 00:08:15,794
can be simply divided from top to bottom into

123
00:08:16,494 --> 00:08:20,558
action application layer, infrastructure layer,

124
00:08:20,646 --> 00:08:24,590
and hardware layer. In the application layer,

125
00:08:24,782 --> 00:08:28,674
it's divided into machine learning part and data parts.

126
00:08:29,294 --> 00:08:33,174
Let's talk about machine learning part first. Typically, a machine

127
00:08:33,214 --> 00:08:37,086
learning part is divided into four data

128
00:08:37,150 --> 00:08:40,834
engineering experiments, training, and inference.

129
00:08:41,374 --> 00:08:45,054
Data engineering is responsible for collecting,

130
00:08:45,174 --> 00:08:48,950
cleaning, and preparing data to ensure it's reliable

131
00:08:49,022 --> 00:08:52,902
and suitable for machine learning tasks. The experiment

132
00:08:52,958 --> 00:08:57,202
phase involves exploring and analyzing data,

133
00:08:57,358 --> 00:09:01,170
testing different algorithms, and creating

134
00:09:01,242 --> 00:09:04,734
model prototypes to find the best solution.

135
00:09:05,514 --> 00:09:09,826
The training phase works on training models using historical data

136
00:09:09,930 --> 00:09:14,394
and optimizing their parameters

137
00:09:14,474 --> 00:09:18,506
to achieve the best performance. The inference phase

138
00:09:18,690 --> 00:09:22,514
involves developing deploying trained models into

139
00:09:22,594 --> 00:09:26,634
product environments to make predictions on new data.

140
00:09:27,654 --> 00:09:30,854
Dataparts is also an indispensable part

141
00:09:30,894 --> 00:09:35,190
of the machine learning platform. Data related subparts

142
00:09:35,342 --> 00:09:38,662
usually includes a feature store,

143
00:09:38,798 --> 00:09:42,886
model management, and data lake. The feature store

144
00:09:42,910 --> 00:09:46,846
is responsible for storing and serving data, serving feature

145
00:09:46,910 --> 00:09:50,868
data consistently across streaming, and inference

146
00:09:51,046 --> 00:09:52,964
to ensure reproductibility.

147
00:09:53,904 --> 00:09:57,844
Model management involves tracking and visioning

148
00:09:58,384 --> 00:10:02,504
machine learning models, driving collaboration, and managing

149
00:10:02,544 --> 00:10:05,720
model deployment pipelines. The data lake

150
00:10:05,752 --> 00:10:09,880
serves as a centralized repository for serving

151
00:10:09,992 --> 00:10:13,364
vast amounts of flow and process data,

152
00:10:14,064 --> 00:10:18,164
enabling efficient data material and analysis.

153
00:10:18,884 --> 00:10:22,584
However, to create an affordable version platform,

154
00:10:23,044 --> 00:10:26,884
we have decided not to include the data components

155
00:10:27,044 --> 00:10:30,892
at this date. The reason for this is that our

156
00:10:30,948 --> 00:10:34,460
target users typically handle smaller data sites,

157
00:10:34,612 --> 00:10:38,124
and there is no immediate need to

158
00:10:38,204 --> 00:10:41,820
establish in a dedicated data platform

159
00:10:41,892 --> 00:10:45,332
at this point. Moreover, data platforms and

160
00:10:45,388 --> 00:10:48,614
machine learning platforms can be decoupled

161
00:10:49,354 --> 00:10:53,146
that as our basis scales up in the

162
00:10:53,170 --> 00:10:57,026
future, we can build a dedicated data platform

163
00:10:57,130 --> 00:11:00,546
separately. So for the affordable machine

164
00:11:00,570 --> 00:11:04,882
learning platform, the scope is a dark color part in

165
00:11:04,898 --> 00:11:07,610
the diagram, the machine learning part,

166
00:11:07,682 --> 00:11:11,354
and the infrastructure part. I will

167
00:11:11,514 --> 00:11:15,516
deep dive into the most critical technical points

168
00:11:15,700 --> 00:11:19,668
in these two scalable container

169
00:11:19,716 --> 00:11:23,588
environments and GPU sharing. To better

170
00:11:23,636 --> 00:11:27,244
understand the requirements for scalable container

171
00:11:27,324 --> 00:11:30,772
environments, let's revisit some typical

172
00:11:30,828 --> 00:11:34,860
basis scenarios. Educational institutions

173
00:11:35,052 --> 00:11:38,764
often operate with a single machine equipped with a few GPU

174
00:11:38,804 --> 00:11:42,544
cards suitable for classroom use and small skill

175
00:11:42,584 --> 00:11:46,304
research projects. Startups in small business

176
00:11:46,424 --> 00:11:50,304
typically have a setup consisting of a few PCs

177
00:11:50,424 --> 00:11:54,364
each with GPU's ideal for initial product development

178
00:11:54,784 --> 00:11:59,048
and smart skill deployment. Freelancers and

179
00:11:59,136 --> 00:12:02,296
consultants usually work with a single PC

180
00:12:02,360 --> 00:12:05,944
equipped with only one gpu, which is perfect

181
00:12:06,024 --> 00:12:09,472
for individual projects and consultancy

182
00:12:09,528 --> 00:12:12,756
work. Then we could find that there is a

183
00:12:12,780 --> 00:12:16,996
conflict. From the basics scenario perspective,

184
00:12:17,140 --> 00:12:21,108
the hardware setup may consist of only one

185
00:12:21,236 --> 00:12:23,716
or few PCs. However,

186
00:12:23,900 --> 00:12:27,124
building a machine learning platform requires multiple container

187
00:12:27,164 --> 00:12:31,092
environments, including experimental training and inference

188
00:12:31,148 --> 00:12:34,972
environments. To manage these environments effectively,

189
00:12:35,068 --> 00:12:38,650
we need to introduce kubernetes. The instruction

190
00:12:38,682 --> 00:12:42,094
of kubernetes requires at least three nodes.

191
00:12:42,634 --> 00:12:46,562
The challenge now becomes how to deploy kubernetes

192
00:12:46,658 --> 00:12:50,434
on a single PC while ensuring compatibility

193
00:12:50,594 --> 00:12:53,894
for potential multinode expansion.

194
00:12:54,354 --> 00:12:57,794
The answer is to introduce OpenStack to

195
00:12:57,834 --> 00:13:02,180
further illustrate our needs consider typical basis scenario

196
00:13:02,322 --> 00:13:06,296
where the initial hardware setup consists of only one PC.

197
00:13:06,480 --> 00:13:09,880
As the basis grows, the hardware may expand

198
00:13:09,952 --> 00:13:13,864
to multiple physical machines or virtual machines and

199
00:13:13,944 --> 00:13:17,364
potentially transition to a cloud environment.

200
00:13:18,144 --> 00:13:22,684
This can give shift to ensuring compatibility with

201
00:13:23,184 --> 00:13:27,444
heterogeneous hardware environments while maintaining scalability.

202
00:13:28,404 --> 00:13:31,652
OpenStack is well suited to drive issue as

203
00:13:31,668 --> 00:13:35,064
shown in the diagram on the OpenStack website,

204
00:13:35,484 --> 00:13:38,980
it excels in heterogeneous hardware

205
00:13:39,052 --> 00:13:42,764
compatibility. OpenStack also provides a dedicated

206
00:13:42,844 --> 00:13:46,892
single machine deployment tutorial,

207
00:13:47,068 --> 00:13:50,424
making it a perfect fit for our requirements.

208
00:13:51,084 --> 00:13:54,620
Additionally, since OpenStack provides a virtual machine

209
00:13:54,652 --> 00:13:58,858
environment, it allows for stimulus transition to

210
00:13:58,986 --> 00:14:03,050
either self host cloud or public cloud environment

211
00:14:03,162 --> 00:14:06,706
in the future without disrupting the operation

212
00:14:06,770 --> 00:14:10,374
of the Kubernetes cluster and machine learning platform.

213
00:14:11,034 --> 00:14:14,130
This diagram illustrates the potential lifecycle

214
00:14:14,202 --> 00:14:17,334
of a typical affordable machine learning platform.

215
00:14:17,794 --> 00:14:21,538
In the initial phase, OpenStack is used to support single

216
00:14:21,586 --> 00:14:26,000
machine setups and ensure compatibility with

217
00:14:26,152 --> 00:14:30,328
heterogeneous hardware. As a platform evolves,

218
00:14:30,416 --> 00:14:33,864
Overstack continue to provide compatibility

219
00:14:33,984 --> 00:14:37,328
with more complex environments, including cloud

220
00:14:37,376 --> 00:14:40,552
infrastructure. In Lithostix,

221
00:14:40,728 --> 00:14:44,912
OpenStack can be seamlessly removed to other container

222
00:14:44,968 --> 00:14:48,600
environments, directly enhancing flexibility and

223
00:14:48,672 --> 00:14:52,180
scalability. Next, I will introduce one of

224
00:14:52,212 --> 00:14:56,324
the most app critical technologies in building an affordable machine learning

225
00:14:56,364 --> 00:15:00,380
platform. Before diving deep into the technical details,

226
00:15:00,452 --> 00:15:04,516
lets first understand the mainstream GPU sharing solutions

227
00:15:04,620 --> 00:15:08,188
and their applicable scenarios from the

228
00:15:08,236 --> 00:15:11,684
big picture perspective. Machine learning tasks

229
00:15:11,764 --> 00:15:15,684
use Nvidia GPU's we will start by looking at several

230
00:15:15,764 --> 00:15:19,276
official Nvidia solutions, including multi instance

231
00:15:19,340 --> 00:15:23,460
GPU, which is MIG GPU time sharing

232
00:15:23,572 --> 00:15:27,064
and multiprocessed service, which is NP's.

233
00:15:27,644 --> 00:15:31,644
We can see that MIG allows two parallelism for

234
00:15:31,724 --> 00:15:35,700
multiply multiple tasks on the same GPU with the highest

235
00:15:35,772 --> 00:15:39,444
level of isolation, making it suitable for

236
00:15:39,484 --> 00:15:43,524
inference tasks and small scale training tasks, although it's

237
00:15:43,564 --> 00:15:47,494
also the most expensive option which we will discuss in

238
00:15:47,614 --> 00:15:51,718
detail later. GPU time sharing involves time slicing

239
00:15:51,766 --> 00:15:55,790
in a single GPU, which causes content switching between different

240
00:15:55,862 --> 00:15:59,278
tasks, leading to increased

241
00:15:59,446 --> 00:16:02,558
total task time, making it suitable

242
00:16:02,606 --> 00:16:06,238
for, let us say, sensitive inference tasks, but more suitable for

243
00:16:06,286 --> 00:16:10,114
relatively synchronous training tasks.

244
00:16:10,414 --> 00:16:13,842
The last solution, NP's, is the earliest

245
00:16:13,898 --> 00:16:17,450
GPU sharing solution which merges multiple tasks

246
00:16:17,522 --> 00:16:21,410
into a single GPU contest that if one

247
00:16:21,442 --> 00:16:25,138
task fails, all tasks will fail. Thus,

248
00:16:25,186 --> 00:16:28,454
it's only suitable for experimental scenarios.

249
00:16:28,994 --> 00:16:32,938
Among third party solutions, they will primarily introduce

250
00:16:33,026 --> 00:16:37,090
Tencent KKe Gagia GPU.

251
00:16:37,242 --> 00:16:41,040
Let's now dive into the principles of those solutions

252
00:16:41,202 --> 00:16:44,464
and their advantages and disadvantages.

253
00:16:45,204 --> 00:16:49,068
MIG is a technology that allows a single Nvidia GPU

254
00:16:49,116 --> 00:16:52,784
to be partitioned into multiple isolated instances

255
00:16:53,364 --> 00:16:57,476
each of this instance has its own dedicated resources

256
00:16:57,660 --> 00:17:01,384
such as memory, compute cores, and bandwidth.

257
00:17:01,884 --> 00:17:05,524
This separation for such multiple workloads can

258
00:17:05,564 --> 00:17:09,544
run simultaneously on the same GPU without affecting each other,

259
00:17:09,914 --> 00:17:13,994
thus maximizing resource utilization

260
00:17:14,114 --> 00:17:17,482
and performance. The primary advantage

261
00:17:17,538 --> 00:17:20,978
of IMIG is the strong isolation it provides

262
00:17:21,026 --> 00:17:24,890
between different tasks by dedicating specific

263
00:17:25,042 --> 00:17:28,494
resources to each instance,

264
00:17:29,314 --> 00:17:33,002
one task from impacting the performance or stability

265
00:17:33,138 --> 00:17:37,130
of another. This makes imaging particularly suitable for

266
00:17:37,162 --> 00:17:40,470
environments that revere the workloads.

267
00:17:40,642 --> 00:17:44,262
Additionally, MNG allows true for precise

268
00:17:44,318 --> 00:17:47,902
resources allocation, enabling efficient

269
00:17:47,958 --> 00:17:51,454
use of GPU capabilities and improving overall

270
00:17:51,494 --> 00:17:55,070
system scalabilities. However, here is

271
00:17:55,102 --> 00:17:58,726
an important drawback to consider. The cost of using

272
00:17:58,790 --> 00:18:02,742
MnG can be very high as only Nvidia's high

273
00:18:02,798 --> 00:18:05,554
performance professional GPU support this technology.

274
00:18:06,314 --> 00:18:10,122
As shown in the table on the right, the minimal requirement

275
00:18:10,218 --> 00:18:13,330
is the nadia, a 30 toothpod mnG,

276
00:18:13,522 --> 00:18:16,850
which doesnt align with the goal of an affordable machine learning

277
00:18:16,882 --> 00:18:20,282
platform. Lets talk about the next candidate.

278
00:18:20,418 --> 00:18:23,754
GPU type sharing immediate time slashing

279
00:18:23,794 --> 00:18:27,170
is a feature that allows a single GPU to be shared by

280
00:18:27,202 --> 00:18:30,378
multiple processes or users.

281
00:18:30,466 --> 00:18:34,534
By dividing the GPU's computer resources into typesclass,

282
00:18:35,044 --> 00:18:38,180
each process or user gets a

283
00:18:38,252 --> 00:18:41,580
dedicated time slice during which they have full access

284
00:18:41,732 --> 00:18:45,444
to the GPU's resources. This enables multiple

285
00:18:45,484 --> 00:18:49,784
tasks to run on the same GPU in a sequential manner,

286
00:18:50,084 --> 00:18:53,564
providing the illusion of parallel processing

287
00:18:53,684 --> 00:18:57,100
while ensuring that each task gets a fair share

288
00:18:57,172 --> 00:18:58,824
of the GPU's capabilities.

289
00:19:00,884 --> 00:19:05,308
Advantage of time pricing is increased flexibility

290
00:19:05,476 --> 00:19:07,664
and better results utilization.

291
00:19:08,404 --> 00:19:11,892
It allows multiple users or processes

292
00:19:11,988 --> 00:19:16,460
to share a single GPU with zasket. The need for partitioning

293
00:19:16,572 --> 00:19:19,844
is hardware resources. Physically, there are some

294
00:19:19,884 --> 00:19:22,804
disadvantages to time slicing.

295
00:19:22,964 --> 00:19:26,420
In a time slicing setup, multiple processes share the

296
00:19:26,452 --> 00:19:30,492
same proof vram, leading to potential memory

297
00:19:30,548 --> 00:19:33,884
contention. If one process consumes a large amount

298
00:19:33,924 --> 00:19:37,748
of vram, it can leave insufficient memory for

299
00:19:37,796 --> 00:19:41,804
other tasks, causing performance degradation

300
00:19:41,924 --> 00:19:45,596
of failures. Additionally, the shared memory

301
00:19:45,660 --> 00:19:49,644
spaces increases the risk of where

302
00:19:49,724 --> 00:19:53,572
inefficient memory management by one profile can

303
00:19:53,628 --> 00:19:57,180
gradually consume more vram, impacting the performance and

304
00:19:57,212 --> 00:19:59,704
the stability of other processes.

305
00:20:00,334 --> 00:20:04,542
Regarding for the isolation, time slicing doesn't provide a strict isolation

306
00:20:04,598 --> 00:20:08,358
between processes. If one process encounters

307
00:20:08,406 --> 00:20:12,414
a fault or crashes, it can potentially affect

308
00:20:12,494 --> 00:20:16,034
other processes sharing the same GPU resources.

309
00:20:16,734 --> 00:20:20,886
Lack of isolation can lead to system insertability and

310
00:20:20,990 --> 00:20:24,334
unpredictable performance, making it

311
00:20:24,374 --> 00:20:28,114
challenging to ensure reliable operation in production environments.

312
00:20:28,624 --> 00:20:32,072
Next solution is Nvidia NP's the core principle

313
00:20:32,128 --> 00:20:36,440
of NP's in

314
00:20:36,472 --> 00:20:40,404
allowing multiple processes to share single GPU context.

315
00:20:40,824 --> 00:20:44,600
Traditionally, each CUDA application would create

316
00:20:44,712 --> 00:20:48,704
its own GPU context, leading to resource message

317
00:20:48,824 --> 00:20:51,604
and context switching overhead.

318
00:20:52,064 --> 00:20:55,596
By sharing a single GPU context NP's reduces

319
00:20:55,660 --> 00:20:58,804
its inefficiencies. Additionally,

320
00:20:58,964 --> 00:21:02,956
NP's Merc command queue from different processes

321
00:21:03,020 --> 00:21:06,484
into a single queue, thereby facilitating more

322
00:21:06,524 --> 00:21:10,784
efficient scheduling and execution of command

323
00:21:11,524 --> 00:21:14,904
midping turn minimize GPU add time.

324
00:21:15,524 --> 00:21:18,676
Whilst the primary advantage of Nvidia

325
00:21:18,860 --> 00:21:22,574
NPM is performance improvement, by reducing

326
00:21:22,614 --> 00:21:26,350
context switching and managing command more

327
00:21:26,382 --> 00:21:29,534
efficiently, NP's can significantly enhance

328
00:21:29,574 --> 00:21:32,674
the performance of concurrent processes.

329
00:21:33,094 --> 00:21:37,438
Furthermore, compared to MIG and templating

330
00:21:37,606 --> 00:21:42,182
NP's both consumer grid gpu notable

331
00:21:42,238 --> 00:21:45,934
disadvantages associated with NP's while

332
00:21:45,974 --> 00:21:49,254
the pay issue its memory isolation,

333
00:21:49,554 --> 00:21:53,034
the shared GPU contest results in less strict memory

334
00:21:53,074 --> 00:21:56,654
oscillation compared to a separate contest.

335
00:21:57,314 --> 00:22:01,418
This can lead to memory contention and potential data security

336
00:22:01,506 --> 00:22:05,114
concerns. Another significant drawback is fault

337
00:22:05,154 --> 00:22:09,466
isolation. If one process encounters fault,

338
00:22:09,650 --> 00:22:13,298
it can impact other processes sharing the same GPU

339
00:22:13,346 --> 00:22:17,330
context. Configuring and debugging NP's

340
00:22:17,442 --> 00:22:21,642
can be complex and requires specialized knowledge

341
00:22:21,738 --> 00:22:26,098
and experience outside of official solutions.

342
00:22:26,266 --> 00:22:29,914
Many third party vendors have proposed their own GPU

343
00:22:29,954 --> 00:22:35,106
sharing solution. A typical example is Tencent Gaia

344
00:22:35,170 --> 00:22:38,866
GpU. Tencent provides a

345
00:22:39,050 --> 00:22:42,310
complete Fido GPU sharing permissions, which is a

346
00:22:42,342 --> 00:22:46,406
fully open source GPU services. Inga GPU

347
00:22:46,470 --> 00:22:50,782
is a GPU resource limitation component and

348
00:22:50,958 --> 00:22:54,374
belongs to Cuda hijacking. We could manage

349
00:22:54,494 --> 00:22:58,214
each container's memory usage by intercepting

350
00:22:58,334 --> 00:23:01,914
CUDA's memory allocation and release requests,

351
00:23:02,454 --> 00:23:05,674
thereby achieving memory isolation.

352
00:23:06,254 --> 00:23:12,636
There is the only thing to load is that context

353
00:23:12,700 --> 00:23:16,172
application doesn't go through the malloc function,

354
00:23:16,308 --> 00:23:19,532
so it's impossible to know how much memory the

355
00:23:19,588 --> 00:23:23,644
process uses in the condesce. Therefore, we could

356
00:23:23,684 --> 00:23:27,140
accurate the current memory usage from

357
00:23:27,172 --> 00:23:31,052
the GPU each time. In terms of computing power isolation,

358
00:23:31,228 --> 00:23:36,604
users can specify the GPU utilization rate for container.

359
00:23:36,904 --> 00:23:40,896
MacUDa will monitor utilization and take some actions

360
00:23:40,960 --> 00:23:44,124
when it exceeds the limit.

361
00:23:44,424 --> 00:23:48,004
Both hard isolation and soft isolation are supported.

362
00:23:48,384 --> 00:23:52,752
Since a monitoring adjustment scheme is used, computing power

363
00:23:52,928 --> 00:23:55,524
cannot be limited in a short period.

364
00:23:56,104 --> 00:23:59,736
Only long term efficiency fairness can be guaranteed.

365
00:23:59,920 --> 00:24:03,674
Therefore, it's not suitable for scenarios they are tested.

366
00:24:03,824 --> 00:24:07,366
Task times are extremely short, such as

367
00:24:07,550 --> 00:24:11,742
inference tasks. Machine learning platforms are extremely

368
00:24:11,798 --> 00:24:15,790
broad topic and even if we limit the scope

369
00:24:15,822 --> 00:24:19,382
to building an affordable machine learning platform, there are still

370
00:24:19,438 --> 00:24:22,998
many sets to share. However, I believe

371
00:24:23,046 --> 00:24:26,294
that GPU sharing and scalable container

372
00:24:26,334 --> 00:24:29,714
environments are among the most critical packs.

373
00:24:30,124 --> 00:24:33,484
A few years ago, well, I was

374
00:24:33,564 --> 00:24:37,084
developing a machine learning platform. We successfully run

375
00:24:37,124 --> 00:24:40,804
our platforms mostly on a small PC cluster with

376
00:24:40,844 --> 00:24:43,876
only few machines shown in the

377
00:24:43,900 --> 00:24:47,876
picture. This demonstrated that by applying

378
00:24:47,940 --> 00:24:51,624
the technological solutions previously mentioned,

379
00:24:52,004 --> 00:24:56,012
we can indeed build an affordable machine learning platform aimed at

380
00:24:56,068 --> 00:24:59,764
education institutions. Ngo's freelancers

381
00:24:59,844 --> 00:25:03,844
and setups. Finally, I hope my

382
00:25:03,884 --> 00:25:08,204
insights have provided valuable guidance and inspiration

383
00:25:08,364 --> 00:25:10,700
for your own affordable machine learning platform.

384
00:25:10,772 --> 00:25:13,996
Endorse and thank you all for joining

385
00:25:14,020 --> 00:25:14,764
this online session.

