1
00:00:27,320 --> 00:00:31,112
Hi everyone, my name is Nikhil Taroni, head of AI products

2
00:00:31,168 --> 00:00:34,592
at GCor. First of all, thank you very much to the organizer of

3
00:00:34,608 --> 00:00:37,720
Conf 42 for having me here today, and I'm really

4
00:00:37,752 --> 00:00:41,336
excited to talk to you about our pioneering work building our global

5
00:00:41,400 --> 00:00:45,664
intelligence pipeline and its cornerstone technology inference

6
00:00:45,704 --> 00:00:48,880
at the edge. And during the talk, feel free

7
00:00:48,912 --> 00:00:52,136
to ping me on slack with any questions or comments and I'll do

8
00:00:52,160 --> 00:00:55,904
my best to answer. But for now, let's dive in.

9
00:00:56,444 --> 00:00:59,940
As I'm here on behalf of GCor, let me begin by telling

10
00:00:59,972 --> 00:01:03,700
you a little bit more about us. We are a global

11
00:01:03,812 --> 00:01:08,300
cloud edge and AI solutions provider headquartered

12
00:01:08,332 --> 00:01:11,956
in Luxembourg, but with a global presence and

13
00:01:11,980 --> 00:01:15,796
our vision is to connect the world to AI anywhere,

14
00:01:15,940 --> 00:01:19,766
anytime. The cornerstone of the company is

15
00:01:19,790 --> 00:01:24,454
our secure low latency network, which consists of over 180

16
00:01:24,534 --> 00:01:28,594
CDN points of presence and 25 cloud locations,

17
00:01:28,974 --> 00:01:32,994
giving us an average response time of just 30 milliseconds.

18
00:01:33,494 --> 00:01:37,022
We offer a range of cloud services covering infrastructure

19
00:01:37,038 --> 00:01:39,634
as a service, multi platform as a service.

20
00:01:40,534 --> 00:01:43,678
But today, as we go at a machine learning conference,

21
00:01:43,846 --> 00:01:47,178
I'm going to focus on our AI as a service offerings and some of

22
00:01:47,186 --> 00:01:51,094
the work we've done there. To begin, we offer a range of

23
00:01:51,634 --> 00:01:54,850
AI services including GPU, bare metal

24
00:01:54,882 --> 00:01:58,450
and virtual machine instances, managed kubernetes clusters,

25
00:01:58,642 --> 00:02:02,282
and also a 5g platform that helps reduce the latency between

26
00:02:02,338 --> 00:02:05,762
your device and the model. But all of these services

27
00:02:05,858 --> 00:02:09,654
underpins what we call our global intelligence pipeline,

28
00:02:10,314 --> 00:02:14,304
which is designed to follow the steps taken by machine learning practitioners to

29
00:02:14,344 --> 00:02:17,656
train, deploy and scale AI applications

30
00:02:17,720 --> 00:02:20,924
in production. So starting on the left there,

31
00:02:21,504 --> 00:02:24,832
you can see that we offer direct access to high performance

32
00:02:24,968 --> 00:02:28,524
AI infrastructure acted by Nvidia GPU's.

33
00:02:29,144 --> 00:02:32,648
This provides the raw computing power needed for intense AI training

34
00:02:32,696 --> 00:02:36,224
workloads. We then also offer a managed Kubernetes

35
00:02:36,264 --> 00:02:39,744
service which can also then utilize the GPU

36
00:02:39,784 --> 00:02:43,714
nodes to help you orchestrate your machine learning and AI workloads.

37
00:02:44,254 --> 00:02:48,206
So we then enable the deployment of workspaces in mlots platforms

38
00:02:48,350 --> 00:02:52,794
from our vendor partners and this helps you manage your machine learning lifecycle.

39
00:02:53,414 --> 00:02:57,054
And finally, we specialize in the serving and inference of pre

40
00:02:57,094 --> 00:03:00,526
trained AI models and that's going to be the focus of my

41
00:03:00,550 --> 00:03:03,934
talk today. So, as many of you know, when it comes

42
00:03:03,974 --> 00:03:06,762
to training your large machine learning AI models,

43
00:03:06,918 --> 00:03:10,698
you really care about compute performance in order to accelerate

44
00:03:10,746 --> 00:03:14,770
your research and development. So to give you the best possible performance,

45
00:03:14,962 --> 00:03:18,058
we partner with Nvidia so that all of our

46
00:03:18,106 --> 00:03:21,914
GPU clusters are powered by 800 way a 100 or H

47
00:03:21,954 --> 00:03:24,654
100 gpu's within Vlink.

48
00:03:25,114 --> 00:03:29,114
But for really large training jobs that span multiple compute nodes,

49
00:03:29,274 --> 00:03:33,510
it's equally important to have a very fast interconnect, providing direct

50
00:03:33,582 --> 00:03:36,674
GPU connection across multiple nodes.

51
00:03:37,214 --> 00:03:40,554
All of our clusters are also connected with the latest Infiniband.

52
00:03:41,094 --> 00:03:44,766
But now, once you've trained or perhaps fine tuned a large model

53
00:03:44,830 --> 00:03:47,714
for your use case, the question arises,

54
00:03:48,254 --> 00:03:51,114
where can I serve my model with low latency?

55
00:03:51,774 --> 00:03:54,474
This is a really important question for several reasons,

56
00:03:55,094 --> 00:03:59,160
the main one being because as you go from research and development to

57
00:03:59,192 --> 00:04:01,844
production of a business application of AI,

58
00:04:02,384 --> 00:04:06,184
your primary compute workload will move from training to

59
00:04:06,224 --> 00:04:09,724
inference. For many business critical applications,

60
00:04:10,104 --> 00:04:14,032
the end user needs a real time response, no matter

61
00:04:14,088 --> 00:04:18,400
where they are in the world. And this is becoming more and more important

62
00:04:18,592 --> 00:04:21,336
as the market is, of course rapidly evolving,

63
00:04:21,520 --> 00:04:25,104
with an increasing number of businesses not only testing out AI

64
00:04:25,144 --> 00:04:28,660
in pilots or group concepts, actually adopting

65
00:04:28,732 --> 00:04:32,660
AI applications in full scale production at

66
00:04:32,692 --> 00:04:35,708
enterprise scale. So as an example of that,

67
00:04:35,756 --> 00:04:39,224
you can see here a survey last year from McKinsey,

68
00:04:39,644 --> 00:04:42,884
from enterprise leaders, and it showcased how

69
00:04:43,004 --> 00:04:46,276
over half of them are already adopting AI. And again,

70
00:04:46,300 --> 00:04:49,740
it just showcases just how prevalent AI

71
00:04:49,772 --> 00:04:52,824
is becoming, not only in pilots, but in actual production.

72
00:04:53,464 --> 00:04:56,536
This not only drives the demand for inference compute,

73
00:04:56,720 --> 00:05:00,064
but also the need for scalable, reliable, and secure

74
00:05:00,104 --> 00:05:04,044
infrastructure that can maintain a very high level of service availability.

75
00:05:04,904 --> 00:05:08,496
So, to give you an example, the application most of us are most familiar

76
00:05:08,520 --> 00:05:11,680
with is probably chatbots. So take

77
00:05:11,712 --> 00:05:15,232
an example here. You can see that if you take an

78
00:05:15,248 --> 00:05:18,996
off the shelf model such as Mistral Seven B and

79
00:05:19,020 --> 00:05:23,212
Runva locally, and I've taken a reasonably

80
00:05:23,348 --> 00:05:26,996
standard question around about 200 tokens, and got

81
00:05:27,020 --> 00:05:30,396
an answer around 20 tokens back and running

82
00:05:30,420 --> 00:05:33,984
that locally took around about 250 milliseconds.

83
00:05:34,604 --> 00:05:37,784
Now, to an end user, that really feels like real time.

84
00:05:38,284 --> 00:05:42,140
But now, supposing you were sitting, say, in Tokyo, and you were submitting

85
00:05:42,172 --> 00:05:46,314
that request to a data center in the cloud in, say, the United States,

86
00:05:46,974 --> 00:05:50,166
then just the network latency could easily double

87
00:05:50,270 --> 00:05:52,754
or treble that at that time.

88
00:05:53,374 --> 00:05:57,158
So that when you then add in all sort of the inference time, the end

89
00:05:57,206 --> 00:06:00,534
to end processing could get close to a second,

90
00:06:00,694 --> 00:06:02,634
which would really damage the user experience.

91
00:06:03,534 --> 00:06:06,782
Of course, there are many other use cases where you really

92
00:06:06,838 --> 00:06:09,910
care about a real time response, whether that's,

93
00:06:09,942 --> 00:06:13,474
for example, autonomous driving or any sort of virtual live

94
00:06:13,514 --> 00:06:17,522
streaming or even quality inspection use cases in industries

95
00:06:17,578 --> 00:06:21,242
such as mining or manufacturing. For all these applications,

96
00:06:21,338 --> 00:06:25,610
you of course need high performance compute for

97
00:06:25,642 --> 00:06:29,314
the fast inference, but you also need very low

98
00:06:29,354 --> 00:06:33,010
latency, and in many cases, it's also important for

99
00:06:33,042 --> 00:06:37,454
regulatory or privacy reasons, that the data is processed locally.

100
00:06:38,004 --> 00:06:41,772
And for us that means in an edge location that

101
00:06:41,788 --> 00:06:45,092
is geographically close to the end user. So now,

102
00:06:45,108 --> 00:06:48,692
supposing you're looking to run infront of

103
00:06:48,708 --> 00:06:52,628
your model, if you want to achieve that in real time, then there are

104
00:06:52,716 --> 00:06:55,584
three core requirements you have to work on.

105
00:06:56,044 --> 00:06:59,972
The first of those is you need a distributed,

106
00:07:00,068 --> 00:07:02,904
powerful compute infrastructure around the world.

107
00:07:03,704 --> 00:07:07,224
But also you then need a very low latency backbone which

108
00:07:07,264 --> 00:07:10,632
connects a compute. You then also need

109
00:07:10,728 --> 00:07:13,244
runtime deployment and reconfigurability.

110
00:07:13,984 --> 00:07:17,536
So let's start with the inference. And when

111
00:07:17,560 --> 00:07:20,872
we looked at running inference at the edge here at

112
00:07:20,888 --> 00:07:24,992
G core, we faced several challenges that required innovative solutions

113
00:07:25,128 --> 00:07:27,924
to help optimize performance and efficiency.

114
00:07:28,524 --> 00:07:33,220
I'll focus on LLMs here, as they are the technology

115
00:07:33,292 --> 00:07:36,580
that gets the most attention in a minute. But of course, these techniques are

116
00:07:36,612 --> 00:07:40,372
also relevant to other technologies. So the first strategy

117
00:07:40,388 --> 00:07:43,264
that we used was operator fusion,

118
00:07:43,924 --> 00:07:48,100
and by merging edge and operators, this helps streamline computational

119
00:07:48,172 --> 00:07:52,004
tasks, which often leads to better latency and enhancing

120
00:07:52,044 --> 00:07:56,324
the responsiveness of AI models. We also employed quantization,

121
00:07:56,744 --> 00:08:00,352
which involves compressing the activations and weights of neural networks

122
00:08:00,448 --> 00:08:03,704
so they require fewer bits. Another strategy

123
00:08:03,744 --> 00:08:07,024
is compression with techniques such as Sparta T,

124
00:08:07,184 --> 00:08:10,760
where we trim unnecessary connections, or distillation,

125
00:08:10,952 --> 00:08:13,844
where we train smaller models to mimic larger ones.

126
00:08:14,384 --> 00:08:17,960
Of course, with all these techniques, you've also got to balance

127
00:08:18,112 --> 00:08:21,772
the performance of the model with the accuracy and

128
00:08:21,788 --> 00:08:25,324
making sure you don't lose too much accuracy. And finally,

129
00:08:25,444 --> 00:08:28,716
the fourth technique, of course, is very well suited to

130
00:08:28,740 --> 00:08:31,684
GPU's is parallelization. Typically,

131
00:08:31,724 --> 00:08:35,404
we've implemented tense parallelism, distributing computation across

132
00:08:35,444 --> 00:08:39,220
multiple devices, and also pipeline parallelism to help

133
00:08:39,252 --> 00:08:42,524
efficiently manage larger models and help scale up our

134
00:08:42,564 --> 00:08:45,860
AI capabilities. So once we've optimized

135
00:08:45,892 --> 00:08:49,446
a computer model, the second piece of the puzzle is

136
00:08:49,470 --> 00:08:53,134
a network latency. Now, on this slide, I'm just

137
00:08:53,214 --> 00:08:57,062
showing you a map which shows the growth of 5G

138
00:08:57,118 --> 00:09:00,270
around the globe. And the reason I'm sharing that is,

139
00:09:00,342 --> 00:09:04,950
of course, over the last few years, 5G has enabled

140
00:09:05,062 --> 00:09:08,190
many of us, perhaps all of us, to stream really high

141
00:09:08,222 --> 00:09:12,554
quality content, such as videos, straight to our mobile devices,

142
00:09:13,294 --> 00:09:16,678
because of the ultra low latency that 5G

143
00:09:16,726 --> 00:09:20,474
provides. So we've taken those learnings,

144
00:09:21,014 --> 00:09:24,694
look to apply them also to AI. And so by

145
00:09:24,734 --> 00:09:28,182
leveraging our CDN network with 180 points of

146
00:09:28,198 --> 00:09:32,086
presence, what that means is that a user request will

147
00:09:32,110 --> 00:09:35,422
take just an average of 30 milliseconds end to end, to get

148
00:09:35,438 --> 00:09:38,798
to our network and back, and then perhaps a total of around 50

149
00:09:38,846 --> 00:09:42,656
milliseconds to then get to an inference node where the actual

150
00:09:42,800 --> 00:09:46,120
machine inference takes place. And so that round trip

151
00:09:46,152 --> 00:09:49,488
of around 50 milliseconds very fast

152
00:09:49,616 --> 00:09:53,312
compared to perhaps a few hundred milliseconds in a typical public

153
00:09:53,368 --> 00:09:58,136
cloud setting. So what that means is that provided you've also optimized

154
00:09:58,280 --> 00:10:01,912
the model inference, so that it takes perhaps between 104

155
00:10:01,928 --> 00:10:05,248
hundred milliseconds, you can achieve an end to

156
00:10:05,296 --> 00:10:08,484
end processing time of under half a second.

157
00:10:09,144 --> 00:10:12,744
And that's important because around half a second is around

158
00:10:12,784 --> 00:10:15,856
about the time which as a human we perceive that to

159
00:10:15,880 --> 00:10:19,804
be in real time. Any slower, you start to notice that lag.

160
00:10:20,424 --> 00:10:23,920
So again, to put that into more context, I said if you

161
00:10:23,952 --> 00:10:27,432
combine a very low latency network combined with

162
00:10:27,448 --> 00:10:31,364
an optimized inference round trip in under half a second,

163
00:10:31,824 --> 00:10:34,916
and that's applicable to a number of technologies.

164
00:10:35,080 --> 00:10:37,624
So for example, automatic speech recognition,

165
00:10:38,244 --> 00:10:41,644
object detection, or text speech are all

166
00:10:41,684 --> 00:10:45,508
examples where this enables a real time

167
00:10:45,556 --> 00:10:48,628
response. So then the final piece of the puzzle,

168
00:10:48,676 --> 00:10:52,252
we've had to work on runtime deployment

169
00:10:52,308 --> 00:10:55,372
and reconfigurability in a simple and scalable

170
00:10:55,428 --> 00:10:59,504
manner to really enable this inference at the edge, at scale.

171
00:11:00,124 --> 00:11:03,314
So to do this, we achieved this using a container

172
00:11:03,394 --> 00:11:07,202
as a service technology, so that we provide a single

173
00:11:07,298 --> 00:11:11,330
anycast endpoint that can be easily connected to a developer's

174
00:11:11,402 --> 00:11:14,474
application. So what that means is that

175
00:11:14,554 --> 00:11:18,570
no matter where in the world an end user is, the request to that

176
00:11:18,602 --> 00:11:22,074
endpoint is routed to the nearest CDN node and from

177
00:11:22,114 --> 00:11:25,874
there to the nearest inference node. So if we delve

178
00:11:25,914 --> 00:11:29,578
into a little bit more detail, you can see what's happening here under

179
00:11:29,626 --> 00:11:33,704
the hood. I apologize, a little bit small, but hopefully you can follow it.

180
00:11:34,244 --> 00:11:38,020
So at the top here, supposing you have a two end

181
00:11:38,052 --> 00:11:41,828
users, and let's say one of those is in rear vision Aero

182
00:11:41,996 --> 00:11:45,556
and one of those in say Kyoto. Now, the anycast

183
00:11:45,620 --> 00:11:49,116
endpoint is available globally. When each of those

184
00:11:49,140 --> 00:11:53,204
users sends the request, that request will go to their local CDN

185
00:11:53,244 --> 00:11:56,812
node, the user in Kyoto. There might be a CDN node in

186
00:11:56,828 --> 00:12:00,478
Kyoto itself. And similarly for revision Arrow, that request

187
00:12:00,526 --> 00:12:04,758
would go to the revision Arrow CDN node. Now after that

188
00:12:04,926 --> 00:12:07,714
we developed smart routing technology,

189
00:12:08,494 --> 00:12:12,366
and what that does is it routes the request to

190
00:12:12,390 --> 00:12:16,766
the nearest available inference region. So that's where we have some

191
00:12:16,830 --> 00:12:20,910
high performance compute, typically gpu's that will do the actual

192
00:12:21,102 --> 00:12:24,242
optimized inference. Now what that means is

193
00:12:24,258 --> 00:12:28,050
that not only does it guarantee the fastest response time for each

194
00:12:28,082 --> 00:12:31,594
of these individually, but it also ensures that the

195
00:12:31,634 --> 00:12:35,490
processing and the model itself also stay locally.

196
00:12:35,682 --> 00:12:39,434
So in particular, the user in Kyoto, the inference

197
00:12:39,474 --> 00:12:43,458
will be in state Tokyo and their data, and that model

198
00:12:43,546 --> 00:12:46,818
will only ever be in Tokyo. And likewise for the user

199
00:12:46,906 --> 00:12:51,052
in Rio, in Brazil, their model and their arrangement would also take place

200
00:12:51,108 --> 00:12:54,452
locally in Brazil. That also actually means you

201
00:12:54,468 --> 00:12:58,228
could potentially have a different model in each region. So, for example,

202
00:12:58,276 --> 00:13:01,708
you could have a model that was trained only on japanese user data

203
00:13:01,796 --> 00:13:05,424
in Japan, and similarly for other world regions.

204
00:13:05,884 --> 00:13:09,252
And so that also helps maintain that sort of data privacy and

205
00:13:09,268 --> 00:13:12,932
locality that I was talking about earlier. The final piece

206
00:13:12,948 --> 00:13:15,988
of puzzle that I want to talk about here is that in each of these

207
00:13:16,036 --> 00:13:19,600
regions, we also have developed auto scaling

208
00:13:19,672 --> 00:13:22,084
autoscaling functionality.

209
00:13:22,624 --> 00:13:25,976
So what that means is that the amount of compute scales

210
00:13:26,000 --> 00:13:29,520
up and down with demand. And that's really important because

211
00:13:29,552 --> 00:13:32,232
it means that you can scale up the compute when you need it, when there's

212
00:13:32,248 --> 00:13:36,016
lots of demand. But equally importantly, you can scale back down again when there

213
00:13:36,040 --> 00:13:39,976
isn't. And also that means you're not then paying for compute

214
00:13:40,120 --> 00:13:43,684
when you don't use it. So, tying all these things together,

215
00:13:44,464 --> 00:13:48,616
I'll just summarize by putting together our sort of full end to end

216
00:13:48,680 --> 00:13:51,856
architecture, and you'll see that we've combined high

217
00:13:51,880 --> 00:13:55,040
performance AI training infrastructure, and then that

218
00:13:55,072 --> 00:13:58,616
could be in a public cloud setting. Thus, for some customers where

219
00:13:58,640 --> 00:14:02,524
privacy is really important, we could also offer it in a private cloud setting.

220
00:14:02,944 --> 00:14:06,644
And then if you combine that with this infinite set, this low network,

221
00:14:06,944 --> 00:14:09,524
and the infinite edge technology we're talking about today,

222
00:14:10,284 --> 00:14:13,956
you have a comprehensive global AIoT

223
00:14:14,020 --> 00:14:17,780
architecture that we believe is fit for today's most demanding and

224
00:14:17,812 --> 00:14:21,308
scalable AI applications. So with that,

225
00:14:21,356 --> 00:14:24,900
that's probably enough of me talking. What I'll showcase here,

226
00:14:24,932 --> 00:14:27,984
a few demos that showcase technology in action.

227
00:14:28,524 --> 00:14:32,340
So the first of those is an example with a real time face

228
00:14:32,412 --> 00:14:36,532
avatar. And what you'll see here is at the top here, I'll be entering

229
00:14:36,588 --> 00:14:40,218
in some prompts. And what you should look out for is the

230
00:14:40,226 --> 00:14:43,482
way that the avatar of the person changes

231
00:14:43,658 --> 00:14:46,774
in real time as I type in those prompts.

232
00:14:47,154 --> 00:14:50,290
So for example, here, I'm going to type in portrait of Elon Musk.

233
00:14:50,322 --> 00:14:53,834
And you see that the face has changed, Elon Musk

234
00:14:53,874 --> 00:14:58,058
almost immediately. And now as I type Bill Gates again, that change

235
00:14:58,106 --> 00:15:01,854
was done in real time. And now moving on to Madonna

236
00:15:02,354 --> 00:15:05,900
and a few more examples here. But again, what's really impressive

237
00:15:05,932 --> 00:15:09,820
is the way that the change is done in real time compared

238
00:15:09,852 --> 00:15:13,788
to the typing of the prompt there. There's a Jeff Bezos

239
00:15:13,916 --> 00:15:17,212
when he had hair. So, another example I

240
00:15:17,228 --> 00:15:21,212
can show you here is of real time translation from English

241
00:15:21,268 --> 00:15:24,380
to Luxembourgish. Now, like most

242
00:15:24,412 --> 00:15:27,940
people in the world, I don't speak Luxembourgish, which is a

243
00:15:28,052 --> 00:15:31,552
local language of Luxembourg. But if I was in

244
00:15:31,568 --> 00:15:34,480
our head office and I needed to quickly translate something,

245
00:15:34,672 --> 00:15:38,552
then no problem. I could use a typical translation

246
00:15:38,608 --> 00:15:42,724
tool. Hello there. Allow me to introduce myself.

247
00:15:43,024 --> 00:15:46,464
I am an advanced machine learning model specifically designed for

248
00:15:46,504 --> 00:15:50,004
voice to text translation from English to Luxembourgish.

249
00:15:50,424 --> 00:15:54,304
I have been created and powered by the cutting edge technology of G

250
00:15:54,344 --> 00:15:58,344
core AI cloud. Great. And I'll press send,

251
00:15:59,084 --> 00:16:02,220
and you'll see that again in less than a second.

252
00:16:02,252 --> 00:16:06,004
Less half a second, I'd say. The translation happened,

253
00:16:06,044 --> 00:16:09,676
and I've received the luxembourgish translation

254
00:16:09,740 --> 00:16:13,116
of those words again, I really showcased, from the minute

255
00:16:13,220 --> 00:16:16,664
I pressed send, the processing was in almost real time.

256
00:16:17,364 --> 00:16:20,820
So, in the final example I'll show you today is some work

257
00:16:20,852 --> 00:16:24,024
we've done with our colleagues at let's AI.

258
00:16:24,564 --> 00:16:28,100
Let's AI are a creative platform that allows you to generate images for

259
00:16:28,132 --> 00:16:31,064
anything, you know, simply by tagging it in a text.

260
00:16:31,604 --> 00:16:34,908
We've worked with, let's see, I over the last few months, and they've been utilizing

261
00:16:34,956 --> 00:16:38,820
our H 100 AI infrastructure and inference

262
00:16:38,852 --> 00:16:42,076
services to both train their Generali models and

263
00:16:42,100 --> 00:16:45,104
also serve them to their customers worldwide.

264
00:16:45,764 --> 00:16:49,380
So what you see here is an example of a single image that was generated

265
00:16:49,452 --> 00:16:52,690
by the platform. But I think what's really impressive

266
00:16:52,722 --> 00:16:56,386
is that you can actually take a series of images

267
00:16:56,490 --> 00:16:59,978
generated by the platform and then combine

268
00:17:00,026 --> 00:17:03,258
together to form a video. So this is

269
00:17:03,266 --> 00:17:06,174
an example of a video generated with Lepsey Eye,

270
00:17:06,874 --> 00:17:10,522
which I think really shows the power of generative AI powered

271
00:17:10,538 --> 00:17:12,334
by GPU infrastructure.

272
00:17:16,874 --> 00:17:20,412
The rookie sensation Ricky Malone says surprises with an unbelievable

273
00:17:20,468 --> 00:17:23,836
pole position at silver. It looks like Ricky Malone is joining forces with the

274
00:17:23,860 --> 00:17:26,676
legendary f one team for the 1986 season.

275
00:17:26,860 --> 00:17:30,028
Ricky, after our crash, watching you lose yourself

276
00:17:30,076 --> 00:17:33,292
was hard. Not just because of us, but seeing you

277
00:17:33,308 --> 00:17:37,012
give into darkness. It wasn't only about you.

278
00:17:37,188 --> 00:17:40,144
I was hurt, too, trapped by resentment,

279
00:17:40,524 --> 00:17:43,684
encouraging you to race again, to face that trauma.

280
00:17:43,844 --> 00:17:47,554
It's for both of us. Us. I need to see you conquer this.

281
00:17:47,934 --> 00:17:51,406
Not to go back to what we had, but to find closure

282
00:17:51,550 --> 00:17:55,070
for both of us to move on. This is about letting go,

283
00:17:55,222 --> 00:17:58,878
forgiving each other, healing. By helping you return

284
00:17:58,926 --> 00:18:03,110
to the track, I'm also finding my way back. It's about finding

285
00:18:03,142 --> 00:18:06,314
peace, Ricky, for you and for me.

286
00:18:16,234 --> 00:18:20,094
So there you go. A completely generated video that I

287
00:18:20,674 --> 00:18:24,194
certainly found very impressive. So we're almost at the end of the talk

288
00:18:24,234 --> 00:18:27,730
here, and just want to let you know that if

289
00:18:27,802 --> 00:18:31,666
you're interested in learning more about technology, our infrastructure, the edge

290
00:18:31,690 --> 00:18:34,774
product is actually being launched in beta this week.

291
00:18:35,274 --> 00:18:38,522
So this lets you deploy your model globally with

292
00:18:38,538 --> 00:18:41,882
a single endpoint, as I explained. Or you can also

293
00:18:41,938 --> 00:18:45,842
choose an open source model out of the box from a model catalog,

294
00:18:46,018 --> 00:18:49,538
which includes popular models such as mistral unstable

295
00:18:49,586 --> 00:18:53,850
diffusion. So this gives you a serverless AI compute with

296
00:18:53,922 --> 00:18:57,610
very low latency around the world on a pay as you go model

297
00:18:57,762 --> 00:19:01,762
with DDoS endpoint protection. But also, as this is our

298
00:19:01,858 --> 00:19:05,326
beta service, it's also free. So if you'd like to try

299
00:19:05,350 --> 00:19:09,230
it, I'd love to hear from you, find out about your machine learning

300
00:19:09,262 --> 00:19:12,782
use case, and see whether edge computing could

301
00:19:12,798 --> 00:19:16,390
be the right solution for you. And I'll just end the same. But of course,

302
00:19:16,462 --> 00:19:19,934
our work doesn't end here. The team is striving

303
00:19:19,974 --> 00:19:23,550
to push the limits of our network and bring down the latency

304
00:19:23,662 --> 00:19:26,926
to just a few tens of milliseconds, and we believe this

305
00:19:26,950 --> 00:19:30,482
will further benefit sort of really mission

306
00:19:30,538 --> 00:19:33,214
critical use cases of AI at the edge.

307
00:19:33,674 --> 00:19:37,082
So with that, I hope you've enjoyed the talk. Thank you for

308
00:19:37,098 --> 00:19:40,634
joining me today. And again, please do reach out if you have any

309
00:19:40,674 --> 00:19:44,250
questions or would like to discuss any of the technologies that I've spoken

310
00:19:44,282 --> 00:19:47,394
about today, but for now, have a great conference.

