1
00:00:20,880 --> 00:00:24,686
Thank you all for joining the session. Today, I am going to talk about

2
00:00:24,790 --> 00:00:28,824
the large language models and the future of large language

3
00:00:28,864 --> 00:00:32,004
models and productionization of the large language models.

4
00:00:32,744 --> 00:00:36,576
Myself, Deepak. I am working as an associate director for data science and

5
00:00:36,600 --> 00:00:39,656
machine learning projects. I also have more

6
00:00:39,680 --> 00:00:42,736
than 15 years of experience in data science and

7
00:00:42,760 --> 00:00:46,248
machine learning, dominantly working in generative AI

8
00:00:46,296 --> 00:00:49,112
for the past three years. All right,

9
00:00:49,288 --> 00:00:52,404
now take you to the next slide.

10
00:00:52,864 --> 00:00:56,816
So, before I'm going to talk about the large language model productionization or

11
00:00:56,840 --> 00:01:00,400
deploying in cloud, let's understand the traditional

12
00:01:00,432 --> 00:01:04,200
AI model development and deployment, followed by the challenges

13
00:01:04,272 --> 00:01:07,424
we have in deploying or

14
00:01:07,464 --> 00:01:10,632
productionizing the traditional AI models. Then I'll

15
00:01:10,768 --> 00:01:14,168
walk you through on the large language models like GPT four,

16
00:01:14,336 --> 00:01:17,728
and I can explain you the architecture of the large language models

17
00:01:17,776 --> 00:01:21,150
or the generated AI model. Then I can take you through the

18
00:01:21,182 --> 00:01:24,750
concept of lancing framework and how the applications can

19
00:01:24,782 --> 00:01:28,114
be developed with lancing, followed by a demo.

20
00:01:28,734 --> 00:01:32,310
Moving to the next slide, let's talk about the traditional

21
00:01:32,342 --> 00:01:35,654
AI models. When I say traditional AI models,

22
00:01:35,734 --> 00:01:39,590
we began with linear regression, logistic regression, random forest

23
00:01:39,662 --> 00:01:43,262
decision trees, boosting adaboost, exaboost,

24
00:01:43,398 --> 00:01:46,762
neural network, and the evolution started from neural

25
00:01:46,818 --> 00:01:49,770
network to transformers in 2019 or 2017.

26
00:01:49,842 --> 00:01:53,410
I'm sorry. So that is how the industry has a breakthrough,

27
00:01:53,562 --> 00:01:57,434
by coming up with a model called Bert, which is a bidirectional encoder

28
00:01:57,474 --> 00:02:01,986
representation for transformers. I think that has

29
00:02:02,130 --> 00:02:05,362
significantly performed well

30
00:02:05,418 --> 00:02:09,066
in most of the natural language processing tasks. When I

31
00:02:09,090 --> 00:02:12,680
talk about traditional AI models, let me not talk about starting

32
00:02:12,712 --> 00:02:16,000
from linear or logistic regressions or random forest.

33
00:02:16,152 --> 00:02:19,520
Let's begin with a small large language model

34
00:02:19,632 --> 00:02:22,764
which has been known, which I, which I call that as invert

35
00:02:23,264 --> 00:02:26,824
the process involved in model training

36
00:02:26,984 --> 00:02:30,400
or model fine tuning has a requires huge amount

37
00:02:30,432 --> 00:02:33,648
of data set to train the model. So once

38
00:02:33,696 --> 00:02:36,832
we train the model or fine tune the model, we have to fine tune the

39
00:02:36,848 --> 00:02:39,764
model for a specific task or for a specific domain.

40
00:02:40,404 --> 00:02:43,796
Typically, it needs a GPU machine to do the model fine tuning

41
00:02:43,820 --> 00:02:47,036
process. Once we perform the model fine

42
00:02:47,060 --> 00:02:50,884
tuning, we have to do the hyper parameter tuning like learning rate

43
00:02:50,964 --> 00:02:54,876
epoch with multiple additional parameters to come

44
00:02:54,900 --> 00:02:59,132
up with the right ways for the model to classify or question answering

45
00:02:59,268 --> 00:03:01,424
or any of the tasks which it can perform.

46
00:03:02,484 --> 00:03:05,996
So then once we do the fine tuning and optimization of

47
00:03:06,020 --> 00:03:09,378
the model, we have to deploy the model in a cloud

48
00:03:09,426 --> 00:03:12,774
environment. It could be AWS or Azure,

49
00:03:13,754 --> 00:03:16,174
even Google Cloud platform.

50
00:03:16,514 --> 00:03:19,706
But before that, when you are going to deploy the

51
00:03:19,730 --> 00:03:23,490
model, the model has to be serialized, meaning the model

52
00:03:23,562 --> 00:03:27,578
as to when we deploy the model in production, it should have the scalability

53
00:03:27,706 --> 00:03:31,894
and reliability and durability. Considering that

54
00:03:32,374 --> 00:03:35,990
when we move the model to production, we have to serialize the model by having

55
00:03:36,022 --> 00:03:39,294
a pytorch or a tensorflow saved model

56
00:03:39,334 --> 00:03:42,830
format to serve the model. Introduction so,

57
00:03:42,862 --> 00:03:46,630
model serving, as I talked about as a framework called Pytorch serving,

58
00:03:46,742 --> 00:03:50,214
so which is a framework can have a scalability on

59
00:03:50,254 --> 00:03:54,334
performing the inference. This framework provides an API

60
00:03:54,494 --> 00:03:58,018
where the application, once we build a real world application,

61
00:03:58,126 --> 00:04:01,506
the application can invoke the inference or the prediction

62
00:04:01,690 --> 00:04:04,134
by invoking the Pytorch serving part.

63
00:04:04,594 --> 00:04:08,002
Also, we provide that as an API that helps us to

64
00:04:08,018 --> 00:04:11,826
come up with an API design and based on the API design we can start

65
00:04:11,890 --> 00:04:15,514
invoking the model. The model can be a single model or multiple

66
00:04:15,554 --> 00:04:19,434
models can be deployed in production. I will talk about the Pytorch serving

67
00:04:19,474 --> 00:04:23,458
architecture in a minute. Before that I will tell how the scalability

68
00:04:23,546 --> 00:04:26,494
and load balancing will be performed in the cloud environment.

69
00:04:27,054 --> 00:04:30,750
When we deploy these models in production, it can be deployed in elastic

70
00:04:30,782 --> 00:04:34,214
container service, in AWS or in Azure

71
00:04:34,254 --> 00:04:38,478
container app. When we deploy the models, we have a load balancer

72
00:04:38,566 --> 00:04:42,070
has to be created and we have to create the cloud formation template to create

73
00:04:42,102 --> 00:04:45,854
a container and we have to deploy this model as a docker image and

74
00:04:45,894 --> 00:04:48,314
internally it has a Pytorch serving framework.

75
00:04:48,734 --> 00:04:51,974
Once we deploy the model, we have to have a auditability

76
00:04:52,094 --> 00:04:55,460
which is nothing but monitoring and logging. So there most

77
00:04:55,492 --> 00:04:58,924
of the model would be logged along with the number of invocation has been

78
00:04:58,964 --> 00:05:02,984
made to the model along with the throughput and error rates.

79
00:05:03,444 --> 00:05:07,156
So the model should be highly secured where it cannot be having unauthorized

80
00:05:07,220 --> 00:05:10,724
access and attacks. So also once we build

81
00:05:10,764 --> 00:05:14,412
a model, it should have an security along with the

82
00:05:14,428 --> 00:05:18,036
CI CD pipeline for the reinforcement learning whenever the

83
00:05:18,060 --> 00:05:21,734
model trains and fine tune and deployed in production,

84
00:05:21,924 --> 00:05:25,042
if the model has any variation from the data

85
00:05:25,138 --> 00:05:28,378
which it has been trained, then the model has to be

86
00:05:28,506 --> 00:05:31,730
when in the production the model has a deviation

87
00:05:31,882 --> 00:05:35,266
in the data, then it cannot identify the data accurately.

88
00:05:35,410 --> 00:05:39,162
So we have a CACD pipeline to have a reinforcement.

89
00:05:39,218 --> 00:05:42,330
Human learning to ensure if there isn't any deviation,

90
00:05:42,442 --> 00:05:46,506
model has to automatically, automatically, after a certain time it

91
00:05:46,530 --> 00:05:50,156
has to train and fine tune and again it has to be deployed.

92
00:05:50,260 --> 00:05:53,524
That variant is called a b testing or multiple variants

93
00:05:53,564 --> 00:05:57,172
of models will be deployed in production that comes under versioning

94
00:05:57,228 --> 00:06:01,620
and rollback. So we are all talking about the traditional AI.

95
00:06:01,812 --> 00:06:04,476
So this comes under the concept of mlops.

96
00:06:04,620 --> 00:06:07,956
So we design the model, we develop the model

97
00:06:08,020 --> 00:06:11,412
and operationalize the model so in case of design,

98
00:06:11,548 --> 00:06:14,970
we identify the data set, we identify the

99
00:06:15,002 --> 00:06:18,434
model. Then once we have done the identification,

100
00:06:18,514 --> 00:06:22,410
we understand what the model task is. It could be in classification or

101
00:06:22,442 --> 00:06:25,802
summarization, abstraction or like

102
00:06:25,818 --> 00:06:29,854
a question and answering or next sentence for prediction. There could be multiple

103
00:06:30,314 --> 00:06:33,538
kind of tasks the model can perform. So as part of

104
00:06:33,546 --> 00:06:37,458
the recurrent gathering or use case prioritization that has to be identified

105
00:06:37,586 --> 00:06:40,898
along with the data availability to train or fine tune the model.

106
00:06:40,946 --> 00:06:44,808
I think fine tune is the right word, followed by model engineering

107
00:06:44,896 --> 00:06:48,424
which has a technique to identify the model, then perform

108
00:06:48,504 --> 00:06:52,176
Eiffel parameter tuning and fine tune the model and deploy the

109
00:06:52,200 --> 00:06:55,712
model. In operations. That deployment process would

110
00:06:55,728 --> 00:07:00,352
be in a cloud environment by having a CACD pipeline like Azure DevOps,

111
00:07:00,448 --> 00:07:03,764
or then we can monitor via

112
00:07:04,464 --> 00:07:08,404
Amazon Cloudwatch or Azure monitoring logs.

113
00:07:09,084 --> 00:07:12,604
So this traditional AI model development involves

114
00:07:12,644 --> 00:07:16,504
a huge number, there's a certain amount of process has to be followed,

115
00:07:17,124 --> 00:07:20,692
right? So before getting into the large language models,

116
00:07:20,748 --> 00:07:24,900
I would like to touch base on the Pytorch serving. So Pytorch serving is

117
00:07:24,932 --> 00:07:28,308
nothing but a framework where large, sorry, where Bert models

118
00:07:28,356 --> 00:07:31,580
or large language models like Bert

119
00:07:31,692 --> 00:07:34,892
can be deployed. So it is a framework which comes up with

120
00:07:34,908 --> 00:07:38,276
an inference and management API where multiple models can

121
00:07:38,300 --> 00:07:42,192
be deployed inside the container. So again,

122
00:07:42,248 --> 00:07:45,432
this container, when you mean this pyth serving, has to be

123
00:07:45,448 --> 00:07:48,712
built as a docker and it has to be deployed inside a container. It could

124
00:07:48,728 --> 00:07:52,844
be an Amazon elastic container instance or Azure

125
00:07:53,464 --> 00:07:56,752
where we can deploy multiple machine learning models by

126
00:07:56,888 --> 00:08:00,684
using model store. Under the model store we can start

127
00:08:01,384 --> 00:08:05,876
using EBS or elastic storage mechanism,

128
00:08:05,980 --> 00:08:09,140
we can start to save the models

129
00:08:09,292 --> 00:08:12,676
by use by running an API and we can serve the model by

130
00:08:12,700 --> 00:08:16,028
an HTTP endpoint. I think this is the holistic process.

131
00:08:16,156 --> 00:08:19,932
Now you understand the amount of efforts or time we spent in

132
00:08:20,068 --> 00:08:24,464
the whole machine learning or traditional machine learning model development productionization.

133
00:08:25,084 --> 00:08:28,692
So now the offer which we are going to make

134
00:08:28,828 --> 00:08:32,312
is lang chain. But before that

135
00:08:32,368 --> 00:08:36,352
I will give you a few touch base on large language models. See large language

136
00:08:36,408 --> 00:08:39,624
models like GPT-3 or GPT four which has been

137
00:08:39,664 --> 00:08:43,832
trained more than 175 billion parameters. We have other models like Lama

138
00:08:43,888 --> 00:08:47,312
two or Mistral or cloud which comes up with 7

139
00:08:47,368 --> 00:08:50,640
billion or 70 billion parameters. Of the amount of

140
00:08:50,672 --> 00:08:54,336
data has been trained. When it comes to charge DpT,

141
00:08:54,520 --> 00:08:57,874
chart, GPT, we all know it's from OpenAI. It's more like

142
00:08:57,914 --> 00:09:01,162
a very large language model. It is a

143
00:09:01,178 --> 00:09:04,738
foundational model. It has a capability to answer any questions

144
00:09:04,786 --> 00:09:08,410
or any task it can perform without any fine tuning.

145
00:09:08,602 --> 00:09:11,810
So the whole process without fine tuning can be

146
00:09:11,842 --> 00:09:15,698
achieved by providing in context learning to the model. So where the

147
00:09:15,786 --> 00:09:19,666
in context learning would be providing

148
00:09:19,810 --> 00:09:22,414
the model by giving some context.

149
00:09:23,254 --> 00:09:27,406
In context learning means as part of the problem techniques,

150
00:09:27,510 --> 00:09:31,286
instruction can be specified to the GPT four model

151
00:09:31,390 --> 00:09:35,234
to perform a specific task. So when I say

152
00:09:36,094 --> 00:09:40,166
performing a specific task, we can use multiple prompt engineering techniques.

153
00:09:40,310 --> 00:09:44,142
So before the tradition was writing a programming language

154
00:09:44,198 --> 00:09:48,242
in Java or in Python to perform a task for a programming language,

155
00:09:48,398 --> 00:09:52,362
but now natural language process is

156
00:09:52,378 --> 00:09:56,122
a programming language, nothing but, it's an English. So where we

157
00:09:56,138 --> 00:10:00,114
can specify an instruction to the model which is nothing but a prompt along

158
00:10:00,154 --> 00:10:04,282
with the input, and we say if it performs a summarization

159
00:10:04,338 --> 00:10:07,978
or translation task, we specify the task information

160
00:10:08,146 --> 00:10:11,882
by providing in context learning via prompt to along with

161
00:10:11,898 --> 00:10:15,448
an input, we get the relevant answers from the GPT four.

162
00:10:15,626 --> 00:10:19,396
So that's the evolution of large language models. So large language models

163
00:10:19,460 --> 00:10:23,228
are not necessarily need to be fine tuned, which saves the significant

164
00:10:23,356 --> 00:10:26,412
amount of resources like infrastructure and time.

165
00:10:26,548 --> 00:10:29,344
And you know, to have a safer and cleaner environment,

166
00:10:29,644 --> 00:10:32,824
not to fine tune or train the algorithm every time.

167
00:10:33,484 --> 00:10:36,564
Now we know about large language models.

168
00:10:36,644 --> 00:10:39,980
Now we know how we can utilize the large language models

169
00:10:40,132 --> 00:10:43,194
to perform a specific task. But it all

170
00:10:43,234 --> 00:10:46,466
looks good when you are doing some kind of a prototype.

171
00:10:46,610 --> 00:10:49,786
So where you can specify a prompt and

172
00:10:49,810 --> 00:10:52,698
you can give an input and you can get an output on the prompt.

173
00:10:52,826 --> 00:10:56,778
So how do you productionize the large language models? That is an interesting area

174
00:10:56,826 --> 00:11:00,002
to focus on. Okay, that's how we offer lang

175
00:11:00,058 --> 00:11:04,346
chain. But again, before getting into lang chain, let's look into the architecture

176
00:11:04,410 --> 00:11:07,546
of large language models. So before let's

177
00:11:07,570 --> 00:11:11,776
have a small comparison between traditional model and generate the algorithm,

178
00:11:11,850 --> 00:11:15,004
it's nothing but large language model. In traditional model

179
00:11:15,084 --> 00:11:18,996
we have a data pre processing, then we identify the features required

180
00:11:19,060 --> 00:11:22,716
for training or fine tuning

181
00:11:22,740 --> 00:11:26,604
the model. After identifying the features, then we perform a fine tuning

182
00:11:26,644 --> 00:11:30,084
job by having the data. Then once the data has been trained,

183
00:11:30,124 --> 00:11:34,228
then we deploy in production in cloud environment. So typically the model

184
00:11:34,276 --> 00:11:37,348
uses a framework like Tensorflow, Pytorch,

185
00:11:37,396 --> 00:11:40,764
keras, then underlying it could be an IBM

186
00:11:40,804 --> 00:11:44,796
Watson API, or it could be an Pytorch serving which

187
00:11:44,820 --> 00:11:48,676
I was mentioning. Similarly, we would have used multiple databases like no

188
00:11:48,700 --> 00:11:52,704
SQL or SQL database and mlops, avant Docker and Jenkins.

189
00:11:53,044 --> 00:11:56,884
Right now the shift, as in paradigm shift, the reason

190
00:11:56,964 --> 00:12:00,620
we are in the era of more interesting things happening

191
00:12:00,692 --> 00:12:04,490
every day or every week to identify which

192
00:12:04,522 --> 00:12:08,346
is realistic and which can be productionized is a key challenge. So that

193
00:12:08,370 --> 00:12:11,610
would be addressed as part of this demo or as part of this

194
00:12:11,642 --> 00:12:15,090
conversation which we are having now. Even after the conversation you can

195
00:12:15,122 --> 00:12:18,706
reach out to me and have a discussion. Now the

196
00:12:18,770 --> 00:12:22,138
whole process has been converted into prompt tuning or prompt

197
00:12:22,186 --> 00:12:25,482
engineering on neat basis. We can go for fine tuning,

198
00:12:25,538 --> 00:12:29,234
but it's not necessary. But even prompt engineering significantly

199
00:12:29,274 --> 00:12:32,412
performs well on the tasks. Data pre processing it's

200
00:12:32,468 --> 00:12:35,932
all about the input. Data has to be cleansed and given as an input

201
00:12:35,988 --> 00:12:39,636
along with the prompt. Then it has an underlying foundational model like

202
00:12:39,660 --> 00:12:43,276
GPT four or Claude or Mistral. Any of the model can be used.

203
00:12:43,420 --> 00:12:47,236
Then we deploy the model by using orchestration platform

204
00:12:47,300 --> 00:12:50,988
like LangChain or Lama index. So today the offer is about

205
00:12:51,036 --> 00:12:54,580
LangChain. It's not only about developing machine

206
00:12:54,612 --> 00:12:57,956
learning models. Deploying a machine learning model and invoking

207
00:12:57,980 --> 00:13:01,420
a machine learning model is become much more easier than what we have

208
00:13:01,452 --> 00:13:04,474
done earlier. If you have any questions,

209
00:13:04,594 --> 00:13:08,370
I'll move to the next slide. LangChain so

210
00:13:08,402 --> 00:13:12,138
LangChain is a framework to develop the large language

211
00:13:12,186 --> 00:13:15,586
models. It facilitates the creation of applications

212
00:13:15,690 --> 00:13:19,954
that are contextual, aware and capable of reasoning, thereby enhancing

213
00:13:19,994 --> 00:13:23,254
the practical utility of llms in various scenarios.

214
00:13:23,954 --> 00:13:28,082
LangChain has split the job into sequential

215
00:13:28,138 --> 00:13:32,584
steps where the preprocessing could be an independent step and model

216
00:13:32,884 --> 00:13:36,204
invocation would be an independent step. There are

217
00:13:36,324 --> 00:13:39,924
like Azure offers a prompt flow where the model

218
00:13:39,964 --> 00:13:43,612
sequence can be split into multiple steps where if there isn't any

219
00:13:43,668 --> 00:13:47,224
change happens, even each layer could be a plug and play.

220
00:13:47,604 --> 00:13:51,460
So the amount of time it takes from prototype to production

221
00:13:51,612 --> 00:13:54,212
by having a suite of tools like lang chain,

222
00:13:54,348 --> 00:13:58,064
makes the productionization more secure and scalable.

223
00:13:58,804 --> 00:14:02,444
So as I said, LangChain is a framework

224
00:14:02,564 --> 00:14:06,084
to develop machine learning model and by using an

225
00:14:06,124 --> 00:14:08,864
API the models can be invoked.

226
00:14:09,444 --> 00:14:12,940
About the lancing framework, which I said in the previous slide.

227
00:14:13,052 --> 00:14:16,704
Lancing can be developed in Python as well JavaScript.

228
00:14:17,164 --> 00:14:21,244
This offers multiple interfaces and integration with pandas

229
00:14:21,324 --> 00:14:24,928
or numpy or scikit learn. It doesn't offer to integrate

230
00:14:24,976 --> 00:14:28,804
with multiple other panda Python libraries.

231
00:14:29,104 --> 00:14:33,088
Also, these are not having a chain agent. But what do you mean by chain?

232
00:14:33,256 --> 00:14:36,832
Multiple sequential steps can be integrated together

233
00:14:37,008 --> 00:14:40,208
like pre processing invocation, model invocation

234
00:14:40,256 --> 00:14:44,424
and post processing that can be performed by chain agents.

235
00:14:44,464 --> 00:14:47,976
Here are nothing but where collection of activities or multiple

236
00:14:48,040 --> 00:14:51,300
events can be performed without having much trouble in the

237
00:14:51,332 --> 00:14:55,036
execution. And they are ready made chain and they are very good

238
00:14:55,060 --> 00:14:58,468
in agent implementation. Also lang chain

239
00:14:58,556 --> 00:15:02,124
as a Langsmith and the templates and Langserve

240
00:15:02,164 --> 00:15:05,944
is used for serving the model. Introduction by and rest API

241
00:15:06,244 --> 00:15:10,156
Langsmith is for debugging and evaluating and monitoring the chains

242
00:15:10,260 --> 00:15:13,516
within the LLM framework. So this

243
00:15:13,540 --> 00:15:17,402
is all comes as part of the package of lang chain framework.

244
00:15:17,538 --> 00:15:20,962
Lang chain is a sequential chain

245
00:15:21,018 --> 00:15:25,002
where multiple models can be invoked simultaneously, or it can have a sequential

246
00:15:25,058 --> 00:15:28,434
model invocation, or it can also have a parallel model invocation.

247
00:15:28,554 --> 00:15:32,474
So as part of this model lang chain framework, they also offer lang chain

248
00:15:32,514 --> 00:15:35,762
compression language, so where the

249
00:15:35,778 --> 00:15:39,746
amount of code which we write in python could be drastically reduced

250
00:15:39,850 --> 00:15:42,374
by using expression language of lang chain.

251
00:15:43,704 --> 00:15:47,624
So interesting. So after that, let's see how

252
00:15:47,664 --> 00:15:50,992
the generated way application can be developed with LangChain.

253
00:15:51,128 --> 00:15:54,376
So whenever we start with a generative way application, we have to

254
00:15:54,400 --> 00:15:57,844
identify the objective of what is a task we are going to perform.

255
00:15:58,144 --> 00:16:01,872
It could be an prototype to identify

256
00:16:01,968 --> 00:16:06,248
or perform an image classification, or even it can be a natural language processing

257
00:16:06,296 --> 00:16:09,582
task like translation. So where we

258
00:16:09,598 --> 00:16:13,182
have to provide the context to the generative AI model, then we

259
00:16:13,198 --> 00:16:16,926
have to offer a support to have an integrate with multiple platforms.

260
00:16:17,030 --> 00:16:20,966
Then the code which we write should be in a mode to productionize and

261
00:16:20,990 --> 00:16:25,334
we should have a collaborative environment like Azure Notebook or Amazon

262
00:16:25,374 --> 00:16:28,974
Sagemaker. There are many things where there is a platform to develop

263
00:16:29,134 --> 00:16:32,758
the machine learning models. Then after the diversified model

264
00:16:32,806 --> 00:16:36,054
application, it can suit for various range of applications from

265
00:16:36,094 --> 00:16:39,164
chatbot to document summarization or analyzation.

266
00:16:39,664 --> 00:16:43,320
Now the development takes into product, product or productionization.

267
00:16:43,472 --> 00:16:47,304
So whenever we talk about productionization, scalability is a very important

268
00:16:47,384 --> 00:16:50,936
feature where when the model should serve multiple requests

269
00:16:50,960 --> 00:16:54,536
in parallel or in concurrent fashion. Also it have

270
00:16:54,560 --> 00:16:57,992
a framework should have supported testing, we should

271
00:16:58,008 --> 00:17:01,336
have monitoring tools to check how the model is performing in

272
00:17:01,360 --> 00:17:04,960
production. And the deployment should be ease by having an API

273
00:17:04,992 --> 00:17:08,918
as an invocation the model. So also that will be a continuous

274
00:17:08,966 --> 00:17:12,534
improvement for the model by having a prompt versioning where

275
00:17:12,574 --> 00:17:16,414
multiple prompts can be identified and fine tuned

276
00:17:16,454 --> 00:17:20,102
on the prompt and the prompt will go through an evaluation phase and

277
00:17:20,118 --> 00:17:23,514
the prompt will be further fine tuned to deploy into production.

278
00:17:23,934 --> 00:17:27,150
Again, the most interesting thing is deployment,

279
00:17:27,262 --> 00:17:30,822
where LaNC serve can be used to deploy the lank chain.

280
00:17:30,998 --> 00:17:34,726
So lank serve is nothing but like a fast API. It's like a

281
00:17:34,750 --> 00:17:38,164
server, which on top of lank chain where

282
00:17:38,244 --> 00:17:42,180
it acts as a server and it communicates and provides the rest API to

283
00:17:42,212 --> 00:17:45,504
invoke the Lang chain or the agents inside the lancing.

284
00:17:46,164 --> 00:17:50,364
All right, now when we go into lancing, there are multiple deployment

285
00:17:50,404 --> 00:17:54,024
templates which is readily available to consume and where

286
00:17:54,404 --> 00:17:58,004
each and every time we can have a plug and play features

287
00:17:58,044 --> 00:18:01,814
like providing templates in model invocation and scalability

288
00:18:02,004 --> 00:18:05,810
and these of integration and production grades. The production grade support.

289
00:18:05,962 --> 00:18:09,386
I think these are all the features supported by lanching to offer.

290
00:18:09,490 --> 00:18:13,666
Introduction let's see the

291
00:18:13,690 --> 00:18:16,930
difference between the prompting and fine tuning wise and alternatives,

292
00:18:16,962 --> 00:18:20,786
right? In the case of prompting, which you could see,

293
00:18:20,930 --> 00:18:24,858
we specify you are an unbiased professor and your input

294
00:18:24,946 --> 00:18:28,884
score should be zero to ten, and then we pass it to the foundation model

295
00:18:28,994 --> 00:18:31,848
along with an input. Then we can get an output. So where as part of

296
00:18:31,856 --> 00:18:35,600
the problem we are specifying the instruction to the model. In the case

297
00:18:35,632 --> 00:18:38,608
of fine tuning, which we are talking all the time,

298
00:18:38,736 --> 00:18:42,416
where we need the data set and to take the foundational model, and then

299
00:18:42,440 --> 00:18:45,912
we fine tune the model and then we deploy the model in production. That is

300
00:18:45,928 --> 00:18:49,568
all the LLM engineering or the prompting wise fine tuning works.

301
00:18:49,696 --> 00:18:53,200
Still, I am not saying that we should go only for prompt engineering.

302
00:18:53,312 --> 00:18:56,684
That could be a domain specific task where you may require

303
00:18:57,194 --> 00:19:00,450
fine tuning, but typically most of the problem can

304
00:19:00,482 --> 00:19:04,890
be solved well enough by using the right amount of prompting technique

305
00:19:05,002 --> 00:19:08,594
like chain of thought or self consistency tree of thoughts,

306
00:19:08,634 --> 00:19:11,534
the multiple prompt engineering technique can be tried out.

307
00:19:12,034 --> 00:19:15,354
Now let's move on to the Lang chain demo,

308
00:19:15,434 --> 00:19:19,474
and I'll show you how easily a prototype

309
00:19:19,514 --> 00:19:22,134
and productionization of the model can be performed.

310
00:19:22,694 --> 00:19:26,206
As usual, for any libraries to be installed

311
00:19:26,230 --> 00:19:29,994
in Python, it has to follow via pip install or puda install.

312
00:19:30,614 --> 00:19:34,834
But once we install the Lang chain and the LangChain API

313
00:19:35,614 --> 00:19:38,886
installed, you have to procure the open API key,

314
00:19:38,990 --> 00:19:43,834
followed by installing the Lang chain, OpenAI and long chain libraries

315
00:19:44,654 --> 00:19:48,602
in a Python environment, followed by we

316
00:19:48,618 --> 00:19:52,098
have to import the lounging OpenAI and import

317
00:19:52,146 --> 00:19:56,370
the chat OpenAI and create call the function and specify

318
00:19:56,482 --> 00:19:59,494
the instruction via LLM invoke.

319
00:19:59,874 --> 00:20:03,938
So that is the power of three lines of code can effectively

320
00:20:04,026 --> 00:20:07,402
perform a prototyping for you. And when you wanted

321
00:20:07,418 --> 00:20:10,922
to print. How can Langsmith help with testing? You can get an

322
00:20:10,938 --> 00:20:14,706
output saying from the chart GPT model. Okay, this is how

323
00:20:14,850 --> 00:20:18,174
steps required Langsmith can help with the testing.

324
00:20:18,514 --> 00:20:22,434
Now we without a prompt, we have given an input, but by

325
00:20:22,474 --> 00:20:26,258
adding a prompt, we are specifying an instruction saying that what kind of

326
00:20:26,306 --> 00:20:29,754
task the input would be given. So in this

327
00:20:29,794 --> 00:20:32,842
case we are saying in the prompt that you

328
00:20:32,858 --> 00:20:37,054
are a world class technical documentation writer. By providing an input,

329
00:20:38,194 --> 00:20:41,362
it writes the document in a more efficient

330
00:20:41,458 --> 00:20:45,266
way manner like how the technical documentation writer would write.

331
00:20:45,410 --> 00:20:48,594
So that's the power of prompt by specifying the prompt.

332
00:20:48,714 --> 00:20:51,850
So you can see here we are, okay, the same thing. We are importing the

333
00:20:51,882 --> 00:20:55,170
packages libraries and we are invoking a chat open a

334
00:20:55,202 --> 00:20:59,058
function. Typically you have to for a security reason.

335
00:20:59,106 --> 00:21:02,410
I have written all the API key where you have to provide the API keys

336
00:21:02,442 --> 00:21:05,834
in the function, followed by the prompt template where you give the

337
00:21:05,874 --> 00:21:09,180
template as an instruction, as a prompt, followed by the user

338
00:21:09,212 --> 00:21:12,700
input. Now once I give like this your world

339
00:21:12,732 --> 00:21:16,116
class technical documentation writer the system prompt followed by the user

340
00:21:16,140 --> 00:21:20,252
input, user input and specifying the chain dot in put

341
00:21:20,308 --> 00:21:24,196
I can specify very good amount of output,

342
00:21:24,380 --> 00:21:28,260
like how a technical document writer would write it. So most

343
00:21:28,292 --> 00:21:31,660
of the things are very similar. On top of that we can have an output

344
00:21:31,692 --> 00:21:34,932
parser where we can define. The output parser could

345
00:21:34,948 --> 00:21:37,836
be in a JSON format, or it could be an excel, or how do we

346
00:21:37,860 --> 00:21:41,208
define the format for for a large language model,

347
00:21:41,376 --> 00:21:44,924
the output has to be performed. So by using this

348
00:21:45,584 --> 00:21:48,976
input and a prompt template and output parser,

349
00:21:49,160 --> 00:21:52,536
you're all set to get an output from the large language model like GPT

350
00:21:52,600 --> 00:21:56,152
four. If you have any questions, I'm more happier to talk

351
00:21:56,208 --> 00:21:59,784
after the session. Once again, thank you all for

352
00:21:59,824 --> 00:22:03,168
this, for your time and listening to the session. If you

353
00:22:03,176 --> 00:22:05,902
have any doubts, you can reach out to me at any point of time.

354
00:22:06,008 --> 00:22:10,266
Thank you all. Have a nice evening, have a good day and rest

355
00:22:10,290 --> 00:22:10,634
of the week.

