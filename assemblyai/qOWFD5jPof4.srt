1
00:00:29,074 --> 00:00:32,458
Thank you for joining in my talk title getting more toss with your logs

2
00:00:32,506 --> 00:00:35,986
at the 03:00 a.m. yeah. So before we

3
00:00:36,010 --> 00:00:39,362
begin the talk, let's introduce myself to you

4
00:00:39,378 --> 00:00:43,226
guys. So I am Shankma Kazaria, working at the software engineer

5
00:00:43,250 --> 00:00:47,146
at Zendot, an incident management platform which

6
00:00:47,210 --> 00:00:50,898
helps teams to respond to an incident and resolve it quickly.

7
00:00:51,066 --> 00:00:54,734
So I have several like years of experience in the software development.

8
00:00:57,204 --> 00:01:00,948
Today I'm excited to share insight on effective logging practices

9
00:01:01,076 --> 00:01:04,464
to help you resolve the issue quickly and efficiently.

10
00:01:05,764 --> 00:01:07,704
So let's start the talk.

11
00:01:08,324 --> 00:01:11,540
So what, what we are gonna discuss in this talk.

12
00:01:11,612 --> 00:01:15,548
So we will be discussing what and why of the logging, why it's important,

13
00:01:15,716 --> 00:01:19,196
what are the common login standard views which needs to be

14
00:01:19,220 --> 00:01:22,792
logged and what are the common mistakes people

15
00:01:22,848 --> 00:01:26,416
usually do while implementation and logging. And what are the logging best

16
00:01:26,440 --> 00:01:30,040
practices you can follow. And at the end once you

17
00:01:30,072 --> 00:01:34,064
have the better logging implementation, how, how log

18
00:01:34,104 --> 00:01:37,192
can be useful at three m when the constitution will

19
00:01:37,248 --> 00:01:40,760
present about that at the later part. Yeah. So let's

20
00:01:40,792 --> 00:01:44,168
start. So first basics, we'll start with the basic,

21
00:01:44,296 --> 00:01:47,896
yeah, so we'll start by voltage logging. So logging

22
00:01:47,920 --> 00:01:51,424
is, it's a kind of act of capturing and storing

23
00:01:51,464 --> 00:01:54,544
the events and messages or even generated by

24
00:01:54,584 --> 00:01:57,936
an application during its operation. So it's

25
00:01:58,000 --> 00:02:01,712
kind of like gives you traces like what has happened during

26
00:02:01,768 --> 00:02:04,832
the operation of the application, what all things,

27
00:02:04,968 --> 00:02:07,284
what all the event has occurred.

28
00:02:08,184 --> 00:02:11,736
Yeah. Then the next one is why logging

29
00:02:11,760 --> 00:02:15,492
is very important. So like you, you know what is

30
00:02:15,508 --> 00:02:18,780
the logging then why it's important. So logging is important in

31
00:02:18,812 --> 00:02:22,420
the scenarios like when you're double string

32
00:02:22,452 --> 00:02:26,228
the on like ongoing production issues. And so all

33
00:02:26,276 --> 00:02:29,284
you like Mozart and the on call engine all, you know what,

34
00:02:29,364 --> 00:02:32,796
how the crunchy stress and how is fracting situation is to be

35
00:02:32,820 --> 00:02:37,276
in debugging at the night or it like

36
00:02:37,380 --> 00:02:41,462
any, any situation. Then you

37
00:02:41,558 --> 00:02:44,798
like logging helps you in the monitoring the application health and

38
00:02:44,846 --> 00:02:49,070
performance, how the particular end consoling

39
00:02:49,102 --> 00:02:53,206
to a particular request. And then it's useful in

40
00:02:53,230 --> 00:02:56,870
the security auditing. So security, it's kind of

41
00:02:56,982 --> 00:03:01,270
like necessary to all the online systems. So it

42
00:03:01,302 --> 00:03:04,950
helps you to keep compliant to the

43
00:03:04,982 --> 00:03:08,394
auditing and the security like who can access what.

44
00:03:09,004 --> 00:03:12,644
And then it helps you to understand the user behavior

45
00:03:12,684 --> 00:03:16,428
like how users are using your application, how it's growing.

46
00:03:16,476 --> 00:03:19,584
So you can optimize it or you can,

47
00:03:21,164 --> 00:03:24,116
you can plan a better way to improve the user experience.

48
00:03:24,260 --> 00:03:28,796
Yeah. So as I mentioned, like troubleshooting

49
00:03:28,820 --> 00:03:31,956
a producer like in the night or

50
00:03:31,980 --> 00:03:35,316
at the any event time when there is any ongoing

51
00:03:35,340 --> 00:03:39,064
production logs are the only source of truth which tells

52
00:03:39,104 --> 00:03:42,376
what has happened during the particular request. What are the,

53
00:03:42,440 --> 00:03:46,232
you know, what are the operations or what are the function drain and

54
00:03:46,288 --> 00:03:50,048
which code has been like request, like which code

55
00:03:50,056 --> 00:03:53,656
is executed by the request. So it's really helpful.

56
00:03:53,760 --> 00:03:57,136
You'll get the better idea what has happened and what

57
00:03:57,200 --> 00:04:00,632
then. So, oh yeah, then monitoring the

58
00:04:00,648 --> 00:04:04,118
application health, as I like mentioned earlier, like it,

59
00:04:04,256 --> 00:04:07,858
it tells you the efficiency or it tells you how

60
00:04:07,986 --> 00:04:11,802
effectively your particular, your system is working. So what are

61
00:04:11,818 --> 00:04:15,450
the latencies? What are the, like v 99, which is

62
00:04:15,482 --> 00:04:19,146
like 99 percentile of your

63
00:04:19,290 --> 00:04:23,218
response time, like that. So you can monitor like

64
00:04:23,266 --> 00:04:26,534
what, what was before and what was after the

65
00:04:27,274 --> 00:04:31,082
optimize it. And security auditing. It's very critical

66
00:04:31,138 --> 00:04:34,026
part of any online applications then.

67
00:04:34,170 --> 00:04:38,282
Yeah. So now once we know like

68
00:04:38,378 --> 00:04:41,818
why logging is important, we will start about what is the,

69
00:04:41,986 --> 00:04:45,426
what are the components consist of on the login framework

70
00:04:45,490 --> 00:04:49,898
or what are the components are there and then we'll

71
00:04:49,946 --> 00:04:54,066
go and see how we can implement it. And what are the common logging

72
00:04:54,090 --> 00:04:57,626
signatures? Yeah, so in the logging framework there are

73
00:04:57,650 --> 00:05:01,318
like main four components, logger handlers, filters, and format is.

74
00:05:01,426 --> 00:05:05,222
So loggers is kind of like the entry point of the log which

75
00:05:05,278 --> 00:05:08,462
captures the log. So which captures the log event.

76
00:05:08,558 --> 00:05:12,194
And then like that, handlers comes into the picture which,

77
00:05:12,934 --> 00:05:16,430
which is responsible to sending that particular

78
00:05:16,502 --> 00:05:20,350
log event to its destination. So handlers can be like there are

79
00:05:20,382 --> 00:05:24,118
multiple kind of handlers, like a file handler, stream handler which

80
00:05:24,166 --> 00:05:27,570
rise to the particular stream or file then

81
00:05:27,642 --> 00:05:31,674
comes to the filters which filter out the event. So like

82
00:05:31,794 --> 00:05:35,194
you can filter out the particular events. And let's say if

83
00:05:35,274 --> 00:05:39,274
it is the error event then you might need to fill

84
00:05:39,314 --> 00:05:42,786
for some more like contextual data. Then filters

85
00:05:42,810 --> 00:05:46,738
can be useful. Or if you want, if you don't need particular

86
00:05:46,826 --> 00:05:50,666
type of event, then you can filter out at the filter server itself. It won't

87
00:05:50,730 --> 00:05:54,182
go till the log file, then format also formulator

88
00:05:54,238 --> 00:05:57,606
at the end it formats all the log data to

89
00:05:57,630 --> 00:06:01,326
a proper structure with like what are they

90
00:06:01,470 --> 00:06:05,214
required by you? Let's say you want the log data

91
00:06:05,254 --> 00:06:09,054
in the JSON, so it will, it will convert the

92
00:06:09,134 --> 00:06:12,254
raw data formatting to the JSON format

93
00:06:12,294 --> 00:06:16,110
data. This is what the like sample

94
00:06:16,142 --> 00:06:20,110
or their demo framework of the logging way like what

95
00:06:20,142 --> 00:06:22,894
are the Python program or whatever the language you are using.

96
00:06:22,974 --> 00:06:27,144
It generates the log message comes to the logger, logger then

97
00:06:28,204 --> 00:06:31,412
comes to the file and file which will write the logs

98
00:06:31,428 --> 00:06:34,908
to the file. Then it goes to filter which filter

99
00:06:34,956 --> 00:06:38,756
out the particular events as per your logic as per your need,

100
00:06:38,900 --> 00:06:42,316
then go through your format which will form in the

101
00:06:42,340 --> 00:06:46,140
particular, in the particular format and then it will

102
00:06:46,172 --> 00:06:48,624
translate login file. Yeah.

103
00:06:49,484 --> 00:06:52,852
So now more, once we know the what is logging and

104
00:06:52,868 --> 00:06:55,980
what is the login framework, we'll move ahead with the what

105
00:06:56,012 --> 00:06:59,690
are the common login scenario. So as we discussed,

106
00:06:59,882 --> 00:07:03,506
what are what for what are the

107
00:07:03,530 --> 00:07:07,146
purposes you can using the login. So make sure that we log

108
00:07:07,210 --> 00:07:11,066
the event, particular event. So if you want to troubleshooting or

109
00:07:11,090 --> 00:07:14,834
debugging it for debugging purpose, you log

110
00:07:14,914 --> 00:07:18,498
like this particular event

111
00:07:18,546 --> 00:07:22,322
has happened. So in the example as I showed like logger debugger,

112
00:07:22,378 --> 00:07:26,694
this is the debugging message where you write your appropriate message at appropriate time

113
00:07:27,654 --> 00:07:31,486
based on the what are the function it's in. Then you

114
00:07:31,510 --> 00:07:37,166
write the logger for arrow handling where like you,

115
00:07:37,270 --> 00:07:41,886
you are expecting some error or exception

116
00:07:41,990 --> 00:07:45,590
in the particular code block. Then you put it under the try and

117
00:07:45,622 --> 00:07:49,334
catch. Then in the cache or in the when you like case

118
00:07:49,374 --> 00:07:53,356
the error, you write up unlock saying that this operation has

119
00:07:53,380 --> 00:07:56,624
been failed. And then you attach the particular

120
00:07:58,524 --> 00:08:02,420
contextual information. So if the time of debugging can

121
00:08:02,452 --> 00:08:06,044
be useful and the other is a performance monitoring.

122
00:08:06,084 --> 00:08:09,612
So where you want to monitor

123
00:08:09,668 --> 00:08:14,628
a particular endpoint, a particular function blocks or performs

124
00:08:14,676 --> 00:08:18,008
a particular function, you rightly start time,

125
00:08:18,116 --> 00:08:21,536
then perform a particular operation and then put an

126
00:08:21,560 --> 00:08:24,864
end time and log that particular like start time and end time.

127
00:08:24,984 --> 00:08:28,488
So you'll get to know how much time is taking to perform a particular

128
00:08:28,656 --> 00:08:31,840
code block. And the other thing is security

129
00:08:31,912 --> 00:08:35,576
auditing. So as I mentioned earlier, it's very like important

130
00:08:35,680 --> 00:08:39,056
to like maintain the security compliance and all this.

131
00:08:39,200 --> 00:08:42,760
So where you monitors like this particular user is logged in

132
00:08:42,872 --> 00:08:46,724
from this particular user like ip address or this

133
00:08:46,764 --> 00:08:49,972
specific user has performed a particular action.

134
00:08:50,028 --> 00:08:53,104
So, so by with the login,

135
00:08:53,604 --> 00:08:56,988
security auditing login, you adhere to the compliance and

136
00:08:57,036 --> 00:09:01,148
you can like while auditing you can properly show them with

137
00:09:01,316 --> 00:09:04,504
user search and watch what all things. Yeah,

138
00:09:05,004 --> 00:09:08,244
so now like we know the,

139
00:09:08,404 --> 00:09:12,252
what are the common logging scenarios, we move on the common logging metric.

140
00:09:12,308 --> 00:09:15,856
So this is uh, in this particular point we are gonna

141
00:09:15,960 --> 00:09:19,168
talk about what are the basic or what are the common

142
00:09:19,216 --> 00:09:22,376
mistake logging mistakes, uh, like a

143
00:09:22,400 --> 00:09:25,760
developer or like what are like also implementing the

144
00:09:25,792 --> 00:09:29,368
logging does. So one of the biggest message,

145
00:09:29,416 --> 00:09:32,832
ignoring the context with the log message. So while uh,

146
00:09:32,928 --> 00:09:36,720
write the message logins, uh said the uh, operate this

147
00:09:36,752 --> 00:09:40,600
operation has failed. You like what implementation. When you

148
00:09:40,632 --> 00:09:44,126
are a, in that frame of mind which you know

149
00:09:44,150 --> 00:09:47,886
the logic, you know the what, what this function is

150
00:09:47,910 --> 00:09:51,582
about, you you tend to miss the context of the data.

151
00:09:51,758 --> 00:09:55,062
Like you darling generally thinks like it's a

152
00:09:55,118 --> 00:09:59,318
very common issue. It's, it's a generic message.

153
00:09:59,446 --> 00:10:02,774
Anyone can understand what, what this message about.

154
00:10:02,894 --> 00:10:06,174
But once you, like, once you change

155
00:10:06,214 --> 00:10:10,142
the production, once after like 1015 days or like some,

156
00:10:10,198 --> 00:10:13,422
after some time when you start debugging that particular issue,

157
00:10:13,598 --> 00:10:17,462
you don't know like what is this message for?

158
00:10:17,638 --> 00:10:20,246
For which request is linked to what,

159
00:10:20,430 --> 00:10:24,286
which user has faced particular this issue? It's just logging,

160
00:10:24,310 --> 00:10:27,462
it's just taking up your storage. This log is

161
00:10:27,598 --> 00:10:31,254
kind of no useful without the contextual data. So uh,

162
00:10:31,414 --> 00:10:35,194
yeah, generally it tends to like not

163
00:10:36,064 --> 00:10:39,200
during the proper contextual data, then this log is

164
00:10:39,232 --> 00:10:42,920
of no use. Then another thing, it's like

165
00:10:43,112 --> 00:10:47,444
setting up, not setting a proper log level. So while writing the

166
00:10:48,184 --> 00:10:50,484
login generally,

167
00:10:51,624 --> 00:10:55,284
generally like what, what it feels like,

168
00:10:55,704 --> 00:10:59,144
you write the all the logs with the same log

169
00:10:59,184 --> 00:11:02,488
level. Like as I mentioned in the example,

170
00:11:02,536 --> 00:11:06,474
like all the, all the log is right

171
00:11:06,514 --> 00:11:09,898
and that is error. So it can be confusing,

172
00:11:10,026 --> 00:11:14,094
like it's a debugging event or it's like a particular

173
00:11:14,554 --> 00:11:18,090
operation has been successfully completed. There should be info or

174
00:11:18,122 --> 00:11:22,010
debugging. It shouldn't be like error, otherwise it will

175
00:11:22,122 --> 00:11:25,906
create the confusion while debugging the particular issue. Because you

176
00:11:25,930 --> 00:11:29,620
are getting the multiple events as an error log

177
00:11:29,722 --> 00:11:33,160
so you can't filter out the particular

178
00:11:33,232 --> 00:11:37,524
error events. And the other thing is like by mistake

179
00:11:39,104 --> 00:11:42,536
without the contextual data. You market the

180
00:11:42,560 --> 00:11:45,720
particular log as like

181
00:11:45,792 --> 00:11:49,768
warning, but it should be a error. So that is also

182
00:11:49,816 --> 00:11:53,680
happened. So while like what are code

183
00:11:53,712 --> 00:11:57,424
review or bi review? Someone needs to point out

184
00:11:57,464 --> 00:12:01,708
that all developer has to keep in mind all the contextual

185
00:12:01,856 --> 00:12:05,396
operation has been performed before and after that particular code block and

186
00:12:05,460 --> 00:12:09,524
write the logging level as a proper

187
00:12:09,604 --> 00:12:13,388
login level. Yeah, one another mistake.

188
00:12:13,476 --> 00:12:16,932
Generally we were doing the

189
00:12:16,948 --> 00:12:20,868
public log with the contextual data, but that contextual

190
00:12:20,916 --> 00:12:24,164
data of the unstructured data. So what,

191
00:12:24,244 --> 00:12:27,612
what I mean unstructured data is like in the

192
00:12:27,668 --> 00:12:30,304
some lobe they have like a particular,

193
00:12:30,984 --> 00:12:34,480
let's say function name, user id and IP address. But in

194
00:12:34,512 --> 00:12:38,496
the very next or in the later part of the code they tend to miss

195
00:12:38,600 --> 00:12:42,992
that particular info in that or

196
00:12:43,008 --> 00:12:46,408
in the other place. Let's say in the example when

197
00:12:46,576 --> 00:12:49,960
I'm writing the log for the first operation I am mentioning the

198
00:12:49,992 --> 00:12:53,804
function and user id and ip in the log context.

199
00:12:54,144 --> 00:12:57,720
Then in the second operation I'll mention the file name

200
00:12:57,752 --> 00:13:01,842
and then line number in the message. I'm not mentioning the ip

201
00:13:01,898 --> 00:13:05,922
and user id there in the second, so can be

202
00:13:06,058 --> 00:13:10,134
misleading unstructured the data. So like when rmb,

203
00:13:10,434 --> 00:13:14,506
let's say I want to find what are the operation performed

204
00:13:14,530 --> 00:13:17,890
by the user id one. So when I search with the user id one,

205
00:13:17,922 --> 00:13:22,346
I'll get only the operation one. I won't get the operation to because there

206
00:13:22,370 --> 00:13:25,232
is no user id field in the on load.

207
00:13:25,378 --> 00:13:28,460
So kind of unstructured data, you won't get the,

208
00:13:28,572 --> 00:13:32,748
all the, all the events or all the operation

209
00:13:32,796 --> 00:13:35,684
performed at the particular user id or what are the,

210
00:13:35,764 --> 00:13:39,468
as per your requirement. So on logging should be in the structured

211
00:13:39,516 --> 00:13:43,624
way, all the details should be there in the old place

212
00:13:45,284 --> 00:13:50,012
value mismatch for SNK. So this is also kind

213
00:13:50,068 --> 00:13:53,908
of like a repetitive mistake that generally

214
00:13:53,956 --> 00:13:57,376
found out one login. So let's find the example.

215
00:13:57,440 --> 00:14:00,664
I have the proper sub contextual data,

216
00:14:00,784 --> 00:14:04,520
like with the proper key, but in the value of

217
00:14:04,552 --> 00:14:08,024
that particular key is type of that value is

218
00:14:08,064 --> 00:14:11,864
different. So let's say in the line, uh, key, uh, value in the first

219
00:14:11,904 --> 00:14:15,768
operation is 15 in the integer, but in the second and

220
00:14:15,816 --> 00:14:19,336
15 is in the string. And similarly in the

221
00:14:19,360 --> 00:14:23,092
data, the first log has the dictionary order JSON value,

222
00:14:23,208 --> 00:14:27,356
but in the later, like in the second operation it's the uh,

223
00:14:27,460 --> 00:14:30,704
string value. So um,

224
00:14:31,004 --> 00:14:34,820
what's wrong in this is like uh, whenever like

225
00:14:34,852 --> 00:14:38,556
whatever the software you are using, what are the open source application or

226
00:14:38,580 --> 00:14:42,444
whatever the things you are using for visualization code

227
00:14:42,524 --> 00:14:46,380
will reject the second record stating that it's not the

228
00:14:46,412 --> 00:14:48,744
appropriate, uh uh,

229
00:14:49,204 --> 00:14:52,850
like appropriate type of value.

230
00:14:52,962 --> 00:14:55,842
So in the sum record is coming as integer,

231
00:14:55,898 --> 00:14:59,746
some record is coming in the string. So it will tend to

232
00:14:59,770 --> 00:15:04,522
reject the mismatched type of logs.

233
00:15:04,578 --> 00:15:08,650
So you like, while visualization or

234
00:15:08,722 --> 00:15:12,970
while fetching out the logs, this particular engine in

235
00:15:13,002 --> 00:15:16,254
our like list of logs. And it's kind of,

236
00:15:17,334 --> 00:15:21,142
kind of misleading that you tend to think that this particular code

237
00:15:21,158 --> 00:15:24,518
block is like particular request hasn't

238
00:15:24,566 --> 00:15:28,870
touched this particular code block. So, which is kind of not true.

239
00:15:29,022 --> 00:15:32,806
So while logging, be careful

240
00:15:32,870 --> 00:15:36,222
about the value, type of tech type of

241
00:15:36,238 --> 00:15:39,638
the value. And now since we

242
00:15:39,726 --> 00:15:43,286
have like discussed what are the common logging like mistakes

243
00:15:43,390 --> 00:15:46,858
like, we'll see what are the best practices. So you

244
00:15:46,906 --> 00:15:50,626
don't miss out any dogs. So I will

245
00:15:50,650 --> 00:15:54,138
like first decide what to know. So you

246
00:15:54,226 --> 00:15:57,454
decide what are, what's the proper necessary

247
00:15:57,794 --> 00:16:01,514
or message, which someone else can make sense.

248
00:16:01,674 --> 00:16:05,994
So generally people write the like vague messages like request successful

249
00:16:06,074 --> 00:16:09,658
operation, successful operation fail. It should

250
00:16:09,706 --> 00:16:13,232
be writing the proper logging method so

251
00:16:13,288 --> 00:16:17,360
someone else can make sense out of that message

252
00:16:17,392 --> 00:16:21,384
and they can get the idea what has happened wrong or what has happened successfully.

253
00:16:21,424 --> 00:16:25,272
Like that and include all the necessary metadata.

254
00:16:25,408 --> 00:16:28,936
So in the necessary metadata, meaning the contextual data,

255
00:16:29,000 --> 00:16:32,144
what the, what's the function name where this particular

256
00:16:32,224 --> 00:16:36,432
error occurred, the line number and what is the exact error occurred

257
00:16:36,568 --> 00:16:39,864
and all, like what's the username,

258
00:16:39,904 --> 00:16:43,184
what's the like, users is a name who's like making this particular

259
00:16:43,264 --> 00:16:47,232
request, what's this IP address and all this. And so you can get,

260
00:16:47,368 --> 00:16:50,624
you can get the best out of the logs and I recommend

261
00:16:50,664 --> 00:16:54,152
to use one trace id or any correlation id.

262
00:16:54,288 --> 00:16:57,824
So whenever you write a log for that particular request,

263
00:16:57,904 --> 00:17:01,320
include the trace id to all the logs during that

264
00:17:01,352 --> 00:17:05,544
request cycle. So like whenever you want to fetch, whatever events

265
00:17:05,584 --> 00:17:09,208
happen during the trigger cycle, you just need to get the

266
00:17:09,296 --> 00:17:12,885
trace id. And once you search with the test id, get the list

267
00:17:12,909 --> 00:17:16,677
of all the events are for that particular request.

268
00:17:16,845 --> 00:17:20,445
So that the correlation key, it's kind of very useful because

269
00:17:20,509 --> 00:17:24,045
it will help you correlate the matrix log.

270
00:17:24,109 --> 00:17:26,393
So in observability there are like,

271
00:17:27,813 --> 00:17:30,073
it's consist of three main things,

272
00:17:30,493 --> 00:17:34,301
matrix logs and traces, right? So it's,

273
00:17:34,437 --> 00:17:38,160
and if you are have the micro service architecture,

274
00:17:38,192 --> 00:17:41,416
then it's very much useful to correlate the,

275
00:17:41,480 --> 00:17:45,528
or to get the traces of particular requests

276
00:17:45,576 --> 00:17:48,872
like which all micro services went and what are the operation happening,

277
00:17:48,928 --> 00:17:52,600
particular micro services. Then you can, you can link

278
00:17:52,712 --> 00:17:55,896
those events and get the most details how out

279
00:17:55,920 --> 00:17:59,064
of that particular event. And then it's like,

280
00:17:59,144 --> 00:18:03,136
as we talk about, like include all the necessary data or include

281
00:18:03,280 --> 00:18:07,124
more data. It's not like you include all these data which,

282
00:18:07,664 --> 00:18:11,080
which is USD access. Also you need to decide what

283
00:18:11,192 --> 00:18:14,680
are the data needs to be logged, what not least to be dope

284
00:18:14,832 --> 00:18:18,336
in login data. Like just keep in mind that

285
00:18:18,360 --> 00:18:22,024
you don't load the PI data which is like a credit card

286
00:18:22,064 --> 00:18:26,536
number, someone's like id password,

287
00:18:26,600 --> 00:18:30,120
some, something like PI data. So just keep in mind you don't log

288
00:18:30,152 --> 00:18:33,820
the some like PI data. And while writing

289
00:18:33,972 --> 00:18:37,852
define the proper log with the uM, like just

290
00:18:37,908 --> 00:18:41,304
keep in mind the contextual place of that particular code block.

291
00:18:42,084 --> 00:18:46,348
And then as we discussed, like why, like how people are uh,

292
00:18:46,516 --> 00:18:49,452
keeping structure like a log with unstructured.

293
00:18:49,508 --> 00:18:53,012
So keep the star, keep the structure of the

294
00:18:53,028 --> 00:18:56,344
log defined and stick to them. So your,

295
00:18:57,164 --> 00:19:01,070
you include all the necessary details, what you have like

296
00:19:01,212 --> 00:19:04,730
decided to have all the places. So it will be you

297
00:19:04,882 --> 00:19:08,694
easy for you to like debug the concentration

298
00:19:09,074 --> 00:19:12,082
and what are the benefit of the structure log. So like once you have the

299
00:19:12,098 --> 00:19:15,906
structure login, you can even better visualize the all the

300
00:19:15,930 --> 00:19:19,394
events, how many. So you can better like visualize

301
00:19:19,434 --> 00:19:23,626
and send like in the time frame or particular like

302
00:19:23,730 --> 00:19:27,034
1 hour to r how many events are occurred,

303
00:19:27,114 --> 00:19:30,394
how many? Like you can maybe land graph, histogram or

304
00:19:30,434 --> 00:19:34,522
hit map done like that. So you get the better

305
00:19:34,578 --> 00:19:37,986
visualization. Uh, once you have the meta visualization and the

306
00:19:38,010 --> 00:19:41,554
proper login, you can see the

307
00:19:41,714 --> 00:19:46,250
logs pattern and you can set

308
00:19:46,282 --> 00:19:49,594
the proper alerting on top of the panel loads.

309
00:19:49,714 --> 00:19:53,626
So like you can set a alerting,

310
00:19:53,730 --> 00:19:57,710
let's say alerts me. Particular events happen in the particular time frame

311
00:19:57,882 --> 00:20:01,942
it now. So like you get to know if

312
00:20:02,078 --> 00:20:05,742
particular events are failing continuously, then you

313
00:20:05,838 --> 00:20:09,662
get to know why it's failing. And now you can debug and fix it as

314
00:20:09,678 --> 00:20:13,754
soon as possible. And unless you are getting the timely alert

315
00:20:14,934 --> 00:20:18,830
and you have the proper login, your incident response time will be

316
00:20:18,982 --> 00:20:23,686
because you will be able to handle the incident

317
00:20:23,750 --> 00:20:27,746
quickly, efficiently. And proper

318
00:20:27,810 --> 00:20:31,450
logging doesn't only like, enables or help

319
00:20:31,522 --> 00:20:35,234
tech team to deal issue with fastly, but it also

320
00:20:35,274 --> 00:20:40,530
improves the collaboration with the other teams. Let's say the

321
00:20:40,562 --> 00:20:44,354
growth or business thing wants to know how many insert, how many

322
00:20:44,514 --> 00:20:48,458
events are occurring like

323
00:20:48,506 --> 00:20:51,794
in the particular time frame. So let's, I give you example of my accounting.

324
00:20:51,834 --> 00:20:55,728
So as we, as I mentioned, like we are the incident management tool.

325
00:20:55,856 --> 00:20:59,696
So it's kind of useful for us to know

326
00:20:59,760 --> 00:21:03,296
like how many incidents are getting created in 1 hour or

327
00:21:03,320 --> 00:21:06,688
130 minutes, how many instances are getting created for a

328
00:21:06,736 --> 00:21:10,544
particular user or particular customer. So, and we can publish

329
00:21:10,584 --> 00:21:14,360
this data to the self or grow team to look

330
00:21:14,392 --> 00:21:18,312
out for, and from there they can make

331
00:21:18,408 --> 00:21:21,520
their analytical decisions and how to go forward,

332
00:21:21,592 --> 00:21:25,120
how to, how to run campaign or what

333
00:21:25,152 --> 00:21:28,776
are the other things they can watch out for based on the

334
00:21:28,800 --> 00:21:32,304
particular matrix. And then what

335
00:21:32,344 --> 00:21:36,024
are then after that, once we

336
00:21:36,104 --> 00:21:39,688
have the structure data, then we need to decide what are the scenarios you

337
00:21:39,736 --> 00:21:43,192
gonna know. So it's not like you, you are

338
00:21:43,208 --> 00:21:46,968
gonna know each and every scenarios with the each and every message. That's kind

339
00:21:47,016 --> 00:21:50,246
of like information order.

340
00:21:50,310 --> 00:21:53,910
There are two log or too many logs.

341
00:21:53,942 --> 00:21:56,954
It's also a very like a bad thing because,

342
00:21:57,454 --> 00:22:01,134
because you will be get lost while debugging

343
00:22:01,174 --> 00:22:04,798
from those load. So decide what are the scenarios you're

344
00:22:04,806 --> 00:22:08,094
gonna load. So in the example, as I said,

345
00:22:08,134 --> 00:22:11,390
like whenever you are making the require, like it's

346
00:22:11,422 --> 00:22:15,486
just for example, when I am like making a request, I'm catching the timeout

347
00:22:15,550 --> 00:22:18,614
error, writing the appropriate message for

348
00:22:18,654 --> 00:22:22,166
that particular error. Then in the second I

349
00:22:22,190 --> 00:22:25,422
may accept the connection error and then

350
00:22:25,518 --> 00:22:28,934
writing the particular message for that particular error.

351
00:22:29,054 --> 00:22:32,614
So instead of this,

352
00:22:32,694 --> 00:22:39,590
I can do it like try catch the, all the errors in the single exception

353
00:22:39,662 --> 00:22:42,850
and I can log the old, I can log

354
00:22:42,882 --> 00:22:46,690
the message that request failed with the data. So that particular

355
00:22:46,762 --> 00:22:50,586
time like I couldn't get to know

356
00:22:50,650 --> 00:22:54,250
why request failed. So instead of

357
00:22:54,322 --> 00:22:58,450
writing like this and one of like what I recommend,

358
00:22:58,562 --> 00:23:00,014
it's like don't,

359
00:23:01,554 --> 00:23:05,514
don't, don't try to catch the like get

360
00:23:05,554 --> 00:23:09,062
exception, see what the

361
00:23:09,158 --> 00:23:12,638
code block about. Then try to think of

362
00:23:12,686 --> 00:23:16,574
all the exception that that particular core that can

363
00:23:16,734 --> 00:23:21,398
occur and that particular code block and then catch

364
00:23:21,446 --> 00:23:25,238
each, each individually write the appropriate message.

365
00:23:25,366 --> 00:23:28,814
And if you don't know what can't go wrong

366
00:23:28,854 --> 00:23:32,734
in that particular code block, it's better to leave it,

367
00:23:32,854 --> 00:23:36,662
don't catch that particular assessment, let it fail. Let your application

368
00:23:36,718 --> 00:23:40,344
monitoring generate

369
00:23:40,384 --> 00:23:44,244
another, generate an incident and let you know this particular things

370
00:23:45,104 --> 00:23:49,296
went wrong and then you come and fix a particular issue. Otherwise once

371
00:23:49,320 --> 00:23:52,648
you write the blanket accession you don't know what is going

372
00:23:52,696 --> 00:23:56,760
wrong in particular situation and you kind of overlook

373
00:23:56,872 --> 00:23:58,044
the potential,

374
00:24:00,144 --> 00:24:03,304
potential bug or something which impact your

375
00:24:03,344 --> 00:24:07,192
company technically or financially. Anything can be

376
00:24:07,208 --> 00:24:10,936
possible. Yeah. So and one

377
00:24:10,960 --> 00:24:14,888
of the, one of the most important

378
00:24:15,016 --> 00:24:18,504
is like how you storing the law logs.

379
00:24:18,624 --> 00:24:22,604
So load storage also one of the important thing because like

380
00:24:23,344 --> 00:24:26,856
it shouldn't be too less or too less or

381
00:24:26,920 --> 00:24:30,152
too much. So let's say you are

382
00:24:30,248 --> 00:24:33,600
storing the log for seven days or like past seven days or

383
00:24:33,632 --> 00:24:37,444
past ten days. There might be if the,

384
00:24:38,504 --> 00:24:42,552
if you need that past 1520

385
00:24:42,608 --> 00:24:47,680
days of later then you need to restore and restore

386
00:24:47,712 --> 00:24:52,440
the older logs. And in that situation just keep

387
00:24:52,472 --> 00:24:55,592
in mind of the log retention period as

388
00:24:55,608 --> 00:24:59,128
per your requirement or how like how many past days of data

389
00:24:59,176 --> 00:25:02,488
you need at the time. Then once you after

390
00:25:02,536 --> 00:25:05,776
that particular time period move into cold storage. So it

391
00:25:05,800 --> 00:25:08,472
will help to save the cost as well.

392
00:25:08,648 --> 00:25:11,880
And then

393
00:25:11,952 --> 00:25:16,760
add the, in a certain, like add the, like what

394
00:25:16,792 --> 00:25:20,044
are the keys or what are the,

395
00:25:21,024 --> 00:25:24,432
like what are the information you

396
00:25:24,448 --> 00:25:27,958
are using to what search a particular log.

397
00:25:28,046 --> 00:25:32,126
Just make note of that and add

398
00:25:32,150 --> 00:25:35,382
the indexing of that particular information

399
00:25:35,558 --> 00:25:39,198
on that particular key. So it the searching will be a faster

400
00:25:39,366 --> 00:25:42,994
and take the makeup of the data. So like you don't

401
00:25:43,374 --> 00:25:46,474
lost the data in case of any disaster.

402
00:25:47,574 --> 00:25:51,710
So and alerting. Now we have the logging proper login

403
00:25:51,742 --> 00:25:55,966
setup. You can set up the alert roles. You can set up the alerting for

404
00:25:55,990 --> 00:25:59,094
a particular event. So like, but let's say

405
00:25:59,134 --> 00:26:02,574
event a occurred 1015 times. Then you will get the

406
00:26:02,614 --> 00:26:06,674
know, you get the on call engineer will be notified for

407
00:26:07,734 --> 00:26:13,030
that particular alert. Saying this event is the particular

408
00:26:13,102 --> 00:26:17,230
event is getting failed or in error while performing

409
00:26:17,262 --> 00:26:21,006
particular operation so they can see and quickly fix the

410
00:26:21,030 --> 00:26:24,844
issue. And then once we have the login, you can

411
00:26:25,904 --> 00:26:29,176
better visualize it. Once you have a better visualization out of

412
00:26:29,200 --> 00:26:32,792
it, you can, you can write

413
00:26:32,848 --> 00:26:36,920
the alert for an only detection. Let's say usually

414
00:26:36,992 --> 00:26:40,960
your traffic is ready like 30 40 requests

415
00:26:40,992 --> 00:26:44,440
per second and one, and then suddenly,

416
00:26:44,512 --> 00:26:47,792
and there's some particular point of time, there are 100

417
00:26:47,808 --> 00:26:51,032
5200 requests per second. Then it's kind of an ugly.

418
00:26:51,168 --> 00:26:54,632
So there might be out of like due to the needle

419
00:26:54,688 --> 00:26:58,024
someone is doing on your website. So at that particular time

420
00:26:58,064 --> 00:27:02,424
you can set up an only detection alert

421
00:27:02,464 --> 00:27:05,844
rules which based on your requirement and then

422
00:27:06,264 --> 00:27:09,440
it helps you to reduce the alert fatigue.

423
00:27:09,552 --> 00:27:13,024
So let's say like you are,

424
00:27:13,064 --> 00:27:17,092
like you keep the threshold as a 50

425
00:27:17,148 --> 00:27:21,144
requests per second and then you, your application has

426
00:27:21,604 --> 00:27:25,092
grown, grown well, you are onboarding more customers and

427
00:27:25,108 --> 00:27:28,516
all these things. Then your request, now generally

428
00:27:28,540 --> 00:27:32,220
your request, like 50 60 requests per second. Then you can update this

429
00:27:32,252 --> 00:27:36,220
result. And because it's general, it's not the, out of

430
00:27:36,252 --> 00:27:39,580
ddos, you are getting 56 requests. So at that

431
00:27:39,612 --> 00:27:43,592
time you can update the threshold and, and you

432
00:27:43,608 --> 00:27:47,680
can get the proper alerting. And now we

433
00:27:47,752 --> 00:27:51,688
have the like a proper alerting setup with the proper logging,

434
00:27:51,736 --> 00:27:55,488
with the proper contextual data and the proper visualization,

435
00:27:55,656 --> 00:28:00,248
how it can be useful at the three a. So mostly once

436
00:28:00,296 --> 00:28:04,364
you have the proper logging structure and all the visualization

437
00:28:04,744 --> 00:28:08,248
are there, most of the scenarios which you anticipate

438
00:28:08,296 --> 00:28:12,324
can be, can go wrong. You visualize it properly,

439
00:28:12,444 --> 00:28:16,460
so you get the idea of the situation or get to

440
00:28:16,492 --> 00:28:19,948
know like why it's happening from the visualization itself.

441
00:28:19,996 --> 00:28:23,396
You don't need to go to logs and searches

442
00:28:23,420 --> 00:28:28,188
for each and every request. And so like basically

443
00:28:28,236 --> 00:28:31,864
you can find the anomaly from where the visualization itself.

444
00:28:32,204 --> 00:28:35,228
If, if particular errors occur,

445
00:28:35,396 --> 00:28:38,792
occur out like no out. If you

446
00:28:38,808 --> 00:28:42,184
are expected error bounding, let's say like you are not, not expecting

447
00:28:42,224 --> 00:28:46,432
particular data or you want to debug the specific request.

448
00:28:46,528 --> 00:28:49,904
And then you can use this like

449
00:28:49,984 --> 00:28:54,376
trace id instructors in the log. So as

450
00:28:54,400 --> 00:28:58,404
I mentioned previously, trace id or a correlation id, you should

451
00:28:59,624 --> 00:29:03,730
log as part of contextual data in the every law. So let's

452
00:29:03,762 --> 00:29:07,354
say you want to log a particular request, what all happened

453
00:29:07,434 --> 00:29:11,074
during the request second and what like where it went wrong and all the sense

454
00:29:11,114 --> 00:29:14,650
you can get the trace id of that particular request and search

455
00:29:14,682 --> 00:29:18,362
it in their application log. And you will

456
00:29:18,378 --> 00:29:21,618
get the, all the events, all the events

457
00:29:21,746 --> 00:29:25,066
related to that particular trace id. And then you get to

458
00:29:25,090 --> 00:29:28,946
know like what went wrong? So this is the example I'm

459
00:29:28,970 --> 00:29:32,336
drawing. So in the one of our thing we verify

460
00:29:32,360 --> 00:29:35,648
the contact number we by sending the OTP. So in

461
00:29:35,696 --> 00:29:39,000
order we have logged the all the events as occurred.

462
00:29:39,032 --> 00:29:42,592
So let's from the start where API invoked.

463
00:29:42,728 --> 00:29:46,496
Then whether need to be verified or not, whether it's numbers

464
00:29:46,520 --> 00:29:50,248
already verified, whether the user is in the block

465
00:29:50,296 --> 00:29:54,512
list or not due to like some issue

466
00:29:54,568 --> 00:29:58,010
with the telecom provider. And then, then we check

467
00:29:58,042 --> 00:30:01,994
with the telecom provider. What surprising to sending

468
00:30:02,154 --> 00:30:05,874
messages or phone calls to that particular number

469
00:30:06,034 --> 00:30:09,834
whether this particular country is allowed from our side or from

470
00:30:09,874 --> 00:30:12,914
the network provider side. And then once all the checks

471
00:30:12,954 --> 00:30:16,770
pass we, we send the OTP verification

472
00:30:16,842 --> 00:30:20,282
code. So from this kind of login you'll get to know

473
00:30:20,338 --> 00:30:24,358
what are the events happen successfully and over. Like if

474
00:30:24,446 --> 00:30:27,662
something goes wrong then we could have logged like

475
00:30:27,718 --> 00:30:31,798
uh, this particular check space. Not changing the or

476
00:30:31,966 --> 00:30:35,526
no, not changing the automatic verification log. So then when

477
00:30:35,550 --> 00:30:39,630
the customer comes and saying I'm not getting the verification code, then we quickly

478
00:30:39,662 --> 00:30:43,110
check that particular request and then we can tell

479
00:30:43,142 --> 00:30:47,318
them like why it's like what are the like check is failing and

480
00:30:47,406 --> 00:30:50,966
what needs to be done from their side or for our side or based on

481
00:30:50,990 --> 00:30:54,734
that we can reach out to network provider file if it's

482
00:30:54,774 --> 00:30:56,114
failing from their side.

483
00:30:57,454 --> 00:31:00,830
And as we have the like

484
00:31:00,862 --> 00:31:04,514
a logging setup and too many logs are firing and

485
00:31:04,814 --> 00:31:08,710
storing in this storage. Like it can be

486
00:31:08,862 --> 00:31:12,430
like if you have too many logs you can get

487
00:31:12,462 --> 00:31:16,274
lost in the old logs. So what

488
00:31:16,314 --> 00:31:20,106
generally in the big organization do

489
00:31:20,170 --> 00:31:23,786
they run the two parallel stream, one for the info log and what for the

490
00:31:23,810 --> 00:31:27,010
debug? Debug log. Debugger level logs.

491
00:31:27,122 --> 00:31:31,322
So in the info log consists of the info warning and critical

492
00:31:31,458 --> 00:31:35,490
error logs. And debugger logs holds all the logs

493
00:31:35,522 --> 00:31:39,442
with, with the debugging level of verbosity.

494
00:31:39,578 --> 00:31:43,298
So generally developer has access to info logs

495
00:31:43,346 --> 00:31:47,340
and the debug level have access like develop access

496
00:31:47,412 --> 00:31:50,812
to the SRE or DevOps guys or someone

497
00:31:50,868 --> 00:31:54,188
who's managing the operation. So like when the issue

498
00:31:54,276 --> 00:31:57,988
comes, developer tries to debug with the info log.

499
00:31:58,116 --> 00:32:01,860
If that particular if is not make the most sense

500
00:32:01,892 --> 00:32:05,884
out of that, then they ask for a debugger access and to,

501
00:32:05,964 --> 00:32:09,500
to get the like to fix the issue

502
00:32:09,572 --> 00:32:13,716
or to get to know what is the issue. And similarly this

503
00:32:13,740 --> 00:32:17,084
is like example shows like you can

504
00:32:17,124 --> 00:32:21,236
write the particular log level in the particular file

505
00:32:21,380 --> 00:32:25,384
or you can write the logger based on the module. So it's

506
00:32:25,804 --> 00:32:30,596
up to you how you want to, like how

507
00:32:30,620 --> 00:32:34,676
you want to log and yeah, but be

508
00:32:34,740 --> 00:32:38,444
careful you don't get lost in the logs. Don't log every

509
00:32:38,484 --> 00:32:41,044
information, don't log like each and every event.

510
00:32:41,124 --> 00:32:44,384
But look what are the necessary events.

511
00:32:45,044 --> 00:32:48,420
Yeah. And how you can achieve

512
00:32:48,452 --> 00:32:52,196
this particular blogging. So there are like many applications

513
00:32:52,260 --> 00:32:57,356
are available, let's say like new relatable

514
00:32:57,540 --> 00:33:01,124
open telemetry, all are there and you can use

515
00:33:01,164 --> 00:33:04,156
any one of them. They have their own pros and cons.

516
00:33:04,340 --> 00:33:08,572
And I have listed the three most useful like

517
00:33:08,668 --> 00:33:11,024
used open source applications.

518
00:33:11,614 --> 00:33:15,486
One EFK, which consists of three, three open source tool,

519
00:33:15,630 --> 00:33:19,222
Rastrixus, locust, Kibana EFK which is like

520
00:33:19,398 --> 00:33:24,634
instead of locksteps you are using the flower fluently

521
00:33:25,254 --> 00:33:28,494
and the graph analogy. So what is VFK?

522
00:33:28,574 --> 00:33:32,078
So elasticsearch fluent in Kibana.

523
00:33:32,166 --> 00:33:35,478
So elasticsearch is the log storage and where

524
00:33:35,526 --> 00:33:38,918
you can search the particular things quickly.

525
00:33:39,046 --> 00:33:43,088
Fluent is the load connector and collects it, connects the log

526
00:33:43,136 --> 00:33:47,112
and push it to the elastic search. And Kibana on top of kibana on

527
00:33:47,128 --> 00:33:50,432
top of can visualize the logs and you can,

528
00:33:50,528 --> 00:33:53,728
it's kind of will provide you the visual UI or

529
00:33:53,776 --> 00:33:57,280
visualization on top of your logs. The form unlocking

530
00:33:57,312 --> 00:34:00,848
is also the same. So there are only the

531
00:34:00,976 --> 00:34:04,224
components are changing. So instead of flowing bit or

532
00:34:04,304 --> 00:34:07,720
you are using on tail from tail here, then low key as your

533
00:34:07,752 --> 00:34:11,930
local vector and the graphene as your visualization alert.

534
00:34:12,002 --> 00:34:15,514
And you can also use the alert manager to

535
00:34:15,594 --> 00:34:19,074
like to set up the alert on top of it. Yeah.

536
00:34:19,154 --> 00:34:22,010
So that end from my side.

537
00:34:22,082 --> 00:34:25,826
So hope you get the idea

538
00:34:25,890 --> 00:34:29,426
how to set up a proper logging and how it can be

539
00:34:29,490 --> 00:34:32,522
useful during your transition. So yeah,

540
00:34:32,578 --> 00:34:35,458
if you have any question, do let me know.

541
00:34:35,586 --> 00:34:38,706
As I showed my LinkedIn QR code

542
00:34:38,770 --> 00:34:42,286
in my home islet with you can connect with me and we

543
00:34:42,310 --> 00:34:45,798
can get go from there. Yeah. Thank you guys.

544
00:34:45,846 --> 00:34:48,910
Thank you for being around for during my talk.

545
00:34:48,982 --> 00:34:49,654
Yeah, thank you.

