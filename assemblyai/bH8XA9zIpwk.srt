1
00:00:20,890 --> 00:00:24,586
In this presentation, we'll be deep diving into probabilistic programming,

2
00:00:24,698 --> 00:00:27,714
which is a powerful modeling approach that addresses

3
00:00:27,842 --> 00:00:31,590
critical challenges in traditional machine learning and AI techniques.

4
00:00:31,930 --> 00:00:35,254
We'll explore how probabilistic programming enables us

5
00:00:35,292 --> 00:00:39,014
to embrace uncertainty, incorporate expert

6
00:00:39,062 --> 00:00:42,426
knowledge, and enhance transparency in the

7
00:00:42,448 --> 00:00:46,826
decision making. Finally, I will present the implementation of

8
00:00:46,928 --> 00:00:48,970
probabilistic models in Python.

9
00:00:50,450 --> 00:00:54,094
Why do we need probabilistic programming and

10
00:00:54,292 --> 00:00:57,822
what does it has to offer? What does it offer over other

11
00:00:57,876 --> 00:01:01,754
machine learning and AI techniques? So the fundamental challenge

12
00:01:01,802 --> 00:01:05,794
in conventional machine learning and AI techniques is the lack of

13
00:01:05,832 --> 00:01:09,278
uncertainty quantification. These models

14
00:01:09,374 --> 00:01:13,234
typically provide point estimates without accounting for

15
00:01:13,272 --> 00:01:17,538
the uncertainty surrounding their predictions. This limitation

16
00:01:17,634 --> 00:01:21,094
hampers our ability to assess the reliability of

17
00:01:21,132 --> 00:01:24,326
model and undermines our confidence in the

18
00:01:24,348 --> 00:01:27,414
decision making process. The second challenge we

19
00:01:27,452 --> 00:01:31,498
face is that machine learning models are data hungry and often

20
00:01:31,584 --> 00:01:35,194
require correctly labeled data, and these

21
00:01:35,232 --> 00:01:39,530
models tend to struggle with problems where

22
00:01:39,600 --> 00:01:43,230
data is limited. Conventional machine learning

23
00:01:43,300 --> 00:01:47,258
and AI techniques lack of framework to encode

24
00:01:47,434 --> 00:01:51,178
expert domain knowledge or prior beliefs

25
00:01:51,354 --> 00:01:55,230
into the model. So without the ability to leverage

26
00:01:55,390 --> 00:01:58,866
domain specific insights, the model might

27
00:01:58,968 --> 00:02:02,366
overlook crucial nuances in data and tend

28
00:02:02,398 --> 00:02:04,370
to not perform up to its potential.

29
00:02:04,970 --> 00:02:09,154
Lastly, machine learning models are becoming more and more complex

30
00:02:09,282 --> 00:02:13,058
and opaque, while public demands for more transparency

31
00:02:13,154 --> 00:02:16,822
and accountability on decisions being derived from data

32
00:02:16,876 --> 00:02:20,454
and AI. So all of this presents

33
00:02:20,502 --> 00:02:22,490
a need for a modeling framework,

34
00:02:23,150 --> 00:02:26,790
encode expert knowledge, work with limited

35
00:02:26,870 --> 00:02:30,718
data, provide predictions along with

36
00:02:30,804 --> 00:02:34,270
associated uncertainty, and provide

37
00:02:34,340 --> 00:02:37,930
models or enables models which offer more transparency

38
00:02:38,010 --> 00:02:41,754
and explainability. Probabilistic programming

39
00:02:41,882 --> 00:02:46,142
emerges as a game changer, so to understand probabilistic

40
00:02:46,206 --> 00:02:50,290
programming, it is essential to grasp bayesian statistics.

41
00:02:50,950 --> 00:02:54,702
How bayesian statistics differ from the classical

42
00:02:54,766 --> 00:02:58,138
frequentist approach. In frequentist

43
00:02:58,174 --> 00:03:01,266
statistics, model parameters are treated as fixed

44
00:03:01,298 --> 00:03:04,486
quantities, and uncertainty in the

45
00:03:04,508 --> 00:03:08,662
parameter estimation is typically addressed through techniques

46
00:03:08,726 --> 00:03:10,410
such as conference intervals.

47
00:03:11,070 --> 00:03:15,350
However, frequentist methods do not assign probability

48
00:03:15,430 --> 00:03:19,062
distribution to parameters, and their interpretation

49
00:03:19,126 --> 00:03:23,082
of uncertainty is rooted in the long run frequency

50
00:03:23,146 --> 00:03:26,826
properties of the estimators rather than explicit probabilistic

51
00:03:26,858 --> 00:03:30,606
statements about the parameter values, while in

52
00:03:30,628 --> 00:03:34,414
contrast, bayesian statistics. In bayesian statistics,

53
00:03:34,542 --> 00:03:37,954
unknown model parameters are treated as random variables and

54
00:03:37,992 --> 00:03:41,022
are modeled using probability distribution.

55
00:03:41,166 --> 00:03:44,658
So this approach inherently captures uncertainty within the

56
00:03:44,664 --> 00:03:48,290
parameters themselves, and hence this framework

57
00:03:48,370 --> 00:03:52,626
offers a more intuitive and a flexible approach to quantify

58
00:03:52,818 --> 00:03:56,962
uncertainty. How does Bayesian statistics

59
00:03:57,106 --> 00:04:01,226
work? Bayesian statistical methods use

60
00:04:01,328 --> 00:04:05,606
Bayesian theorem to compute and update probabilities

61
00:04:05,718 --> 00:04:09,034
as you obtain new data. This is a simple but

62
00:04:09,072 --> 00:04:12,206
a powerful equation. What we start with is the

63
00:04:12,228 --> 00:04:16,126
prior belief, right? So what's the prior belief or the prior distribution for the

64
00:04:16,148 --> 00:04:19,946
unknown parameter likelihood represents

65
00:04:20,058 --> 00:04:23,662
the information. The new information represents

66
00:04:23,726 --> 00:04:27,006
our updated belief about this unknown parameter,

67
00:04:27,118 --> 00:04:31,090
which incorporates both prior knowledge and observed evidence.

68
00:04:31,990 --> 00:04:36,466
The term in denominator marginal likelihood is more of a normalizing

69
00:04:36,498 --> 00:04:40,630
constant, making sure that posterior also represents a probability distribution.

70
00:04:41,370 --> 00:04:45,286
Now let's look at how inference happens with

71
00:04:45,388 --> 00:04:48,946
bayesian versus non bayesian models. So we'll start with non

72
00:04:48,978 --> 00:04:52,934
bayesian inference, and then we'll go to Bayesian inference. So in case of Bayesian

73
00:04:52,982 --> 00:04:56,682
inference, what we do is we determine the value of

74
00:04:56,736 --> 00:05:01,070
unknown a point estimate of the unknown parameter which

75
00:05:01,140 --> 00:05:04,800
maximizes the likelihood of data. So likelihood is

76
00:05:05,330 --> 00:05:09,162
given the unknown parameter. So we defined the parameter

77
00:05:09,226 --> 00:05:13,930
which maximizes the likelihood of evidence, and it comes as a single point estimate.

78
00:05:14,090 --> 00:05:17,314
And for a new instance, we predict only using that

79
00:05:17,352 --> 00:05:21,406
point estimate. While in case of Bayesian inference,

80
00:05:21,598 --> 00:05:25,370
we start with our prior belief about this parameter, about this unknown parameter,

81
00:05:25,470 --> 00:05:29,298
which here is represented as p theta, and then we compute

82
00:05:29,474 --> 00:05:32,786
posterior distribution, which is p theta given evidence. So it's

83
00:05:32,818 --> 00:05:36,930
an updated distribution about the unknown parameter

84
00:05:37,090 --> 00:05:40,214
given our prior, starting from our prior

85
00:05:40,262 --> 00:05:44,666
and given the new data set. So now for

86
00:05:44,688 --> 00:05:48,826
a new instance, you compute the probability of

87
00:05:48,848 --> 00:05:52,574
the new instance considering the entire posterior distribution rather

88
00:05:52,612 --> 00:05:54,910
than a single point estimate.

89
00:05:55,250 --> 00:05:59,354
So this simple implementation

90
00:05:59,402 --> 00:06:02,750
is a lot more complex. In practice,

91
00:06:03,510 --> 00:06:06,846
the integral here tends to be interactable,

92
00:06:06,958 --> 00:06:10,690
especially when we work with higher on a higher

93
00:06:10,760 --> 00:06:12,420
dimension parameter space.

94
00:06:14,790 --> 00:06:18,894
There's no closed form solution to get this posterior distribution.

95
00:06:19,022 --> 00:06:22,918
So what do we do in that scenario? Right? So if we can't get a

96
00:06:22,924 --> 00:06:26,834
closed form solution, can we get samples from the posterior distribution?

97
00:06:26,882 --> 00:06:31,074
Right. So if we are able to sample from this posterior distribution, we effectively

98
00:06:31,202 --> 00:06:34,130
have its posterior distribution.

99
00:06:34,210 --> 00:06:37,602
So the whole idea is if we can sample from this posterior,

100
00:06:37,746 --> 00:06:42,574
and then we can use that samples to get

101
00:06:42,612 --> 00:06:46,990
inference for a new instance along with the associated,

102
00:06:48,210 --> 00:06:51,854
as we touched earlier, p of y,

103
00:06:51,892 --> 00:06:53,570
which is the normalizing constant.

104
00:06:56,390 --> 00:07:00,254
Normalizing constant, which involves integrals, is generally

105
00:07:00,302 --> 00:07:07,522
not interactable, and then

106
00:07:07,576 --> 00:07:09,830
we don't really have a closed form solution.

107
00:07:10,570 --> 00:07:13,794
Numerical integration techniques also tend to be too computationally

108
00:07:13,842 --> 00:07:17,678
intensive here. How do we sample from here? Right. How do we sample the posterior?

109
00:07:17,714 --> 00:07:21,660
So for this, we rely on a special class of

110
00:07:22,350 --> 00:07:27,770
algorithms called Markov chain Monte Carlo methods,

111
00:07:28,350 --> 00:07:31,510
through which we are able to sample from a probability distribution.

112
00:07:31,590 --> 00:07:35,166
So if we're able to construct a Markov chain that has the

113
00:07:35,188 --> 00:07:39,546
desired distribution as its equilibrium distribution, one can obtain

114
00:07:39,738 --> 00:07:43,220
samples for the desired distribution by recording states

115
00:07:45,350 --> 00:07:50,238
from this Markov chain. The different MCMC samplers

116
00:07:50,254 --> 00:07:53,460
here. So you have metropoliscape sampling and so on,

117
00:07:54,310 --> 00:07:57,906
which can help you which can help generate samples

118
00:07:57,938 --> 00:08:03,062
from this distribution. So now going

119
00:08:03,116 --> 00:08:07,282
back and explaining what is probabilistic programming or probabilistic modeling.

120
00:08:07,346 --> 00:08:10,854
So probabilistic programming is merely a programming framework for bayesian

121
00:08:10,902 --> 00:08:15,686
statistics. It inherently uncertainty

122
00:08:15,718 --> 00:08:19,126
within its parameters. So it tends to thrive in a world of uncertainty.

123
00:08:19,318 --> 00:08:24,142
And as you define your prior beliefs, you built

124
00:08:24,196 --> 00:08:28,058
in your model, your prior beliefs or the expert domain knowledge. So it tends

125
00:08:28,074 --> 00:08:30,894
to work well with little data as well.

126
00:08:31,092 --> 00:08:32,670
And it can be updated.

127
00:08:35,350 --> 00:08:39,874
Your distribution can be updated as you get more and more new

128
00:08:39,912 --> 00:08:44,590
information. And the whole model architecture offers transparency

129
00:08:44,670 --> 00:08:46,630
and more explainable models.

130
00:08:47,290 --> 00:08:50,850
Now a bit about workflow of probabilistic programming.

131
00:08:50,930 --> 00:08:54,870
So the first step is we identify all the unknown parameters.

132
00:08:55,770 --> 00:08:59,206
We define the prior distribution, and while defining the prior

133
00:08:59,238 --> 00:09:02,694
distribution, we encode our prior belief or we encode

134
00:09:02,742 --> 00:09:06,182
our expert knowledge, expert domain knowledge

135
00:09:06,326 --> 00:09:09,706
about the model parameters. Then we specify the

136
00:09:09,728 --> 00:09:13,006
likelihood, which is the probability distribution of observed data

137
00:09:13,108 --> 00:09:17,194
as a function of unknown quantities. And then we can run a suitable

138
00:09:17,242 --> 00:09:20,254
MCMC sampler to get

139
00:09:20,292 --> 00:09:23,390
the posterior distribution for all of these unknown parameters.

140
00:09:24,450 --> 00:09:28,162
And now for any new instance, now we have, instead of point

141
00:09:28,216 --> 00:09:31,826
estimates, we have a distribution, we have the entire distribution for the

142
00:09:31,848 --> 00:09:35,494
unknown parameters, and we can utilize that distribution to

143
00:09:35,532 --> 00:09:38,902
compute the estimate along with this

144
00:09:38,956 --> 00:09:41,670
uncertainty for a new instance.

145
00:09:42,650 --> 00:09:46,630
So now a quick demo on implementing

146
00:09:47,470 --> 00:09:50,330
bayesian models or probabilistic models in python.

147
00:09:51,790 --> 00:09:55,260
For my demo, I'm using a data set

148
00:09:55,950 --> 00:10:00,986
in which from

149
00:10:01,008 --> 00:10:04,234
a sample of population, I have height, weight and gender.

150
00:10:04,362 --> 00:10:09,626
So gender here is in binary form whether the candidate

151
00:10:09,658 --> 00:10:12,834
is female or not, and then you have the height and weight of that

152
00:10:12,872 --> 00:10:17,026
candidate. So in

153
00:10:17,048 --> 00:10:20,466
a non probabilistic world, we will try

154
00:10:20,488 --> 00:10:24,126
to fit a logistic regression model here. And for bayesian

155
00:10:24,158 --> 00:10:27,574
model I'll be using. So we

156
00:10:27,612 --> 00:10:30,902
start with a simple logistic model with your

157
00:10:31,036 --> 00:10:34,482
is female flag as the target,

158
00:10:34,626 --> 00:10:37,510
and then we try to find the coefficients of height and weight,

159
00:10:37,950 --> 00:10:42,538
which best fit our problem.

160
00:10:42,624 --> 00:10:46,726
Right? So in this case, you run a linear model, a logistic regression

161
00:10:46,758 --> 00:10:50,800
model, and then you get coefficients of that logistic regression model.

162
00:10:51,170 --> 00:10:54,814
And again, you're only getting point estimates, you are not getting

163
00:10:54,852 --> 00:10:58,350
an estimate that what is the range of this parameter

164
00:11:01,570 --> 00:11:06,562
and what's the underlying uncertainty? Associated model

165
00:11:06,616 --> 00:11:10,226
coefficients, right? So the next thing I'm going to do is move

166
00:11:10,248 --> 00:11:13,666
on to running a bayesian model. So for bayesian model, as we

167
00:11:13,688 --> 00:11:17,586
discussed earlier, we start with defining the unknown parameters.

168
00:11:17,698 --> 00:11:21,446
We define the prior distribution, we define likelihood, and then

169
00:11:21,468 --> 00:11:25,174
we run an MCMC sampler. So, Stan, it's its

170
00:11:25,212 --> 00:11:28,666
own language. First thing I have done is

171
00:11:28,688 --> 00:11:32,986
I have built a Stan model. So I'll just give a

172
00:11:33,088 --> 00:11:36,374
quick glimpse of that model. So Stan

173
00:11:36,422 --> 00:11:39,926
has a couple of modules. So you have your data

174
00:11:40,048 --> 00:11:43,870
transform data parameters, transform parameters model and generated quantities.

175
00:11:44,610 --> 00:11:48,238
Since that simple model, I'm just models here.

176
00:11:48,324 --> 00:11:51,598
So data is where you define the structure of your data,

177
00:11:51,684 --> 00:11:55,026
data types, parameters where

178
00:11:55,048 --> 00:11:58,866
you define the data types of

179
00:11:58,888 --> 00:12:02,962
your unknown parameters. And then we come to the moderate part. In the moderate part,

180
00:12:03,096 --> 00:12:06,082
the first thing I do is I start with my priors.

181
00:12:06,226 --> 00:12:09,830
So what are the prior beliefs? I hold about

182
00:12:09,980 --> 00:12:12,710
the three coefficients, your interceptor,

183
00:12:13,690 --> 00:12:17,126
your coefficient for weight, and your coefficient for height. And then here is

184
00:12:17,148 --> 00:12:19,210
the part where I have defined likelihood,

185
00:12:20,670 --> 00:12:24,170
right? And again, the target metric here is binary

186
00:12:24,830 --> 00:12:29,094
Bernali. So we fit a Bernoulli logic Bernoulli likelihood

187
00:12:29,142 --> 00:12:32,560
here. That's it, that's how you define your stand model.

188
00:12:33,890 --> 00:12:36,990
Then I can take this model into Python,

189
00:12:37,650 --> 00:12:41,134
run the compiler for this model. The data

190
00:12:41,172 --> 00:12:44,094
which needs to be fed to stan needs to be in form of a dictionary.

191
00:12:44,142 --> 00:12:47,394
So this data is just being transformed. We run

192
00:12:47,432 --> 00:12:51,202
the MCMC sampler, and then you get a

193
00:12:51,256 --> 00:12:55,506
series of estimates or samples for

194
00:12:55,528 --> 00:12:58,580
each of the unknown parameters. And then you can use those,

195
00:12:59,690 --> 00:13:03,574
you can look at the mean median models and

196
00:13:03,612 --> 00:13:06,850
other centrality measures for your parameters.

197
00:13:06,930 --> 00:13:10,182
And along with that you can also understand, okay, what's the standard deviation,

198
00:13:10,326 --> 00:13:13,434
what's the range? So this gives you a lot more information about

199
00:13:13,472 --> 00:13:17,686
your coefficients rather than giving you point estimates, right? And then subsequently,

200
00:13:17,878 --> 00:13:21,882
as I perform predictions, instead of considering

201
00:13:21,946 --> 00:13:26,510
just a point estimate, I can consider the entire distribution of my parameters,

202
00:13:27,010 --> 00:13:30,686
unknown quantities, right? Or my coefficients here,

203
00:13:30,868 --> 00:13:34,618
that can help us get a better prediction

204
00:13:34,714 --> 00:13:38,366
and along with that get the associated uncertainty with

205
00:13:38,388 --> 00:13:41,840
the prediction as well. So that's about it.

206
00:13:42,290 --> 00:13:45,794
If you go onto Stan website, it has a lot of detailed documentation

207
00:13:45,842 --> 00:13:50,294
and you can find out how you can build more complex stand

208
00:13:50,332 --> 00:13:54,550
models as well. So that's about it from, in terms of my presentation.

209
00:13:55,130 --> 00:13:55,700
Thank you everyone.

