1
00:00:20,490 --> 00:00:25,070
Hello, my name is Duncan Blythe. I'm the founder and CTO of Superduperdb.

2
00:00:25,530 --> 00:00:29,394
In this talk, I'll present to you superduperdb, our vision

3
00:00:29,442 --> 00:00:32,742
and mission, the way we work, the way

4
00:00:32,796 --> 00:00:36,566
the technology works, Python snippets how

5
00:00:36,588 --> 00:00:40,540
to get started with superduperdb and a short demo.

6
00:00:41,310 --> 00:00:45,078
So this quotation you see on the screen perfectly describes

7
00:00:45,174 --> 00:00:48,902
what we're aiming to do, and that is to transform

8
00:00:48,966 --> 00:00:52,926
the simplicity and ease with which developers can

9
00:00:52,948 --> 00:00:56,400
get started using AI together with their data.

10
00:00:57,330 --> 00:01:01,034
So a fundamental problem is that data and AI live in separate

11
00:01:01,082 --> 00:01:05,310
silos. Current state of the art methods

12
00:01:05,470 --> 00:01:09,442
for productionizing AI require a lot of data

13
00:01:09,496 --> 00:01:13,250
migration into complex pipelines and infrastructure.

14
00:01:13,910 --> 00:01:17,910
That means maintaining duplicated data in multiple locations

15
00:01:18,570 --> 00:01:22,738
and also various steps and tools, including specialized

16
00:01:22,834 --> 00:01:26,534
vector databases. So as

17
00:01:26,572 --> 00:01:30,122
a result, bringing machine learning to production and AI is

18
00:01:30,176 --> 00:01:33,754
very complex. What often happens is that

19
00:01:33,792 --> 00:01:37,770
deployments take on a character similar to this

20
00:01:37,840 --> 00:01:42,170
depiction here, where data databases

21
00:01:42,330 --> 00:01:45,770
are the initial input nodes to a complex graph

22
00:01:45,930 --> 00:01:49,550
of deployments, tools, steps and processes.

23
00:01:50,690 --> 00:01:54,514
And the current trend of including vector databases in

24
00:01:54,552 --> 00:01:58,580
this setup only make things worse. In 2024,

25
00:01:59,430 --> 00:02:03,138
AI, in order to be simple to use,

26
00:02:03,224 --> 00:02:06,882
needs to come into contact with data, and our

27
00:02:06,936 --> 00:02:10,790
thesis at super Duper DB is this can be greatly simplified

28
00:02:11,130 --> 00:02:14,834
if we can provide a unified data and AI

29
00:02:14,882 --> 00:02:19,190
environment where no duplication and migration

30
00:02:19,530 --> 00:02:23,686
or etl transformations, mlops, pipelines and infrastructure

31
00:02:23,718 --> 00:02:27,574
is necessary. So one environment combining

32
00:02:27,622 --> 00:02:30,700
AI and data as well as vector search.

33
00:02:31,490 --> 00:02:35,082
So we do that by bringing your AI to your database

34
00:02:35,146 --> 00:02:38,810
data deployment. So it's the environment

35
00:02:38,970 --> 00:02:42,566
in which data and AI are unified and that greatly

36
00:02:42,618 --> 00:02:46,574
simplifies the process of AI development and adoption

37
00:02:46,702 --> 00:02:50,414
and allows you to unlock the full potential of your existing

38
00:02:50,462 --> 00:02:54,046
data. With super Duper DB,

39
00:02:54,158 --> 00:02:57,670
you're able to build AI without moving data.

40
00:02:57,820 --> 00:03:01,270
And when I say building AI, I really mean the current

41
00:03:01,340 --> 00:03:04,962
state of the art AI. So generative AI, including llms

42
00:03:05,026 --> 00:03:09,174
and so forth, also standard machine learning use cases

43
00:03:09,302 --> 00:03:13,130
as well as custom workflows which consist of

44
00:03:13,200 --> 00:03:15,500
combinations of these things.

45
00:03:17,870 --> 00:03:21,514
SuperduperDB aims to become the centerpiece of the modern

46
00:03:21,642 --> 00:03:25,166
data centric AI stack. So this is what

47
00:03:25,188 --> 00:03:28,720
it looks like. On the left we have

48
00:03:29,570 --> 00:03:33,410
data, so databases, data warehouses,

49
00:03:35,110 --> 00:03:38,578
your data. We would like to connect

50
00:03:38,664 --> 00:03:42,190
this with AI vector search, and indeed

51
00:03:42,270 --> 00:03:46,406
the Python ecosystem. And this is currently not possible.

52
00:03:46,508 --> 00:03:50,546
With full generality, with SuperduperDB,

53
00:03:50,738 --> 00:03:54,690
this is possible. So SuperduperDB acts as a centerpiece,

54
00:03:54,850 --> 00:03:58,598
orchestrating and connecting these diverse components.

55
00:03:58,694 --> 00:04:02,394
And when I say this, I really mean that you can bring any piece

56
00:04:02,432 --> 00:04:08,086
of code from the open source ecosystem of Python libraries

57
00:04:08,278 --> 00:04:12,254
and integrate vector search completely

58
00:04:12,372 --> 00:04:15,838
flexibly. It's an all in one platform

59
00:04:15,924 --> 00:04:19,070
for all data centric AI use cases.

60
00:04:19,490 --> 00:04:22,978
So we have a deployment which allows for

61
00:04:23,064 --> 00:04:27,502
inference and scalable model training. You can develop models

62
00:04:27,566 --> 00:04:31,602
in combination with the platform, putting together

63
00:04:31,656 --> 00:04:35,314
very complex workflows, and you can also use the platform for

64
00:04:35,352 --> 00:04:39,278
search, navigation and analytics. And that also includes

65
00:04:39,454 --> 00:04:43,270
modern document Q and a systems.

66
00:04:43,770 --> 00:04:47,538
So it's built for developers with the ecosystem mind and we

67
00:04:47,564 --> 00:04:51,094
can allow you to integrate any AI

68
00:04:51,142 --> 00:04:55,158
models and AI APIs directly with your database.

69
00:04:55,334 --> 00:04:59,430
And so we can leverage the full power of the open source

70
00:04:59,510 --> 00:05:03,630
ecosystem for AI and Python, and that is substantial.

71
00:05:04,850 --> 00:05:08,830
We're open source licensed according to Apache two

72
00:05:08,900 --> 00:05:12,522
on GitHub. Please take a look like

73
00:05:12,676 --> 00:05:15,810
subscribe, contribute and that's very important.

74
00:05:15,880 --> 00:05:19,250
We are very keen to get contributors on board,

75
00:05:19,400 --> 00:05:23,038
improving the quality and the features

76
00:05:23,054 --> 00:05:26,662
in the project. So how does it work?

77
00:05:26,796 --> 00:05:31,254
So SuperduperDB acts in

78
00:05:31,292 --> 00:05:35,846
combination with your databases data

79
00:05:35,948 --> 00:05:39,818
deployment and what happens is you can install

80
00:05:39,904 --> 00:05:43,242
models and AI APIs and configure these

81
00:05:43,376 --> 00:05:47,802
to either form inference on your data as

82
00:05:47,856 --> 00:05:51,760
data comes in, or indeed fine tune themselves

83
00:05:52,130 --> 00:05:56,126
on your data. And developers can interact with the system from

84
00:05:56,228 --> 00:05:59,466
Jupyter notebooks, Python scripts,

85
00:05:59,578 --> 00:06:03,566
as well as we're working on other sdks.

86
00:06:03,758 --> 00:06:07,060
So this will come in hopefully the upcoming months.

87
00:06:07,910 --> 00:06:11,394
And we also are working on rest APIs so that you can

88
00:06:11,432 --> 00:06:15,410
easily integrate this with your applications downstream.

89
00:06:16,170 --> 00:06:19,942
You can also easily interact with this

90
00:06:19,996 --> 00:06:22,630
since we're Python first with fast APIs.

91
00:06:23,290 --> 00:06:26,854
So now let's get a little more technical. So what does

92
00:06:26,892 --> 00:06:30,998
the underlying architecture look like of a SuperduperDB system?

93
00:06:31,164 --> 00:06:35,146
So you'll see later that SuperduperDB is a Python package, but at

94
00:06:35,168 --> 00:06:38,860
the same time it's a deployment system.

95
00:06:40,610 --> 00:06:44,266
In order for your Python package to operate,

96
00:06:44,378 --> 00:06:48,830
it's interacting with the components you see on this diagram.

97
00:06:49,170 --> 00:06:52,926
So are various components. So the

98
00:06:52,948 --> 00:06:56,562
principal component is the data backend which corresponds to your

99
00:06:56,696 --> 00:07:00,306
traditional database. And we also have

100
00:07:00,328 --> 00:07:03,762
a metadata store and artifact store.

101
00:07:03,816 --> 00:07:07,638
So these are for saving information about models and actually saving

102
00:07:07,724 --> 00:07:11,078
model data. So these

103
00:07:11,244 --> 00:07:14,850
three things here together get wrapped in this DB variable

104
00:07:14,930 --> 00:07:17,910
that you'll see in the subsequent code snippets.

105
00:07:18,670 --> 00:07:23,478
Work is carried out by a scalable slave

106
00:07:23,574 --> 00:07:27,290
master slave scheduler. So we're using

107
00:07:27,360 --> 00:07:30,960
ray to do this and

108
00:07:32,450 --> 00:07:36,362
work is submitted to this system via

109
00:07:36,506 --> 00:07:40,942
either directly from the

110
00:07:40,996 --> 00:07:44,990
developer requests or from

111
00:07:45,140 --> 00:07:49,460
a change data capture demon which is listening for incoming data.

112
00:07:49,830 --> 00:07:53,954
And you can also set up a vector search component and

113
00:07:54,072 --> 00:07:57,122
that interacts with the query

114
00:07:57,186 --> 00:08:00,614
API. So when you select data, you can optionally link

115
00:08:00,732 --> 00:08:04,310
the query with the vector search

116
00:08:04,380 --> 00:08:08,434
component. So that's still a very high level

117
00:08:08,492 --> 00:08:12,426
view of what's going on. Suffice to

118
00:08:12,448 --> 00:08:16,186
say you can read more about this on our docs and get

119
00:08:16,368 --> 00:08:20,250
into as much detail as you like by exploring our examples

120
00:08:20,690 --> 00:08:24,206
and exploring the code base. So let's have a look

121
00:08:24,228 --> 00:08:28,074
at the code. To connect to superduperDB, you simply

122
00:08:28,122 --> 00:08:32,350
wrap your standard database URI with our wrapper

123
00:08:32,770 --> 00:08:36,450
and you get an object DB which you can do many of the standard

124
00:08:36,520 --> 00:08:40,334
things you would do with a database client, but much much more. So that's

125
00:08:40,382 --> 00:08:44,546
sort of the super duper of your database in

126
00:08:44,568 --> 00:08:47,240
order to query your system.

127
00:08:47,690 --> 00:08:51,110
It's very similar to doing a standard database query.

128
00:08:51,850 --> 00:08:55,414
What you do is you simply connect with the backend of your

129
00:08:55,452 --> 00:08:59,094
choice, so in this case MongDB and you execute

130
00:08:59,142 --> 00:09:03,130
your query object. So the query object is here between brackets.

131
00:09:03,470 --> 00:09:07,286
And this is completely analogous to a standard Pymongo

132
00:09:07,318 --> 00:09:11,006
query for instance. And SQL is a bit more

133
00:09:11,108 --> 00:09:15,838
involved because you need to set up a schema first and

134
00:09:15,924 --> 00:09:19,390
add a table to the system. But after you have this table,

135
00:09:19,810 --> 00:09:24,206
we can perform SQL queries via the Ibis

136
00:09:24,398 --> 00:09:28,366
library. You can create custom data types,

137
00:09:28,478 --> 00:09:31,826
so that allows you to do much more than you do than you

138
00:09:31,848 --> 00:09:35,086
would do with a standard database. For instance here I'm creating

139
00:09:35,118 --> 00:09:38,902
an MP3 data type and the way this works

140
00:09:38,956 --> 00:09:42,982
is that you have this encoder object and you tell

141
00:09:43,116 --> 00:09:47,010
your encoder how it should handle the bytes

142
00:09:47,090 --> 00:09:50,460
from a data point. So here we're just doing a

143
00:09:52,510 --> 00:09:55,446
simple encoding via pickle,

144
00:09:55,558 --> 00:09:59,794
but you can do whatever you like. And that's a theme in superduperdb.

145
00:09:59,942 --> 00:10:03,646
You can define whatever functionality you need via our

146
00:10:03,748 --> 00:10:07,258
system of wrappers and encoders

147
00:10:07,434 --> 00:10:10,926
and connectors. So creating a

148
00:10:10,948 --> 00:10:14,640
model is very flexible. So here

149
00:10:15,330 --> 00:10:19,534
for instance, a very simple model just involving a regular expression.

150
00:10:19,662 --> 00:10:23,122
So we have to think of models in super duper DB. It's not just

151
00:10:23,256 --> 00:10:26,646
a Pytorch model or a hugging face model, but really a

152
00:10:26,668 --> 00:10:30,882
generalized computation. And this computation

153
00:10:30,946 --> 00:10:34,614
can either have auxiliary data, can be trainable or

154
00:10:34,652 --> 00:10:38,378
not, but that's the sort of sense of

155
00:10:38,544 --> 00:10:41,894
the whole project is to link different types of computation

156
00:10:41,942 --> 00:10:45,450
together, which might or might not involve traditional

157
00:10:46,430 --> 00:10:50,194
AI models. So here for instance,

158
00:10:50,342 --> 00:10:53,662
we import the object model wrapper, give it a name,

159
00:10:53,716 --> 00:10:57,162
my extractor, and we passing

160
00:10:57,226 --> 00:11:01,006
as the heavy lifting component of

161
00:11:01,028 --> 00:11:04,398
this model a mapping

162
00:11:04,494 --> 00:11:09,490
which extracts from an input string URLs.

163
00:11:10,070 --> 00:11:13,342
So you can go much more involved than this. For instance,

164
00:11:13,406 --> 00:11:17,222
here we are using Spacey to

165
00:11:17,276 --> 00:11:21,602
pass text and doing essentially named

166
00:11:21,666 --> 00:11:24,280
entity recognition on this text.

167
00:11:24,890 --> 00:11:28,430
SuperduperDb handles saving these diverse

168
00:11:28,450 --> 00:11:31,914
bits of data and code into the

169
00:11:31,952 --> 00:11:35,562
system. So you're now able to use spacey to do

170
00:11:35,616 --> 00:11:39,030
parsing. And the cool thing about this is there's no necessity

171
00:11:39,110 --> 00:11:43,078
for us as superduperdb maintainers

172
00:11:43,094 --> 00:11:46,362
to have already made this spacey integration.

173
00:11:46,426 --> 00:11:50,490
You can really just bring it to your super Duperdb

174
00:11:50,570 --> 00:11:54,302
deployment and it can go even deeper than this. For instance,

175
00:11:54,366 --> 00:12:00,190
you could do something like a custom APIs request handling

176
00:12:00,270 --> 00:12:03,682
logic of how exactly individual data

177
00:12:03,736 --> 00:12:06,886
points. So this would be this predict function

178
00:12:07,068 --> 00:12:10,200
or multiple data points are handled by your model.

179
00:12:10,730 --> 00:12:13,970
So completely versatile,

180
00:12:14,130 --> 00:12:17,850
completely flexible, you'll see through and through that. We use

181
00:12:17,920 --> 00:12:19,100
the data class,

182
00:12:20,750 --> 00:12:24,186
data class decorator around our classes and the reason for

183
00:12:24,208 --> 00:12:27,734
this is this creates a very nice way to expose

184
00:12:27,782 --> 00:12:31,506
these models then to rest API

185
00:12:31,558 --> 00:12:34,906
functionality. And so then you can nicely build front ends

186
00:12:34,938 --> 00:12:38,734
on top of this. So applying a model to data

187
00:12:38,772 --> 00:12:42,830
in a database is simple via the predict in DB.

188
00:12:43,590 --> 00:12:47,422
So you simply say which key you would like to operate

189
00:12:47,486 --> 00:12:51,140
over and also what data you would like to select.

190
00:12:51,750 --> 00:12:55,290
And superduperDb will then under the hood,

191
00:12:55,390 --> 00:12:58,934
load the data, efficiently pass that

192
00:12:58,972 --> 00:13:02,274
data through your model and save the outputs

193
00:13:02,322 --> 00:13:05,702
back in the database. And this can actually

194
00:13:05,756 --> 00:13:09,146
be done in a sort of asynchronous streaming fashion where you

195
00:13:09,168 --> 00:13:12,934
don't need to necessarily even be activating

196
00:13:12,982 --> 00:13:17,542
the model yourself. So this will actually start essentially

197
00:13:17,606 --> 00:13:20,970
getting a life of its own via the listener wrapper.

198
00:13:21,330 --> 00:13:24,970
So you would wrap your model with a listener

199
00:13:25,050 --> 00:13:28,750
and tell the listener to listen to a certain query.

200
00:13:29,170 --> 00:13:33,314
And that means that the system will then listen for incoming data

201
00:13:33,512 --> 00:13:36,914
on this query, and when it comes in, it will

202
00:13:36,952 --> 00:13:40,062
apply that model to that data and populate

203
00:13:40,126 --> 00:13:42,980
the database with outputs over that data.

204
00:13:43,670 --> 00:13:46,946
And a vector index operates together with this listener

205
00:13:46,978 --> 00:13:50,582
component. A vector index in itself needs

206
00:13:50,636 --> 00:13:54,454
to be always up to date, so that's why it operates together with

207
00:13:54,492 --> 00:13:58,314
listener. So you wrap a listener with

208
00:13:58,352 --> 00:14:02,262
a vector index and you instantly make that data underneath

209
00:14:02,326 --> 00:14:06,330
this select query searchable.

210
00:14:09,890 --> 00:14:14,074
Creating more complex functionality where multiple

211
00:14:14,122 --> 00:14:17,882
models interact happens via the stack API.

212
00:14:18,026 --> 00:14:22,240
So you can simply list the components you would like to

213
00:14:22,870 --> 00:14:26,334
like to add to your stack. And as before, add the stack

214
00:14:26,382 --> 00:14:30,146
to the system and

215
00:14:30,248 --> 00:14:34,082
you can even parameterize these stacks in order to make a higher

216
00:14:34,136 --> 00:14:36,914
level interface to your AI functionality.

217
00:14:37,042 --> 00:14:40,694
So what you would do then is essentially perform surgery on your

218
00:14:40,732 --> 00:14:44,514
stack, replacing certain variables

219
00:14:44,562 --> 00:14:48,150
or certain values with variables

220
00:14:48,750 --> 00:14:52,458
become available as parameters in the higher level

221
00:14:52,624 --> 00:14:56,058
app API. So now this app has

222
00:14:56,144 --> 00:15:00,590
two free variables identify and select from

223
00:15:00,740 --> 00:15:06,094
this app. I can very easily share

224
00:15:06,292 --> 00:15:10,526
high level AI functionality. And how do I share it?

225
00:15:10,708 --> 00:15:14,642
Simply export and SuperduperDb will then

226
00:15:14,696 --> 00:15:18,146
inspect the model or models that

227
00:15:18,168 --> 00:15:21,822
you've created and create a very nice JSoN serialized format

228
00:15:21,886 --> 00:15:25,606
with references to artifacts. So we're now

229
00:15:25,628 --> 00:15:29,794
going to see a demo of this system. So imagine

230
00:15:29,842 --> 00:15:33,240
you have a library of video recordings or video,

231
00:15:33,850 --> 00:15:37,882
and you would like to search this video using

232
00:15:37,936 --> 00:15:41,402
natural language for important scenes. Or search

233
00:15:41,456 --> 00:15:45,094
these videos and these could potentially be sensitive recordings,

234
00:15:45,142 --> 00:15:48,570
so you don't necessarily want to send requests off to

235
00:15:48,640 --> 00:15:52,766
externally hosted APIs. So in this

236
00:15:52,948 --> 00:15:56,394
case, what we can do with superduperdb

237
00:15:56,522 --> 00:15:59,994
with very few Python commands is simply

238
00:16:00,122 --> 00:16:03,086
add the videos to the database,

239
00:16:03,198 --> 00:16:07,170
specifying only the uris of where the videos are located.

240
00:16:07,830 --> 00:16:11,886
We can create our own custom model to extract and subsample

241
00:16:11,918 --> 00:16:15,910
video frames for them from the video, vectorize these

242
00:16:15,980 --> 00:16:20,070
frames using computer vision models via Pytorch,

243
00:16:20,570 --> 00:16:24,134
and then once this is set up, we're ready to search through these

244
00:16:24,172 --> 00:16:27,586
frames and return answers to queries such as

245
00:16:27,628 --> 00:16:31,754
this. Show me scenes where the main actor throws a ball as

246
00:16:31,792 --> 00:16:35,318
just a simple example, and we'll get results

247
00:16:35,494 --> 00:16:39,766
in the form of references to places

248
00:16:39,798 --> 00:16:44,286
in the video where this may have happened. So this

249
00:16:44,308 --> 00:16:47,406
is simply just one example of what you can do with superduperdb. There are

250
00:16:47,428 --> 00:16:50,606
numerous examples which you can see on our website.

251
00:16:50,788 --> 00:16:54,306
Suffice to say that if you can think of

252
00:16:54,328 --> 00:16:57,890
it, you can probably do it. So let's start.

253
00:16:58,040 --> 00:17:01,902
So this is a Jupyter notebook and we're going to be interacting with Superduperdb

254
00:17:01,966 --> 00:17:03,410
from this notebook.

255
00:17:05,850 --> 00:17:10,390
Let's connect, we're connecting to MongoDB

256
00:17:11,290 --> 00:17:14,450
and we're going to use the collection

257
00:17:14,530 --> 00:17:17,800
videos to store the data about the videos.

258
00:17:18,430 --> 00:17:20,410
We have this DB connector.

259
00:17:21,150 --> 00:17:25,526
Let's create a special data type video on file

260
00:17:25,718 --> 00:17:29,340
which essentially tells us where our videos are.

261
00:17:29,970 --> 00:17:33,774
So you'll see here that we have a URL of

262
00:17:33,812 --> 00:17:37,360
a video. Let's have a look at what it is.

263
00:17:40,530 --> 00:17:44,606
It's a video of lots of different animals and we

264
00:17:44,628 --> 00:17:48,002
could potentially add multiple here, but we're just going to add this video

265
00:17:48,136 --> 00:17:51,650
to the database. So you'll see that adding a video

266
00:17:51,720 --> 00:17:55,410
from a public Uri, or could be an s three Uri,

267
00:17:56,570 --> 00:18:00,710
is a simple matter of simply inserting that uri

268
00:18:01,610 --> 00:18:05,090
under the hood. The system is actually downloading and caching

269
00:18:05,170 --> 00:18:09,610
the data so it can be used in computations.

270
00:18:10,350 --> 00:18:14,140
All of this happens automatically. You don't need to specify anything.

271
00:18:15,710 --> 00:18:19,386
So we can see here that we have a single document in

272
00:18:19,408 --> 00:18:23,450
the collection now which contains the reference to CRI

273
00:18:23,610 --> 00:18:27,150
and the data on file. So you can see that in more detail

274
00:18:27,220 --> 00:18:30,640
here. There is the cached video.

275
00:18:31,330 --> 00:18:35,470
So now I'm going to use the OpenCV

276
00:18:35,550 --> 00:18:39,106
library to create my own custom model

277
00:18:39,288 --> 00:18:43,582
which takes the data from this video and subsamples frames,

278
00:18:43,726 --> 00:18:46,390
saving those frames back in the database.

279
00:18:47,690 --> 00:18:51,414
So you'll see the logic here isn't important. Suffice to

280
00:18:51,452 --> 00:18:55,670
say. Suffice to say that I can create

281
00:18:55,740 --> 00:18:57,560
any custom logic I like.

282
00:19:00,430 --> 00:19:02,730
So now that model has been created,

283
00:19:03,390 --> 00:19:06,746
let's apply the model to the data in the

284
00:19:06,768 --> 00:19:10,554
database. So you'll see it's iterated through

285
00:19:10,592 --> 00:19:14,270
the frames. And now we've actually extracted one data

286
00:19:14,340 --> 00:19:17,774
point here and verified that a frame has

287
00:19:17,812 --> 00:19:21,454
been saved in that data. So you can see that in more

288
00:19:21,492 --> 00:19:25,390
detail here. If I take one document out of the outputs

289
00:19:25,550 --> 00:19:29,886
collection, you'll see there's actually a Python native image

290
00:19:29,918 --> 00:19:33,442
in there which we extract with

291
00:19:33,496 --> 00:19:37,174
this execute query. And now I

292
00:19:37,212 --> 00:19:41,042
would like to make those frames searchable.

293
00:19:41,186 --> 00:19:46,102
So we're going to use a pytorch model which

294
00:19:46,156 --> 00:19:49,698
is imported from the clip package of OpenAI.

295
00:19:49,874 --> 00:19:53,562
So this is a self hostable model and

296
00:19:53,696 --> 00:19:57,114
we're actually going to use two model components. So one

297
00:19:57,152 --> 00:20:00,380
for the visual part and one for the textual part.

298
00:20:01,070 --> 00:20:04,254
So now those models have been set up, let's wrap them

299
00:20:04,292 --> 00:20:07,962
with a vector index, and we're going to create a vector

300
00:20:08,026 --> 00:20:11,918
index which is essentially multimodal, so it has an indexing listener and a

301
00:20:11,924 --> 00:20:15,380
compatible listener. That means the models can,

302
00:20:16,630 --> 00:20:20,366
sorry. The images can be searched either with a textual

303
00:20:20,478 --> 00:20:22,850
interface or an image interface.

304
00:20:24,790 --> 00:20:28,834
So now the vector index has been set up and the images

305
00:20:28,882 --> 00:20:32,322
have been vectorized. Search through those frames.

306
00:20:32,466 --> 00:20:36,534
So let's look for for instance elephants in

307
00:20:36,572 --> 00:20:42,026
the woods. So this

308
00:20:42,128 --> 00:20:45,290
query here is searching the output collection

309
00:20:45,790 --> 00:20:49,286
using the search term referring

310
00:20:49,318 --> 00:20:52,686
to the index that we've created, and we're able

311
00:20:52,708 --> 00:20:56,922
to extract the timestamp from the results. These are simply Mongodbe

312
00:20:56,986 --> 00:20:59,870
documents returned in the results.

313
00:21:01,090 --> 00:21:05,154
And once you have that timestamp we can actually find

314
00:21:05,272 --> 00:21:08,180
the position in the original video.

315
00:21:08,790 --> 00:21:14,530
So let's confirm elephant.

316
00:21:15,850 --> 00:21:19,026
So we search for elephants in the woods.

317
00:21:19,058 --> 00:21:22,630
And now we have an elephant in the woods.

318
00:21:23,370 --> 00:21:27,202
Let's check this wasn't a fluke monkey

319
00:21:27,266 --> 00:21:28,330
is playing.

320
00:21:38,590 --> 00:21:41,550
So there you have it in very few python commands.

321
00:21:41,890 --> 00:21:44,810
Videos searchable, completely configurable,

322
00:21:44,970 --> 00:21:48,526
self hosting. Configure all steps of

323
00:21:48,548 --> 00:21:52,254
logic yourself via Python, but follow this template and

324
00:21:52,292 --> 00:21:55,070
save the results in superduperdb.

325
00:21:55,570 --> 00:21:59,166
Would you like to know more about superduperdb then

326
00:21:59,348 --> 00:22:03,418
find us on GitHub at superduperdb slash superduperdb.

327
00:22:03,594 --> 00:22:07,042
You can check out our document and example. Use cases

328
00:22:07,106 --> 00:22:10,822
at docs superduperdb.com and try

329
00:22:10,876 --> 00:22:14,978
out the code with a simple pip install. Pip install super duper

330
00:22:14,994 --> 00:22:17,010
db happy coding.

