1
00:00:20,650 --> 00:00:23,934
Hi and welcome to my talk. My name is Ivan Popov and

2
00:00:23,972 --> 00:00:27,206
today I will be talking about balancing speed and accuracy in

3
00:00:27,228 --> 00:00:29,830
model development. In the beginning,

4
00:00:30,730 --> 00:00:34,406
a couple of words about myself. I'm a data scientist at

5
00:00:34,428 --> 00:00:38,350
these boundary render. It's a fintech company based in London.

6
00:00:38,530 --> 00:00:42,710
I have three years of machine learning experience in fintech

7
00:00:42,790 --> 00:00:46,698
and computer vision sectors. I also have an extensive experience

8
00:00:46,784 --> 00:00:50,490
as a data engineer data analyst and I also completed project

9
00:00:50,560 --> 00:00:54,238
as a DevOps. Also earlier in my career I have

10
00:00:54,324 --> 00:00:57,850
created an online service and grew it to 80,000 users.

11
00:00:57,930 --> 00:01:01,294
So in today's talk, I will be talking about the

12
00:01:01,332 --> 00:01:04,466
two main factors when you develop a machine learning model,

13
00:01:04,648 --> 00:01:07,922
speed and accuracy, and how we can balance them.

14
00:01:08,056 --> 00:01:12,260
I will talk to you about how you can identify which

15
00:01:12,790 --> 00:01:16,582
things to focus on in your model and how

16
00:01:16,636 --> 00:01:20,098
to optimize your model when you have identified

17
00:01:20,274 --> 00:01:23,814
so when we talk about model performance, we usually think of

18
00:01:23,852 --> 00:01:27,266
model accuracy, how well it can make predictions.

19
00:01:27,458 --> 00:01:31,178
However, there is another angle, the speed with which the model

20
00:01:31,264 --> 00:01:35,146
creates, the prognosis. The main factors that impact the

21
00:01:35,168 --> 00:01:38,522
model accuracy and speed are the complexity of the model

22
00:01:38,576 --> 00:01:42,286
architecture, the amount and quality of input data,

23
00:01:42,388 --> 00:01:46,366
and the hardware. Please note that when I say

24
00:01:46,468 --> 00:01:50,138
accuracy, I don't just mean the percentage of the correct predictions.

25
00:01:50,314 --> 00:01:53,914
I use it as an umbrella term for all metrics

26
00:01:53,962 --> 00:01:57,710
such as f one score, rock, oak, iou, et cetera.

27
00:01:57,870 --> 00:02:01,870
In the ideal world, we want a model that has 100% accuracy

28
00:02:01,950 --> 00:02:05,038
and can return. These result in a nanosecond,

29
00:02:05,214 --> 00:02:08,502
and we have to balance accuracy and speed to achieve the best

30
00:02:08,556 --> 00:02:12,614
value for the business. In today's talk, I'm going to give real world

31
00:02:12,652 --> 00:02:16,018
examples of this balance and provide a step by step instruction

32
00:02:16,114 --> 00:02:19,574
how to identify your model's needs as well as the ways to optimize

33
00:02:19,622 --> 00:02:22,986
it. In some situations, speed is not

34
00:02:23,008 --> 00:02:26,922
the most important factor, for example, in academic research. In that

35
00:02:26,976 --> 00:02:31,182
case, the priority is finding state of the art model that

36
00:02:31,236 --> 00:02:34,800
can push the boundaries of science and advance the field of

37
00:02:35,250 --> 00:02:39,322
machine learning. However, when creating a model for commercial

38
00:02:39,386 --> 00:02:42,926
purposes, it is important to consider the experience of

39
00:02:42,948 --> 00:02:46,286
the end user and their satisfaction. In today's

40
00:02:46,318 --> 00:02:50,034
fast paced world, people have shorter attention spans and are not

41
00:02:50,072 --> 00:02:53,474
willing to wait for more than a few seconds for a page

42
00:02:53,512 --> 00:02:57,158
to load. So your model must be able to quickly return

43
00:02:57,244 --> 00:03:00,534
results to keep the customer engaged on your web page or in your

44
00:03:00,572 --> 00:03:04,454
app. Accuracy should not be compromised entirely for

45
00:03:04,492 --> 00:03:07,842
speed, as reliable and trustworthy predictions

46
00:03:07,906 --> 00:03:11,180
are essential to gain customers trust in your product or service.

47
00:03:11,710 --> 00:03:16,170
So let me provide you some real world examples so you can get a context

48
00:03:16,590 --> 00:03:20,106
in the loan industry, when a person looks for a loan on

49
00:03:20,128 --> 00:03:23,854
the aggregator website, the loan providers must return the quote within a few

50
00:03:23,892 --> 00:03:27,450
seconds, otherwise, their offer won't be shown. In this case, the speed

51
00:03:27,530 --> 00:03:30,990
is prioritized because this is usually not the final offer,

52
00:03:31,060 --> 00:03:34,738
and the underwriters can later review the case in more detail to make a

53
00:03:34,744 --> 00:03:38,110
final decision. But when it comes to ecommerce,

54
00:03:38,270 --> 00:03:41,646
instant recommendations require

55
00:03:41,678 --> 00:03:45,830
a stricter balance between speed and accuracy. A system that recommends products

56
00:03:45,900 --> 00:03:50,310
too slowly may cause customers to lose interest or seek recommendations elsewhere,

57
00:03:50,730 --> 00:03:54,294
while a system that recommends irrelevant products may result in

58
00:03:54,332 --> 00:03:57,000
poor customer experiences and lost sales.

59
00:03:57,470 --> 00:04:01,270
Imagine a heavy metal fan getting Taylor Swift tickets as a recommendation.

60
00:04:01,350 --> 00:04:04,540
That would be hilarious, but not for the ticket website.

61
00:04:06,830 --> 00:04:10,394
Medical diagnosis models are an excellent example where

62
00:04:10,432 --> 00:04:13,850
accuracy is more crucial than speed. Doctors usually spend

63
00:04:13,920 --> 00:04:17,546
considerable time examining and analyzing the outcomes

64
00:04:17,578 --> 00:04:20,894
before making a diagnosis. Therefore, the model can take

65
00:04:20,932 --> 00:04:24,846
more time to provide results as long as the accuracy is not compromised.

66
00:04:25,038 --> 00:04:28,606
As with many other things in life, the problem of balancing accuracy

67
00:04:28,638 --> 00:04:31,970
and speed can be solved with money. Investing in better

68
00:04:32,040 --> 00:04:35,538
hardware, such as cpus and gpus can improve these

69
00:04:35,544 --> 00:04:39,558
inference speed without sacrificing accuracy. However, it is important

70
00:04:39,644 --> 00:04:43,906
to carefully weigh the cost and benefit of each component before making a decision.

71
00:04:44,098 --> 00:04:47,526
Sometimes investing a large amount of money in hardware may

72
00:04:47,548 --> 00:04:51,766
only yield a small speed improvement. Additionally, as budgets

73
00:04:51,798 --> 00:04:55,718
are typically limited, there are only a few options for hardware upgrades.

74
00:04:55,894 --> 00:04:59,738
And again, like with many other things in life, not every problem can be

75
00:04:59,744 --> 00:05:03,258
solved with money. Upgrading hardware can certainly improve

76
00:05:03,274 --> 00:05:06,494
the performance of a model, but it won't fix issues that

77
00:05:06,532 --> 00:05:10,026
stem from poor data quality or feature selection.

78
00:05:10,218 --> 00:05:13,746
The accuracy of a model is heavily reliant on the quality of the

79
00:05:13,768 --> 00:05:17,634
data it's trained on. Furthermore, a model's architecture can also

80
00:05:17,672 --> 00:05:21,506
impact its accuracy and efficiency. If the architecture is

81
00:05:21,528 --> 00:05:25,922
too complex or simple, the model can suffer from overfitting or underfitting,

82
00:05:25,986 --> 00:05:29,346
respectively. This can result in slow inference times and poor

83
00:05:29,378 --> 00:05:32,786
accuracy, even with hive hardware. The choice

84
00:05:32,818 --> 00:05:36,630
of algorithm or learning method used can also

85
00:05:36,700 --> 00:05:39,842
impact a model's efficiency. Some algorithms

86
00:05:39,906 --> 00:05:43,466
may be inherently slow or may perform better only on certain types of

87
00:05:43,488 --> 00:05:47,622
data. For example, using a fully connected network for image segmentation

88
00:05:47,686 --> 00:05:50,934
may not be the best choice. It can be impractical

89
00:05:50,982 --> 00:05:54,726
due to the large number of parameters involved. In image,

90
00:05:54,758 --> 00:05:58,442
every pixel is a feature, and in a fully connected network, each neuron

91
00:05:58,506 --> 00:06:01,402
in one layer is connected to every neuron in the next layer,

92
00:06:01,466 --> 00:06:04,510
leading to a very high number of connections and parameters.

93
00:06:04,670 --> 00:06:07,774
This can result in a computationally expensive and memory

94
00:06:07,822 --> 00:06:11,438
intensive model, making it difficult to train and it will be prone

95
00:06:11,454 --> 00:06:15,374
to overfitting. So mastering the balance between

96
00:06:15,512 --> 00:06:19,202
model speed and accuracy can serve as a significant competitive

97
00:06:19,266 --> 00:06:23,202
advantage for your company. By determining which aspect

98
00:06:23,266 --> 00:06:26,454
is more crucial in your case and investing wisely in

99
00:06:26,492 --> 00:06:30,662
optimization technologies and techniques, you can fine tune your model to deliver

100
00:06:30,726 --> 00:06:34,090
the best output for the end user. This will give the business

101
00:06:34,160 --> 00:06:37,674
the flexibility to succeed in a fiercely competitive market.

102
00:06:37,792 --> 00:06:41,502
How to identify your model's needs you need to understand your business

103
00:06:41,556 --> 00:06:45,006
objectives first step in understanding how to optimize your

104
00:06:45,028 --> 00:06:48,170
model model performance with business goals

105
00:06:48,250 --> 00:06:52,254
and objectives so you need to answer the question, what is

106
00:06:52,292 --> 00:06:55,922
the purpose of the model? Is it for internal users or

107
00:06:56,056 --> 00:07:00,126
is it customer facing? What are the desired outcomes

108
00:07:00,158 --> 00:07:03,214
of the model? Is it to increase revenue, reduce costs,

109
00:07:03,262 --> 00:07:06,114
improve customer satisfaction, or something else?

110
00:07:06,312 --> 00:07:09,874
What are the key performance indicators, or KPIs,

111
00:07:10,002 --> 00:07:14,066
that the business is tracking? How does the model fit into those KPIs

112
00:07:14,258 --> 00:07:17,570
and who are the end users of the model? What are their

113
00:07:17,660 --> 00:07:21,354
expectations and needs? Let me give you two

114
00:07:21,392 --> 00:07:25,082
main scenarios for using ML models. The main ones

115
00:07:25,136 --> 00:07:28,726
are customer facing and internal customer facing

116
00:07:28,758 --> 00:07:32,542
applications. Hear speed is often more critical than

117
00:07:32,596 --> 00:07:36,254
accuracy. For example, in an ecommerce application,

118
00:07:36,372 --> 00:07:40,174
a recommendation engine that takes too long to recommend products can lead to

119
00:07:40,212 --> 00:07:43,470
customers losing interest and seeking recommendations elsewhere.

120
00:07:43,630 --> 00:07:47,566
Similarly for online chat bots that become more and more popular

121
00:07:47,678 --> 00:07:51,060
due to Chat GPT and similar.

122
00:07:51,670 --> 00:07:55,410
So for online chat bots, speed is critical as customers expect

123
00:07:55,480 --> 00:07:58,898
quick responses and don't want to wait too long for a chatbot

124
00:07:58,914 --> 00:08:02,038
to answer. For internal analytics, on the other hand,

125
00:08:02,124 --> 00:08:04,870
accuracy is often more critical than speed.

126
00:08:05,210 --> 00:08:08,842
Financial forecasting accuracy is crucial for making

127
00:08:08,896 --> 00:08:12,730
informed business decisions and in supply chain management,

128
00:08:13,150 --> 00:08:17,290
accurate predictions are necessary to optimize inventory management.

129
00:08:18,510 --> 00:08:22,126
In those scenarios, you can spend a longer time waiting for

130
00:08:22,148 --> 00:08:25,840
the result because the model can run overnight and

131
00:08:26,930 --> 00:08:30,240
you have a lot more time to get the correct answer.

132
00:08:31,490 --> 00:08:35,362
So let's go

133
00:08:35,416 --> 00:08:39,074
from more general things to the actual things

134
00:08:39,112 --> 00:08:42,462
you can do. First and foremost,

135
00:08:42,606 --> 00:08:46,342
get yourself a good data set with quality data and good labels. Of course,

136
00:08:46,396 --> 00:08:50,562
this mainly applies to supervised learning, but commercial models

137
00:08:50,626 --> 00:08:54,022
are usually supervised. The more data you get,

138
00:08:54,076 --> 00:08:56,600
the better, as long as you can ensure its quality.

139
00:08:57,050 --> 00:09:00,418
Let's say you're working on a model that classifies hundreds

140
00:09:00,434 --> 00:09:04,086
and digits. A good data set is a data set of handwritten

141
00:09:04,118 --> 00:09:07,734
digits that includes samples from multiple writers

142
00:09:07,782 --> 00:09:11,194
and different writing styles. It should also have a balanced distribution of

143
00:09:11,232 --> 00:09:14,526
digits, meaning that each digit occurs roughly the same number of

144
00:09:14,548 --> 00:09:18,094
times. All images have a clear label associated with these

145
00:09:18,292 --> 00:09:21,674
bed data set in this case would be the one that only includes

146
00:09:21,722 --> 00:09:25,602
handwritten digits from a single writer because these the model would be

147
00:09:25,656 --> 00:09:29,746
biased towards the writing style of that particular writer and would not be

148
00:09:29,768 --> 00:09:33,790
able to generalize well to other handwriting styles

149
00:09:33,950 --> 00:09:37,686
or if this data set was missing certain digits or labels for

150
00:09:37,708 --> 00:09:41,494
the images. It is always better to have a smaller good

151
00:09:41,532 --> 00:09:45,046
quality data set than a larger bad quality one because you

152
00:09:45,068 --> 00:09:48,406
can always use data augmentation to generate more data

153
00:09:48,588 --> 00:09:52,182
from data you already have. Good raw data alone

154
00:09:52,246 --> 00:09:56,006
is not enough to ensure a good model. The data needs to be processed

155
00:09:56,038 --> 00:10:00,102
to fit your model. This step includes data

156
00:10:00,176 --> 00:10:04,410
cleaning such as removing redundant data and null values, data normalization

157
00:10:04,490 --> 00:10:07,982
such as tokenization, stowboard removal, and embedding in

158
00:10:08,036 --> 00:10:11,530
NLP feature generation such as aggregations,

159
00:10:11,610 --> 00:10:15,246
onepot encoding, and finding trends like recurring transactions

160
00:10:15,278 --> 00:10:18,546
and financial data. Data preprocessing is

161
00:10:18,568 --> 00:10:21,874
a part of model development where a lot of code is

162
00:10:21,912 --> 00:10:25,418
written, and this is also one of the biggest sources

163
00:10:25,534 --> 00:10:29,574
of inefficiencies in the model. Of course, when you

164
00:10:29,692 --> 00:10:33,158
preprocess data for training, it won't impact the

165
00:10:33,164 --> 00:10:36,770
model speed, but remember that the data used for inference

166
00:10:36,850 --> 00:10:39,910
also must undergo the same preprocessing steps.

167
00:10:40,250 --> 00:10:43,718
So how do we find inefficiencies in data preprocessing?

168
00:10:43,814 --> 00:10:47,754
Well, the simplest way is to use time function in Python. You just

169
00:10:47,792 --> 00:10:50,970
surround parts of code with it and see how quickly it runs.

170
00:10:51,310 --> 00:10:55,114
But what if you have a large code base and your data preprocessing

171
00:10:55,162 --> 00:10:59,162
is spread across multiple classes and files? You can't surround

172
00:10:59,226 --> 00:11:03,070
all functions with time. It will be very tedious and messy.

173
00:11:03,490 --> 00:11:07,326
Luckily, there are out of the box solutions such as Python's

174
00:11:07,358 --> 00:11:10,014
built in, cpython and Yappy.

175
00:11:10,142 --> 00:11:12,930
Yappy is a profiler that is written in c,

176
00:11:13,000 --> 00:11:16,514
it's super fast, and most importantly, it lets you profile asynchronous

177
00:11:16,562 --> 00:11:20,934
code. It is my profiler of choice. Here is an example

178
00:11:21,052 --> 00:11:24,210
of these basic usage of yapi,

179
00:11:24,370 --> 00:11:27,750
where foo is a function you want to profile

180
00:11:28,350 --> 00:11:31,580
can be cost, method, or anything

181
00:11:32,030 --> 00:11:35,418
sophisticated. The best thing is that

182
00:11:35,504 --> 00:11:39,546
you will see all of the functions and all

183
00:11:39,568 --> 00:11:43,230
of the files that are called when this function is executed.

184
00:11:43,970 --> 00:11:47,406
Let's go over some basic things when it comes down to

185
00:11:47,428 --> 00:11:50,880
yappy. First of all, let's understand the difference between

186
00:11:51,810 --> 00:11:55,346
times that you can use. Clock types can

187
00:11:55,368 --> 00:11:59,010
be CpU time or wall time. Cpu time or process

188
00:11:59,080 --> 00:12:03,010
time is the amount of time for which cpu

189
00:12:04,390 --> 00:12:08,294
has been used for processing instructions of a computer program

190
00:12:08,412 --> 00:12:11,686
or operating system, or in our case, a function as

191
00:12:11,708 --> 00:12:14,550
opposed to elapsed time, which includes, for example,

192
00:12:14,620 --> 00:12:18,354
waiting for input output operations or entering

193
00:12:18,402 --> 00:12:21,974
low voltime is the actual time taken from the start

194
00:12:22,012 --> 00:12:25,546
of a computer program to the end. In other words, it is difference

195
00:12:25,648 --> 00:12:29,098
between the time at which a task finishes and the time at which the

196
00:12:29,104 --> 00:12:32,714
task started. When you're providing asynchronous

197
00:12:32,762 --> 00:12:35,120
code, you should use the wall time.

198
00:12:36,290 --> 00:12:39,950
Then at the bottom in green you can see the simple

199
00:12:40,020 --> 00:12:44,498
output of yappy. It includes function

200
00:12:44,664 --> 00:12:47,970
name with a function file.

201
00:12:48,390 --> 00:12:51,602
For more sophisticated programs it will be

202
00:12:51,656 --> 00:12:55,486
running. It will have a lot more functions

203
00:12:55,518 --> 00:12:59,254
and files in there. Then you will see n

204
00:12:59,292 --> 00:13:03,334
calls. N calls is the number of function calls how many

205
00:13:03,372 --> 00:13:06,982
times this function has been called. It's a great way to see if

206
00:13:07,036 --> 00:13:10,442
some function has been called a lot more times than you would expect.

207
00:13:10,576 --> 00:13:13,900
Then you would know that maybe there is a way to optimize it.

208
00:13:14,430 --> 00:13:17,846
T sub is the time spent in the function excluding

209
00:13:17,878 --> 00:13:21,566
subc calls. Function includes inside of it

210
00:13:21,668 --> 00:13:25,022
some other functions. The t sub

211
00:13:25,076 --> 00:13:28,766
will not account the time. So t sub usually is if

212
00:13:28,788 --> 00:13:32,698
it's big then you have a problem or it means that the

213
00:13:32,724 --> 00:13:36,482
function doesn't call other functions and then ttot is

214
00:13:36,536 --> 00:13:39,970
ttotal total time spent in the function,

215
00:13:40,040 --> 00:13:43,810
including subcools. Obviously if it's a function like main,

216
00:13:43,880 --> 00:13:47,286
it will have a very large total time. But then you need to

217
00:13:47,308 --> 00:13:50,598
go and see all of the

218
00:13:50,764 --> 00:13:54,326
functions that are inside main to see which

219
00:13:54,348 --> 00:13:57,814
one takes the most time. By using a profiler you can

220
00:13:57,852 --> 00:14:01,914
get a complete overview of how your code is running and which parts of it

221
00:14:01,952 --> 00:14:06,006
are the slope. This is the quickest and simplest way of finding bottlenecks

222
00:14:06,038 --> 00:14:10,194
in your code. Go and see some examples of inefficiencies

223
00:14:10,262 --> 00:14:12,910
that often happen in data preprocessing.

224
00:14:13,890 --> 00:14:17,774
It's no secret we all use pandas. It's great for

225
00:14:17,892 --> 00:14:21,086
data analysis and data preprocessing, and one

226
00:14:21,108 --> 00:14:24,554
of the most useful functions of

227
00:14:24,612 --> 00:14:26,580
pandas is apply.

228
00:14:27,110 --> 00:14:31,470
However, it's not the most efficient. While pandas

229
00:14:31,550 --> 00:14:35,234
itself is a great package, apply does not take

230
00:14:35,272 --> 00:14:38,786
advantage of vectorization. Also apply returns

231
00:14:38,818 --> 00:14:42,466
a new series of data frame objects. So with a very large data frame

232
00:14:42,498 --> 00:14:45,586
you have considerable input output overhead.

233
00:14:45,698 --> 00:14:49,722
A couple of ways to solve it is to instead

234
00:14:49,776 --> 00:14:52,730
of using apply, try using numpy set,

235
00:14:52,800 --> 00:14:56,890
especially if you are just

236
00:14:57,040 --> 00:15:01,174
performing operation on a single column

237
00:15:01,222 --> 00:15:04,686
independence data frame. Alternatively, you can

238
00:15:04,868 --> 00:15:08,126
find a simpler multiply column by

239
00:15:08,148 --> 00:15:11,790
two. It can be done with these built in function.

240
00:15:11,940 --> 00:15:15,778
Also, if you want to use apply to multiple columns in

241
00:15:15,784 --> 00:15:19,490
the pandas data frame, try to avoid using access

242
00:15:19,560 --> 00:15:22,914
equals one format in apply and write

243
00:15:22,952 --> 00:15:26,898
a standalone function that can take in multiple numpy

244
00:15:26,914 --> 00:15:30,966
arrays as inputs and then directly use it on the

245
00:15:31,068 --> 00:15:34,242
values attribute

246
00:15:34,306 --> 00:15:38,314
of the panda series. Sometimes you can be performing calculations more

247
00:15:38,352 --> 00:15:41,722
times than needed. Sometimes you may have metadata in your data

248
00:15:41,776 --> 00:15:45,674
set like gender, city, car type, and you

249
00:15:45,712 --> 00:15:49,770
can be performing a calculation for

250
00:15:49,840 --> 00:15:53,102
every single data point. While you only need to

251
00:15:53,156 --> 00:15:56,320
perform calculation once per these.

252
00:15:57,730 --> 00:16:02,286
So you should consider using and

253
00:16:02,308 --> 00:16:06,034
filtering in pandas and only performing a calculation once per group.

254
00:16:06,152 --> 00:16:09,714
This could significantly improve the speed of your data pre

255
00:16:09,752 --> 00:16:13,154
processing. Finally, wherever possible, it's best

256
00:16:13,192 --> 00:16:16,866
to use numpy instead of pandas. While pandas is

257
00:16:16,888 --> 00:16:20,582
very user friendly and intuitive, numpy is written in c

258
00:16:20,636 --> 00:16:24,386
and it's the champion when it comes down to efficiency. Feature selection

259
00:16:24,418 --> 00:16:28,614
is essentially the final step of data pre processing and it has

260
00:16:28,732 --> 00:16:32,460
a very large impact on the accuracy and speed of a model.

261
00:16:32,830 --> 00:16:36,154
As the name suggests, it's the process of

262
00:16:36,192 --> 00:16:39,626
determining which features in a data set are most relevant to

263
00:16:39,648 --> 00:16:43,386
the output. Your first instinct may be to

264
00:16:43,408 --> 00:16:47,034
take all of the data and throw it in the model because just a minute

265
00:16:47,082 --> 00:16:49,760
ago I told you the more data these better,

266
00:16:50,290 --> 00:16:54,002
but I was talking about the number of data points, not the data

267
00:16:54,056 --> 00:16:58,014
from each particular data point. By selecting

268
00:16:58,062 --> 00:17:01,374
the most important features and removing irrelevant

269
00:17:01,422 --> 00:17:05,026
ones, you can simplify your model and reduce the risk of

270
00:17:05,048 --> 00:17:09,142
overfitting. This not only improves the accuracy of the model, but also

271
00:17:09,196 --> 00:17:12,646
makes it more efficient and less complex, which can

272
00:17:12,668 --> 00:17:15,926
be critical for real world applications where time and

273
00:17:15,948 --> 00:17:19,158
resources are limited. So one of the

274
00:17:19,164 --> 00:17:23,346
methods I users for feature selection is sharply

275
00:17:23,378 --> 00:17:27,002
additive explanations or sharp values. They are

276
00:17:27,056 --> 00:17:30,118
a way to explain the output of any machine learning model.

277
00:17:30,304 --> 00:17:34,618
It uses a game theoretic approach that measures each player's

278
00:17:34,714 --> 00:17:37,806
or features contribution to the outcome machine learning.

279
00:17:37,908 --> 00:17:41,562
Each feature is assigned an importance value representing

280
00:17:41,706 --> 00:17:45,150
its contribution to the model's output. Features with

281
00:17:45,220 --> 00:17:48,862
positive shock values positively impact the prediction,

282
00:17:48,926 --> 00:17:51,950
while those with negative values have a negative impact.

283
00:17:52,030 --> 00:17:55,380
These magnitude is a measure of how strong the effect is.

284
00:17:56,150 --> 00:18:00,102
When I say positive or negative, I don't mean good or bad, I just mean

285
00:18:00,156 --> 00:18:04,354
plus or minus. Sharp values are model agnostic,

286
00:18:04,482 --> 00:18:08,038
so it means they can be used to interpret any machine learning model,

287
00:18:08,124 --> 00:18:11,586
including linear regression, decision trees, random forests,

288
00:18:11,698 --> 00:18:15,510
grading, boosting models, neural networks so they are universal.

289
00:18:15,590 --> 00:18:19,146
Obviously for more complex architectures it is

290
00:18:19,248 --> 00:18:22,686
harder to calculate them and increases the number of

291
00:18:22,708 --> 00:18:26,094
calculations. So even though they can be used

292
00:18:26,132 --> 00:18:29,454
for neural networks, for example, these best

293
00:18:29,492 --> 00:18:32,670
work for simpler models like gradient boosted trees.

294
00:18:33,750 --> 00:18:37,250
Shack values are particularly useful for

295
00:18:37,320 --> 00:18:40,802
feature selection when dealing with high dimensional complex data

296
00:18:40,856 --> 00:18:44,734
sets. By prioritizing features with high shack

297
00:18:44,782 --> 00:18:48,534
values, both positive and negative, we are

298
00:18:48,572 --> 00:18:52,994
looking at magnitude here, you can streamline the model by removing less impactful

299
00:18:53,042 --> 00:18:56,054
features and highlighting the most influential ones.

300
00:18:56,252 --> 00:19:00,486
You can make the model simpler and faster without sacrificing the accuracy.

301
00:19:00,678 --> 00:19:03,962
This method not only enhances model performance, but also

302
00:19:04,016 --> 00:19:07,642
helps to improve the explainability of a model. It also

303
00:19:07,696 --> 00:19:10,922
helps understanding the driving forces behind predictions,

304
00:19:11,066 --> 00:19:13,870
making the model more transparent and transworthy.

305
00:19:14,450 --> 00:19:18,250
You can say that using sharp values for feature selection

306
00:19:18,330 --> 00:19:21,934
is a form of regular prediction, and you will not be

307
00:19:21,972 --> 00:19:24,020
wrong. That's pretty much it.

308
00:19:24,550 --> 00:19:28,370
What's best? Sharp values do not change when the model

309
00:19:28,440 --> 00:19:31,570
changes unless the contribution of the feature changes.

310
00:19:31,720 --> 00:19:35,286
So this means that sharp values provide a

311
00:19:35,308 --> 00:19:38,886
consistent interpretation of the model's behavior even when

312
00:19:38,908 --> 00:19:41,560
the model architecture or parameters change.

313
00:19:41,930 --> 00:19:46,050
You do not need to study game theory to calculate sharp values.

314
00:19:46,210 --> 00:19:49,698
All necessary tools can be found in a shape package in python,

315
00:19:49,794 --> 00:19:53,526
and using it you can calculate shapalis and visualize feature importance,

316
00:19:53,638 --> 00:19:57,370
feature dependence, force, and make decision lets.

317
00:19:58,190 --> 00:20:02,582
And for example, the visualization you see on the slide right now is directly

318
00:20:02,646 --> 00:20:05,934
from d. When we talk about machine learning

319
00:20:05,972 --> 00:20:09,402
models today, we usually talk about llms

320
00:20:09,466 --> 00:20:13,598
and transformers, and while they are great at many tasks,

321
00:20:13,774 --> 00:20:17,234
most businesses don't need such sophisticated architectures for

322
00:20:17,272 --> 00:20:21,060
their purposes, especially because llms are

323
00:20:22,070 --> 00:20:25,430
very expensive to train and maintain. And most

324
00:20:25,500 --> 00:20:29,314
tasks even today, can be easily executed

325
00:20:29,442 --> 00:20:33,042
using much simpler models such as gradient boosted trees

326
00:20:33,106 --> 00:20:35,830
like Xgboost and LightGBM.

327
00:20:36,170 --> 00:20:39,934
Xgboost and LightGBM are two of the most popular gradient

328
00:20:40,002 --> 00:20:43,994
boosting frameworks users in machine learning. Both models are

329
00:20:44,032 --> 00:20:47,606
designed to improve the speed and accuracy of machine learning models.

330
00:20:47,718 --> 00:20:51,926
Xgboost is known for its scalability and flexibility, while LGBM

331
00:20:51,958 --> 00:20:55,686
is known for its high speed performance. Xgboost is a well

332
00:20:55,728 --> 00:20:59,790
established framework with a large user base, and LGBM is

333
00:20:59,860 --> 00:21:03,430
relatively new, but has gained popularity due

334
00:21:03,450 --> 00:21:05,090
to its impressive performance.

335
00:21:05,990 --> 00:21:09,970
Xgboost has been widely used since its release in 2014.

336
00:21:10,390 --> 00:21:14,254
It is flexible because it can handle multiple input data types

337
00:21:14,302 --> 00:21:18,038
and works well with sparse data. Xgboost has

338
00:21:18,044 --> 00:21:21,814
an internal data structure called dmetrics that

339
00:21:21,852 --> 00:21:25,266
is optimized for both memory efficiency and training speed.

340
00:21:25,458 --> 00:21:29,018
You can construct a dmetrics from multiple different sources of data.

341
00:21:29,184 --> 00:21:32,886
Xgboost also has a regularization feature that helps prevent

342
00:21:32,918 --> 00:21:35,980
overfitting common problem in machine learning.

343
00:21:36,830 --> 00:21:40,518
However, Xgboost can be slower than other models when dealing

344
00:21:40,534 --> 00:21:43,662
with larger data sets. This is because when training

345
00:21:43,716 --> 00:21:47,582
a gradient boosted tree model, Xgboost uses level by

346
00:21:47,636 --> 00:21:50,910
tree growth, where it grows the tree by one level

347
00:21:50,980 --> 00:21:54,926
for each branch in what is known in depth

348
00:21:54,958 --> 00:21:58,546
first order. This will usually result in more leaves and

349
00:21:58,568 --> 00:22:03,010
therefore more splits, as in a computational overhead.

350
00:22:03,990 --> 00:22:07,574
So for each leaf, as you can see on the diagram, for each

351
00:22:07,612 --> 00:22:12,022
leaf on the level, it will grow even

352
00:22:12,076 --> 00:22:15,814
if it's not needed there. On the other hand, LGBM is

353
00:22:15,852 --> 00:22:20,022
known for its lightning fast performance. This is because when training a gradient

354
00:22:20,086 --> 00:22:23,766
boosted tree model, LGBM uses leafwise tree

355
00:22:23,798 --> 00:22:27,142
growth where it grows. The tree using a liftwise approach

356
00:22:27,286 --> 00:22:30,506
uses best first order by constructing the splits for

357
00:22:30,528 --> 00:22:33,790
each branch until the perfect split is reached.

358
00:22:34,530 --> 00:22:38,142
LGBM is designed to handle large data sets efficiently and

359
00:22:38,196 --> 00:22:42,250
in certain cases can be much faster than Xgboost.

360
00:22:42,410 --> 00:22:45,950
LGBM also has a feature that allows it to handle categorical

361
00:22:46,030 --> 00:22:49,710
data efficiently, which is a significant advantage over HgBoost,

362
00:22:49,790 --> 00:22:53,678
and it also has a built in regularization support to prevent overfitting.

363
00:22:53,854 --> 00:22:57,106
However, LGBM can be more memory intensive,

364
00:22:57,218 --> 00:23:00,534
which can be a problem when dealing with larger data lets with

365
00:23:00,572 --> 00:23:04,582
limited memory resources. There is no obvious way to choose

366
00:23:04,716 --> 00:23:08,674
one model over the other, so you'll have to experiment

367
00:23:08,722 --> 00:23:12,474
and decide these result. Fortunately, the models can be

368
00:23:12,512 --> 00:23:16,454
set up and trade very quickly, and you can get the testing swiftly

369
00:23:16,502 --> 00:23:20,794
out of the way and move to optimizing the model for let's

370
00:23:20,842 --> 00:23:24,506
make a quick recap of today's balancing speed

371
00:23:24,538 --> 00:23:28,334
and accuracy often depends on the context or the

372
00:23:28,372 --> 00:23:32,026
field of application. We may need quick results in user

373
00:23:32,058 --> 00:23:35,994
focused applications and on the other hand, require the highest degree

374
00:23:36,042 --> 00:23:39,762
of accuracy in fields like medical to identify your model's needs,

375
00:23:39,816 --> 00:23:43,454
align its purpose with business goals, define desired outcomes,

376
00:23:43,502 --> 00:23:46,870
and consider KPIs. Tailor the model to meet

377
00:23:46,940 --> 00:23:51,154
user expectations, prioritizing speed for customer facing applications

378
00:23:51,202 --> 00:23:54,694
and accuracy for internal analytics. Look at your

379
00:23:54,732 --> 00:23:57,986
particular case and sometimes in internal applications

380
00:23:58,018 --> 00:24:02,038
you also need speed, and in customer facing applications you need accuracy.

381
00:24:02,214 --> 00:24:06,218
Make sure you acquire a robust data set with quality

382
00:24:06,304 --> 00:24:10,262
data and accurate labels. The quantity of data is important,

383
00:24:10,416 --> 00:24:14,160
but the emphasis should always be on maintaining its quality.

384
00:24:14,530 --> 00:24:18,314
Consider experimenting with simpler models like Xgboost or light GPM

385
00:24:18,362 --> 00:24:21,514
instead of complex architectures like llms and transformers.

386
00:24:21,642 --> 00:24:25,086
These frameworks, known for enhancing both speed and

387
00:24:25,108 --> 00:24:28,666
accuracy, can be suitable for quite a variety of use cases.

388
00:24:28,858 --> 00:24:32,826
And when you have a simple model that makes accurate

389
00:24:32,858 --> 00:24:36,754
predictions but works too slowly by looking for the bottlenecks

390
00:24:36,802 --> 00:24:40,770
in hours code using these profilers such as cprfile or Yappy,

391
00:24:40,850 --> 00:24:44,680
the most frequent place for the bottlenecks is the data preprocessing step.

392
00:24:45,130 --> 00:24:48,854
Thank you for joining me today and I hope you find this

393
00:24:48,892 --> 00:24:51,780
talk useful. Hopefully see you in the future.

