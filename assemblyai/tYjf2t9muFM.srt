1
00:00:27,240 --> 00:00:31,812
Hello. Hello. Welcome everyone to this talk on decentralized

2
00:00:31,868 --> 00:00:35,864
monitoring and why it matters. I am Shyam Srivalsan.

3
00:00:36,284 --> 00:00:40,068
I work at Netdata. NetData is an open source

4
00:00:40,116 --> 00:00:43,532
observability platform, and our goal is

5
00:00:43,548 --> 00:00:47,424
to shake up the observability landscape and make monitoring easy for everyone.

6
00:00:50,284 --> 00:00:53,624
So let's start by talking about observability.

7
00:00:54,324 --> 00:00:57,104
What is observability in a nutshell? So,

8
00:00:57,484 --> 00:01:01,220
to start with, there's all the stuff that you care about, and this

9
00:01:01,252 --> 00:01:04,188
could be your data center,

10
00:01:04,356 --> 00:01:08,264
your applications, your databases, your servers,

11
00:01:08,964 --> 00:01:13,124
your IoT networks, your Kubernetes clusters, all of those things which

12
00:01:13,244 --> 00:01:16,716
are keeping whatever that you care about, your business, your company,

13
00:01:16,860 --> 00:01:20,704
all of it running. So here's all the stuff that you care about,

14
00:01:21,114 --> 00:01:24,706
and you want to observe this stuff. And how do you do that? So you

15
00:01:24,730 --> 00:01:28,106
do it at a high level using these three things.

16
00:01:28,210 --> 00:01:32,414
So there's metrics that you get out of your infrastructure,

17
00:01:33,114 --> 00:01:36,722
and metrics are usually in the form of time series data, and there's numeric

18
00:01:36,778 --> 00:01:40,010
data associated with different counters that you look at.

19
00:01:40,202 --> 00:01:44,330
And then you have logs which are string texture data,

20
00:01:44,442 --> 00:01:47,704
which are again, talking about what's happening in your infrastructure.

21
00:01:47,834 --> 00:01:51,108
And then you have traces which go a bit deeper into the flow

22
00:01:51,156 --> 00:01:54,812
between a particular event happening across different

23
00:01:54,868 --> 00:01:58,304
parts of your infrastructure, your front end, your black end, for example.

24
00:01:59,404 --> 00:02:03,180
So now you have your metrics, logs and traces. They exist on these

25
00:02:03,212 --> 00:02:06,476
devices. What do you do next? So the next step is

26
00:02:06,500 --> 00:02:10,508
to collect it all. So this is where your

27
00:02:10,676 --> 00:02:13,694
observability tools and platforms generally enter

28
00:02:13,734 --> 00:02:17,542
the picture. And they have you, they usually have some sort

29
00:02:17,558 --> 00:02:21,134
of agents in place, or collectors or exporters which

30
00:02:21,214 --> 00:02:25,126
are doing this job of collecting all of this data into

31
00:02:25,270 --> 00:02:29,070
some sort of repository or storage. And once you collect

32
00:02:29,102 --> 00:02:31,678
it, what next? What do you do with it?

33
00:02:31,846 --> 00:02:35,262
So there's two main things that you do, and one is to

34
00:02:35,278 --> 00:02:39,078
visualize it. So to have it up in a dashboard, have it

35
00:02:39,166 --> 00:02:43,114
in some form that you can look at it. And this is how,

36
00:02:44,294 --> 00:02:48,030
whether you're a DevOps or a developer or an SRE, this is how

37
00:02:48,062 --> 00:02:51,646
you observe the stuff that you care about. It's mostly in the form

38
00:02:51,670 --> 00:02:55,510
of dashboards. And also another really important component

39
00:02:55,542 --> 00:02:59,262
is alerting, because you can't be expected to always be looking at

40
00:02:59,278 --> 00:03:03,238
a dashboard about this stuff. You have other

41
00:03:03,286 --> 00:03:06,462
important tasks to do. You're human, so you have to go to

42
00:03:06,478 --> 00:03:10,102
sleep. You have other things outside of work that you need

43
00:03:10,118 --> 00:03:13,638
to be doing, and you cannot always be observing

44
00:03:13,766 --> 00:03:17,254
the things to make sure that everything is going okay, so here,

45
00:03:17,294 --> 00:03:21,006
that's where alerting comes in, which means that if something important happens

46
00:03:21,070 --> 00:03:24,634
that you need to know about, the alerts will make sure that that happens.

47
00:03:26,094 --> 00:03:29,326
So this, in a nutshell is observability.

48
00:03:29,470 --> 00:03:32,518
So I just wanted to set the stage with this because

49
00:03:32,646 --> 00:03:35,710
we're going to be talking about a lot of the different aspects of the things

50
00:03:35,742 --> 00:03:37,074
that we just discussed.

51
00:03:39,494 --> 00:03:42,894
And what about the observability landscape? It's pretty

52
00:03:42,934 --> 00:03:46,502
crowded in here, to be honest. You can see all those logos of all

53
00:03:46,518 --> 00:03:48,714
the different observability companies.

54
00:03:50,774 --> 00:03:54,142
To try and break this down a little bit, I'm going

55
00:03:54,158 --> 00:03:57,878
to be dividing these solutions into different generations.

56
00:03:58,006 --> 00:04:01,846
So you can talk about the first generation observability platforms which

57
00:04:01,870 --> 00:04:05,238
are focused on checks. So you have your nagios, your zabbix,

58
00:04:05,286 --> 00:04:08,342
your peer DG check MK, they're focused on

59
00:04:08,358 --> 00:04:11,766
checks. Is something working or not? Is something running or not?

60
00:04:11,830 --> 00:04:15,326
Is something up or down? That's their main role.

61
00:04:15,390 --> 00:04:18,494
Of course they're branching out and doing other things as well, but that's where they

62
00:04:18,534 --> 00:04:21,554
started with, or that's their core philosophy.

63
00:04:22,174 --> 00:04:26,144
And then you have the second generation which are more focused on the metrics themselves.

64
00:04:26,334 --> 00:04:29,988
So this is where Prometheus, for example, is really

65
00:04:30,036 --> 00:04:34,084
famous for. And most of my audience has heard about Prometheus.

66
00:04:34,124 --> 00:04:37,828
Most of you have used it. Prometheus community is pretty active as well.

67
00:04:37,916 --> 00:04:41,708
So you know how it works. There's metrics that are exported

68
00:04:41,756 --> 00:04:44,700
from different services, applications,

69
00:04:44,772 --> 00:04:48,580
servers, devices, and it's time series metrics.

70
00:04:48,732 --> 00:04:52,546
There's a metadata associated to it. And there is a Prometheus server

71
00:04:52,660 --> 00:04:56,326
which collects all of this data and stores it there as a time

72
00:04:56,350 --> 00:04:59,934
series database. And it's not just Prometheus. There are other agents

73
00:04:59,974 --> 00:05:02,914
and other monitoring platforms that do this as well.

74
00:05:03,654 --> 00:05:07,150
Then you have the observability

75
00:05:07,222 --> 00:05:11,246
tools that focus on logs. So you have things like splunk or elastic

76
00:05:11,350 --> 00:05:14,958
where though they do other things as well, logs is kind of their

77
00:05:15,006 --> 00:05:18,542
core competence. And then finally you

78
00:05:18,558 --> 00:05:22,060
have the fourth generation of integrated

79
00:05:22,212 --> 00:05:25,860
monitoring tools where you have a

80
00:05:25,892 --> 00:05:28,540
mix of all of these things. You have metrics, you have logs, you have traces,

81
00:05:28,612 --> 00:05:32,284
checks and all these things mixed in. And this is where the

82
00:05:32,324 --> 00:05:35,700
big commercial tools such as Datadog,

83
00:05:35,772 --> 00:05:39,844
Dynatrace and stanar and Eurelic and so on really

84
00:05:40,004 --> 00:05:43,612
make their mark. So this is the observatory

85
00:05:43,668 --> 00:05:49,196
landscape as most of you are used to it. So what

86
00:05:49,220 --> 00:05:52,524
is in common with most of these tools? Right? So when you think about

87
00:05:52,564 --> 00:05:56,124
it, this is where the philosophy of centralized monitoring really

88
00:05:56,164 --> 00:06:00,076
comes in. Centralized monitoring or centralized observability

89
00:06:00,260 --> 00:06:03,844
is the default setting today, which means

90
00:06:03,884 --> 00:06:07,732
that the tools that we're talking about are centralizing

91
00:06:07,868 --> 00:06:11,748
metrics, logs, traces, checks, all of this information into

92
00:06:11,836 --> 00:06:14,024
some sort of a central monitoring server.

93
00:06:15,384 --> 00:06:19,192
There are benefits, this approach, of course, this is why people do it.

94
00:06:19,368 --> 00:06:22,632
It gives you comprehensive visibility because you have all of those things in

95
00:06:22,648 --> 00:06:26,152
one place. So you have the metrics, you have the logs, you have

96
00:06:26,168 --> 00:06:30,048
the traces, all of that information. So when you're looking at a particular timeline,

97
00:06:30,176 --> 00:06:32,324
you get all the information in one place.

98
00:06:33,864 --> 00:06:37,464
You can use this to correlate trends across various different data

99
00:06:37,504 --> 00:06:40,804
types, whether it's a metric, whether it's a log or a trace.

100
00:06:41,284 --> 00:06:44,548
The trend correlation becomes really important when you're troubleshooting something,

101
00:06:44,716 --> 00:06:48,196
and it also gives you a deeper understanding of what's really going on in the

102
00:06:48,220 --> 00:06:52,036
system, in the infrastructure. So this is centralized observability,

103
00:06:52,140 --> 00:06:56,220
this is the underlying architecture or philosophy of

104
00:06:56,372 --> 00:06:59,860
most of, or almost all of the

105
00:06:59,892 --> 00:07:03,532
main observability platforms out there today. So it

106
00:07:03,548 --> 00:07:07,236
sounds pretty good, doesn't it? So what's the issue? Right,

107
00:07:07,420 --> 00:07:11,980
so that's what I'm going to get to. There is a big bucket and

108
00:07:12,092 --> 00:07:16,156
that's what we're going to have a deep dive into. So let's talk about

109
00:07:16,340 --> 00:07:19,584
the seven deadly sins of centralized monitoring, if you will.

110
00:07:20,844 --> 00:07:24,124
What are the limitations of centralized monitoring, which in many

111
00:07:24,164 --> 00:07:28,476
cases, unless you think about it, it's not something that's

112
00:07:28,620 --> 00:07:32,180
obvious and on top of your mind or in front of your mind,

113
00:07:32,252 --> 00:07:36,160
right, sometimes it can go under the carpet and that's where the troubles start

114
00:07:36,192 --> 00:07:39,936
creeping in. So let's go over these one by one, and let's

115
00:07:39,960 --> 00:07:43,296
start with fidelity. So what, what is fidelity?

116
00:07:43,360 --> 00:07:46,792
What does fidelity mean? So one

117
00:07:46,808 --> 00:07:50,928
way to think about this is fidelity is sort of a mixture of granularity

118
00:07:51,056 --> 00:07:54,776
and cardinality. And what does this mean? So granularity

119
00:07:54,840 --> 00:07:58,432
means, let's, let's say we're talking about metric data

120
00:07:58,488 --> 00:08:02,100
here. How often do you have the metric data? Are you collecting it

121
00:08:02,172 --> 00:08:05,876
every second, or are you collecting it every 10 seconds?

122
00:08:06,020 --> 00:08:09,140
Or are you collecting it every 60 seconds? Right.

123
00:08:09,292 --> 00:08:13,180
There is a very big difference between these three things, between getting per

124
00:08:13,212 --> 00:08:16,804
second data, per ten second data and per 62nd data.

125
00:08:16,964 --> 00:08:20,316
So this is what data granularity means, how granular

126
00:08:20,380 --> 00:08:23,908
your data is. And if you have low granularity, which means that let's say

127
00:08:23,916 --> 00:08:26,064
you're collecting data every 60 seconds,

128
00:08:26,744 --> 00:08:29,896
this is in effect blurry data, because you

129
00:08:29,920 --> 00:08:33,728
don't get the full picture. There's a lot of stuff that's happening.

130
00:08:33,776 --> 00:08:37,688
Let's say for example, they'll be looking at a specific metric, which is a counter

131
00:08:37,736 --> 00:08:41,392
over some important data coming from your database and you're only looking at

132
00:08:41,408 --> 00:08:44,404
it at snapshots that happen every 60 seconds.

133
00:08:45,064 --> 00:08:48,712
There could be a lot that happened within that time period which you're either

134
00:08:48,768 --> 00:08:51,764
aggregating away or you're not getting to see.

135
00:08:51,944 --> 00:08:55,676
So this is why I call it blurry data. And then

136
00:08:55,700 --> 00:08:57,984
the second part of this is cardinality.

137
00:08:58,284 --> 00:09:01,956
And you know what cardinality means? It means how

138
00:09:02,020 --> 00:09:04,884
much data that you're getting, right?

139
00:09:04,964 --> 00:09:08,964
So if you have ten

140
00:09:09,004 --> 00:09:12,628
metrics or 100 metrics or 1000 metrics coming

141
00:09:12,676 --> 00:09:16,744
from a particular server or from a particular application, there's again a big difference.

142
00:09:17,444 --> 00:09:21,286
In many cases with the traditional

143
00:09:21,350 --> 00:09:24,622
monitoring aspect, you're cherry picking what you want to monitor.

144
00:09:24,718 --> 00:09:28,478
So you're saying that I think, or somebody else on the

145
00:09:28,486 --> 00:09:32,350
team thinks, or this particular observability tool

146
00:09:32,462 --> 00:09:36,166
thinks that these are the ten most important metrics that

147
00:09:36,190 --> 00:09:39,758
I need to collect from my postgres database or

148
00:09:39,846 --> 00:09:43,230
from this Linux server. There might

149
00:09:43,262 --> 00:09:45,942
be a lot of other metrics, there might be 100, there might be a thousand,

150
00:09:45,998 --> 00:09:49,744
there might be 2000 other metrics that are collectible

151
00:09:50,484 --> 00:09:54,024
from that machine. But you're choosing not to.

152
00:09:54,524 --> 00:09:57,224
And we talk about why you're choosing not to do this.

153
00:09:57,844 --> 00:10:01,548
But the important thing to understand here is if you have low cardinality,

154
00:10:01,636 --> 00:10:05,356
that means that you have blind spots. There's metrics out there which are talking

155
00:10:05,380 --> 00:10:08,508
about things which you're not collecting and it's completely blind to you.

156
00:10:08,556 --> 00:10:12,020
So you might think that I have observability, I have all

157
00:10:12,052 --> 00:10:15,736
this data that I'm looking at, but there's a ton of other data that

158
00:10:15,760 --> 00:10:18,244
you're not looking at and that you're not even thinking about.

159
00:10:19,304 --> 00:10:23,032
So think about it this way. When an actual issue happens and

160
00:10:23,048 --> 00:10:26,672
you're troubleshooting it and you're now doing the post mortem

161
00:10:26,728 --> 00:10:30,456
of why didn't we catch this issue before it became a problem?

162
00:10:30,640 --> 00:10:34,576
Sometimes you end up taking this action, or we should have been monitoring X

163
00:10:34,680 --> 00:10:38,312
and Y and we didn't have those metrics, let's go and add them to

164
00:10:38,328 --> 00:10:42,046
the dashboard so that this won't happen again. So this is a problem with

165
00:10:42,110 --> 00:10:45,446
low cardinality data where you're not collecting all of those

166
00:10:45,470 --> 00:10:48,894
things to start with. And when you have a combination of

167
00:10:48,974 --> 00:10:51,874
low granularity and low cardinality,

168
00:10:52,174 --> 00:10:55,430
then what you get is like the first half of this

169
00:10:55,462 --> 00:10:59,794
horse over here, you get low fidelity data, which means it's abstract,

170
00:11:00,374 --> 00:11:03,634
it doesn't have the detail and it doesn't have the coverage,

171
00:11:04,094 --> 00:11:07,824
which means that you think you have observability, but in

172
00:11:07,944 --> 00:11:09,484
actual fact you don't.

173
00:11:11,064 --> 00:11:14,592
Now fidelity and centralization

174
00:11:14,688 --> 00:11:18,656
are both deeply linked to each other. In a way centralization

175
00:11:18,760 --> 00:11:22,304
makes cost and fidelity proportional to each other. And this is the

176
00:11:22,344 --> 00:11:26,232
root cause of all of the problems that we talked about. Because if

177
00:11:26,248 --> 00:11:30,088
you increase the fidelity, which means that you're increasing the granularity

178
00:11:30,216 --> 00:11:33,488
and the cardinality, then you're by default

179
00:11:33,536 --> 00:11:37,236
increasing the cost. Because think about it, you have a

180
00:11:37,260 --> 00:11:41,268
centralized monitoring server where all of this information needs to reside.

181
00:11:41,436 --> 00:11:45,404
Higher granularity means that you need to send more data over

182
00:11:45,444 --> 00:11:48,684
time. So instead of sending one sample every 60 seconds,

183
00:11:48,804 --> 00:11:51,264
you're now sending 60 samples.

184
00:11:52,084 --> 00:11:54,824
So it's 60 times the amount of data that you're sending.

185
00:11:55,844 --> 00:12:00,156
And the same with cardinality. Instead of collecting ten metrics, if you're collecting 1000

186
00:12:00,220 --> 00:12:03,656
metrics, it's 100 times the amount of data that you're collecting.

187
00:12:03,800 --> 00:12:07,296
And that central server needs to be able to handle all of this data.

188
00:12:07,400 --> 00:12:10,520
Your network connection needs to be able to handle all of this new data going

189
00:12:10,552 --> 00:12:11,444
out of the egress.

190
00:12:13,184 --> 00:12:17,444
So there's a direct link here. Reducing costs

191
00:12:17,784 --> 00:12:21,688
leads to a decrease in fidelity, increasing fidelity

192
00:12:21,776 --> 00:12:24,904
leads to higher costs. So in effect what's

193
00:12:24,944 --> 00:12:27,764
happening is we're building in low fidelity by design.

194
00:12:32,384 --> 00:12:34,764
The second point is scalability.

195
00:12:35,784 --> 00:12:39,128
So again, when you think about it,

196
00:12:39,296 --> 00:12:42,084
think about a central server and what happens,

197
00:12:42,864 --> 00:12:46,552
there is a clear bottleneck here, which is your central monitoring server.

198
00:12:46,688 --> 00:12:50,432
What happens when something happens to that central monitoring server?

199
00:12:50,568 --> 00:12:54,220
That's when you start facing all of your issues. If you want to

200
00:12:54,252 --> 00:12:57,344
scale again, you run into these problems.

201
00:12:57,964 --> 00:13:01,932
So as an example you can think about setting up your own

202
00:13:02,028 --> 00:13:06,348
central Prometheus monitoring server. And what

203
00:13:06,396 --> 00:13:09,564
happens when you keep scaling. And especially for companies

204
00:13:09,644 --> 00:13:13,564
where you're adding a lot of compute or a lot of storage very quickly

205
00:13:13,724 --> 00:13:17,372
and you want to scale very fast. This means that you end up spending more

206
00:13:17,428 --> 00:13:20,772
time trying to figure out how to scale your monitoring environment and

207
00:13:20,788 --> 00:13:24,346
your observability platform. Then you should, you should actually be

208
00:13:24,370 --> 00:13:28,146
spending that time on what you need to do to make sure that all

209
00:13:28,170 --> 00:13:31,410
of your applications and your business logic is working properly.

210
00:13:31,602 --> 00:13:35,774
So scalability is a major issue. With centralized monitoring solutions

211
00:13:36,114 --> 00:13:39,458
you can run into bottlenecks. Obviously there's capacity

212
00:13:39,506 --> 00:13:42,826
limits, there's also latency and delays because all of

213
00:13:42,850 --> 00:13:46,290
this information from all of these different servers and

214
00:13:46,322 --> 00:13:50,138
applications and databases, all of this needs to be relayed into

215
00:13:50,186 --> 00:13:54,052
a central repository somewhere for it to be observed

216
00:13:54,148 --> 00:13:57,464
alerted on and anything else that you want to do on that data,

217
00:13:58,244 --> 00:14:01,892
which means that all of this data needs to travel there. So there is a

218
00:14:01,908 --> 00:14:05,260
latency associated to this. And of

219
00:14:05,292 --> 00:14:08,788
course if you want to get fancy and if you want to build this out,

220
00:14:08,836 --> 00:14:12,156
make this more scalable, then you might have to end up doing a lot of

221
00:14:12,180 --> 00:14:17,196
complex load balance. So centralization makes

222
00:14:17,340 --> 00:14:20,864
scalability harder the higher scale that you're looking at,

223
00:14:20,944 --> 00:14:23,976
because if you're looking at this at a very small scale, and let's say that

224
00:14:24,000 --> 00:14:27,216
I have ten servers and I want to monitor this, you don't really have any

225
00:14:27,240 --> 00:14:31,048
scalability issues with a centralized monitoring solution. But as

226
00:14:31,096 --> 00:14:34,640
your infrastructure becomes more complex, we're talking about

227
00:14:34,752 --> 00:14:38,800
multi cloud environments, hybrid cloud or kubernetes clusters,

228
00:14:38,872 --> 00:14:42,552
along with other, you know, an IoT network. This is

229
00:14:42,568 --> 00:14:46,624
when scalability starts hitting you really badly with centralized monitoring solutions.

230
00:14:46,744 --> 00:14:49,804
And this is again one of those things which is a silent killer

231
00:14:50,304 --> 00:14:53,344
to start with. You might not see it as a problem at all, but over

232
00:14:53,384 --> 00:14:55,324
time it becomes more and more of a problem.

233
00:14:57,304 --> 00:15:00,624
Let's go to the next one. So this is a fun one cost,

234
00:15:00,784 --> 00:15:04,884
right? So as you can see from all those

235
00:15:05,224 --> 00:15:08,824
news headline clippings that I've based to the observability

236
00:15:08,904 --> 00:15:12,228
is really expensive and why is it so.

237
00:15:12,316 --> 00:15:15,852
Right. So centralized data storage is expensive.

238
00:15:15,988 --> 00:15:19,864
If you want to store huge amounts of data, especially at higher fidelity,

239
00:15:20,484 --> 00:15:23,852
you're going to have to spend a huge amount of money on it, whether it's

240
00:15:23,908 --> 00:15:28,188
you self hosting it yourself or your observability provider

241
00:15:28,236 --> 00:15:31,316
who has to host all of this on their cloud and they're going to

242
00:15:31,340 --> 00:15:34,996
transfer that cost to you. Of course there's also

243
00:15:35,060 --> 00:15:38,498
centralized compute. It's not just storage, but let's say you have

244
00:15:38,546 --> 00:15:42,234
huge amount of data, petabytes of data, and then you want to do compute

245
00:15:42,274 --> 00:15:46,194
ordered, you want to compute whether there's certain

246
00:15:46,234 --> 00:15:49,682
correlations happening between different data points

247
00:15:49,778 --> 00:15:53,850
or if you want to understand if something should be alerted on all of

248
00:15:53,882 --> 00:15:57,306
this is again compute that you're doing in a centralized location over

249
00:15:57,370 --> 00:16:00,442
a very large dataset. Of course there's

250
00:16:00,578 --> 00:16:04,342
architectural ways in which you can make this easier, but in

251
00:16:04,358 --> 00:16:08,582
most cases still there is a large compute cost and again that cost gets

252
00:16:08,718 --> 00:16:10,754
passed on to the user.

253
00:16:12,014 --> 00:16:15,910
The third thing is high data egress. So in

254
00:16:15,942 --> 00:16:19,454
many deployments there's a big cost to the amount of data

255
00:16:19,494 --> 00:16:23,142
that you're sending up to the central repository, especially if

256
00:16:23,158 --> 00:16:26,534
it being hosted by the observability provider on their cloud,

257
00:16:26,574 --> 00:16:30,944
for example. So if you want to get higher accuracy,

258
00:16:31,094 --> 00:16:34,364
it is possible, right? So the, the observability tool,

259
00:16:34,404 --> 00:16:37,820
let's take, you know, datadog, for example. You can collect data

260
00:16:37,852 --> 00:16:40,996
at a higher granularity or a better granularity,

261
00:16:41,180 --> 00:16:45,156
but it means that you're going to be sending that much data up to Datadog's

262
00:16:45,180 --> 00:16:49,100
cloud, and that means more egress costs for you and also more

263
00:16:49,132 --> 00:16:53,060
costs in general. And then we just talked about

264
00:16:53,092 --> 00:16:56,356
scaling. Scaling costs grow disproportionately.

265
00:16:56,460 --> 00:17:00,464
So you can see some of these articles here mention things like this that

266
00:17:00,844 --> 00:17:04,380
they didn't start up with this. They started off with paying a few

267
00:17:04,412 --> 00:17:07,344
hundred, a few thousand dollars for monitoring and observability.

268
00:17:07,684 --> 00:17:11,268
And as the company scaled, the costs grew

269
00:17:11,316 --> 00:17:15,388
disproportionately. So very soon they have a $1 million observability

270
00:17:15,476 --> 00:17:18,764
bill. And now the company is trying to make sense of why are we paying

271
00:17:18,804 --> 00:17:22,064
a million dollars for observability? This doesn't make any sense.

272
00:17:23,584 --> 00:17:27,352
So this has happened multiple times in multiple companies

273
00:17:27,528 --> 00:17:30,960
where a lot of smart people are working. So you can see how

274
00:17:30,992 --> 00:17:35,520
this is something that, it's not something that's obvious or evident

275
00:17:35,712 --> 00:17:38,936
on day one, but over time it's going to catch

276
00:17:38,960 --> 00:17:42,176
up to you. And what's the result of

277
00:17:42,200 --> 00:17:46,256
all of these things? So very often what happens is people

278
00:17:46,320 --> 00:17:50,904
start questioning the value of observability because

279
00:17:51,644 --> 00:17:54,492
it doesn't make sense that you have to pay millions for this stuff.

280
00:17:54,668 --> 00:17:58,396
So teams decide that either we

281
00:17:58,420 --> 00:18:01,956
don't need observability, just the most drastic option, of course,

282
00:18:02,100 --> 00:18:05,748
or what happens more often is that they decide we're going

283
00:18:05,756 --> 00:18:09,396
to cherry pick what to observe. We can't just grab all the data

284
00:18:09,460 --> 00:18:12,852
because it's just costing us too much and we're not seeing the value

285
00:18:12,908 --> 00:18:16,628
from all the data all the time. So let's cherry pick based

286
00:18:16,676 --> 00:18:20,764
on our subject matter expertise, or expertise

287
00:18:20,804 --> 00:18:24,308
that we're getting from somewhere that these are the things that I want to monitor,

288
00:18:24,476 --> 00:18:27,824
and we'll only monitor this. And this,

289
00:18:28,164 --> 00:18:32,172
as we discussed earlier, can be a bad move because nobody, no matter how

290
00:18:32,188 --> 00:18:35,668
much of an expert you are, can anticipate what metric

291
00:18:35,716 --> 00:18:39,384
would be useful while you're troubleshooting an outage at 03:00 a.m. in the morning.

292
00:18:40,244 --> 00:18:43,384
That's when you wish that you had all the data already there.

293
00:18:44,264 --> 00:18:47,856
So cost is a huge problem when it comes to

294
00:18:47,880 --> 00:18:51,304
centralized monitoring solutions, even the ones that are open

295
00:18:51,344 --> 00:18:55,216
source, like Prometheus, there is still cost in terms of what

296
00:18:55,240 --> 00:18:58,848
you are self hosting and what you're maintaining and what you have

297
00:18:58,856 --> 00:19:00,204
to scale out over time.

298
00:19:03,144 --> 00:19:06,856
And number four is accuracy. So this is linked to

299
00:19:06,920 --> 00:19:10,410
fidelity in a way, because when you have reduced

300
00:19:10,482 --> 00:19:13,714
granularity or reduced coverage,

301
00:19:13,834 --> 00:19:17,266
then by proxy, by default, your accuracy

302
00:19:17,330 --> 00:19:20,714
becomes lower because you're not getting data every second,

303
00:19:20,874 --> 00:19:24,338
because you're not getting all the metrics, you cannot be,

304
00:19:24,386 --> 00:19:27,970
by definition, as accurate as you could be. But it's

305
00:19:28,002 --> 00:19:31,866
not just about fidelity. There's also other issues that

306
00:19:31,890 --> 00:19:35,154
could happen. For example, let's just think about

307
00:19:35,194 --> 00:19:38,844
alerts for a second. If all of the data is spread

308
00:19:38,884 --> 00:19:42,180
across all of these different servers or nodes

309
00:19:42,252 --> 00:19:45,916
or applications, rather than triggering

310
00:19:45,940 --> 00:19:49,748
the alerts when something anomalous is discovered in

311
00:19:49,796 --> 00:19:53,228
one particular location, if you have to centralize all of these information

312
00:19:53,316 --> 00:19:56,940
in a single place, then the thresholds that you're applying to it

313
00:19:57,012 --> 00:20:00,652
might again be generic, it might not be precise enough,

314
00:20:00,748 --> 00:20:04,916
it might not be customized enough to the metric in question that

315
00:20:04,940 --> 00:20:07,940
you trigger the alert at the right time, which means that you could be triggering

316
00:20:07,972 --> 00:20:10,628
the alert later, you could be missing it completely,

317
00:20:10,796 --> 00:20:14,264
and you might miss the actual event there.

318
00:20:15,444 --> 00:20:18,956
When it comes to things like machine learning, for example, you must be

319
00:20:18,980 --> 00:20:23,020
hearing about a lot of machine learning related to observability,

320
00:20:23,212 --> 00:20:26,516
maybe even some discussions during this conference.

321
00:20:26,700 --> 00:20:30,232
So when it comes to machine learning, as you,

322
00:20:30,328 --> 00:20:33,288
I'm sure you must have heard, it's all about the data,

323
00:20:33,416 --> 00:20:36,760
right? So how accurate your machine

324
00:20:36,792 --> 00:20:40,696
learning is, is based on how much data you have, how granular it is,

325
00:20:40,800 --> 00:20:45,288
and how clean it is, and how good it is. So when

326
00:20:45,336 --> 00:20:48,848
let's, as an example, let's think about something like anomaly detection.

327
00:20:49,016 --> 00:20:52,296
So anomaly detection is basically a way in

328
00:20:52,320 --> 00:20:56,308
which you can detect if something, a metric value, for example,

329
00:20:56,416 --> 00:21:00,244
is anomalous or not. Is this something that's expected or is it

330
00:21:00,284 --> 00:21:04,212
unexpected? Now, when you do all of this in a centralized

331
00:21:04,268 --> 00:21:07,940
location, you need to have so much context built

332
00:21:07,972 --> 00:21:11,716
in, because the metric could be coming from a raspberry

333
00:21:11,740 --> 00:21:14,900
PI or it could be coming from an

334
00:21:14,932 --> 00:21:18,044
Nvidia H 100 GPU

335
00:21:18,084 --> 00:21:21,932
rig. And it's a very different environment in

336
00:21:21,948 --> 00:21:25,254
both those cases. So the value of the metric, whether that's

337
00:21:25,294 --> 00:21:28,998
expected, whether that's unexpected. Again, having to do it in

338
00:21:29,006 --> 00:21:32,294
a centralized fashion means that you have to have so much context built

339
00:21:32,334 --> 00:21:35,862
in, and that increases the processing load of what you're trying to

340
00:21:35,878 --> 00:21:39,702
do and

341
00:21:39,838 --> 00:21:43,366
putting together all of these things, what does this lead to? So this leads to

342
00:21:43,390 --> 00:21:46,990
obviously outages, it leads to downtime, and in general, it leads

343
00:21:47,022 --> 00:21:50,734
to pain for DevOps and for sres and developers

344
00:21:50,774 --> 00:21:52,854
who have to deal with this.

345
00:21:56,594 --> 00:22:00,294
And that brings us to the next point, which is resilience.

346
00:22:01,834 --> 00:22:05,218
And this is again, one of those terms which is bandied about a

347
00:22:05,226 --> 00:22:08,578
lot and misused

348
00:22:08,666 --> 00:22:13,066
a lot as well. But again,

349
00:22:13,130 --> 00:22:16,690
this is something that's built into the definition of what

350
00:22:16,722 --> 00:22:20,136
a centralized monitoring solution is. The centralized monitoring

351
00:22:20,160 --> 00:22:23,744
solution has a single point of failure, which is your single server.

352
00:22:23,904 --> 00:22:27,320
Something happens to that server, everything goes down and you have

353
00:22:27,352 --> 00:22:31,044
cascading failures across your infrastructure, which means that

354
00:22:31,384 --> 00:22:33,844
in the worst case, if a disaster happens,

355
00:22:34,264 --> 00:22:37,672
you're left with no way to monitor what you

356
00:22:37,688 --> 00:22:40,724
care about and the recovery time.

357
00:22:41,344 --> 00:22:43,284
There are no guarantees on this either.

358
00:22:45,104 --> 00:22:49,032
Recent example is what happened last year when there

359
00:22:49,048 --> 00:22:52,680
was an outage on datadog service which

360
00:22:52,712 --> 00:22:56,000
took down their monitoring for most of

361
00:22:56,032 --> 00:22:59,364
their customers for many hours. And again,

362
00:22:59,904 --> 00:23:03,808
in these cases, these users were left without a direct way of easily

363
00:23:03,856 --> 00:23:07,536
understanding what was going on. Because you have the centralized view,

364
00:23:07,600 --> 00:23:11,240
all of your data is in one place. Your window into what's

365
00:23:11,272 --> 00:23:15,280
happening and how to observe these systems is that single

366
00:23:15,432 --> 00:23:18,904
point of entry, right? So a single pane of grass is often

367
00:23:18,984 --> 00:23:22,304
touted as a good thing because you get all your information in one place,

368
00:23:22,384 --> 00:23:26,496
and it is, but it also has its drawback if that's the only window

369
00:23:26,560 --> 00:23:30,400
that you have into observability on those infrastructures. Now, if you

370
00:23:30,432 --> 00:23:33,480
had more localized ways into looking at your

371
00:23:33,552 --> 00:23:37,844
individual pieces of your infrastructure, you're centralized

372
00:23:38,184 --> 00:23:41,832
view could be down. It could be down for a day, for example. But as

373
00:23:41,848 --> 00:23:45,804
long as you have the ability to look into those things through other means,

374
00:23:46,184 --> 00:23:49,600
through localized means, you wouldn't be

375
00:23:49,632 --> 00:23:52,244
as impacted as an observability user.

376
00:23:55,784 --> 00:23:59,800
And then we have efficiency. So efficiency

377
00:23:59,832 --> 00:24:03,724
is again a thing that on day one

378
00:24:03,864 --> 00:24:07,540
of your observability journey, maybe you're not

379
00:24:07,612 --> 00:24:11,612
paying a lot of attention to efficiency. But over time, as it

380
00:24:11,708 --> 00:24:15,580
grows, as your observability setup grows along with your infrastructure,

381
00:24:15,692 --> 00:24:19,804
you've added lots of new collectors or exporters, you have

382
00:24:19,884 --> 00:24:21,904
different kinds of data types in there.

383
00:24:22,884 --> 00:24:26,124
This is when the efficiency

384
00:24:26,164 --> 00:24:30,152
gains start becoming more and more important for you. If there

385
00:24:30,168 --> 00:24:33,480
are delays in data processing, if the data handling is

386
00:24:33,512 --> 00:24:37,120
inefficient across your data pipeline, all of this starts to add up

387
00:24:37,152 --> 00:24:40,784
over time. And resource overload

388
00:24:40,824 --> 00:24:44,880
becomes a real challenge, a real issue, because how

389
00:24:44,912 --> 00:24:48,352
much resources do you allocate your monitoring server? And how much

390
00:24:48,368 --> 00:24:50,284
can you scale up when you need to?

391
00:24:51,144 --> 00:24:55,484
And another often underlooked or overlooked

392
00:24:56,674 --> 00:25:00,794
part of this is energy consumption. So you

393
00:25:00,834 --> 00:25:04,014
have it. Infrastructure in general,

394
00:25:04,594 --> 00:25:08,490
I think, takes up around 30% of the total energy consumption

395
00:25:08,522 --> 00:25:10,694
in the world. Today, and that's a lot.

396
00:25:12,514 --> 00:25:16,694
And your observability platform

397
00:25:17,514 --> 00:25:21,362
itself is intended to monitor that you're running

398
00:25:21,458 --> 00:25:24,870
optimally all the time. Now, if your observability platform itself

399
00:25:25,022 --> 00:25:27,758
adds to your energy consumption in a significant way,

400
00:25:27,886 --> 00:25:31,394
then this becomes a problematic scenario to be in.

401
00:25:33,254 --> 00:25:37,142
So I think we've covered six

402
00:25:37,278 --> 00:25:41,238
of the deadly sins and we've landed at the final one, which is data

403
00:25:41,286 --> 00:25:44,774
privacy. So this one's obvious,

404
00:25:44,894 --> 00:25:49,278
right? So this is something that is

405
00:25:49,326 --> 00:25:54,924
often talked about, that large tech companies have

406
00:25:55,744 --> 00:25:59,768
an often unhealthy liking for customer data.

407
00:25:59,936 --> 00:26:03,336
And there are

408
00:26:03,360 --> 00:26:06,564
different ways to look at this problem. So for one,

409
00:26:07,584 --> 00:26:11,764
thinking about centralized systems in general, there is a concentration of risk.

410
00:26:12,624 --> 00:26:17,332
You have a single repository where if

411
00:26:17,348 --> 00:26:20,756
there was an attack that happened there, then the attacker

412
00:26:20,860 --> 00:26:24,332
suddenly gains access to all of your data or all

413
00:26:24,348 --> 00:26:28,316
of the data that exists there. And the

414
00:26:28,340 --> 00:26:31,492
concentration of risk is something that you

415
00:26:31,508 --> 00:26:35,104
should be thinking about more and more when it comes to

416
00:26:36,124 --> 00:26:40,308
your data and your data security. The other

417
00:26:40,356 --> 00:26:42,844
aspect to this is compliance challenges.

418
00:26:43,004 --> 00:26:46,884
So you've heard about your gdprs

419
00:26:46,964 --> 00:26:51,052
and your ctpas and all of these different compliance standards

420
00:26:51,228 --> 00:26:53,864
that your company, your business has to meet,

421
00:26:54,244 --> 00:26:58,060
and you want to understand whether the

422
00:26:58,092 --> 00:27:02,316
tool that you're using for observability is supporting all of these

423
00:27:02,460 --> 00:27:06,476
standards. Now, if it's a centralized

424
00:27:06,620 --> 00:27:11,144
observability tool, which means that they have access to your data, your data

425
00:27:11,304 --> 00:27:14,824
is being stored somewhere, and by your data, it could be your end

426
00:27:14,864 --> 00:27:18,664
user's data, which means it's your customer's data that you're now

427
00:27:18,784 --> 00:27:22,608
storing, in a way, in a third party company side.

428
00:27:22,776 --> 00:27:26,456
So it becomes a question of trust as well. If the company

429
00:27:26,520 --> 00:27:29,888
big enough, maybe you trust them, maybe you're trusting that these large

430
00:27:30,016 --> 00:27:33,896
public listed companies are going to treat this data well, and that's

431
00:27:33,920 --> 00:27:35,234
a choice, right?

432
00:27:36,934 --> 00:27:40,638
And then finally, there's also the question of deployment options.

433
00:27:40,766 --> 00:27:45,246
Maybe you do not want all

434
00:27:45,270 --> 00:27:49,006
of your networks to be exposed, all of your devices or

435
00:27:49,030 --> 00:27:53,094
all of your servers to be exposed to the outside world or exposed

436
00:27:53,134 --> 00:27:56,606
to the centralized monitoring. So you want to have a way in which

437
00:27:56,630 --> 00:28:00,422
you can cordon off certain parts of your network or parts of your infrastructure

438
00:28:00,478 --> 00:28:03,910
into a demillage zone, for example. Are you

439
00:28:03,942 --> 00:28:07,326
able to do this? Are you able to achieve this with your monitoring

440
00:28:07,350 --> 00:28:10,766
solution? This becomes another question that you should be thinking

441
00:28:10,790 --> 00:28:14,406
about. So I think we've

442
00:28:14,430 --> 00:28:18,470
now talked about all of these different problems

443
00:28:18,582 --> 00:28:22,702
and what's the solution. So the one solution that I'm proposing here on this talk

444
00:28:22,798 --> 00:28:27,054
is to decentralize. And what does it mean to decentralized.

445
00:28:27,174 --> 00:28:30,034
So let's try and understand this a little bit better.

446
00:28:30,214 --> 00:28:34,414
So on the left here, you have a centralized network.

447
00:28:35,194 --> 00:28:38,282
As you can see, there is a central authority or

448
00:28:38,298 --> 00:28:42,410
a central node, and all the other nodes are connected

449
00:28:42,522 --> 00:28:45,174
in one way or another to the single authority.

450
00:28:46,434 --> 00:28:50,050
On the right, we have a decentralized network. There is no

451
00:28:50,082 --> 00:28:53,978
single authority server who controls all the nodes. Every single

452
00:28:54,026 --> 00:28:57,610
node has individual identity and entity.

453
00:28:57,802 --> 00:29:01,018
This is really important. So every single node that you see

454
00:29:01,066 --> 00:29:04,210
here on the decentralized network can operate on its

455
00:29:04,242 --> 00:29:07,562
own. It's independent and it's fully capable.

456
00:29:07,738 --> 00:29:11,090
So this is the main difference. Each node is fully capable.

457
00:29:11,242 --> 00:29:15,842
As you can see, there are still many centralization points that can exist

458
00:29:16,018 --> 00:29:19,314
where multiple nodes are connected to a single node,

459
00:29:19,354 --> 00:29:22,554
which means that this node now has access to the data of these other

460
00:29:22,594 --> 00:29:25,894
nodes, and then these nodes could be connected

461
00:29:25,934 --> 00:29:29,326
to each other. And you can have how many number of these connections as you

462
00:29:29,350 --> 00:29:32,750
want. It's up to you. But the really important thing is that

463
00:29:32,822 --> 00:29:36,126
each of those individual nodes are a

464
00:29:36,150 --> 00:29:38,274
capable entity in and of itself.

465
00:29:40,254 --> 00:29:43,966
So what does this mean for the problems

466
00:29:43,990 --> 00:29:47,230
that we talked about, the problems of fidelity, scalability, cost,

467
00:29:47,342 --> 00:29:50,270
accuracy, resilience, efficiency, and data privacy?

468
00:29:50,462 --> 00:29:53,806
So think about it. Let's think about

469
00:29:53,830 --> 00:29:57,214
fidelity. So if we are storing

470
00:29:57,254 --> 00:30:00,394
the data on the individual node itself,

471
00:30:00,734 --> 00:30:04,430
this gives us a lot more option to have higher

472
00:30:04,462 --> 00:30:08,294
fidelity data because you can collect more data, you can collect

473
00:30:08,374 --> 00:30:11,662
data more in a more granular fashion because you're just storing it on

474
00:30:11,678 --> 00:30:15,310
the device itself. Of course, you need to think about if you're storing

475
00:30:15,342 --> 00:30:18,462
it in an efficient way or not, but you're not sending it

476
00:30:18,478 --> 00:30:20,994
to be stored in a centralized server somewhere else.

477
00:30:22,254 --> 00:30:25,790
And decentralized networks are by definition built to be highly

478
00:30:25,822 --> 00:30:29,394
scalable. You can scale them up, you can scale them down as you wish to

479
00:30:30,094 --> 00:30:33,782
and cost. So again, there is no central server,

480
00:30:33,958 --> 00:30:37,654
which is contributing to the cost by being

481
00:30:37,694 --> 00:30:41,766
directly proportional to the fidelity. It's completely decentralized, so you

482
00:30:41,790 --> 00:30:43,834
have the option to keep the costs down.

483
00:30:46,004 --> 00:30:49,396
Higher fidelity is one reason why it could

484
00:30:49,420 --> 00:30:53,508
be more accurate, but also if you want to do things like alerting

485
00:30:53,556 --> 00:30:57,116
or anomaly detection, since each node is individually

486
00:30:57,180 --> 00:31:00,692
capable of doing this, this means that you're doing it on device,

487
00:31:00,748 --> 00:31:04,092
you're doing it on the edge. So if there were alerts being triggered on one

488
00:31:04,108 --> 00:31:07,276
of these nodes, that decision is being taken on the edge,

489
00:31:07,340 --> 00:31:10,604
which means it's, that it's, it's, it's more accurate.

490
00:31:12,824 --> 00:31:16,312
And again, these are architectural definitions of

491
00:31:16,328 --> 00:31:19,664
a decentralized network. Is that it's more resilient by nature,

492
00:31:19,784 --> 00:31:22,736
which means that you can take off one of these things, but the nodes would

493
00:31:22,760 --> 00:31:26,312
still operate. You can break the connections, but the

494
00:31:26,328 --> 00:31:30,204
nodes would again be able to operate or connect to other nodes

495
00:31:30,784 --> 00:31:34,204
as and when needed. The efficiency

496
00:31:34,544 --> 00:31:38,244
is another important factor. So you can cut down on things like

497
00:31:38,864 --> 00:31:42,928
centralized bottlenecks, you can cut down on things like latency

498
00:31:42,976 --> 00:31:46,004
issues by having a decentralized network

499
00:31:46,744 --> 00:31:51,004
and data privacy because you're storing all your data on device,

500
00:31:51,544 --> 00:31:55,080
your data privacy requirements look very different all of a sudden.

501
00:31:55,272 --> 00:31:59,256
You don't even need to worry about a lot of the regulations because

502
00:31:59,360 --> 00:32:03,204
you are not exporting your data to be stored in a third party cloud somewhere.

503
00:32:05,184 --> 00:32:09,128
So there's a lot of advantages to be had from decentralized

504
00:32:09,176 --> 00:32:12,724
networks, and you don't need to be scared about

505
00:32:13,464 --> 00:32:17,160
decentralized being something that's very complex or hard

506
00:32:17,192 --> 00:32:19,044
to understand or hard to deploy.

507
00:32:21,304 --> 00:32:24,688
You know, let's dive into this a

508
00:32:24,696 --> 00:32:27,440
little bit more so that we can understand this better.

509
00:32:27,552 --> 00:32:31,500
So let's talk about decentralized design for high fidelity.

510
00:32:31,612 --> 00:32:35,484
Specifically, the main important aspect

511
00:32:35,524 --> 00:32:39,308
of this is keeping data at the edge. So you have compute

512
00:32:39,396 --> 00:32:43,428
and storage already available on these things that you're monitoring,

513
00:32:43,556 --> 00:32:47,624
whether that's a container, whether that's a virtual machine, or whether that's

514
00:32:47,924 --> 00:32:51,724
a high end server. These things have compute available,

515
00:32:51,844 --> 00:32:55,516
they have storage available, and this is enough to keep the data at the

516
00:32:55,540 --> 00:32:59,006
edge. You can keep the data stored there, and you can also have

517
00:32:59,030 --> 00:33:02,742
the processing happening there, and you can optimize it

518
00:33:02,758 --> 00:33:06,074
in such a way that the monitoring doesn't affect

519
00:33:06,374 --> 00:33:09,794
the actual business logic that needs to operate on those devices.

520
00:33:10,414 --> 00:33:13,694
So keep data at the edge. That's number one. Number two,

521
00:33:13,814 --> 00:33:17,174
make the data highly available across the network, right?

522
00:33:17,334 --> 00:33:20,830
Because you might have ephemeral nodes that are not going to exist

523
00:33:20,862 --> 00:33:24,520
forever. They might come up, they might go down. Once they vanish, you still

524
00:33:24,552 --> 00:33:28,324
need access to their data, which means that their data needs to be stored somewhere.

525
00:33:29,144 --> 00:33:32,684
That's where those other nodes in your decentralized network becomes important.

526
00:33:33,824 --> 00:33:37,680
They also help with higher availability. So if a node goes down, you know that

527
00:33:37,752 --> 00:33:40,884
there's another node which has access to data of this node.

528
00:33:41,304 --> 00:33:45,160
And you can also use this for more flexible deployment scenarios

529
00:33:45,232 --> 00:33:48,400
where you can offload sensitive production systems

530
00:33:48,432 --> 00:33:52,602
from observability work. You can say that I have these ten servers here which

531
00:33:52,618 --> 00:33:56,730
are doing top secret work. I don't want to do any monitoring

532
00:33:56,842 --> 00:34:00,306
logic on this. Just export the data somewhere else,

533
00:34:00,490 --> 00:34:04,494
export the alerting, export the anomaly detection, and do it all elsewhere.

534
00:34:05,754 --> 00:34:09,418
And then number three, you need a way to unify and integrate

535
00:34:09,466 --> 00:34:12,842
all of this at query time. So you have all of this data,

536
00:34:12,978 --> 00:34:16,692
it's stored, it's being processed in a decentralized

537
00:34:16,748 --> 00:34:19,904
fashion across different nodes in different places.

538
00:34:20,324 --> 00:34:23,868
How do you get that single pane of glass

539
00:34:23,916 --> 00:34:27,132
view when you need it? So there has to be

540
00:34:27,148 --> 00:34:30,860
a way that you can unify and integrate everything at query time.

541
00:34:31,052 --> 00:34:35,692
These are some of the challenges also of making decentralized observability

542
00:34:35,788 --> 00:34:39,620
work. Now we'll talk about net

543
00:34:39,652 --> 00:34:43,515
data. Nadata is

544
00:34:43,539 --> 00:34:47,555
the company that I work for. It started off as an open source project

545
00:34:47,699 --> 00:34:50,619
and it became very popular on GitHub.

546
00:34:50,691 --> 00:34:53,183
It has more than 68,000 likes,

547
00:34:53,803 --> 00:34:57,403
stars on GitHub, and people

548
00:34:57,443 --> 00:35:00,983
are using it for all kinds of things. They're using it to monitor

549
00:35:01,763 --> 00:35:05,395
tiny things such as their home labs or their raspberry PI's. But also there

550
00:35:05,419 --> 00:35:09,063
are teams and companies using the data to monitor entire data centers,

551
00:35:09,404 --> 00:35:12,932
kubernetes, clusters, multi cloud environment, high performance

552
00:35:13,028 --> 00:35:16,836
clusters. So really it's completely

553
00:35:16,860 --> 00:35:18,784
up to you how you use net data.

554
00:35:20,324 --> 00:35:23,908
So how does net data aim

555
00:35:23,956 --> 00:35:27,452
to achieve the decentralized philosophy that

556
00:35:27,468 --> 00:35:28,584
we've been talking about?

557
00:35:30,644 --> 00:35:33,964
The main component of the net data monitoring solution

558
00:35:34,004 --> 00:35:38,046
is the net data agent. And I have agent and double quotes here because

559
00:35:38,220 --> 00:35:42,098
the netifier agent is so much more than what normal

560
00:35:42,226 --> 00:35:46,034
monitoring agents are. So it's open source, it collects data

561
00:35:46,074 --> 00:35:49,850
in real time, which means that the granularity is 1 second. By default.

562
00:35:49,922 --> 00:35:53,618
All the metrics are collected per second. So it

563
00:35:53,666 --> 00:35:57,466
auto discovers what's there to monitor in

564
00:35:57,610 --> 00:36:01,122
the environment where it's been installed. And it collects all of this data

565
00:36:01,178 --> 00:36:05,734
every second, and it stores this data in its own time series database

566
00:36:06,744 --> 00:36:09,112
on all of this is open source, so you can look at it if you

567
00:36:09,128 --> 00:36:11,760
want to. And it collects metrics,

568
00:36:11,792 --> 00:36:15,240
analogs, and it also does alerting and the

569
00:36:15,272 --> 00:36:19,144
notification of those alerts are being sent. All of this happens on the agent,

570
00:36:19,304 --> 00:36:22,496
and anomaly detection and machine learning also happens within the

571
00:36:22,520 --> 00:36:26,484
agent at the edge. This is again something that's not very common,

572
00:36:27,224 --> 00:36:30,552
and the agent can also stream data to other agents. So this is where

573
00:36:30,568 --> 00:36:34,490
the decentralized concept comes in. The agent is a fully functioning

574
00:36:34,522 --> 00:36:38,186
entity of itself, but it can also send its data to be stored on

575
00:36:38,210 --> 00:36:40,974
another agent via configuration.

576
00:36:41,914 --> 00:36:45,418
And you can have a cloud which unifies

577
00:36:45,586 --> 00:36:48,970
all of these different agents and gives you the ability to

578
00:36:49,002 --> 00:36:53,042
query from any agent across all agents in real time.

579
00:36:53,178 --> 00:36:55,814
And we'll talk a little bit more about the cloud component.

580
00:36:57,834 --> 00:37:01,270
So this is what distributed metrics pipeline looks

581
00:37:01,302 --> 00:37:04,398
like inside Netdata, it's, you can think of it in

582
00:37:04,406 --> 00:37:08,022
a way like Lego building blocks. So you have local netdata,

583
00:37:08,158 --> 00:37:11,486
it's discovering metrics, it's collecting these metrics and then

584
00:37:11,510 --> 00:37:15,670
it's detecting anomalies on them, it's storing them in the time series database.

585
00:37:15,862 --> 00:37:19,478
And you know, it's checking for alert transitions, it's querying

586
00:37:19,526 --> 00:37:23,094
for anomaly score or correlations and things like this.

587
00:37:23,214 --> 00:37:27,384
And it's also able to visualize this in a dashboard. It's all inside this agent.

588
00:37:27,884 --> 00:37:31,396
But then at the same time it can also collect metrics from

589
00:37:31,420 --> 00:37:34,144
a remote net data, which means another agent.

590
00:37:34,524 --> 00:37:38,084
So this is the decentralized aspect of it where you can plug these

591
00:37:38,124 --> 00:37:42,824
agents together into sort of a Lego creation.

592
00:37:43,884 --> 00:37:47,892
So you can collect data from a remote net data, you can stream all

593
00:37:47,908 --> 00:37:51,860
of this data from both the collected one and the current one to

594
00:37:51,892 --> 00:37:55,266
another remote net data. So it's really up to you

595
00:37:55,290 --> 00:37:58,826
on how you construct this network, this monitoring network of

596
00:37:58,850 --> 00:37:59,534
yours.

597
00:38:03,994 --> 00:38:07,386
And the really important thing which allows Netdata

598
00:38:07,530 --> 00:38:11,050
to deploy this decentralized philosophy is

599
00:38:11,082 --> 00:38:15,066
that the netdata agent is really lightweight, even though

600
00:38:15,090 --> 00:38:18,282
it's highly capable. So we ran a

601
00:38:18,298 --> 00:38:21,688
full, very detailed analysis and I'll share the link

602
00:38:21,776 --> 00:38:25,112
along with this presentation when you can take a look

603
00:38:25,128 --> 00:38:28,536
at it on how this was done. But you have some of the data points

604
00:38:28,600 --> 00:38:31,968
here. You can see that the cpu usage, the memory

605
00:38:32,016 --> 00:38:35,984
usage, the disk usage, and the egress bandwidth that's generated

606
00:38:36,104 --> 00:38:39,816
is all really, really low, even though it's

607
00:38:39,840 --> 00:38:43,392
doing all of those things that we talked about. It's doing the metric collection,

608
00:38:43,448 --> 00:38:46,864
the storage, the alerting, the anomaly detection and machine

609
00:38:46,904 --> 00:38:50,632
learning and the dashboarding. All of that is happening on each agent.

610
00:38:50,768 --> 00:38:54,496
But it's still very light in terms of the number of resources,

611
00:38:54,640 --> 00:38:57,952
and you can configure it to make it lighter still. So if

612
00:38:57,968 --> 00:39:00,604
you say that this is an IoT node,

613
00:39:01,264 --> 00:39:05,120
I want to make sure that it runs super light. Then you can configure

614
00:39:05,152 --> 00:39:08,456
it so that it doesn't run alerting, it doesn't run ML,

615
00:39:08,640 --> 00:39:12,488
it doesn't do any storage, it's just streaming the data to another more powerful

616
00:39:12,536 --> 00:39:14,604
node which does all of those things for it.

617
00:39:16,134 --> 00:39:20,798
And just by installing the data on an empty vm, you get 2000

618
00:39:20,846 --> 00:39:24,294
plus metrics. You get more than 50 pre configured

619
00:39:24,334 --> 00:39:27,486
alerts. There's anomaly detection running for every metric.

620
00:39:27,670 --> 00:39:31,478
And by default, if you just have three gb of

621
00:39:31,566 --> 00:39:34,934
disk space, you get up to two weeks of

622
00:39:34,974 --> 00:39:38,510
this per second real time data. You get three months of per

623
00:39:38,542 --> 00:39:42,790
minute data, and you get two years of per our data. So that's,

624
00:39:42,862 --> 00:39:46,670
you know, in terms of your data retention, that's a pretty

625
00:39:46,702 --> 00:39:47,434
good deal.

626
00:39:50,294 --> 00:39:54,534
Now, we've talked about the net data agent. The other component

627
00:39:54,574 --> 00:39:57,686
to this, which allows this decentralized architecture, is in the

628
00:39:57,710 --> 00:40:01,710
data parent. So net data parents are nothing

629
00:40:01,742 --> 00:40:05,414
but other net data agents which are aggregating data across multiple

630
00:40:05,454 --> 00:40:08,794
children. So you can start to see the decentralized network build

631
00:40:08,834 --> 00:40:12,202
out here. You have three parents. Each parent

632
00:40:12,258 --> 00:40:16,114
has multiple children. So this

633
00:40:16,154 --> 00:40:19,666
parent, for example, has children that are part of a data center.

634
00:40:19,810 --> 00:40:23,386
The other parent has children that are part of a cloud provider, and the

635
00:40:23,410 --> 00:40:26,322
third parent has children that are part of another data center.

636
00:40:26,498 --> 00:40:30,114
And all of these parents could be connected to each other so that they

637
00:40:30,154 --> 00:40:34,032
have access to the data across these three different environments.

638
00:40:34,208 --> 00:40:38,224
Now, having access to these parents or

639
00:40:38,344 --> 00:40:41,124
mini centralization points gives you,

640
00:40:41,984 --> 00:40:45,648
obviously, it gives you enhanced scalability and flexibility, because now you can

641
00:40:45,776 --> 00:40:48,684
really build the Lego blocks into something magnificent.

642
00:40:49,504 --> 00:40:53,336
It ensures that all of the data remains always on Prem.

643
00:40:53,480 --> 00:40:56,324
You're always storing all of your data on your own premises.

644
00:40:57,884 --> 00:41:01,076
And by design, it's resilient and fault

645
00:41:01,100 --> 00:41:05,012
tolerant. You can take out any of these instances, but the other remaining

646
00:41:05,068 --> 00:41:07,344
instances would continue to function on its own.

647
00:41:09,284 --> 00:41:12,692
And this really helps you to build

648
00:41:12,868 --> 00:41:16,276
a monitoring network which is optimized in terms of

649
00:41:16,300 --> 00:41:19,828
performance, in terms of cost, and also if you want to isolate

650
00:41:19,876 --> 00:41:23,716
certain parts of your network from the rest, from your broader network

651
00:41:23,780 --> 00:41:26,874
and from the Internet, it allows you to do this as well.

652
00:41:29,254 --> 00:41:32,822
And the third and final component of Netada's decentralized

653
00:41:32,878 --> 00:41:36,150
architecture is netida cloud. So netdata cloud,

654
00:41:36,222 --> 00:41:39,314
again, cloud in double quotes or air codes,

655
00:41:39,934 --> 00:41:43,558
because it's not a centralized repository.

656
00:41:43,686 --> 00:41:47,478
Netadata cloud does not centralize any observability data. It doesn't store

657
00:41:47,566 --> 00:41:51,362
any data in the cloud. All it

658
00:41:51,378 --> 00:41:54,946
does is it maintains a map of the infrastructure. So the cloud is

659
00:41:54,970 --> 00:41:58,642
the one entity that knows where everybody, all the other nodes, all the parents,

660
00:41:58,698 --> 00:42:02,594
and all the agents are. And it has the ability to

661
00:42:02,634 --> 00:42:06,050
query any of these agents

662
00:42:06,162 --> 00:42:10,146
or all of those agents or any grouping of those agents at

663
00:42:10,210 --> 00:42:13,546
any time, in real time, right? Which means that

664
00:42:13,650 --> 00:42:16,834
I could be just logged into the cloud and say, I want to see

665
00:42:16,874 --> 00:42:20,170
all the data from all the nodes in data center

666
00:42:20,202 --> 00:42:23,458
one and data center two. I don't want to see cloud provider one, or I

667
00:42:23,466 --> 00:42:26,626
could say I want to see all of it together. So the

668
00:42:26,650 --> 00:42:30,034
cloud is able to send this query to these

669
00:42:30,074 --> 00:42:33,774
nodes. And since you have these parent agents.

670
00:42:34,194 --> 00:42:37,738
The cloud doesn't need to query 15 different servers

671
00:42:37,786 --> 00:42:42,290
here, it just needs to query three. So this architecture, this decentralized

672
00:42:42,362 --> 00:42:46,330
architecture keeps things much more efficient in query and

673
00:42:46,362 --> 00:42:49,706
quickly get the data back within a second, because nobody wants to

674
00:42:49,730 --> 00:42:53,114
wait multiple seconds or multiple minutes

675
00:42:53,194 --> 00:42:55,374
to get a dashboard updating.

676
00:42:56,674 --> 00:43:00,314
So the cloud in effect enables horizontal scalability

677
00:43:00,474 --> 00:43:04,322
because you could have any number of these parent

678
00:43:04,378 --> 00:43:07,722
agent clusters, and as long as all of them

679
00:43:07,738 --> 00:43:11,170
are connected to the cloud, it should be relatively easy to just query them within

680
00:43:11,202 --> 00:43:14,220
a second and see the data, which means that you have high

681
00:43:14,252 --> 00:43:18,084
fidelity data across your entire infrastructure. It's super

682
00:43:18,124 --> 00:43:21,932
easy to scale and you have access all of it, access to all

683
00:43:21,948 --> 00:43:25,580
of it from a single central cloud without having

684
00:43:25,612 --> 00:43:28,940
to store your data in the cloud, right? So the cloud is just querying

685
00:43:28,972 --> 00:43:32,784
the data in real time and it's just showing it to you.

686
00:43:37,084 --> 00:43:40,564
So some of the common concerns about decentralized design

687
00:43:40,644 --> 00:43:44,374
are, one, the agent will be really heavy, you have to run this thing

688
00:43:44,994 --> 00:43:48,450
on your servers, on the machines that are hosting your

689
00:43:48,482 --> 00:43:51,298
application. But clearly we saw that, no,

690
00:43:51,386 --> 00:43:54,266
the data agent processes thousands of metrics per second.

691
00:43:54,410 --> 00:43:57,858
It's super light. The second concern

692
00:43:57,906 --> 00:44:01,974
is that querying will increase load on production time on production systems.

693
00:44:02,274 --> 00:44:05,014
So each agent serves only its data,

694
00:44:05,554 --> 00:44:09,370
so the queries do not increase load on the production systems

695
00:44:09,402 --> 00:44:13,034
themselves. Querying for small data sets is lightweight,

696
00:44:13,154 --> 00:44:16,618
and you can use the parent agents

697
00:44:16,706 --> 00:44:20,154
as a centralization point within your decentralized network,

698
00:44:20,314 --> 00:44:23,602
so that certain nodes are isolated from query. The queries do

699
00:44:23,618 --> 00:44:27,626
not even reach them. The third concern is that the queries will

700
00:44:27,650 --> 00:44:31,134
be slower. This isn't the case either.

701
00:44:31,434 --> 00:44:35,194
Actually the queries are faster because we're distributing tiny

702
00:44:35,234 --> 00:44:38,494
queries in parallel, massively to multiple systems,

703
00:44:38,944 --> 00:44:42,640
which means that your refresh times and your load times are

704
00:44:42,712 --> 00:44:46,432
much, much better. And the final thing is that it'll require

705
00:44:46,488 --> 00:44:50,352
more bandwidth. But this is again not true, because the querying is selected,

706
00:44:50,528 --> 00:44:53,456
you're only querying for data that you're seeing on the screen.

707
00:44:53,560 --> 00:44:57,392
It doesn't need to query for all the 2000 metrics that it's collecting,

708
00:44:57,528 --> 00:45:01,040
it just needs to query for what the user is looking at right now.

709
00:45:01,152 --> 00:45:04,160
And if the user goes to a different chart or a different dashboard, then queries

710
00:45:04,192 --> 00:45:05,404
for that instead.

711
00:45:09,824 --> 00:45:14,244
So this is a quick look at the Netdata dashboard.

712
00:45:14,904 --> 00:45:18,344
I'm not going to go into a detailed demo. We have a public demo

713
00:45:18,384 --> 00:45:23,072
space that's available that you can check out yourself from our website without

714
00:45:23,128 --> 00:45:25,960
even logging in. But if you want to log in, if you want to create

715
00:45:25,992 --> 00:45:30,942
a login, then you get access to our

716
00:45:31,088 --> 00:45:34,234
a space in netadda where you can copy a single

717
00:45:34,274 --> 00:45:38,122
command. And when you base that command, it automatically installs the Netadata

718
00:45:38,178 --> 00:45:41,770
agent on your server, on your device. And you get

719
00:45:41,802 --> 00:45:45,306
this dashboard, which is out of the box, right? So this is not a custom

720
00:45:45,370 --> 00:45:49,194
dashboard or a created dashboard, it's what you get immediately

721
00:45:49,234 --> 00:45:53,226
after you install netat on your system. And here

722
00:45:53,250 --> 00:45:56,426
you can see that the data that's coming in is

723
00:45:56,450 --> 00:46:00,048
from 16 nodes, it's across two labels

724
00:46:00,136 --> 00:46:04,104
from 16 different systems, and all of that data is

725
00:46:04,144 --> 00:46:08,208
stored on those nodes, on those systems in completely decentralized

726
00:46:08,256 --> 00:46:12,592
fashion. And it's being queried by the cloud in real time without

727
00:46:12,648 --> 00:46:14,844
any of that data being stored on the cloud.

728
00:46:16,064 --> 00:46:19,784
So I would welcome you to

729
00:46:19,904 --> 00:46:23,650
explore how decentralized monitoring looks and feels like

730
00:46:23,752 --> 00:46:27,074
by trying out net data and in general,

731
00:46:28,934 --> 00:46:32,394
think about how to make your own monitoring

732
00:46:32,694 --> 00:46:36,754
setups more decentralized, even if you're not using nitate.

733
00:46:38,734 --> 00:46:40,754
So where does this take us to?

734
00:46:42,134 --> 00:46:45,154
The last section is about the future, about the road ahead.

735
00:46:47,014 --> 00:46:51,494
So what's the catch? Where are all the other decentralized observability

736
00:46:51,574 --> 00:46:54,924
platforms? So part of the reason

737
00:46:54,964 --> 00:46:58,564
for this is that creating a decentralized observability

738
00:46:58,644 --> 00:47:00,264
platform is not easy.

739
00:47:01,884 --> 00:47:05,388
Changing from a centralized architecture to a decentralized

740
00:47:05,436 --> 00:47:09,084
architecture is even harder because you put

741
00:47:09,124 --> 00:47:12,452
all your eggs into the centralized basket and you

742
00:47:12,468 --> 00:47:15,508
don't really want to change. Even if you do,

743
00:47:15,556 --> 00:47:19,212
it's not easy to do because you have to ensure that resource consumption at

744
00:47:19,228 --> 00:47:22,492
the edge is minimal. You have to

745
00:47:22,508 --> 00:47:25,812
handle complex queries and aggregation. And all the

746
00:47:25,828 --> 00:47:28,944
while the deployment has to be really simple. Right?

747
00:47:29,804 --> 00:47:33,324
And this is something that's hard for a lot of commercial companies to do.

748
00:47:33,404 --> 00:47:37,100
You have to learn to relinquish control. You have to say that I'm

749
00:47:37,132 --> 00:47:40,956
okay with not having control over the data or

750
00:47:40,980 --> 00:47:45,020
over the processing. It all happens on the customer's own promises, on the

751
00:47:45,052 --> 00:47:49,214
user's, on premises. And this is not an easy thing to do.

752
00:47:49,514 --> 00:47:53,178
So this is part, or maybe a big part

753
00:47:53,226 --> 00:47:57,154
of why we're not seeing more decentralized observability platform.

754
00:47:57,314 --> 00:48:00,854
And also, like I said, the big players in the industry

755
00:48:01,954 --> 00:48:05,522
will find it really hard to move away from their existing architecture to

756
00:48:05,538 --> 00:48:08,734
do something like a decentralized monitoring solution.

757
00:48:10,994 --> 00:48:14,906
I believe that the future is decentralized and

758
00:48:14,930 --> 00:48:17,802
that hard problems can be solved,

759
00:48:17,858 --> 00:48:21,354
and they should be solved. I would

760
00:48:21,394 --> 00:48:25,178
ask all of the listeners to not

761
00:48:25,306 --> 00:48:29,514
compromise infidelity, because it will compromising

762
00:48:29,554 --> 00:48:32,854
infidelity will only create more problems for you in the long term.

763
00:48:34,594 --> 00:48:38,522
I would ask you to demand more and demand better from your observability provider,

764
00:48:38,578 --> 00:48:41,678
who, whoever that is. And if you're operating your

765
00:48:41,726 --> 00:48:45,286
own monitoring stack, then try to

766
00:48:45,310 --> 00:48:49,230
apply some of the decentralized principles that we talked about in this talk today

767
00:48:49,422 --> 00:48:51,954
and you would see a long term benefit.

768
00:48:53,214 --> 00:48:56,974
And think about when your

769
00:48:57,014 --> 00:49:00,614
environment, when your infrastructure is distributed. It's multi

770
00:49:00,654 --> 00:49:04,314
cloud, it's hybrid, it's auto scaling environments.

771
00:49:04,494 --> 00:49:06,694
Why is your observability centralized?

772
00:49:07,394 --> 00:49:10,826
Why is it not decentralized? That should be the question that you're

773
00:49:10,850 --> 00:49:14,642
asking yourself. So thank

774
00:49:14,658 --> 00:49:17,534
you so much for listening to this talk.

775
00:49:19,034 --> 00:49:22,114
If you have any questions about. Net data, or if you'd like to find out

776
00:49:22,154 --> 00:49:25,258
more, this is our website that you can visit.

777
00:49:25,426 --> 00:49:28,986
And here's the link to our GitHub page where you can download the

778
00:49:29,010 --> 00:49:32,818
open source. Net data agent and you can run it on whatever

779
00:49:32,866 --> 00:49:35,906
system that you have and get immediate access to

780
00:49:35,970 --> 00:49:39,170
thousands of metrics in a decentralized way.

781
00:49:39,322 --> 00:49:42,734
So I really hope that you try this out.

782
00:49:43,114 --> 00:49:46,654
And if you have any questions, if you have any suggestions,

783
00:49:47,034 --> 00:49:50,370
or if you have any disagreements about anything that I've spoke about

784
00:49:50,442 --> 00:49:54,066
on this talk, here's my email id and my

785
00:49:54,090 --> 00:49:57,362
LinkedIn profile as well. So I'd love

786
00:49:57,378 --> 00:50:00,522
to hear some feedback. Thanks for listening.

787
00:50:00,578 --> 00:50:03,994
Thanks for being a good audience. Thank you and have a good day.

