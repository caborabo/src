1
00:00:20,650 --> 00:00:24,266
Hi, my name is Jay Ron Plumade, and I'm an AI researcher

2
00:00:24,298 --> 00:00:28,562
based in Chicago. Though I'll begin by introducing

3
00:00:28,626 --> 00:00:32,214
visual generative AI at a technical level, and I'll then talk

4
00:00:32,252 --> 00:00:35,862
about several interesting papers that have been published in the last twelve

5
00:00:35,916 --> 00:00:39,370
months that really demonstrate the capabilities of visual generative AI.

6
00:00:39,950 --> 00:00:43,270
Finally, I'll discuss its future implications,

7
00:00:43,350 --> 00:00:47,366
its current limitations, and several ways researchers are preparing

8
00:00:47,398 --> 00:00:51,520
for the rise of photorealistic generative AI. Let's begin.

9
00:00:53,810 --> 00:00:56,190
Let me give you a brief introduction.

10
00:00:56,770 --> 00:01:00,030
Diffusion models are a powerful class of

11
00:01:00,100 --> 00:01:04,018
generative models that are capable of producing high quality

12
00:01:04,184 --> 00:01:07,362
images, and these implementations have

13
00:01:07,416 --> 00:01:10,658
broad applications in art, design and

14
00:01:10,824 --> 00:01:14,382
related fields. The most popular

15
00:01:14,446 --> 00:01:17,670
consumer diffusion models on the market currently are

16
00:01:17,740 --> 00:01:21,974
dolly three, mid journey and stable diffusion in

17
00:01:22,012 --> 00:01:25,074
seconds. These powerful models can input

18
00:01:25,122 --> 00:01:28,070
text and output high quality images.

19
00:01:29,290 --> 00:01:33,170
And most recently, OpenAI has released Sora,

20
00:01:33,250 --> 00:01:36,954
which is a text to video platform that really blows away

21
00:01:37,072 --> 00:01:40,206
any previous models. This was actually so

22
00:01:40,228 --> 00:01:44,062
recent, in fact, that I had to modify a decent portion of my

23
00:01:44,116 --> 00:01:47,806
slideshow and the talk so

24
00:01:47,988 --> 00:01:51,120
what are diffusion models and how do they work?

25
00:01:52,770 --> 00:01:56,194
Diffusion essentially boils down to two steps. We have

26
00:01:56,232 --> 00:01:59,790
a forward process that gradually adds noise to an input image,

27
00:01:59,870 --> 00:02:03,054
and then we have a reverse process that attempts to remove

28
00:02:03,102 --> 00:02:04,770
noise from an input image.

29
00:02:06,170 --> 00:02:09,366
So we begin with an input image that we'll call

30
00:02:09,468 --> 00:02:12,642
x zero x zero. This is the original image,

31
00:02:12,706 --> 00:02:15,080
or the image at step zero.

32
00:02:16,410 --> 00:02:20,006
We then add noise to our original image

33
00:02:20,038 --> 00:02:24,300
to form x one, the image at step one.

34
00:02:24,910 --> 00:02:29,114
We can do this again and again. In this case, we form x

35
00:02:29,152 --> 00:02:32,510
two until we get random noise.

36
00:02:33,010 --> 00:02:36,254
And a value known as the schedule controls the rate

37
00:02:36,292 --> 00:02:40,074
at which noise is added. So there have been multiple approaches

38
00:02:40,122 --> 00:02:44,254
used. But the most straightforward approach is a linear schedule,

39
00:02:44,382 --> 00:02:48,034
which adds the same amount of noise in each step. There are also

40
00:02:48,152 --> 00:02:51,458
non linear schedules that change

41
00:02:51,624 --> 00:02:55,654
the rate at which noise is added depending on where

42
00:02:55,692 --> 00:02:59,078
you are in

43
00:02:59,084 --> 00:03:02,662
the forward process. So most

44
00:03:02,716 --> 00:03:07,330
notably, researchers at OpenAI used a nonlinear model that

45
00:03:07,500 --> 00:03:11,142
added a lot of noise at the beginning and then gradually decreased

46
00:03:11,286 --> 00:03:14,890
tapered off as it approached the final time step.

47
00:03:15,040 --> 00:03:19,450
And this really improved the

48
00:03:19,520 --> 00:03:24,030
effectiveness of the diffusion model by

49
00:03:24,100 --> 00:03:26,400
preserving the image quality for longer.

50
00:03:27,490 --> 00:03:30,858
And this process forms a Markov chain,

51
00:03:31,034 --> 00:03:34,330
wherein the image at any given time only depends on

52
00:03:34,340 --> 00:03:38,194
the previous image. In the reverse process, we start

53
00:03:38,232 --> 00:03:42,546
with an input image from some time step, let's say x two,

54
00:03:42,728 --> 00:03:45,790
and this image is fed into the diffusion model, which essentially

55
00:03:45,870 --> 00:03:49,510
predicts the noise that separates this image from the original image.

56
00:03:49,930 --> 00:03:53,814
So we feed this into the diffusion model, and we get a

57
00:03:53,852 --> 00:03:57,218
prediction of noise. We can then subtract this noise

58
00:03:57,314 --> 00:04:00,650
to get some approximation of the original image,

59
00:04:01,790 --> 00:04:05,750
and we want to get a better approximation

60
00:04:05,830 --> 00:04:09,698
of the original image. So counterintuitively,

61
00:04:09,894 --> 00:04:13,598
we add some noise back to

62
00:04:13,684 --> 00:04:17,818
get to a step between the original image

63
00:04:17,914 --> 00:04:21,502
and our original input. Let's say

64
00:04:21,556 --> 00:04:25,090
x one, and then once again we

65
00:04:25,240 --> 00:04:28,946
predict the noise that

66
00:04:29,048 --> 00:04:32,338
separates x one from x zero, which is the

67
00:04:32,344 --> 00:04:36,262
original image. Once we subtract this value again, we're left

68
00:04:36,316 --> 00:04:40,918
with an even better approximation of our input. Image and

69
00:04:41,084 --> 00:04:44,962
diffusion models through this process enable nearly

70
00:04:45,026 --> 00:04:47,880
every modern visual generative AI on the market.

71
00:04:48,510 --> 00:04:52,442
And used in combination with other neural components like text

72
00:04:52,496 --> 00:04:56,282
encoders, this forms the basis of dolly three

73
00:04:56,336 --> 00:04:59,926
and stable diffusion. And imagine which are some of the

74
00:04:59,968 --> 00:05:03,600
most powerful visual generative AI platforms currently.

75
00:05:06,050 --> 00:05:08,400
Let's talk about the current research.

76
00:05:10,290 --> 00:05:13,298
Nvidia is popping up everywhere these days,

77
00:05:13,384 --> 00:05:17,630
and really they got into visual generative AI and AI

78
00:05:17,710 --> 00:05:20,862
more generally because their hardware is highly specialized

79
00:05:20,926 --> 00:05:24,530
for AI. Gpus are essential to training large

80
00:05:24,600 --> 00:05:28,146
AI models, and Nvidia is, of course, a GPU

81
00:05:28,178 --> 00:05:32,146
manufacturer. Nvidia Picasso

82
00:05:32,178 --> 00:05:35,686
is a platform that allows the creation of custom

83
00:05:35,788 --> 00:05:38,954
generative AI for visual content. So this

84
00:05:38,992 --> 00:05:42,294
includes images, videos, 3d models,

85
00:05:42,342 --> 00:05:46,074
and 360 degree content. And this

86
00:05:46,112 --> 00:05:49,258
allows enterprise users to train

87
00:05:49,344 --> 00:05:53,610
custom visual generative AI models through the Nvidia edify

88
00:05:53,690 --> 00:05:57,466
platform. So, for example, a stock photo

89
00:05:57,498 --> 00:06:01,514
company could train an edify model on their current library to automatically

90
00:06:01,562 --> 00:06:04,820
generative AI stock content. And in fact,

91
00:06:05,190 --> 00:06:08,974
several such companies have entered partnerships

92
00:06:09,022 --> 00:06:13,182
with Nvidia. So, for example, Shutterstock, Getty Images, and Adobe

93
00:06:13,246 --> 00:06:17,160
have been the earliest adopters of the Nvidia Picasso platform.

94
00:06:20,330 --> 00:06:23,542
So I'll first give some background on

95
00:06:23,596 --> 00:06:26,310
neural radiance fields, or neRfs.

96
00:06:26,650 --> 00:06:30,014
A NERF is a neural network that can synthesize

97
00:06:30,082 --> 00:06:33,402
views of complex 3d scenes from

98
00:06:33,456 --> 00:06:37,066
just a partial set of other

99
00:06:37,088 --> 00:06:40,414
words. It can predict angles of a 3d scene that it was not

100
00:06:40,452 --> 00:06:44,122
trained on. And this is a very powerful and versatile

101
00:06:44,186 --> 00:06:47,146
method for 3d scenes representation.

102
00:06:47,338 --> 00:06:51,354
It works by taking input images and interpolating

103
00:06:51,402 --> 00:06:54,814
between them to create a complete scene. So a NERF

104
00:06:54,862 --> 00:06:58,350
is trained to map directly from a 5d input,

105
00:06:58,510 --> 00:07:02,098
which is three coordinates in x, y and z,

106
00:07:02,184 --> 00:07:05,170
and then two coordinates for viewing direction,

107
00:07:06,470 --> 00:07:09,918
and it outputs opacity and color. So this is a 4d

108
00:07:09,944 --> 00:07:13,110
output, three color channels, and opacity.

109
00:07:14,090 --> 00:07:17,962
And what I'm showing you right now is a project called live

110
00:07:18,016 --> 00:07:22,042
portrait 3d, which is a project by Nvidia that

111
00:07:22,096 --> 00:07:26,022
presents a new method to render photorealistic 3d facial

112
00:07:26,086 --> 00:07:29,730
representations from just a single face portrait.

113
00:07:29,910 --> 00:07:34,026
And it does this in real time. So, given a single rgb

114
00:07:34,138 --> 00:07:38,302
input, their image encoder directly produces a

115
00:07:38,436 --> 00:07:41,646
Nerf of the target face. And this

116
00:07:41,668 --> 00:07:45,214
method is quite fast. So they achieved 24 frames

117
00:07:45,262 --> 00:07:48,990
per second on consumer hardware, and it produces

118
00:07:49,070 --> 00:07:52,350
much higher quality results than previous methods.

119
00:07:52,510 --> 00:07:56,146
So what you're seeing right now is an input

120
00:07:56,178 --> 00:08:00,278
image on the left, and the right is an output model

121
00:08:00,444 --> 00:08:03,480
from the LP 3D model.

122
00:08:04,410 --> 00:08:08,970
And so this is just an example of current

123
00:08:09,040 --> 00:08:11,660
research. Moving on.

124
00:08:12,270 --> 00:08:16,214
This project is a system that learns physically simulated

125
00:08:16,262 --> 00:08:19,850
tennis skills from large scale demonstrations of real tennis play.

126
00:08:20,000 --> 00:08:23,406
And this is harvested at a large scale from broadcast videos.

127
00:08:23,588 --> 00:08:26,974
So the approach is based on hierarchical models that

128
00:08:27,012 --> 00:08:30,346
combines several policy networks to steer the character

129
00:08:30,458 --> 00:08:34,242
in a motion embedding learned from these broadcast videos. And the system

130
00:08:34,296 --> 00:08:37,726
can learn complex tennis skills and realistically chain together multiple

131
00:08:37,758 --> 00:08:40,130
shots into extended rallies.

132
00:08:41,030 --> 00:08:45,306
Importantly, this model only uses simpler rewards and doesn't require

133
00:08:45,438 --> 00:08:48,934
explicit annotations on stroke types and movements. That's what makes

134
00:08:48,972 --> 00:08:53,750
this so powerful, it reduces dependency on human annotators.

135
00:08:54,170 --> 00:08:57,838
And this model is applicable to other motion capture applications

136
00:08:57,874 --> 00:09:02,054
as well. It also works despite the relatively

137
00:09:02,102 --> 00:09:06,300
low quality of motion extracted from the broadcast videos.

138
00:09:06,670 --> 00:09:10,654
And the end result of this is that the

139
00:09:10,692 --> 00:09:14,362
system can synthesize two completely simulated

140
00:09:14,426 --> 00:09:17,946
characters playing extended tennis rallies with realistic

141
00:09:17,978 --> 00:09:21,950
physics. Diffusion models really struggle with realistic physics,

142
00:09:22,710 --> 00:09:24,930
and more on that shortly.

143
00:09:32,790 --> 00:09:36,726
So, aside from image and video generation, diffusion models have

144
00:09:36,748 --> 00:09:40,278
also been used in a variety of applications outside

145
00:09:40,364 --> 00:09:43,138
of this domain. So very recently,

146
00:09:43,234 --> 00:09:47,670
researchers at Singh Hua University have used diffusion models to generate

147
00:09:48,190 --> 00:09:51,354
3d models. So, in other words,

148
00:09:51,472 --> 00:09:55,562
each of the models pictured here can be moved around and

149
00:09:55,616 --> 00:09:58,010
manipulated in six degrees of freedom.

150
00:10:00,510 --> 00:10:04,398
So what are the future implications? For the

151
00:10:04,404 --> 00:10:07,946
sake of demonstration, I'm going to use videos generated by Sora,

152
00:10:07,978 --> 00:10:11,438
because they are by far the most advanced videos currently available.

153
00:10:11,604 --> 00:10:14,962
So what does the future look like with such powerful AI now

154
00:10:15,016 --> 00:10:18,466
becoming so readily available? And how does this affect the

155
00:10:18,488 --> 00:10:22,206
various aspects of the media we consume? Maybe this marks

156
00:10:22,238 --> 00:10:26,146
the end of an entire tier of digital creators, or maybe it simply

157
00:10:26,178 --> 00:10:29,670
augments the capabilities of skilled videographers.

158
00:10:32,090 --> 00:10:35,746
We've seen the rise of several innovations that really shifted the landscape

159
00:10:35,778 --> 00:10:39,306
of entire industries, the most impactful of which is likely sitting in your hand

160
00:10:39,328 --> 00:10:42,438
or pocket if it's not off chart. And visual.

161
00:10:42,534 --> 00:10:45,686
Generative AI could very well follow the paradigm established

162
00:10:45,718 --> 00:10:49,686
by smartphones of the last two or so decades. By placing

163
00:10:49,718 --> 00:10:53,038
a camera in the hands of pretty much every single person on the

164
00:10:53,044 --> 00:10:57,262
planet, it significantly reduced the barrier of entry for

165
00:10:57,316 --> 00:11:01,198
photography. And similarly, it could very well be the case that

166
00:11:01,284 --> 00:11:05,742
videography and even 3d modeling continues along this trajectory without necessarily

167
00:11:05,886 --> 00:11:08,830
displacing a significant portion of the industry.

168
00:11:08,990 --> 00:11:12,510
But what about licensing drone stock footage?

169
00:11:12,670 --> 00:11:16,870
We've seen that Sora is capable of generating highly realistic

170
00:11:17,770 --> 00:11:21,366
that mimics the sort of footage that you would get from a

171
00:11:21,388 --> 00:11:23,320
drone that is several hundred dollars.

172
00:11:24,890 --> 00:11:27,590
So here's another parallel with Photoshop.

173
00:11:28,330 --> 00:11:31,546
Before Photoshop, it was much easier to accept the

174
00:11:31,568 --> 00:11:35,066
credibility of images to take them at face value.

175
00:11:35,248 --> 00:11:38,806
And at least for now, it's possible to find weaknesses in generates

176
00:11:38,838 --> 00:11:42,366
video, especially those that involve object interactions and

177
00:11:42,388 --> 00:11:45,722
realistic physics. This AI

178
00:11:45,786 --> 00:11:49,086
content isn't really fooling anyone. No one is going to

179
00:11:49,108 --> 00:11:54,046
see this particular video and think this

180
00:11:54,068 --> 00:11:57,506
is real content. We can see the chair morphing and

181
00:11:57,528 --> 00:12:00,610
floating around, but within the next couple of years,

182
00:12:00,760 --> 00:12:04,674
it will be very hard, if not impossible, to discern these

183
00:12:04,712 --> 00:12:07,190
sorts of images with the naked eye.

184
00:12:07,770 --> 00:12:11,554
So it will prompt the creation of several important forensics

185
00:12:11,602 --> 00:12:14,470
tools and more on that shortly.

186
00:12:16,490 --> 00:12:19,514
We can also see a few strange artifacts here with,

187
00:12:19,632 --> 00:12:23,450
for example, a flame that doesn't move with the wind,

188
00:12:23,600 --> 00:12:27,274
wax that morphs downwards as opposed to

189
00:12:27,472 --> 00:12:31,514
just creating, and a character that doesn't really react

190
00:12:31,562 --> 00:12:32,910
to touching a flame.

191
00:12:35,170 --> 00:12:39,386
So we could also see other cultural shifts,

192
00:12:39,418 --> 00:12:42,894
like being desensitized to AI content. This is

193
00:12:42,932 --> 00:12:46,642
predictable because it happens with everything. And actually,

194
00:12:46,696 --> 00:12:50,014
I lied about this video here. This is a drone

195
00:12:50,062 --> 00:12:53,630
video. This was collected by a drone over the city of Chicago.

196
00:12:53,790 --> 00:12:57,334
And the odd thing is, it doesn't really seem out of place

197
00:12:57,372 --> 00:13:00,870
with the quality brought by advanced generative AI.

198
00:13:01,290 --> 00:13:04,310
So what does that mean for the industry?

199
00:13:07,610 --> 00:13:11,334
So let's go over some of the current limitations of visual generative

200
00:13:11,382 --> 00:13:14,826
AI, and I'd encourage you to keep this in

201
00:13:14,848 --> 00:13:18,026
mind, but stay updated, because I'd expect that many of

202
00:13:18,048 --> 00:13:21,294
these issues will have been vastly improved even within the next year

203
00:13:21,332 --> 00:13:22,080
or two.

204
00:13:24,210 --> 00:13:28,270
So why are we staring at several images of ramen?

205
00:13:29,170 --> 00:13:32,474
I asked Dolly several times to generate

206
00:13:32,522 --> 00:13:36,002
an image of ramen without chopsticks, and it's completely

207
00:13:36,136 --> 00:13:40,340
unable to. And I'd encourage you to try this yourself.

208
00:13:41,270 --> 00:13:44,898
That seems pretty odd at first glance, but it really makes sense when

209
00:13:44,904 --> 00:13:47,750
you consider the underlying diffusion architecture.

210
00:13:49,130 --> 00:13:53,366
This Dali model is highly reliant on training data,

211
00:13:53,548 --> 00:13:56,710
and in this case, it appears that the training data

212
00:13:56,860 --> 00:14:00,918
has chopsticks in every image of ramen.

213
00:14:01,094 --> 00:14:04,874
So in this case, Dali is actually unable to create an

214
00:14:04,912 --> 00:14:08,874
image without chopsticks somehow making their way

215
00:14:08,912 --> 00:14:13,114
into the image. And this begs the question, can AI truly

216
00:14:13,162 --> 00:14:16,526
be creative? Or does it just rely on the creativity of

217
00:14:16,548 --> 00:14:19,742
its training data? And this is more relevant when

218
00:14:19,796 --> 00:14:23,346
considering the ongoing battle between large AI companies

219
00:14:23,448 --> 00:14:27,266
and individual artists, artists are unhappy because of

220
00:14:27,288 --> 00:14:31,026
the usage of their artwork. And as far as

221
00:14:31,048 --> 00:14:35,098
I know, human artists don't mindlessly synthesize images they've

222
00:14:35,134 --> 00:14:38,786
seen before. I don't have any radically different ideas

223
00:14:38,818 --> 00:14:42,454
on this particular debate, but it's definitely worth taking into account the

224
00:14:42,492 --> 00:14:44,870
underlying mechanics of diffusion models.

225
00:14:46,890 --> 00:14:50,806
In this last scenario, I gave Dolly the task of illustrating Winograd

226
00:14:50,838 --> 00:14:54,582
schemas, and these are basically phrases that test Dolly's

227
00:14:54,646 --> 00:14:58,890
ability to interpret and use context clues.

228
00:14:59,230 --> 00:15:03,262
So Dolly was actually able to get several of these, which surprised me,

229
00:15:03,316 --> 00:15:07,146
but I basically told Dolly to describe

230
00:15:07,258 --> 00:15:10,686
the following scenarios. So, in the top

231
00:15:10,788 --> 00:15:14,254
left, the scenario I used was I dropped a bowling

232
00:15:14,302 --> 00:15:18,254
ball on a table, and it broke. This requires understanding

233
00:15:18,302 --> 00:15:21,010
of bowling balls, tables,

234
00:15:22,550 --> 00:15:26,054
and what it means for this interaction. So,

235
00:15:26,092 --> 00:15:29,542
obviously, if I asked a human to

236
00:15:29,596 --> 00:15:33,320
tell me, to explain to me what this means,

237
00:15:34,090 --> 00:15:37,138
they'll probably tell me that bowling balls are heavy,

238
00:15:37,314 --> 00:15:40,858
tables can be fragile, and especially when you drop a

239
00:15:40,864 --> 00:15:44,186
bowling ball from a height, it's very possible that

240
00:15:44,208 --> 00:15:48,342
the table breaks. But that requires understanding

241
00:15:48,406 --> 00:15:50,974
of these objects and the interactions at play.

242
00:15:51,092 --> 00:15:54,846
And this is actually very impressive for a

243
00:15:54,868 --> 00:15:58,574
visual, generative AI model, because especially Dolly one

244
00:15:58,612 --> 00:16:01,886
and two struggled with this. In the

245
00:16:01,908 --> 00:16:05,358
top right, I used the phrase, my trophy couldn't

246
00:16:05,374 --> 00:16:08,100
fit into the brown suitcase because it was too large.

247
00:16:08,550 --> 00:16:12,478
Once again, this requires understanding of suitcases

248
00:16:12,574 --> 00:16:13,810
and objects.

249
00:16:15,850 --> 00:16:19,366
Really, I think chat GBT is serving as

250
00:16:19,388 --> 00:16:22,470
an intermediary here, sort of guiding Dali.

251
00:16:23,530 --> 00:16:27,270
So it's actually perhaps reducing

252
00:16:28,170 --> 00:16:31,422
the computational load on specifically Dali,

253
00:16:31,586 --> 00:16:34,310
but this is still pretty impressive.

254
00:16:34,470 --> 00:16:38,522
So, in the left, bottom left,

255
00:16:38,656 --> 00:16:42,266
I use the phrase, the large ball crashed right through the

256
00:16:42,288 --> 00:16:45,454
table because it was made of styrofoam. And this is really

257
00:16:45,492 --> 00:16:48,766
the first one that I saw that Dolly struggled with.

258
00:16:48,948 --> 00:16:52,566
It appears that everything in this room is made of styrofoam,

259
00:16:52,618 --> 00:16:57,060
the table, the ball. So that doesn't really make sense.

260
00:16:57,750 --> 00:17:01,122
This is similar

261
00:17:01,176 --> 00:17:04,558
to the first prompt, but it incorporated materials,

262
00:17:04,654 --> 00:17:08,086
different materials, and it looks like Dolly is still struggling with

263
00:17:08,108 --> 00:17:11,254
that a little bit. But the fourth one

264
00:17:11,452 --> 00:17:15,014
surprised me by far the most. The phrase I used was

265
00:17:15,132 --> 00:17:18,426
the painting in Mark's living room shows an oak tree. It is

266
00:17:18,448 --> 00:17:22,634
to the right of a house. So this is

267
00:17:22,752 --> 00:17:26,442
a very complex sentence. It's probably

268
00:17:26,496 --> 00:17:28,730
a little bit confusing, even for humans,

269
00:17:29,870 --> 00:17:33,662
because it refers to several objects and

270
00:17:33,716 --> 00:17:37,326
their relative positions. The painting in Mark's living room

271
00:17:37,508 --> 00:17:40,590
chose an oak tree. It is to the right of a house,

272
00:17:40,740 --> 00:17:43,986
and it looks like Dolly nailed this. And I

273
00:17:44,008 --> 00:17:46,260
was very surprised to see this.

274
00:17:47,110 --> 00:17:50,562
So Dolly 4 may in fact mark

275
00:17:50,616 --> 00:17:53,390
the end of gullible visual generative AI,

276
00:17:53,550 --> 00:17:56,998
but I suppose we'll have to wait and see. The rate at

277
00:17:57,004 --> 00:18:02,626
which is advancing is remarkable. And let's

278
00:18:02,658 --> 00:18:06,710
see how researchers are planning to navigating AI growth.

279
00:18:07,950 --> 00:18:11,882
So along with the rise of visual generative AI, we also

280
00:18:11,936 --> 00:18:16,490
see a natural increase in the spread of misinformation and fraudulent activity.

281
00:18:16,910 --> 00:18:20,554
So, for example, security footage and evidence

282
00:18:20,602 --> 00:18:24,474
tampering. This could require additional forensic tools specifically

283
00:18:24,522 --> 00:18:28,174
designed to mitigate generative AI. If I asked

284
00:18:28,292 --> 00:18:32,206
sora to create security footage of a

285
00:18:32,228 --> 00:18:35,570
bank robbery, that could be very dangerous.

286
00:18:35,910 --> 00:18:40,018
And I'm sure OpenAI will impose restrictions and

287
00:18:40,104 --> 00:18:43,566
use chat GBT as a bouncer, sort of filtering

288
00:18:43,598 --> 00:18:48,230
the inputs that it's allowed to take in. But nonetheless,

289
00:18:49,050 --> 00:18:52,434
I am excited to see the capabilities

290
00:18:52,482 --> 00:18:56,006
of sora and the lengths to

291
00:18:56,028 --> 00:18:59,906
which people will go to break the safety protocols

292
00:18:59,938 --> 00:19:01,750
that OpenAI is decoding.

293
00:19:02,910 --> 00:19:06,330
So, on a slightly more lighthearted note,

294
00:19:07,070 --> 00:19:11,114
advertisements for fake products could lead to widespread

295
00:19:11,162 --> 00:19:15,018
scams, as well as false information or propaganda designed

296
00:19:15,034 --> 00:19:18,474
to mimic educational videos. So in addition

297
00:19:18,522 --> 00:19:21,806
to low quality, mass produced content churned out with

298
00:19:21,828 --> 00:19:23,650
the help of generative AI,

299
00:19:26,390 --> 00:19:37,974
we can along

300
00:19:38,012 --> 00:19:41,494
with the rise of visual generative AI, we will also see a natural

301
00:19:41,542 --> 00:19:45,562
increase in the spread of misinformation and fraudulent activity in the next half

302
00:19:45,616 --> 00:19:47,820
decade or so. For example,

303
00:19:48,590 --> 00:19:52,350
security footage and evidence tampering. This could require the

304
00:19:52,500 --> 00:19:57,018
usage of additional forensic tools specifically designed to mitigate generative AI.

305
00:19:57,194 --> 00:20:01,146
So if I asked Sora to generate security footage

306
00:20:01,178 --> 00:20:04,514
for a bank robbery that didn't exist, this could be very

307
00:20:04,552 --> 00:20:08,180
dangerous. And I'm excited to see not only

308
00:20:08,950 --> 00:20:12,882
the security protocols that openad puts in place, but also the ways

309
00:20:12,936 --> 00:20:16,706
that people exploit these security models

310
00:20:16,738 --> 00:20:18,390
to find vulnerabilities.

311
00:20:20,090 --> 00:20:23,394
So this has actually been pretty amusing

312
00:20:23,442 --> 00:20:26,360
in chachibt. So, for example,

313
00:20:27,210 --> 00:20:30,886
asking chachibt to give you a recipe for napalm

314
00:20:30,918 --> 00:20:35,020
brownies, and it's generally gotten better

315
00:20:35,870 --> 00:20:39,226
at dealing with these prompts.

316
00:20:39,258 --> 00:20:42,240
But I still know for a fact,

317
00:20:42,610 --> 00:20:46,094
because I tried, that it is

318
00:20:46,132 --> 00:20:49,626
possible to get chattvt to do some pretty wacky

319
00:20:49,658 --> 00:20:52,610
stuff that OpenAI probably wouldn't endorse.

320
00:20:53,030 --> 00:20:56,494
So in addition to evidence tampering,

321
00:20:56,542 --> 00:20:59,586
what about advertisements for fake products?

322
00:20:59,768 --> 00:21:05,398
This could lead to widespread scamming and also

323
00:21:05,484 --> 00:21:09,478
false information or propaganda designed to mimic educational videos.

324
00:21:09,564 --> 00:21:13,238
That's another way that visual generative AI could be

325
00:21:13,324 --> 00:21:16,794
applied conceivably within the next couple of

326
00:21:16,832 --> 00:21:20,394
years. So with the help

327
00:21:20,432 --> 00:21:23,882
of visual generative AI, the entire process, from start

328
00:21:23,936 --> 00:21:27,574
to finish, of content creation can be streamlined including

329
00:21:27,622 --> 00:21:30,110
writing a script, generative AI footage,

330
00:21:30,930 --> 00:21:34,506
generating stock footage, perhaps with the usage of sora,

331
00:21:34,618 --> 00:21:36,910
and editing it together seamlessly.

332
00:21:38,850 --> 00:21:46,138
And so I'd

333
00:21:46,154 --> 00:21:49,874
also like to briefly touch on the touch

334
00:21:49,912 --> 00:21:53,294
on the images that I used here. These images

335
00:21:53,342 --> 00:21:57,254
were generated by Dali with the prompts of misinformation, and I think it did a

336
00:21:57,292 --> 00:22:01,014
pretty good job. I think. So does

337
00:22:01,052 --> 00:22:04,486
this mean that all media goes downhill from here?

338
00:22:04,668 --> 00:22:07,894
Not quite. I am personally

339
00:22:08,022 --> 00:22:11,606
quite optimistic because there are several safeguards

340
00:22:11,638 --> 00:22:15,014
being discussed right now that could drastically reduce the impact

341
00:22:15,062 --> 00:22:18,540
of aigenerated misinformation. For example,

342
00:22:19,330 --> 00:22:22,654
C two PA is a proposed media specification that

343
00:22:22,692 --> 00:22:26,186
would be able to verify the origin and edit

344
00:22:26,218 --> 00:22:29,742
history of any media. So these

345
00:22:29,796 --> 00:22:33,342
corporations shown at the bottom are pushing for legislation

346
00:22:33,406 --> 00:22:37,646
mandating the usage of verification standards, and it's supported

347
00:22:37,678 --> 00:22:41,346
by several of the largest media giants. This would

348
00:22:41,448 --> 00:22:45,090
ensure that every pixel edited would leave a footprint.

349
00:22:45,250 --> 00:22:48,294
So here's how it would work. Let's say

350
00:22:48,412 --> 00:22:52,440
we have a camera that uses the C two PA standard.

351
00:22:52,970 --> 00:22:56,610
We could put this into Photoshop, which would then

352
00:22:56,700 --> 00:23:00,646
create a seal. This could then be uploaded

353
00:23:00,678 --> 00:23:04,506
to YouTube, and you would be able to click

354
00:23:04,528 --> 00:23:07,850
on an icon in the top right. This would show you

355
00:23:07,920 --> 00:23:10,858
exactly where the content came from,

356
00:23:11,024 --> 00:23:14,842
and it would let you decide for yourself whether or not it's

357
00:23:14,986 --> 00:23:18,430
legit. So you'd see whether or not it came from

358
00:23:18,500 --> 00:23:21,474
the New York Times or someone pretending to be the New York Times.

359
00:23:21,512 --> 00:23:25,294
And this is actually very promising.

360
00:23:25,422 --> 00:23:29,300
So in the case of generative AI, OpenAI or

361
00:23:29,830 --> 00:23:33,410
any other company that uses C two PA would stamp its media with

362
00:23:33,480 --> 00:23:37,570
a unique signature that would ensure it's accurately classified

363
00:23:37,650 --> 00:23:40,710
as having come from that company. So,

364
00:23:40,780 --> 00:23:43,906
specifically, it's worth mentioning that OpenAI has pledged

365
00:23:43,938 --> 00:23:47,450
it here to the C two PA standard once sora is formally

366
00:23:48,030 --> 00:23:51,274
released to the public. So what

367
00:23:51,312 --> 00:23:55,530
else can we do to prevent the rise of AI generated misinformation?

368
00:23:57,310 --> 00:24:00,526
Well, like many things, the solution turns out

369
00:24:00,548 --> 00:24:03,902
to be more AI. So TikTok is one of the first

370
00:24:03,956 --> 00:24:08,094
platforms that requires users to tag AI generated content.

371
00:24:08,292 --> 00:24:12,234
And this is very convenient because it forms

372
00:24:12,282 --> 00:24:16,242
a large set of labeled content that could then be used to train

373
00:24:16,296 --> 00:24:19,666
a large media classifier. And in

374
00:24:19,688 --> 00:24:23,570
fact, the licensing of such a tool could prove to be highly valuable

375
00:24:24,090 --> 00:24:25,720
several years down the line.

376
00:24:27,050 --> 00:24:31,074
So quick summary diffusion

377
00:24:31,122 --> 00:24:34,694
models are a type of generative AI that are highly effective in

378
00:24:34,732 --> 00:24:38,058
creating realistic media. This media can

379
00:24:38,064 --> 00:24:42,220
be used in a variety of ways, from filmmaking to education

380
00:24:42,750 --> 00:24:46,262
and far beyond. Within the next decade,

381
00:24:46,326 --> 00:24:50,262
this will likely have far creating implications in nearly

382
00:24:50,326 --> 00:24:53,034
every industry that relies on digital media.

383
00:24:53,232 --> 00:24:56,854
And there are several proposed countermeasures to AI

384
00:24:56,902 --> 00:25:00,426
content, including metadata verification and

385
00:25:00,528 --> 00:25:02,690
AI thin classifiers.

386
00:25:04,550 --> 00:25:08,382
I really enjoyed preparing this talk and I'm really grateful

387
00:25:08,446 --> 00:25:10,638
for the opportunity to share my thoughts.

388
00:25:10,814 --> 00:25:14,594
Please feel free to reach out to me with any questions.

389
00:25:14,792 --> 00:25:18,770
My name is Jayram Palamadai and thank you for watching

390
00:25:18,840 --> 00:25:19,310
my session.

