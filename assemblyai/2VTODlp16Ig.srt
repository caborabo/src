1
00:00:24,730 --> 00:00:28,310
Welcome to today's conference where we delve into

2
00:00:28,460 --> 00:00:32,134
modern data architecture. In an era where data has

3
00:00:32,172 --> 00:00:35,798
become the cornerstone of innovation and

4
00:00:35,884 --> 00:00:39,922
competitive advantage, organizations worldwide

5
00:00:39,986 --> 00:00:43,894
are facing to dont modern data strategies that can

6
00:00:43,932 --> 00:00:48,774
unlocking its full potential. But why

7
00:00:48,812 --> 00:00:52,314
do we need modern data architecture? Today? We will explore

8
00:00:52,362 --> 00:00:56,206
this question and more understanding how the landscape of

9
00:00:56,228 --> 00:00:59,546
data has evolved and why traditional approaches

10
00:00:59,578 --> 00:01:04,186
are no longer sufficient. From the explosion of data sources

11
00:01:04,378 --> 00:01:08,302
to the demand of no real time insights,

12
00:01:08,446 --> 00:01:12,146
the challenges facing organizations in harnessing the

13
00:01:12,168 --> 00:01:16,802
data and the solution lying the modernizing

14
00:01:16,946 --> 00:01:20,994
our approaches to data architecture. Our discussion

15
00:01:21,042 --> 00:01:24,786
will not only focus on the theoretical aspects of modern

16
00:01:24,818 --> 00:01:28,550
data strategy, but will also delve into practical

17
00:01:28,630 --> 00:01:31,962
insights on how to build modern data

18
00:01:32,016 --> 00:01:36,250
architecture that are agile, scalable and resilient.

19
00:01:36,990 --> 00:01:39,794
We will explore the latest technologies,

20
00:01:39,942 --> 00:01:43,802
methodologies and the frameworks that empower

21
00:01:43,866 --> 00:01:48,330
organizations to transfer data into actionable insights,

22
00:01:48,490 --> 00:01:51,150
driving innovation and failing growth.

23
00:01:51,490 --> 00:01:55,182
Throughout this talk, we will share best practices

24
00:01:55,326 --> 00:01:58,670
gleaned from industry leaders and experts,

25
00:01:58,830 --> 00:02:02,082
offering key takeaways that you can implement within

26
00:02:02,136 --> 00:02:06,354
your own organizations. So whether you are a seasoned

27
00:02:06,402 --> 00:02:09,894
data professional or just beginning your journey into the world

28
00:02:09,932 --> 00:02:13,698
of modern data architecture, I encourage you to engage,

29
00:02:13,794 --> 00:02:17,598
collaborate and immerse yourself in the wealth of knowledge

30
00:02:17,714 --> 00:02:20,618
and the insight that this talk has to offer.

31
00:02:20,784 --> 00:02:24,250
Welcome to the journey of building modern data architectures.

32
00:02:24,910 --> 00:02:28,474
So, before going

33
00:02:28,512 --> 00:02:34,494
into the details of how

34
00:02:34,532 --> 00:02:38,510
we architect the modern data organizations,

35
00:02:39,010 --> 00:02:42,190
let's take a step by step approach.

36
00:02:42,630 --> 00:02:46,274
So in every organization there will be a need to kind

37
00:02:46,312 --> 00:02:50,674
of write the data in

38
00:02:50,712 --> 00:02:54,770
some area and then read that data by the consumer.

39
00:02:55,190 --> 00:02:58,530
So in a very simple format, the data lake

40
00:02:59,430 --> 00:03:02,914
is like a simple storage where structured and unstructured

41
00:03:02,962 --> 00:03:05,560
data can be stored and consumed from.

42
00:03:06,010 --> 00:03:09,258
So in short, in its basic form,

43
00:03:09,344 --> 00:03:13,146
an organization will need a place to write data

44
00:03:13,248 --> 00:03:17,082
that will be consistent and durable. They will also need

45
00:03:17,136 --> 00:03:21,440
a way from consumer to consume that data in a consistent way.

46
00:03:21,890 --> 00:03:24,990
This could be as simple as s three in AWS.

47
00:03:25,650 --> 00:03:28,954
This will store data that can be structured,

48
00:03:29,082 --> 00:03:32,446
unstructured or semi structured. However,

49
00:03:32,548 --> 00:03:35,490
this simple storage will have a number of challenges.

50
00:03:36,310 --> 00:03:39,950
If every publisher writes data in their native format,

51
00:03:40,030 --> 00:03:44,260
then consumer will not have a consistent way to read the data.

52
00:03:44,890 --> 00:03:48,966
Organizing the data to be written so

53
00:03:48,988 --> 00:03:53,190
that consumer know where to read from data

54
00:03:53,260 --> 00:03:56,946
is added at some interval. This creates

55
00:03:56,978 --> 00:04:01,114
a problem to present a comprehensive view going

56
00:04:01,152 --> 00:04:04,522
a little bit detail on this part. In its rudimentary form,

57
00:04:04,576 --> 00:04:08,282
data lake serves as a reservoir for both structured and unstructured data,

58
00:04:08,336 --> 00:04:11,486
providing a centralized repository from which the

59
00:04:11,508 --> 00:04:14,906
information can be stored and retrieved. Writers deposit

60
00:04:14,938 --> 00:04:19,070
the data into the data lake while readers access

61
00:04:19,140 --> 00:04:22,802
and extract the data from its storage. However,

62
00:04:22,856 --> 00:04:26,238
this seemingly straightforward approach presents

63
00:04:26,414 --> 00:04:29,010
the number of challenges that we discussed.

64
00:04:29,510 --> 00:04:33,166
So addressing these challenges is essential

65
00:04:33,278 --> 00:04:36,690
for optimizing the functionality and efficiency

66
00:04:36,770 --> 00:04:40,242
of the data lake, thereby enhancing its utility

67
00:04:40,306 --> 00:04:43,734
and value within the organization ecosystem. Now,

68
00:04:43,772 --> 00:04:47,602
I will be talking a little bit more about how to

69
00:04:47,756 --> 00:04:50,090
organize the data storage.

70
00:04:50,830 --> 00:04:54,314
So basically, in this talk, the way we are

71
00:04:54,352 --> 00:04:58,234
kind of trying to achieve this one is in a step by

72
00:04:58,272 --> 00:05:01,838
step way. So we were talking about the simple form.

73
00:05:02,004 --> 00:05:06,222
Now going a little bit deeper, how can we store

74
00:05:06,276 --> 00:05:09,760
the data? And we'll follow this pattern in our entire talk.

75
00:05:10,290 --> 00:05:13,570
Coming back to organizing data storage.

76
00:05:14,230 --> 00:05:18,274
So if you think that every data publisher will

77
00:05:18,472 --> 00:05:22,466
need to write data to the

78
00:05:22,488 --> 00:05:25,934
storage, right, but we do not want all of them to invent

79
00:05:25,982 --> 00:05:29,286
a new way to write this data. Hence, there will

80
00:05:29,308 --> 00:05:32,520
be a need for a standard way to write the data.

81
00:05:33,210 --> 00:05:37,222
This can be fulfilled by creating a component called

82
00:05:37,276 --> 00:05:41,142
data writer. Data writers are assigned

83
00:05:41,286 --> 00:05:45,450
specific areas within data lake designated for their

84
00:05:45,520 --> 00:05:49,386
respective suborganizations. They must

85
00:05:49,488 --> 00:05:52,926
authenticate their identity and specify the

86
00:05:52,948 --> 00:05:56,410
data set they are writing to. Meanwhile,

87
00:05:56,490 --> 00:06:00,222
the data readers allow consumers to access data based on their

88
00:06:00,276 --> 00:06:04,254
identity and the data set that they are authorized

89
00:06:04,302 --> 00:06:08,334
to access. Data within each server organizations

90
00:06:08,382 --> 00:06:11,490
is structured into distinct data sets.

91
00:06:11,990 --> 00:06:15,926
Within each dataset's designated area,

92
00:06:16,028 --> 00:06:19,830
data is organized into delta files and aggregates.

93
00:06:20,490 --> 00:06:24,402
Additionally, data is partitioned for efficient

94
00:06:24,466 --> 00:06:28,670
storage and retrieval. Standard formats such as Parquet

95
00:06:28,690 --> 00:06:32,774
are employed for storage, while advanced formats like Iceberg

96
00:06:32,822 --> 00:06:36,646
and Hoodie may be utilized to enable transactional

97
00:06:36,678 --> 00:06:40,540
capabilities. In short,

98
00:06:42,290 --> 00:06:46,270
I'm right now focusing on the right

99
00:06:46,340 --> 00:06:49,918
part, how the publisher will kind of

100
00:06:50,004 --> 00:06:54,082
leverage and data writer component to

101
00:06:54,136 --> 00:06:57,266
store that data, and then a little bit

102
00:06:57,368 --> 00:07:01,666
how to organize that data within an organization and

103
00:07:01,688 --> 00:07:04,942
then how to manage the delta. That will come

104
00:07:05,096 --> 00:07:09,062
on a constant basis or

105
00:07:09,116 --> 00:07:12,806
some predefined frequency. But we need

106
00:07:12,828 --> 00:07:16,822
to make sure that the deltas are coming and the deltas are getting

107
00:07:16,956 --> 00:07:20,362
updated into the master files. Now,

108
00:07:20,416 --> 00:07:24,250
let's transition from storage to utilization.

109
00:07:24,830 --> 00:07:28,906
So basically, we'll be talking here on the

110
00:07:28,928 --> 00:07:32,206
consumption side, right? And then before going into

111
00:07:32,228 --> 00:07:36,046
the details, the very high level questions

112
00:07:36,148 --> 00:07:39,966
would be who will consume it, where from they

113
00:07:39,988 --> 00:07:43,458
will consume it and how will they consume it.

114
00:07:43,624 --> 00:07:46,834
Now there will be various personnel who will consume this

115
00:07:46,872 --> 00:07:50,142
data. This could be data analyst,

116
00:07:50,286 --> 00:07:54,158
data scientist, applications, or consumers

117
00:07:54,254 --> 00:07:57,558
that we do not know of and why. I'm saying

118
00:07:57,644 --> 00:08:01,206
that the consumer that we do not know of, there could

119
00:08:01,228 --> 00:08:05,110
be an innovative kind of scenario where

120
00:08:05,180 --> 00:08:08,730
the consumer fit into

121
00:08:08,800 --> 00:08:12,486
data analyst, data scientist, or application bucket. So that's why I'm

122
00:08:12,518 --> 00:08:16,394
creating another bucket for the consumer that

123
00:08:16,432 --> 00:08:19,754
we do not know right now. And all this would

124
00:08:19,792 --> 00:08:23,102
need the data to be stored in a fit for

125
00:08:23,156 --> 00:08:26,862
consumption platform. This means that

126
00:08:26,916 --> 00:08:30,618
the data for a batch application will be stored

127
00:08:30,714 --> 00:08:33,954
in a data lake. Data for real time

128
00:08:33,992 --> 00:08:37,458
access would most likely be accessed from a

129
00:08:37,464 --> 00:08:41,090
database or from a low latency storage.

130
00:08:41,750 --> 00:08:45,118
Additionally, for consumer patents that we

131
00:08:45,144 --> 00:08:48,562
do not know of, this platform will have to enable

132
00:08:48,626 --> 00:08:51,894
the creation of custom syncs to fulfill the

133
00:08:51,932 --> 00:08:55,554
specific consumption needs. Further, the consumers

134
00:08:55,602 --> 00:08:58,742
will need tools that are appropriate

135
00:08:58,806 --> 00:09:02,586
for their consumption. This could be API for

136
00:09:02,608 --> 00:09:05,770
real time access or notebooks for data scientists.

137
00:09:06,270 --> 00:09:10,570
Now, even with this upgrade,

138
00:09:11,090 --> 00:09:15,530
the challenges are consistency

139
00:09:15,610 --> 00:09:19,166
of data in various things, the duplication of

140
00:09:19,188 --> 00:09:22,474
effort to write this data and the

141
00:09:22,532 --> 00:09:24,820
control to prevent unauthorized access.

142
00:09:26,070 --> 00:09:29,458
And we will be kind of moving forward.

143
00:09:29,544 --> 00:09:33,726
We will see how to address these challenges in a more holistic

144
00:09:33,758 --> 00:09:37,542
view or with a more holistic view. Now let's discuss

145
00:09:37,676 --> 00:09:41,270
how the challenges that we talked earlier can be addressed.

146
00:09:41,690 --> 00:09:45,346
Multiple data publishers write data in various formats.

147
00:09:45,538 --> 00:09:49,674
For example, a publisher may write data in

148
00:09:49,712 --> 00:09:53,082
CSV format to the data lake, and there

149
00:09:53,136 --> 00:09:56,442
could be another publisher that may want to write

150
00:09:56,496 --> 00:09:59,050
data in avro format in streams.

151
00:09:59,550 --> 00:10:03,834
So we would need a component that will enable publisher

152
00:10:03,882 --> 00:10:07,486
to publish in an uniform way. We will

153
00:10:07,508 --> 00:10:11,214
need a component that will be able

154
00:10:11,252 --> 00:10:15,090
to move the published data into any of the data

155
00:10:15,160 --> 00:10:18,494
platforms that consumer want to consume

156
00:10:18,542 --> 00:10:22,514
from. This component will also write data in

157
00:10:22,552 --> 00:10:25,860
a standard format. For example, part three.

158
00:10:26,330 --> 00:10:29,350
We call this as the data pipeline.

159
00:10:29,930 --> 00:10:33,880
Some of the challenges that this component will have to solve are

160
00:10:34,410 --> 00:10:37,698
ensure that there is consistency in the data

161
00:10:37,884 --> 00:10:40,890
that is being written to these various platforms.

162
00:10:42,030 --> 00:10:45,786
Excuse me, ensure that the data is being written in

163
00:10:45,808 --> 00:10:49,434
a standard format so that there is a predictability from

164
00:10:49,472 --> 00:10:53,614
consumer perspective. Implement governance so

165
00:10:53,652 --> 00:10:57,630
that there is a trust in the data and then the burden on each

166
00:10:57,700 --> 00:11:01,120
platform to implement governance is not there.

167
00:11:01,650 --> 00:11:05,122
Hence, it is important for us to establish a first class

168
00:11:05,176 --> 00:11:08,786
data storage platform. If there

169
00:11:08,808 --> 00:11:12,354
is an inconsistency in the data, then this

170
00:11:12,392 --> 00:11:15,640
is the word we will use to rebuild the platform.

171
00:11:16,250 --> 00:11:19,318
For the consumer to consume data,

172
00:11:19,404 --> 00:11:22,898
we will need to have a discoverability

173
00:11:22,994 --> 00:11:26,962
of the data. This can be achieved by establishing

174
00:11:27,026 --> 00:11:30,870
a data catalog. Data catalog tells consumer

175
00:11:31,030 --> 00:11:33,850
what data is there for consumption,

176
00:11:34,190 --> 00:11:38,042
but we will have to solve for who can access what

177
00:11:38,096 --> 00:11:41,614
data. This is where data access control will come

178
00:11:41,652 --> 00:11:46,302
into picture. Now, before going into or

179
00:11:46,356 --> 00:11:50,206
doing a deep dive on data pipeline, let's talk or spend a

180
00:11:50,228 --> 00:11:53,410
few minutes about the role of data writer.

181
00:11:53,910 --> 00:11:57,374
So the data writer component

182
00:11:57,422 --> 00:12:01,522
plays an pivotal role in managing both real time and past data

183
00:12:01,576 --> 00:12:05,054
stream. Within the data ecosystem, real time

184
00:12:05,112 --> 00:12:08,754
streams are critical for instantaneous decision

185
00:12:08,802 --> 00:12:12,134
making and monitoring processes. Data can

186
00:12:12,172 --> 00:12:15,634
originate from APIs or change data capture

187
00:12:15,682 --> 00:12:19,386
mechanisms regardless of the source. Real time

188
00:12:19,568 --> 00:12:23,478
data is channeled into a continuous flow,

189
00:12:23,654 --> 00:12:27,322
forming a dynamic data stream. Bash data

190
00:12:27,376 --> 00:12:31,098
processing is essential for handling large volumes

191
00:12:31,114 --> 00:12:34,110
of data in scheduled intervals.

192
00:12:34,610 --> 00:12:38,494
Bash data may originate from file based storage logs or

193
00:12:38,532 --> 00:12:42,566
databases. Bass datasets are directed towards a centralized

194
00:12:42,618 --> 00:12:46,290
data lake for storage and subsequent analysis.

195
00:12:46,870 --> 00:12:49,540
Let's discuss about the data pipeline now.

196
00:12:50,070 --> 00:12:53,394
The basic idea of the data pipeline is to be

197
00:12:53,432 --> 00:12:57,254
able to preprocess the data before it is written to

198
00:12:57,292 --> 00:13:00,822
the storage platforms. The basic steps that

199
00:13:00,876 --> 00:13:04,790
will be within the pipeline are implementation of the data governance

200
00:13:05,450 --> 00:13:09,610
platform or sorry perform light transformation,

201
00:13:10,110 --> 00:13:13,958
route data to the appropriate syncs. Additionally,

202
00:13:14,054 --> 00:13:18,234
more steps should be addible in the future, so it

203
00:13:18,272 --> 00:13:21,966
should be flexible enough custom steps down the

204
00:13:21,988 --> 00:13:25,946
line as well. All of these tapes are organized

205
00:13:26,058 --> 00:13:29,866
using an orchestrator. This could be something like infling

206
00:13:30,058 --> 00:13:33,714
or airflow. Now let's talk about

207
00:13:33,912 --> 00:13:37,214
each of these steps. Data governance

208
00:13:37,342 --> 00:13:40,498
implement policies and procedures for managing data access,

209
00:13:40,584 --> 00:13:43,746
security and compliance. It will also do

210
00:13:43,768 --> 00:13:47,286
a schema check to

211
00:13:47,308 --> 00:13:51,302
validate the schema and to ensure the conformity with

212
00:13:51,356 --> 00:13:54,774
predefined standards and structures. It will also

213
00:13:54,812 --> 00:13:58,486
do and quality checks to identify and address the data integrity issues

214
00:13:58,588 --> 00:14:02,442
or anomalies. It will also do and scan for

215
00:14:02,496 --> 00:14:06,106
sensitive data and conduct in order to

216
00:14:06,128 --> 00:14:10,250
identify and protect sensitive data and ensuring the compliance

217
00:14:11,090 --> 00:14:13,230
with privacy regulations.

218
00:14:14,290 --> 00:14:18,190
Data pipeline will also routing the data into the destination,

219
00:14:18,610 --> 00:14:22,798
so direct data stream to diverse destination based on predefined routing

220
00:14:22,814 --> 00:14:26,322
roles and criteria ensure data is

221
00:14:26,376 --> 00:14:30,094
eventually retained. So it's like implementing

222
00:14:30,142 --> 00:14:33,886
mechanisms to ensure data persistence reliability

223
00:14:34,078 --> 00:14:37,526
even under challenging conditions. It will also do

224
00:14:37,548 --> 00:14:41,510
an light transformation, so applying lightweight transformation

225
00:14:43,050 --> 00:14:46,710
to optimize the data for downstream processing

226
00:14:47,130 --> 00:14:50,570
such as changing data types or formatting.

227
00:14:50,910 --> 00:14:54,634
Now, achieving all the above can be streamlined with

228
00:14:54,672 --> 00:14:58,346
a data pipeline offering a plug and play approach for adding and

229
00:14:58,368 --> 00:15:01,978
updating data governance. For batch data, it routes

230
00:15:02,074 --> 00:15:05,902
both streaming and batch platforms for real time

231
00:15:05,956 --> 00:15:09,386
data. It batches before routing to the lake

232
00:15:09,498 --> 00:15:14,050
and sent it to the other downstream storage syncs simultaneously.

233
00:15:14,630 --> 00:15:18,980
Before sharing the responsibilities of a data transformation platform,

234
00:15:19,590 --> 00:15:23,554
let's revisit the challenges associated with it.

235
00:15:23,752 --> 00:15:27,320
If each team must develop its own computing platform,

236
00:15:27,850 --> 00:15:31,110
optimal capacity utilization becomes challenges.

237
00:15:31,850 --> 00:15:36,050
Second, each team will expand effort in maintaining infrastructure

238
00:15:36,130 --> 00:15:39,514
individually. And the third, there is

239
00:15:39,552 --> 00:15:43,242
no standardized approach to implementing data

240
00:15:43,296 --> 00:15:47,210
ops, leading to inconsistency across teams.

241
00:15:47,630 --> 00:15:50,762
To tackle these challenges, a data transformation

242
00:15:50,826 --> 00:15:54,394
platform can be utilized to accept transformation

243
00:15:54,442 --> 00:15:58,910
scripts in various formats via a Ci CD pipeline.

244
00:15:59,410 --> 00:16:03,182
Initiate and execute jobs on an on demand

245
00:16:03,246 --> 00:16:05,010
computing infrastructure.

246
00:16:06,550 --> 00:16:09,906
Third is dynamically grant access to specific

247
00:16:10,008 --> 00:16:13,314
data based on job context and the

248
00:16:13,352 --> 00:16:16,882
lastly, the fourth one transfer the transform

249
00:16:16,946 --> 00:16:20,262
data to a data writer for storage on data

250
00:16:20,316 --> 00:16:23,750
platforms. As you see in this diagram,

251
00:16:25,690 --> 00:16:29,594
I'm going to talk about the data organization and

252
00:16:29,632 --> 00:16:33,180
then the left hand side of this diagram. It's more like

253
00:16:33,790 --> 00:16:37,242
talking about how to organize the data in different

254
00:16:37,296 --> 00:16:41,146
lobs and what's the role of individual publishers

255
00:16:41,258 --> 00:16:44,800
within that lob. The right hand side is like

256
00:16:46,690 --> 00:16:50,766
how that enterprise catalog would look like and what

257
00:16:50,788 --> 00:16:55,570
would be the role of different sublob within

258
00:16:55,720 --> 00:16:58,818
the overall framework in general,

259
00:16:58,904 --> 00:17:03,710
right? Every organization is composed

260
00:17:03,790 --> 00:17:07,682
of sublobs. So when I'm talking about lobs

261
00:17:07,746 --> 00:17:12,034
means each of these sublobs will have multiple

262
00:17:12,082 --> 00:17:15,842
publisher and each of these publisher may publish

263
00:17:15,906 --> 00:17:19,994
various data sets. So these data sets may

264
00:17:20,032 --> 00:17:24,230
in turn be sent to various data platforms like the lake

265
00:17:24,310 --> 00:17:28,650
or database. We will call this data distribution.

266
00:17:29,310 --> 00:17:32,682
Further, there can be multiple

267
00:17:32,746 --> 00:17:36,510
consumers in each of these sublobs which

268
00:17:36,580 --> 00:17:40,942
may want to access data from any of the data

269
00:17:40,996 --> 00:17:44,210
sets across any sublob.

270
00:17:44,550 --> 00:17:48,398
So it is imperative that this data is discoverable

271
00:17:48,494 --> 00:17:52,350
by the consumers. Hence, the publisher will be responsible

272
00:17:52,430 --> 00:17:57,114
for registering the data in the catalog. In many organizations,

273
00:17:57,182 --> 00:18:00,774
each sublob may have invested in

274
00:18:00,812 --> 00:18:03,922
their own catalog. In such cases,

275
00:18:04,066 --> 00:18:07,310
they will need to roll up all this registration

276
00:18:07,490 --> 00:18:11,770
in a central catalog so that the data is discoverable.

277
00:18:12,510 --> 00:18:15,594
Now let's take a look at things that are

278
00:18:15,632 --> 00:18:19,450
needed to enable publishers and consumers.

279
00:18:19,970 --> 00:18:23,706
We will need an UI for the data users.

280
00:18:23,898 --> 00:18:27,390
We will call this as data portal.

281
00:18:27,970 --> 00:18:31,390
Many organizations will call it by different names.

282
00:18:31,970 --> 00:18:36,194
As we have already discussed, we will need a place for

283
00:18:36,232 --> 00:18:39,842
the admins to define an organization and

284
00:18:39,896 --> 00:18:44,002
its organization. We will also need

285
00:18:44,136 --> 00:18:48,150
to let the publisher define their data in the catalog.

286
00:18:48,650 --> 00:18:52,374
This would be data sets, data distribution and

287
00:18:52,412 --> 00:18:56,386
other things. After defining

288
00:18:56,418 --> 00:18:59,930
the catalog, the publishers may be asked

289
00:19:00,000 --> 00:19:03,274
to define the data quality, lineage and

290
00:19:03,312 --> 00:19:06,906
other things so that the consumer have confidence in

291
00:19:06,928 --> 00:19:10,910
the data that they are consuming. The basic idea

292
00:19:10,980 --> 00:19:15,150
here is to reduce friction for the data to be published.

293
00:19:18,370 --> 00:19:22,174
Hence the tiers just publish but no

294
00:19:22,212 --> 00:19:26,306
consumption. Publish data with basic governance information

295
00:19:26,408 --> 00:19:29,954
like schema and tier three, additional information like

296
00:19:29,992 --> 00:19:33,566
data quality. Besides supporting the publisher

297
00:19:33,598 --> 00:19:37,542
and consumer, the data portal would need to support other

298
00:19:37,596 --> 00:19:40,870
use cases like reporting, searching and access

299
00:19:40,940 --> 00:19:44,280
request. Let's now focus on machine learning.

300
00:19:44,970 --> 00:19:49,050
How can this platform that we have defined till now support

301
00:19:49,200 --> 00:19:52,970
machine learning? As we can see in the diagram

302
00:19:54,510 --> 00:19:58,022
at a rudimentary level, there are a few stages.

303
00:19:58,166 --> 00:20:00,830
Raw data will be available in the data lake.

304
00:20:01,250 --> 00:20:05,326
A transformation job within the data transformation platform

305
00:20:05,428 --> 00:20:09,520
will extract the features and store them in a feature platform.

306
00:20:10,210 --> 00:20:13,970
This feature platform may be created using the following

307
00:20:14,310 --> 00:20:18,494
feature registration in the data portal, data storage

308
00:20:18,542 --> 00:20:22,702
in data lake and perhaps a low latency cache feature

309
00:20:22,766 --> 00:20:26,262
serving using an API. A job to train

310
00:20:26,316 --> 00:20:29,480
the model within the transformation platform.

311
00:20:30,090 --> 00:20:33,666
Then the trained model will create offline predictions

312
00:20:33,858 --> 00:20:37,238
and store that in a data lake and a

313
00:20:37,244 --> 00:20:40,954
low latency storage here. One other thing needs

314
00:20:40,992 --> 00:20:45,274
to be noted here that we are not covering the real time prediction here.

315
00:20:45,472 --> 00:20:49,162
Now putting everything together, we will have these

316
00:20:49,216 --> 00:20:52,270
components. The data writer,

317
00:20:52,930 --> 00:20:57,054
it will be able to read and write data from various sources and

318
00:20:57,092 --> 00:21:01,054
various format, be able to scale up and down according to

319
00:21:01,092 --> 00:21:04,722
load and cater to both batch as well as real time

320
00:21:04,776 --> 00:21:08,706
data. The storage platforms it will

321
00:21:08,728 --> 00:21:11,762
be something like the data lakes, streams and other

322
00:21:11,816 --> 00:21:15,206
storage things. A data pipeline to

323
00:21:15,228 --> 00:21:18,982
implement various preprocessing steps and

324
00:21:19,036 --> 00:21:22,870
binding all these steps through an orchestration.

325
00:21:24,250 --> 00:21:27,706
Now potentially there

326
00:21:27,728 --> 00:21:31,180
could be a data transformation layer or platform

327
00:21:32,110 --> 00:21:36,010
which will help us to do the needed transformation

328
00:21:36,510 --> 00:21:40,514
to send the data in various things. The consumption

329
00:21:40,582 --> 00:21:43,646
tools for consuming data from the various platforms in a

330
00:21:43,668 --> 00:21:47,360
way that is convenient for the consumer and finally

331
00:21:47,730 --> 00:21:51,534
having an UI that binds all this together,

332
00:21:51,732 --> 00:21:58,738
that is the data portal. Now I

333
00:21:58,744 --> 00:22:02,626
will spend some time to share

334
00:22:02,808 --> 00:22:06,422
some of the challenges related to the cloud

335
00:22:06,556 --> 00:22:12,118
or with how

336
00:22:12,204 --> 00:22:16,402
how each lob will create or share their AWS accounts.

337
00:22:16,466 --> 00:22:19,610
And then it's a common challenge.

338
00:22:20,190 --> 00:22:24,202
And then the solutions option could be like create one central account

339
00:22:24,256 --> 00:22:28,380
to store the data and each lob creates their own account.

340
00:22:30,190 --> 00:22:33,914
There could be a potential challenge with data security. There is an added

341
00:22:33,962 --> 00:22:37,994
concern of data breach right sensitive data. The solution

342
00:22:38,042 --> 00:22:41,194
for that one is like sensitive data should be scanned,

343
00:22:41,322 --> 00:22:45,522
an appropriate remediation performed such

344
00:22:45,576 --> 00:22:47,890
that even if the data is compromised,

345
00:22:48,470 --> 00:22:51,380
it will be of very little use.

346
00:22:51,830 --> 00:22:55,090
Some options are encryption, masking or deletion.

347
00:22:55,590 --> 00:22:59,382
Data integration, integrating on premise data

348
00:22:59,436 --> 00:23:03,526
with new data. The solution could be

349
00:23:03,708 --> 00:23:07,714
historical data remain on premise, migrating historical

350
00:23:07,762 --> 00:23:11,340
data to cloud and disconnect with on prem data.

351
00:23:12,510 --> 00:23:16,314
There could be challenge with workflow migration and that could be

352
00:23:16,352 --> 00:23:19,642
potentially solved by migrating on premise job to the cloud

353
00:23:19,696 --> 00:23:23,726
so that new data can be created on cloud and also let

354
00:23:23,748 --> 00:23:27,386
the job remain on premise and create new data on premise.

355
00:23:27,498 --> 00:23:31,470
Create another job to move the data from an

356
00:23:31,540 --> 00:23:35,758
ops point of view. Each service and storage are tagged

357
00:23:35,934 --> 00:23:39,394
with the Subalobi's identity so that this information can be

358
00:23:39,432 --> 00:23:42,722
used for billing. There could be challenge with data

359
00:23:42,776 --> 00:23:46,162
governance. So in that case

360
00:23:46,296 --> 00:23:50,342
I would recommend know CDMC as a standard and

361
00:23:50,396 --> 00:23:52,950
also use custom data governance component.

362
00:23:53,450 --> 00:23:57,222
Cloud capacity is

363
00:23:57,276 --> 00:24:00,982
also a concern. Cloud is someone

364
00:24:01,036 --> 00:24:04,614
else data center. Although it's good abstraction, we will need

365
00:24:04,652 --> 00:24:08,694
to plan for the capacity as each account has

366
00:24:08,732 --> 00:24:11,840
a soft and hard limit on capacity available.

367
00:24:12,450 --> 00:24:15,978
Well, thank you once again for your time and attention.

368
00:24:16,154 --> 00:24:19,454
It has been a pleasure to share my

369
00:24:19,492 --> 00:24:23,358
talk with you all today. I have shared my email

370
00:24:23,444 --> 00:24:27,374
and I would be happy to take any questions you may

371
00:24:27,412 --> 00:24:28,540
have. Thank you.

