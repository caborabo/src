1
00:00:25,170 --> 00:00:29,094
All right, so let's get started by looking at some terms that are

2
00:00:29,132 --> 00:00:33,800
used in streaming. I've put this slide because it's important to

3
00:00:34,170 --> 00:00:38,630
look at those different terms so that we think about streaming

4
00:00:38,970 --> 00:00:42,610
in the same dictionary as we are talking in the slides

5
00:00:42,690 --> 00:00:46,742
going forward. So let's look at some terms. Event time.

6
00:00:46,876 --> 00:00:50,906
The time at which the event actually occurred. For example, if you're

7
00:00:50,938 --> 00:00:54,846
playing a video game, when did you actually hit a

8
00:00:54,868 --> 00:00:58,394
button? When did you actually hit pause?

9
00:00:58,522 --> 00:01:02,602
When did you actually do an activity?

10
00:01:02,746 --> 00:01:06,686
The actual time of that activity is the time that's

11
00:01:06,718 --> 00:01:10,402
called the event time processing time. It's different from event

12
00:01:10,456 --> 00:01:14,242
time in that it's the time at which a streaming data

13
00:01:14,296 --> 00:01:17,460
ingestion pipeline will actually process your event.

14
00:01:17,930 --> 00:01:22,118
The event time in an ideal terms, which is not usually the case,

15
00:01:22,284 --> 00:01:26,134
or it could be the time after a few seconds or milliseconds of

16
00:01:26,172 --> 00:01:30,042
event time when the stream processor gets the data

17
00:01:30,176 --> 00:01:32,886
and processes it. Watermark.

18
00:01:32,998 --> 00:01:36,714
That's the notion of completeness. We are going to discuss this much

19
00:01:36,752 --> 00:01:40,870
in detail, but for now, just assume that watermark indicates

20
00:01:40,950 --> 00:01:45,194
when the processing of a given chunk of streaming data is considered

21
00:01:45,242 --> 00:01:49,354
complete by a pipelines, chopping up the data into temporal

22
00:01:49,402 --> 00:01:52,946
boundaries for groupings and aggregations. What it means is

23
00:01:53,048 --> 00:01:57,230
if you have to aggregate a certain chunk

24
00:01:57,310 --> 00:02:01,650
of a streaming data, you window it basically similar

25
00:02:01,720 --> 00:02:05,098
to batch, where you have group buys aggregates.

26
00:02:05,214 --> 00:02:08,774
You need a certain start and stop to group

27
00:02:08,812 --> 00:02:12,454
by and aggregate. And that certain start and stop in

28
00:02:12,492 --> 00:02:15,762
streaming data world is called a window. Triggers.

29
00:02:15,906 --> 00:02:18,966
Triggers are when in the processing times are

30
00:02:18,988 --> 00:02:22,694
the results materialized? When do you actually materialize the results?

31
00:02:22,822 --> 00:02:26,918
Accumulation is the relationship between multiple results in the same window.

32
00:02:27,014 --> 00:02:30,330
Do you want to create a running sum? Do you want to create

33
00:02:30,400 --> 00:02:33,950
just one sum in a given window? What do you want to do?

34
00:02:34,020 --> 00:02:37,566
So these are all the different terms that we are going to use in the

35
00:02:37,588 --> 00:02:40,846
world of streaming data. Let's look at

36
00:02:40,868 --> 00:02:44,162
some interesting challenges in streaming, as these

37
00:02:44,216 --> 00:02:49,822
challenges form the fundamentals

38
00:02:49,886 --> 00:02:55,022
of how we look at streaming. So let's look at this stream.

39
00:02:55,086 --> 00:02:58,342
So those line, those black line below that

40
00:02:58,396 --> 00:03:01,958
shows the actual wall time or the clock time.

41
00:03:02,124 --> 00:03:04,966
And then you see the events that are coming,

42
00:03:05,068 --> 00:03:08,582
the red boxes that are there. One was an event

43
00:03:08,636 --> 00:03:12,442
that happened at 08:00 a.m. But came into the pipeline after

44
00:03:12,496 --> 00:03:16,410
11:00 a.m. Then there was one event that actually

45
00:03:16,480 --> 00:03:20,362
happened at 09:00 a.m. And that came into the streaming pipeline between

46
00:03:20,416 --> 00:03:24,286
11:00 a.m. And 11:10 a.m. And then the third one

47
00:03:24,388 --> 00:03:27,200
was actually an event that happened at 10:00 a.m.

48
00:03:27,730 --> 00:03:31,294
That came in the streaming data pipeline between 11:00 a.m.

49
00:03:31,332 --> 00:03:34,574
And 11:15 a.m. So what does it show us?

50
00:03:34,692 --> 00:03:38,674
It shows that this data is out of order. Why it

51
00:03:38,712 --> 00:03:42,114
came late. The event happened at 08:00 a.m. But came after

52
00:03:42,152 --> 00:03:45,566
11:00 a.m. So should we process this because it's late

53
00:03:45,598 --> 00:03:48,726
data? Should we not process it? So there are

54
00:03:48,748 --> 00:03:52,422
these challenges where you have data almost every time coming

55
00:03:52,476 --> 00:03:55,906
out of order late because of networking

56
00:03:55,938 --> 00:03:59,574
issues, because of some issues of bandwidth between where the stream

57
00:03:59,622 --> 00:04:03,450
processor is versus where the data is originating. So you will not

58
00:04:03,520 --> 00:04:06,794
always get the data or the

59
00:04:06,832 --> 00:04:09,978
stream in time whenever it happened. You can have a

60
00:04:09,984 --> 00:04:12,986
lot of use cases where the data is coming late.

61
00:04:13,178 --> 00:04:16,814
So this forms the basis of interesting challenges in streaming where

62
00:04:16,852 --> 00:04:20,814
now we are worried about how do we process this late arriving data

63
00:04:20,932 --> 00:04:24,146
or should we actually process it? That's the

64
00:04:24,168 --> 00:04:27,502
dilemma between correctness and completion. In batch

65
00:04:27,566 --> 00:04:31,346
world. The correctness is like I process the

66
00:04:31,368 --> 00:04:34,260
bounded data based on the business logic, right?

67
00:04:34,630 --> 00:04:38,066
Whenever we write our unit best cases and whenever

68
00:04:38,098 --> 00:04:42,146
we do the actual testing before productionizing those code. The notion

69
00:04:42,178 --> 00:04:46,214
of correctness in a batch platform is that, okay, I had

70
00:04:46,252 --> 00:04:50,502
a bounded data set, and the word bounded here means that I had finite boundaries.

71
00:04:50,646 --> 00:04:53,542
Whether it's a table, it's a file,

72
00:04:53,606 --> 00:04:57,354
whatever it is, I have a bounded data set and

73
00:04:57,392 --> 00:05:00,942
I have to apply a set business logic to that bounded data set.

74
00:05:01,076 --> 00:05:04,394
If the business logic that sits behind the input

75
00:05:04,442 --> 00:05:08,554
and the output gives me correct results, I'm good. That's correctness

76
00:05:08,602 --> 00:05:12,526
for me. In the world of batch, in the

77
00:05:12,548 --> 00:05:16,306
world of streaming, it's all what I said in

78
00:05:16,328 --> 00:05:19,762
batch plus figuring out how to handle out of order

79
00:05:19,816 --> 00:05:23,202
data. We saw in the previous slide that there was out of order

80
00:05:23,256 --> 00:05:26,398
data. If we have to think about correctness,

81
00:05:26,494 --> 00:05:29,970
should we add that data because it is being missing,

82
00:05:30,050 --> 00:05:33,798
or should we discard that data because it's late? What would be more correct?

83
00:05:33,964 --> 00:05:38,034
That's dependent on your business logic and that adds additional complexity.

84
00:05:38,082 --> 00:05:41,770
In the world of streaming that we did not have in the world of batch.

85
00:05:42,270 --> 00:05:45,754
Second is completion. In the world of batch, we think about

86
00:05:45,792 --> 00:05:49,082
completion as I processing all the records in a bounded data

87
00:05:49,136 --> 00:05:52,734
set, be it a file, be it a table, et cetera. If a file had

88
00:05:52,772 --> 00:05:56,974
10,000 records, I process all those 10,000 records with

89
00:05:57,012 --> 00:06:00,682
the business logic, that's completion. If I'm partially

90
00:06:00,746 --> 00:06:05,022
processing a file, that means, for example, I just process 6000 records

91
00:06:05,086 --> 00:06:08,642
and I have 4000 unprocessed records because of some schema issues,

92
00:06:08,696 --> 00:06:12,318
et cetera. That's completion in the world of batch.

93
00:06:12,414 --> 00:06:15,714
But in the world of streaming, the completion is like I

94
00:06:15,752 --> 00:06:19,426
try to process all the data on time and late

95
00:06:19,458 --> 00:06:23,654
arriving, or how much of late arriving data should I process?

96
00:06:23,852 --> 00:06:27,542
Should I process the data that's late arriving by 30 minutes or 40

97
00:06:27,596 --> 00:06:31,654
minutes? What's completion in the world of streaming? So there's always a dilemma

98
00:06:31,702 --> 00:06:35,434
between correctness and completion in the world of streaming that we

99
00:06:35,472 --> 00:06:39,142
struggle with and to delineate some of that and to demystify

100
00:06:39,206 --> 00:06:43,242
some of that. We have those important concepts in the world of streaming.

101
00:06:43,386 --> 00:06:46,986
The first one is watermark. Watermark is the notion

102
00:06:47,018 --> 00:06:51,022
of completeness. What it means is all the input data with event

103
00:06:51,076 --> 00:06:54,270
time less than X has been processed.

104
00:06:54,350 --> 00:06:57,874
X is the watermark here not perfect because

105
00:06:57,912 --> 00:07:00,894
it needs strategy to process late arrivals,

106
00:07:01,022 --> 00:07:04,430
right? The second term is trigger,

107
00:07:04,590 --> 00:07:07,990
when to emit the aggregated results for a window.

108
00:07:08,410 --> 00:07:11,506
Event time triggers are there. There are processing time triggers,

109
00:07:11,538 --> 00:07:14,726
and then there are data driven triggers. And then the

110
00:07:14,748 --> 00:07:18,902
third term is the accumulation. The accumulation mode determines

111
00:07:19,046 --> 00:07:22,486
whether the system accumulates the window panes or discards

112
00:07:22,518 --> 00:07:27,062
them. Basically what it means is if I'm processing buttons

113
00:07:27,126 --> 00:07:30,742
in my video game console or PlayStation

114
00:07:30,806 --> 00:07:34,618
or anything, does every click accumulate to my score

115
00:07:34,714 --> 00:07:37,806
for that given window? Or should I discard everything and

116
00:07:37,828 --> 00:07:41,502
just consider the last one? So those are the three terms that we

117
00:07:41,556 --> 00:07:45,342
use and that help us clarify

118
00:07:45,486 --> 00:07:48,706
for a given streaming pipeline. Should we focus on correctness and

119
00:07:48,728 --> 00:07:52,098
completion, bringing all these concepts together?

120
00:07:52,264 --> 00:07:55,954
Let's look at them from a completely different angle. The first

121
00:07:55,992 --> 00:07:59,286
one is what results are being calculated. So if you have to

122
00:07:59,308 --> 00:08:02,614
define what results are you calculating, you can define them with the help

123
00:08:02,652 --> 00:08:05,010
of transformations, filtering,

124
00:08:05,090 --> 00:08:08,698
aggregation. You can run some SQL queries, or you

125
00:08:08,704 --> 00:08:12,330
can do some training if it's a machine learning pipelines.

126
00:08:12,830 --> 00:08:16,954
The second concept is when in the event time are they being

127
00:08:16,992 --> 00:08:20,394
calculated? So here is where we think

128
00:08:20,432 --> 00:08:23,822
about the windowing part. There is event

129
00:08:23,876 --> 00:08:27,102
time windowing, which is similar to sharding in batch, where now

130
00:08:27,156 --> 00:08:30,394
we are windowing the data based on the window

131
00:08:30,442 --> 00:08:33,978
logic. There are fixed windows, there are sliding windows, there are session windows.

132
00:08:34,074 --> 00:08:37,806
And I've also given an example here in Apache Beam,

133
00:08:37,838 --> 00:08:41,006
which is an open source framework where you have a window

134
00:08:41,038 --> 00:08:44,226
that is defined as a fixed window for 60 seconds, for example,

135
00:08:44,328 --> 00:08:48,166
in the event time. And then comes the third one which is

136
00:08:48,268 --> 00:08:52,738
when in the processing time are the results being materialized.

137
00:08:52,914 --> 00:08:56,214
You have watermark, which is the notion of completeness that we went through

138
00:08:56,252 --> 00:08:59,606
before. You have triggers, which means when

139
00:08:59,628 --> 00:09:03,686
do you trigger the results in a given window? And then you have accumulations,

140
00:09:03,798 --> 00:09:08,086
which means how do you actually compute the results across a window?

141
00:09:08,198 --> 00:09:10,890
Do you do a running sum, do you not do anything?

142
00:09:10,960 --> 00:09:15,114
And do you just do the last value in a given window,

143
00:09:15,162 --> 00:09:18,574
et cetera? And then you have later rivals as well, which are also

144
00:09:18,612 --> 00:09:22,430
defined by triggers. So if you see here, there is an example

145
00:09:22,500 --> 00:09:25,650
where we have a fixed window. We have a trigger that after

146
00:09:25,720 --> 00:09:29,266
processing time, 60 seconds after the processing time, you just

147
00:09:29,288 --> 00:09:33,406
do the triggering of the results, which means we are allowing for late arrivals

148
00:09:33,518 --> 00:09:36,786
for up to 60 seconds in this example. And the

149
00:09:36,808 --> 00:09:40,102
accumulation mode is you do not accumulate the results, you just give me the final

150
00:09:40,156 --> 00:09:43,638
result. You don't accumulate or do a running total. So this

151
00:09:43,644 --> 00:09:47,350
is how we bring all the concepts together in the world of streaming.

152
00:09:47,930 --> 00:09:51,786
Now, we spoke about some terms. We spoke about how

153
00:09:51,808 --> 00:09:55,686
do you bring those concepts together. I showed you some code examples

154
00:09:55,718 --> 00:10:00,198
as well. Let's talk about some real examples of

155
00:10:00,384 --> 00:10:04,510
how streaming at scale has some challenges

156
00:10:05,250 --> 00:10:09,470
and how we can sort of solve them using infrastructure and code considerations.

157
00:10:10,130 --> 00:10:13,614
The first one is auto scaling. We have heard of auto scaling in terms

158
00:10:13,652 --> 00:10:17,038
of batch, and most of us, when we speak about auto scaling,

159
00:10:17,134 --> 00:10:21,150
it's mostly horizontal auto scaling, where we keep addressing more machines

160
00:10:21,230 --> 00:10:24,482
of the same type to a given cluster to

161
00:10:24,536 --> 00:10:27,906
improve the performance. In the world of streaming,

162
00:10:28,018 --> 00:10:31,558
that may or may not work because there might be a particular

163
00:10:31,644 --> 00:10:35,606
instance in which a stream needs much more memory on the

164
00:10:35,628 --> 00:10:39,686
same machine itself, rather than having additional

165
00:10:39,878 --> 00:10:43,418
workload distribution across different machines. That happens in

166
00:10:43,424 --> 00:10:46,794
the horizontal scaling. In that case, it is good

167
00:10:46,832 --> 00:10:50,510
to have infrastructure that supports both horizontal and

168
00:10:50,580 --> 00:10:54,046
vertical auto scaling in flight. What does in

169
00:10:54,068 --> 00:10:58,234
flight mean is while the stream is being processed,

170
00:10:58,362 --> 00:11:02,046
because we cannot stop the stream, then do the

171
00:11:02,068 --> 00:11:05,506
auto scaling and then come back and start processing the

172
00:11:05,528 --> 00:11:08,834
stream again. It's running water, it's running data.

173
00:11:08,952 --> 00:11:12,894
So you need capabilities to be able to auto

174
00:11:12,942 --> 00:11:17,446
scale your existing machines, Ram and

175
00:11:17,548 --> 00:11:21,682
number of cores in flight while the stream is being processed.

176
00:11:21,746 --> 00:11:25,702
And that's why vertical auto scaling is important. Dynamic work

177
00:11:25,756 --> 00:11:28,830
rebalancing in the world of batch,

178
00:11:28,930 --> 00:11:32,634
we have multiple machines that are working in the same

179
00:11:32,672 --> 00:11:35,914
world of distributed compute. With streaming, there could be

180
00:11:35,952 --> 00:11:40,026
some machines that are working on some data. Let's assume it's x,

181
00:11:40,128 --> 00:11:43,434
and there are some machines that are working on some other streams, which is

182
00:11:43,472 --> 00:11:47,086
y. It could very well be possible that there are

183
00:11:47,108 --> 00:11:50,362
some machines that are not working as much as other machines,

184
00:11:50,426 --> 00:11:55,362
which means those is disparity of

185
00:11:55,416 --> 00:11:58,754
work between some worker nodes. This can

186
00:11:58,792 --> 00:12:02,434
cause to SKUs, this can cause SKUs in the machines, this can

187
00:12:02,472 --> 00:12:05,490
cause performance issues, et cetera.

188
00:12:06,010 --> 00:12:10,070
To avoid this, it's good to have dynamic work rebalancing.

189
00:12:11,130 --> 00:12:14,962
Have infrastructure that allows for workers to redistribute

190
00:12:15,026 --> 00:12:18,566
work amongst each other so

191
00:12:18,588 --> 00:12:22,498
that you can avoid stragglers, or you can avoid nodes

192
00:12:22,514 --> 00:12:25,830
that are not working and you have dynamic work rebalancing.

193
00:12:26,410 --> 00:12:30,046
Window processing decoupling this is a very important one

194
00:12:30,108 --> 00:12:34,750
because a stream pipeline comprises different

195
00:12:34,820 --> 00:12:37,530
things. You have the code logic.

196
00:12:37,690 --> 00:12:41,406
Suppose you have a code of filtering and then you have windows which

197
00:12:41,428 --> 00:12:45,266
have to be processed, right? It's always good to have a

198
00:12:45,288 --> 00:12:48,834
decoupling between the windows that are being processed from other

199
00:12:48,872 --> 00:12:52,782
stream operations because it helps you scale

200
00:12:52,846 --> 00:12:56,662
better. You can have your primary SDKs for streaming on one

201
00:12:56,716 --> 00:13:00,626
machine and then you can decouple the processing that can goes to different machines

202
00:13:00,738 --> 00:13:03,990
which can help you scale your operations.

203
00:13:04,970 --> 00:13:08,006
Giving memory and GPU related resource hints for

204
00:13:08,028 --> 00:13:11,334
a particular pipeline or specific steps in a pipeline

205
00:13:11,382 --> 00:13:14,790
can be really helpful. You can give python hints

206
00:13:14,870 --> 00:13:18,586
or any other language that your streaming pipeline supports and

207
00:13:18,608 --> 00:13:21,898
say hey, this function would need ideally this much memory in

208
00:13:21,904 --> 00:13:25,434
GPU so that your streaming pipeline can scale up or down accordingly

209
00:13:25,562 --> 00:13:28,778
without relying too much on the auto scaling capabilities. Because now you're

210
00:13:28,794 --> 00:13:32,240
being very definitive on what the stream will need.

211
00:13:32,610 --> 00:13:36,114
I think this is a ubiquitous principle that applies both to batch and

212
00:13:36,152 --> 00:13:40,334
streaming, where it's good to use combined

213
00:13:40,382 --> 00:13:43,970
by instead of group by because combined by reduces shuffle

214
00:13:44,950 --> 00:13:48,726
unless you have to use a group by because group by introduces a

215
00:13:48,748 --> 00:13:52,358
lot of shuffle. Retrying forever I had one of

216
00:13:52,364 --> 00:13:56,310
the customers who had a default retry forever

217
00:13:56,730 --> 00:14:00,790
and their streaming pipelines would hang a lot because they had this option.

218
00:14:00,940 --> 00:14:04,486
Always have a time period or a retry count which returns

219
00:14:04,518 --> 00:14:07,818
an error and ideally sends the element to a dead letter queue after a

220
00:14:07,824 --> 00:14:11,694
particular number of retries, and you can always reprocess those

221
00:14:11,732 --> 00:14:16,606
dead letter queues later. This helps you to have

222
00:14:16,788 --> 00:14:20,606
some amount of definitive time period after which you don't have

223
00:14:20,628 --> 00:14:24,446
to retry and your streaming pipelines will not hang. File I

224
00:14:24,468 --> 00:14:28,382
o this is a very typical one that is common between batch and streaming

225
00:14:28,526 --> 00:14:31,806
right size your files, input files, output files,

226
00:14:31,838 --> 00:14:35,138
et cetera, between 100 mb and one GB shard, depending on

227
00:14:35,144 --> 00:14:38,674
the total size. Network. It's always good to have all the sources

228
00:14:38,722 --> 00:14:42,854
and syncs in the same region to reduce network latency might

229
00:14:42,892 --> 00:14:46,550
not be possible all the time, but if it is, please follow this

230
00:14:46,700 --> 00:14:50,454
type hints when you use type hints. Apache beam raises exceptions during

231
00:14:50,492 --> 00:14:53,794
pipeline construction time itself, so you don't have to worry

232
00:14:53,842 --> 00:14:56,966
about runtime issues because you're catching most of the

233
00:14:56,988 --> 00:15:00,414
issues in the compile time itself. So those are some of

234
00:15:00,452 --> 00:15:04,110
the nuanced infrastructure and code

235
00:15:04,180 --> 00:15:07,594
considerations that I have learned across throughout

236
00:15:07,642 --> 00:15:11,200
my field experience in streaming, and I do hope that

237
00:15:11,970 --> 00:15:15,166
some of these resonated with you as well, and you could use them in

238
00:15:15,188 --> 00:15:18,494
your production pipelines. Thank you so much for having me today,

239
00:15:18,532 --> 00:15:19,850
and I hope you enjoyed the session.

