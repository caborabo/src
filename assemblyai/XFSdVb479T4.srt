1
00:00:20,760 --> 00:00:24,230
Hello, I'm Bongwani Shangwe and I'm here to present to you

2
00:00:24,262 --> 00:00:28,034
about running an open source large learning model on your own infrastructure.

3
00:00:28,514 --> 00:00:31,562
I first want to kick off with a short introduction about the company I

4
00:00:31,578 --> 00:00:35,338
work for at Aventa and what is it we do? Because a lot

5
00:00:35,346 --> 00:00:38,906
of people might know about our secondhand on seller

6
00:00:38,970 --> 00:00:42,306
marketplaces, but they do not know much about the

7
00:00:42,330 --> 00:00:45,666
brand adventure behind them. Adavinte is

8
00:00:45,690 --> 00:00:49,054
one of the world's leading online classified specialists with market

9
00:00:49,594 --> 00:00:53,094
across three continents containing over 25

10
00:00:53,674 --> 00:00:57,594
marketplace brands. Our marketplaces range from

11
00:00:57,714 --> 00:01:01,074
consumer goods, vehicles, real estate and

12
00:01:01,114 --> 00:01:04,426
jobs. Adventure consists of several marketplace

13
00:01:04,490 --> 00:01:07,826
brands like Gleanzeigen in Germany, Mark Blatz in the

14
00:01:07,850 --> 00:01:11,574
Netherlands, Lebencoin in France, Kijiji in Canada.

15
00:01:12,234 --> 00:01:15,122
Adavinte is a champion for sustainable commerce,

16
00:01:15,178 --> 00:01:19,214
making a positive impact on the environment, the economy and society

17
00:01:20,894 --> 00:01:24,950
by the numbers we get about 2.5

18
00:01:25,022 --> 00:01:28,154
billion monthly visits across our website.

19
00:01:28,974 --> 00:01:33,110
We have 25 plus marketplaces in our digital portfolio

20
00:01:33,182 --> 00:01:36,406
and over 5700 employees

21
00:01:36,510 --> 00:01:39,966
across ten continents. Now at Delavinto,

22
00:01:39,990 --> 00:01:43,150
we've been working on conversational search assistant which is

23
00:01:43,182 --> 00:01:46,854
geared to launch on Levin Quan, France for a b testing

24
00:01:46,934 --> 00:01:49,634
during the second quarter of this year.

25
00:01:50,154 --> 00:01:54,098
Conversational search is about building a smarter sharpening experience by allowing

26
00:01:54,146 --> 00:01:57,706
users to ask arbitrary questions and being guided to relevant

27
00:01:57,770 --> 00:02:01,626
recommendations and search results by an assistant in

28
00:02:01,650 --> 00:02:04,970
order to serve a greater user experience.

29
00:02:05,082 --> 00:02:08,794
Outside of the normal chat bots which are in use by

30
00:02:08,834 --> 00:02:12,322
most websites, the conversational search assistance will

31
00:02:12,338 --> 00:02:14,694
be backed by a large learning model service.

32
00:02:15,494 --> 00:02:19,446
Conversational search can be defined as a chat interface

33
00:02:19,630 --> 00:02:23,286
to enhance the user experience by allowing natural language interactions

34
00:02:23,350 --> 00:02:27,594
with software agents or virtual assistants to retrieve information.

35
00:02:28,654 --> 00:02:32,678
The product we envisioned is kind of like this.

36
00:02:32,726 --> 00:02:37,150
This is an example I drew up and

37
00:02:37,262 --> 00:02:41,234
with the conversational assistant, when it pops up or when you interact with it,

38
00:02:41,854 --> 00:02:45,534
you ask a general question. In this case, this user is looking for

39
00:02:45,574 --> 00:02:49,006
a Ford Focus or Fiesta and it's

40
00:02:49,110 --> 00:02:52,878
for the conversational search assistance to assist

41
00:02:52,926 --> 00:02:56,278
the user in narrowing down the search products and

42
00:02:56,326 --> 00:02:59,606
also asking for more preferences and what the user would like

43
00:02:59,710 --> 00:03:03,594
is specific. In this vehicle he's looking for

44
00:03:04,534 --> 00:03:08,614
basic infrastructure overview of how it works.

45
00:03:09,514 --> 00:03:12,890
The user would get of course

46
00:03:13,082 --> 00:03:17,242
interact with the conversational search assistant

47
00:03:17,418 --> 00:03:20,570
and which would in the background generate

48
00:03:20,602 --> 00:03:23,802
a query to call a conversational large learning model.

49
00:03:23,978 --> 00:03:27,146
The large learning model also gathers the history of

50
00:03:27,290 --> 00:03:31,402
the conversation which has been happening and it extracts

51
00:03:31,498 --> 00:03:35,564
that information in order to consolidate it and

52
00:03:36,824 --> 00:03:39,880
push the information to a search API

53
00:03:39,952 --> 00:03:42,484
to get relevant search results.

54
00:03:45,424 --> 00:03:49,248
Everything here the conversation large render model and extraction large

55
00:03:49,296 --> 00:03:53,724
learning model is backwards GPT 3.5

56
00:03:54,504 --> 00:03:57,800
outside from OpenAI, the team has also been looking

57
00:03:57,832 --> 00:04:01,044
at other providers for large learning model APIs.

58
00:04:02,214 --> 00:04:05,518
However, we found that there are some downsides in having to

59
00:04:05,566 --> 00:04:09,422
use a service provided largely model. Some points

60
00:04:09,518 --> 00:04:13,126
are the readiness of the service as this is still a new

61
00:04:13,150 --> 00:04:16,350
field and some providers are slow to

62
00:04:16,382 --> 00:04:19,502
open up to more customers to a

63
00:04:19,518 --> 00:04:23,150
larger scale. Thus it takes quite a while to get onboarded

64
00:04:23,182 --> 00:04:26,554
onto these services. There's the cost factor.

65
00:04:27,824 --> 00:04:30,968
Of course, this is a new additional cost for the team and

66
00:04:30,976 --> 00:04:34,496
the company and we also had to think about

67
00:04:34,560 --> 00:04:38,976
latency given that the services outside of adventure infrastructure,

68
00:04:39,040 --> 00:04:42,256
there's an additional latency we have to account

69
00:04:42,320 --> 00:04:45,384
for. Given some of these factors,

70
00:04:45,464 --> 00:04:49,432
we decided to investigate the use of open large learning models

71
00:04:49,488 --> 00:04:53,324
which we could possibly deploy into our own infrastructure.

72
00:04:53,814 --> 00:04:58,046
So during the proof of concept phase where

73
00:04:58,070 --> 00:05:01,354
we're using paid go to services, we also started exploring

74
00:05:02,974 --> 00:05:07,318
looking at hosting a large learning model with

75
00:05:07,366 --> 00:05:11,114
an enterprise service.

76
00:05:12,654 --> 00:05:16,214
You get the top, best quality and the latest

77
00:05:16,374 --> 00:05:19,674
large learning models which are being produced

78
00:05:20,724 --> 00:05:24,684
and versus for

79
00:05:24,724 --> 00:05:28,104
us. When you do a self hosted solution,

80
00:05:29,324 --> 00:05:32,940
the benefit is that you complete control of your application

81
00:05:33,092 --> 00:05:36,384
and your team is responsible for the system

82
00:05:37,164 --> 00:05:40,940
versus using an external API. You're dependent on that

83
00:05:40,972 --> 00:05:45,144
other system being up all the time for your service to run.

84
00:05:45,704 --> 00:05:49,724
Some benefits in using

85
00:05:52,224 --> 00:05:56,296
your own hosted large learning model is that you have greater privacy

86
00:05:56,400 --> 00:06:00,096
and compliance and you also avoid vendor lock

87
00:06:00,120 --> 00:06:03,936
in. We started exploring

88
00:06:04,120 --> 00:06:07,696
models to use by going to

89
00:06:07,720 --> 00:06:11,204
hugging face. Hugging face is currently the main platform

90
00:06:11,514 --> 00:06:15,442
or a website for building and using machine learning based models

91
00:06:15,498 --> 00:06:19,218
such as large render models. It also provides a platform to run these

92
00:06:19,266 --> 00:06:23,094
models on a smaller scale. In our case, we considered

93
00:06:23,434 --> 00:06:26,962
text generation based models. We first started

94
00:06:27,018 --> 00:06:30,938
off with the Falcon 7 billion meter perimeter tuned

95
00:06:30,986 --> 00:06:35,214
model to get familiar with deploying a large learning model.

96
00:06:36,194 --> 00:06:40,756
It's a lightweight model and it's

97
00:06:40,780 --> 00:06:44,664
quite easy to get started and set up. Though it is lightweight,

98
00:06:45,284 --> 00:06:49,068
we did find at lack depth when answering

99
00:06:49,196 --> 00:06:50,744
specific questions,

100
00:06:51,844 --> 00:06:55,796
specifically when we're looking at to use it for as a conversational

101
00:06:55,860 --> 00:06:59,884
search assistance. So then we started looking at other models

102
00:06:59,964 --> 00:07:03,492
which were out there which we could use. We looked at the Falcon

103
00:07:03,548 --> 00:07:07,432
40 millimeter chat tune model and the Loma 270

104
00:07:07,488 --> 00:07:09,084
billion chat tune model.

105
00:07:11,024 --> 00:07:14,400
Aside from being chat tuned, these models also provide multi

106
00:07:14,432 --> 00:07:18,856
language support which was a requirement for us as marketplaces

107
00:07:18,960 --> 00:07:23,144
across different countries with different language customer

108
00:07:23,264 --> 00:07:29,960
customers who speak different languages. So on

109
00:07:29,992 --> 00:07:33,924
deploying a model or hosting the large learner model. In this case,

110
00:07:34,754 --> 00:07:39,506
we found text generation interface or TGI for

111
00:07:39,530 --> 00:07:43,314
short. TGI is a fast optimized interface

112
00:07:43,354 --> 00:07:46,654
solution built for deploying and serving large learning models.

113
00:07:47,234 --> 00:07:50,694
TGI enables high performance text generation

114
00:07:51,074 --> 00:07:55,210
using tensor parallelism, dynamic batching

115
00:07:55,322 --> 00:07:59,210
for most popular and dynamic batch for most popular

116
00:07:59,402 --> 00:08:03,600
open source larger models. TGI also has

117
00:08:03,672 --> 00:08:07,256
a docker image which can be used to launch the text generation

118
00:08:07,320 --> 00:08:10,648
service. One of some of

119
00:08:10,656 --> 00:08:13,776
the benefits we found using TGI is that it's

120
00:08:13,800 --> 00:08:17,284
a simple launcher service to host your model.

121
00:08:17,904 --> 00:08:21,844
It's production ready as it provides tracing

122
00:08:22,424 --> 00:08:26,520
with open telemetry and Prometheus, this token

123
00:08:26,592 --> 00:08:30,172
streaming, you can have continuous batching

124
00:08:30,188 --> 00:08:34,772
of incoming requests for increased total throughput. There's quantitization

125
00:08:34,868 --> 00:08:38,236
with bits and bytes, stop sequences,

126
00:08:38,380 --> 00:08:42,624
and you can have custom prompt generation,

127
00:08:43,244 --> 00:08:47,236
and it also provides fine tuning support to

128
00:08:47,340 --> 00:08:51,100
fine tune your models. The easiest way to get

129
00:08:51,132 --> 00:08:54,784
started with the text generation interface

130
00:08:57,204 --> 00:09:01,188
tick generation inference is to run

131
00:09:01,276 --> 00:09:05,184
the simple command line, which is

132
00:09:06,324 --> 00:09:09,504
you just point to the model which you'd like to use.

133
00:09:10,204 --> 00:09:14,340
You set up where to store the model and

134
00:09:14,372 --> 00:09:17,836
all the dots, such as the model weights. And when you

135
00:09:17,860 --> 00:09:21,920
run this, it's a docker command. It will launch the docker,

136
00:09:22,052 --> 00:09:27,256
the TGI docker, and if

137
00:09:27,280 --> 00:09:30,684
your model is not present on your machine, it will download it.

138
00:09:31,064 --> 00:09:34,984
If it is present, it will just start. Yeah, it will start

139
00:09:35,024 --> 00:09:38,392
running. And to get going you

140
00:09:38,408 --> 00:09:42,024
can just, to test it out, you can use a simple curl

141
00:09:42,064 --> 00:09:44,284
command and as you can see,

142
00:09:45,624 --> 00:09:49,724
you submit the JSON and you get a response from the model.

143
00:09:50,124 --> 00:09:53,820
Now our team based our

144
00:09:53,852 --> 00:09:57,524
experimentations on GCP. Initially we

145
00:09:57,564 --> 00:10:02,012
tried to run the deployments on our european region of GCP.

146
00:10:02,188 --> 00:10:05,548
However, it appears that with the rise of GPU

147
00:10:05,676 --> 00:10:09,424
use applications such as large rental models, there's a scarcity

148
00:10:09,844 --> 00:10:14,460
of GPU variability and of GPU availability

149
00:10:14,612 --> 00:10:18,396
in Europe and north american regions. Actually, if you

150
00:10:18,540 --> 00:10:22,492
research this more, it seems to be occurring

151
00:10:22,548 --> 00:10:26,316
across all the cloud providers because of

152
00:10:26,500 --> 00:10:30,264
the current popularity of just GPU based applications.

153
00:10:30,964 --> 00:10:34,484
So outside of, we scanned outside of Europe and North

154
00:10:34,524 --> 00:10:38,304
America, and we found that there were two zones that could provide

155
00:10:40,324 --> 00:10:44,272
GPU's, which we could gets in

156
00:10:44,408 --> 00:10:48,080
for spot instances, and those were in

157
00:10:48,192 --> 00:10:51,564
East Asia and also

158
00:10:52,584 --> 00:10:56,080
in the Middle east one.

159
00:10:56,232 --> 00:11:00,304
To be specific, large training models require high

160
00:11:00,344 --> 00:11:04,192
GPU to train for inference. It can be less,

161
00:11:04,248 --> 00:11:07,624
but it's still generating a large amount

162
00:11:07,664 --> 00:11:11,400
of GPU memory required. The higher parameters

163
00:11:11,432 --> 00:11:15,464
the model contains, the more GPU memory required to run the inference

164
00:11:15,584 --> 00:11:19,440
of the model. There's also the possibility

165
00:11:19,512 --> 00:11:23,040
to run the model inference in eight bits and four bits mode,

166
00:11:23,232 --> 00:11:27,552
which decreases the amount of GPU memory required based

167
00:11:27,608 --> 00:11:30,992
on current GPU availability in cloud environments, the best options

168
00:11:31,048 --> 00:11:35,798
are machines that contain Nvidia a 180gb.

169
00:11:35,976 --> 00:11:39,922
However, these gpu's are extremely scarce, even in the cloud

170
00:11:39,978 --> 00:11:43,562
environment. We had then settled for

171
00:11:43,658 --> 00:11:47,186
using machines that contain a Nvidia a

172
00:11:47,210 --> 00:11:50,810
140gb. It is also possible to

173
00:11:50,842 --> 00:11:54,226
run a model on multiple gpu's in order to meet the requirement

174
00:11:54,290 --> 00:11:57,494
memory requirements of the model inference.

175
00:11:57,834 --> 00:12:01,154
So there's two ways we could have

176
00:12:01,274 --> 00:12:04,514
deployed these models

177
00:12:04,594 --> 00:12:07,814
and TGI inference.

178
00:12:08,874 --> 00:12:12,746
The first one was using Kubernetes or running

179
00:12:12,770 --> 00:12:17,114
it on a virtual machine. For doing on kubernetes,

180
00:12:17,274 --> 00:12:20,922
we just created a simple deployment yaml

181
00:12:21,098 --> 00:12:24,506
and we allowed for autoscaling because we didn't want

182
00:12:24,530 --> 00:12:29,058
to keep track of all the machines. So this

183
00:12:29,186 --> 00:12:32,332
seemed a bit better because when

184
00:12:32,428 --> 00:12:36,460
the models are not in use, it's easy to downscale as

185
00:12:36,532 --> 00:12:39,820
versus for if we're going to do it using

186
00:12:39,852 --> 00:12:43,196
a virtual machine. These were self managed machines

187
00:12:43,260 --> 00:12:47,704
and we'd have to keep track if on the capacity.

188
00:12:48,444 --> 00:12:52,704
Basically, if machines were not being used, then we'd have to shut them off ourselves.

189
00:12:53,364 --> 00:12:57,524
So we went into the because of this, we specifically started looking

190
00:12:57,644 --> 00:13:01,144
with doing on Kubernetes, on GKE.

191
00:13:02,404 --> 00:13:06,744
This is kind of a high level overview of GKE setup.

192
00:13:07,284 --> 00:13:11,588
You'd have the text generation inference deployment

193
00:13:11,636 --> 00:13:15,224
running in a docker image, pointing to

194
00:13:16,964 --> 00:13:20,820
a volume where all the weights

195
00:13:20,852 --> 00:13:22,664
were fetched from gcs.

196
00:13:23,284 --> 00:13:27,492
But we actually came into an issue when we

197
00:13:27,588 --> 00:13:31,524
started using GKe as

198
00:13:31,564 --> 00:13:35,020
we started, of course, we started with the

199
00:13:35,092 --> 00:13:38,544
Falcon seven, with the lightest Falcon model.

200
00:13:39,084 --> 00:13:43,024
Even with that, we found that the deployment time took longer than expected.

201
00:13:44,644 --> 00:13:48,268
GK GPU machines were also not really available

202
00:13:48,356 --> 00:13:52,572
for use even with kubernetes. And the

203
00:13:52,668 --> 00:13:56,074
highest or the highest

204
00:13:56,774 --> 00:13:59,954
GPU machine, or in this case

205
00:14:00,774 --> 00:14:04,670
node, GPU node which could be hosted with.

206
00:14:04,822 --> 00:14:08,974
We were able to get with GKE was a twelve gigabyte

207
00:14:09,014 --> 00:14:12,894
GPU, which was not enough to push on with

208
00:14:12,974 --> 00:14:16,838
other large rental models. So given

209
00:14:16,886 --> 00:14:20,654
that, we went back and said

210
00:14:20,694 --> 00:14:23,914
well, we're going to do the virtual machine deployment strategy.

211
00:14:24,814 --> 00:14:28,494
And with detection inference, it allows

212
00:14:28,534 --> 00:14:32,806
you to host the model on a

213
00:14:32,830 --> 00:14:36,718
vm with a single GPU, or parallelized

214
00:14:36,806 --> 00:14:39,554
inference across multiple gpu's.

215
00:14:41,134 --> 00:14:44,910
One of the benefits of parallelizing it across multiple gpu's

216
00:14:44,942 --> 00:14:48,790
is that you can meet using multiple gpu's.

217
00:14:48,822 --> 00:14:52,508
You can actually get have

218
00:14:52,556 --> 00:14:56,704
more higher memory across all the GPU's for inference.

219
00:14:57,884 --> 00:15:01,348
We started looking at running our

220
00:15:01,396 --> 00:15:05,064
experiments and we set up our experiments.

221
00:15:05,524 --> 00:15:09,780
When we deployed the models in a virtual machine,

222
00:15:09,932 --> 00:15:14,236
we set up notebooks to run different experiments

223
00:15:14,300 --> 00:15:18,260
so we can be able to track the data and compare the results

224
00:15:18,332 --> 00:15:19,304
later on.

225
00:15:20,964 --> 00:15:24,004
Yeah, so we looked at,

226
00:15:24,044 --> 00:15:28,180
so we had different cases where we deployed the model.

227
00:15:28,372 --> 00:15:31,676
The models on a single v, of a VM with

228
00:15:31,700 --> 00:15:35,344
a single GPU and a VM with multiple gpu's.

229
00:15:35,884 --> 00:15:38,868
Yeah, we had, for the single vm,

230
00:15:38,956 --> 00:15:42,508
we had a short response latency, which was really perfect

231
00:15:42,556 --> 00:15:45,194
for us. The deployment was quite quick,

232
00:15:46,254 --> 00:15:49,910
but the number of max tokens which could be processed

233
00:15:49,942 --> 00:15:53,790
was limited to the GPU memory. With a vm

234
00:15:53,862 --> 00:15:58,070
with multiple gpu's, we increased the GPU

235
00:15:58,102 --> 00:16:01,514
memory footprint so we had more GPU memory.

236
00:16:02,574 --> 00:16:06,154
And this in turn increased the Max token processing.

237
00:16:06,774 --> 00:16:10,294
The time it took for the model to be ready increased. So the deployment time

238
00:16:10,334 --> 00:16:13,488
also increased and the response latency also

239
00:16:13,576 --> 00:16:17,480
increased. There were several

240
00:16:17,552 --> 00:16:22,352
factors which we looked into why latency?

241
00:16:22,448 --> 00:16:26,520
The factors which came into latency with

242
00:16:26,552 --> 00:16:30,248
this and some of the during our

243
00:16:30,256 --> 00:16:33,952
experience, we found out that it

244
00:16:33,968 --> 00:16:36,764
was due to prompt size and complexity,

245
00:16:37,424 --> 00:16:41,024
so similar to if the

246
00:16:41,064 --> 00:16:44,124
prompt was long and quite complex,

247
00:16:44,824 --> 00:16:48,840
it will take longer for any

248
00:16:48,872 --> 00:16:50,164
of the models to process.

249
00:16:51,424 --> 00:16:55,136
However, the less indirect and short prompt it is,

250
00:16:55,280 --> 00:17:00,004
it opens up the ability for side effects such as hallucinations,

251
00:17:01,184 --> 00:17:04,904
specifically with latency. With GPU

252
00:17:04,944 --> 00:17:09,089
setup, we tracked this down that

253
00:17:09,201 --> 00:17:14,433
using more GPU's across vmware,

254
00:17:14,473 --> 00:17:17,733
more GPU's increased latency.

255
00:17:18,673 --> 00:17:22,121
We figured out that this could be between that there's a

256
00:17:22,137 --> 00:17:26,825
lot of IO happening between the GPU's,

257
00:17:26,849 --> 00:17:30,921
so there's a lot of dot exchange happening versus when you

258
00:17:30,977 --> 00:17:33,813
do an inference on a single GPU machine.

259
00:17:34,404 --> 00:17:37,904
And another thing which we tracked

260
00:17:38,564 --> 00:17:41,708
with regards to latency is the max token

261
00:17:41,756 --> 00:17:45,724
output. So we were able to set the max

262
00:17:45,764 --> 00:17:48,940
token output of the deployment,

263
00:17:49,132 --> 00:17:52,380
and the higher token output length, the response from the model

264
00:17:52,412 --> 00:17:56,624
becomes more detailed and longer, which requires more processing time.

265
00:17:57,284 --> 00:17:59,904
But if the token length is extremely short,

266
00:18:00,654 --> 00:18:04,554
the response time might not be complete or make sense to a human.

267
00:18:04,974 --> 00:18:08,990
Thus, there has to be a balance between how detailed the model response

268
00:18:09,022 --> 00:18:13,278
is and the quickness of the response, and more

269
00:18:13,326 --> 00:18:15,914
detailed analysis on this.

270
00:18:17,694 --> 00:18:21,174
As I mentioned, it is possible to set the number of tokens used

271
00:18:21,214 --> 00:18:24,302
for the output. All the previous

272
00:18:24,358 --> 00:18:27,566
tests we've done, we set the token outputs

273
00:18:27,590 --> 00:18:30,730
to 256, and once

274
00:18:30,762 --> 00:18:34,402
we doubled it, we were able to observe different

275
00:18:34,458 --> 00:18:36,734
effects from the models tested.

276
00:18:38,154 --> 00:18:42,186
So for the Falcon model with 256 tokens,

277
00:18:42,290 --> 00:18:45,946
maximum tokens, the model seemed to perform adequately. Once we

278
00:18:45,970 --> 00:18:49,082
increased the number of tokens, the response time initially

279
00:18:49,138 --> 00:18:52,722
seemed to be the same as before, but as the conversation

280
00:18:52,778 --> 00:18:56,214
continued with the user, the response time began to increase.

281
00:18:56,724 --> 00:19:00,532
Another effect was that the response became longer and

282
00:19:00,548 --> 00:19:03,156
the model started to hallucinate,

283
00:19:03,300 --> 00:19:06,544
appearing to have a conversation with itself.

284
00:19:07,764 --> 00:19:11,156
Llama two, we also again

285
00:19:11,220 --> 00:19:15,156
started with 256 maximum tokens and the models performed

286
00:19:15,180 --> 00:19:19,436
adequately. But we'd noted some instances that detect response

287
00:19:19,460 --> 00:19:23,156
would appear to cut off and with an increase in the

288
00:19:23,220 --> 00:19:27,040
token size, delicacy also increased quite

289
00:19:27,192 --> 00:19:30,912
extremely and the response was more complete.

290
00:19:31,008 --> 00:19:34,864
But as the conversation would continue, we get

291
00:19:34,904 --> 00:19:37,804
some instances where the response would get cut off again.

292
00:19:40,544 --> 00:19:44,456
As mentioned again, we have marketplaces

293
00:19:44,520 --> 00:19:48,352
across different countries or different languages, so it

294
00:19:48,368 --> 00:19:52,216
is important for us to test how this performs, and since we're

295
00:19:52,240 --> 00:19:55,838
launching first in Devonkwan, we looked at

296
00:19:55,926 --> 00:19:59,814
using french. So we tested the Falcon and Loma

297
00:19:59,854 --> 00:20:03,254
two on its ability to do inference with

298
00:20:03,294 --> 00:20:07,006
french users. The main adjustment

299
00:20:07,070 --> 00:20:11,254
was with the system prompt, directing the model to respond only

300
00:20:11,334 --> 00:20:12,274
in French.

301
00:20:14,054 --> 00:20:17,902
For Falcon, if the users question was in English

302
00:20:18,038 --> 00:20:22,634
and the model proceeded to respond

303
00:20:22,674 --> 00:20:26,226
in English, but if the user asked questions

304
00:20:26,250 --> 00:20:30,002
in French, the model responded in French, and if the

305
00:20:30,098 --> 00:20:33,282
preceding user questions was in English, the model remained

306
00:20:33,338 --> 00:20:36,882
only speaking in French, and if all

307
00:20:36,898 --> 00:20:40,418
the user questions were in French, the model start to

308
00:20:40,466 --> 00:20:43,778
respond to French. However, there were some side effects we

309
00:20:43,826 --> 00:20:48,174
observed and some hallucinations

310
00:20:48,674 --> 00:20:51,734
within the model when using the french language.

311
00:20:52,654 --> 00:20:56,554
In the case of lama two, if the user

312
00:20:56,854 --> 00:21:00,350
question if the user's first question was in English, the model

313
00:21:00,382 --> 00:21:04,154
proceeded to respond in English. If the user's

314
00:21:04,454 --> 00:21:08,486
first question was in French, the model responded in

315
00:21:08,510 --> 00:21:11,782
French. However, if the preceding questions were switched

316
00:21:11,798 --> 00:21:14,994
to English, the model reverted to responding in English,

317
00:21:15,734 --> 00:21:19,002
and if all the user's questions were

318
00:21:19,018 --> 00:21:23,066
in French, the model stopped responding in French. And what

319
00:21:23,090 --> 00:21:27,082
we can conclude from this is that both models are quite adequate in

320
00:21:27,258 --> 00:21:31,378
using responding to users

321
00:21:31,426 --> 00:21:35,374
in French. One thing to note here is that

322
00:21:35,874 --> 00:21:39,226
within our discovery, we found out there's no one to one

323
00:21:39,250 --> 00:21:42,634
switch between using different models. It is not easy to

324
00:21:42,674 --> 00:21:46,274
switch using the same prompts or techniques in

325
00:21:46,314 --> 00:21:49,722
one conversational search model

326
00:21:49,858 --> 00:21:53,122
and use it with another without experiencing any type

327
00:21:53,138 --> 00:21:56,854
of side effects. For example, the prompts we used

328
00:21:57,234 --> 00:22:01,626
for these experiments had to be adjusted to what we're using with

329
00:22:01,690 --> 00:22:06,122
OpenAPI conversational search login model and

330
00:22:06,178 --> 00:22:10,034
these experiments show though these experiments show points of success

331
00:22:10,414 --> 00:22:13,590
for more adjustments will need it

332
00:22:13,622 --> 00:22:16,902
to get responses to the quality,

333
00:22:17,038 --> 00:22:20,326
we need to do more adjustments to get responses to the

334
00:22:20,350 --> 00:22:22,794
quality which we're using with OpenAI.

335
00:22:23,574 --> 00:22:27,086
We also adjust the properties to launching the model service and the

336
00:22:27,110 --> 00:22:31,094
properties of the API calls, but we did not do an in depth evaluation

337
00:22:31,174 --> 00:22:35,004
with these properties. It is evident that with some

338
00:22:35,044 --> 00:22:38,864
slight adjustments, the latency and quality of the larger model changes.

339
00:22:39,284 --> 00:22:42,548
But, you know, further investigation would

340
00:22:42,556 --> 00:22:47,340
have needed to be done by a team to find the best properties

341
00:22:47,452 --> 00:22:51,544
for the required results.

342
00:22:52,404 --> 00:22:56,796
So you might be the

343
00:22:56,820 --> 00:23:00,184
big question is, how much is this all going to cost you?

344
00:23:00,704 --> 00:23:04,120
In our experience, we're able to run the models on a single a

345
00:23:04,152 --> 00:23:08,224
140 gigabyte gpu,

346
00:23:08,384 --> 00:23:11,784
and the cost calculations for that, for that

347
00:23:11,824 --> 00:23:15,104
model, for using a model, would come around to

348
00:23:15,144 --> 00:23:19,288
around just under $3,000

349
00:23:19,416 --> 00:23:23,232
a month. But since we have discovered that having

350
00:23:23,408 --> 00:23:26,444
more GPU is an advantage,

351
00:23:27,584 --> 00:23:31,776
we would need to select the highest gpu

352
00:23:31,800 --> 00:23:35,044
available would be the Nvidia, a 180 gigabyte,

353
00:23:35,704 --> 00:23:39,136
and that would come down to a round of a cost of

354
00:23:39,200 --> 00:23:40,804
4200.

355
00:23:42,384 --> 00:23:45,904
This estimation is for a single instance. If we were to

356
00:23:45,944 --> 00:23:49,632
scale out using Kubernetes autopilot or manually adding more

357
00:23:49,728 --> 00:23:53,210
vms, the cost of the of running the open logic

358
00:23:53,352 --> 00:23:56,686
model would grow significantly. It's also good to note

359
00:23:56,710 --> 00:24:00,718
that this does not these costs do not include human costs or

360
00:24:00,766 --> 00:24:04,154
networking costs and maintenance costs.

361
00:24:04,574 --> 00:24:07,742
So in our exploration

362
00:24:07,798 --> 00:24:11,670
phase, we looked at hosting

363
00:24:11,702 --> 00:24:16,166
a large learning model and comparing it to an

364
00:24:16,190 --> 00:24:20,124
enterprise large revenue model. Some of the so

365
00:24:20,164 --> 00:24:23,564
the downsides we actually ended up coming out of is that it's difficult

366
00:24:23,644 --> 00:24:27,412
to get adequate gpu and that there's a

367
00:24:27,428 --> 00:24:31,196
lot of high cost associated with running

368
00:24:31,220 --> 00:24:34,588
this model. It also would require internal

369
00:24:34,636 --> 00:24:38,464
support or expertise, and also security maintenance.

370
00:24:39,964 --> 00:24:43,812
On the other side, while we're running this proof of concept,

371
00:24:43,908 --> 00:24:46,420
we had come to the discovery that,

372
00:24:46,532 --> 00:24:50,276
well, OpenAI actually announced

373
00:24:50,340 --> 00:24:54,500
that they were decreasing the cost of chat GPT 3.5

374
00:24:54,612 --> 00:24:58,580
turbo and this made it more promising to

375
00:24:58,612 --> 00:25:03,116
use. And as for the slow API

376
00:25:03,220 --> 00:25:06,260
response time, we are also doing some fine tuning

377
00:25:06,292 --> 00:25:10,196
on the model, and by doing some fine tuning, it actually

378
00:25:10,260 --> 00:25:13,064
sped up the response time.

379
00:25:13,804 --> 00:25:17,460
So my learnings and outcome from

380
00:25:17,492 --> 00:25:20,884
this, from deploying large

381
00:25:20,924 --> 00:25:25,636
learning model, yeah, it's definitely possible based

382
00:25:25,700 --> 00:25:29,092
on a use case, and based on our use case, it's best to start off

383
00:25:29,148 --> 00:25:34,024
with a lightweight model like the Falcon seven

384
00:25:36,364 --> 00:25:40,180
and start using that internally. Just discover,

385
00:25:40,252 --> 00:25:43,664
play with it and figure out

386
00:25:46,044 --> 00:25:49,612
what properties you can use to offload, or what functionality

387
00:25:49,668 --> 00:25:53,356
you can use to offload to your internal model, and use that

388
00:25:53,380 --> 00:25:57,316
and slowly start offloading to it until you

389
00:25:57,340 --> 00:26:04,036
get to a stage where you can either have a balance of a

390
00:26:04,060 --> 00:26:08,024
pay to go service or have,

391
00:26:08,644 --> 00:26:12,508
or having fully running it on your

392
00:26:12,556 --> 00:26:16,636
own infrastructure. Thank you and I

393
00:26:16,660 --> 00:26:19,740
hope you enjoyed the talk. If you have any questions,

394
00:26:19,932 --> 00:26:21,844
please feel free to get in touch with me.

