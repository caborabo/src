1
00:00:20,600 --> 00:00:24,574
Hello and welcome. In this talk, we'll learn about how to use the influxDB 3.0

2
00:00:24,614 --> 00:00:28,168
go client library. My name is Ana Ystodis Giorgio and

3
00:00:28,176 --> 00:00:31,336
I'm a developer at Influx Data. Influx data is the creator

4
00:00:31,360 --> 00:00:35,176
of influxdb and we'll be learning about how to use influxdb's

5
00:00:35,200 --> 00:00:38,760
client libraries, specifically the go one. For those of you who aren't

6
00:00:38,792 --> 00:00:42,560
familiar with developer advocacy, the role of a developer advocate is someone who

7
00:00:42,592 --> 00:00:46,432
represents the community to the company and the company to the community. And the

8
00:00:46,448 --> 00:00:50,088
way that I do that is through giving talks like these, through creating

9
00:00:50,176 --> 00:00:53,920
technical code examples for how to use influxdb with a variety of

10
00:00:53,952 --> 00:00:57,876
other tools for a variety of use cases, whether that's an

11
00:00:57,900 --> 00:01:01,268
industrial iT example, predictive maintenance example,

12
00:01:01,316 --> 00:01:05,504
or data science example, or visualization example.

13
00:01:06,284 --> 00:01:09,796
I also answer questions on forums and Slack. So if

14
00:01:09,820 --> 00:01:13,452
you go to the influxdb forums or community and you have questions,

15
00:01:13,508 --> 00:01:17,164
likely I will be answering one of your questions there. I encourage you

16
00:01:17,204 --> 00:01:20,452
to reach out and connect with me on LinkedIn if you want to and you

17
00:01:20,468 --> 00:01:24,198
have any questions about today's presentation or anything,

18
00:01:24,326 --> 00:01:27,502
any feedback that you want to give about influxDB as well. That's another part of

19
00:01:27,518 --> 00:01:30,070
my role, is to try and give feedback back to products so that they can

20
00:01:30,102 --> 00:01:33,910
make more informed decisions. Before I begin

21
00:01:33,982 --> 00:01:36,902
and talk about how to use the go client library, I do want to take

22
00:01:36,958 --> 00:01:39,754
a quick second to introduce influxdB University.

23
00:01:40,134 --> 00:01:43,358
So influxdB University offers free, live and self

24
00:01:43,406 --> 00:01:47,086
paced training on a wide variety of topics related to influxdb.

25
00:01:47,190 --> 00:01:51,070
You know, things on how to get started, you know, how to

26
00:01:51,142 --> 00:01:54,774
build a hybrid architecture within FluxdB, how to

27
00:01:54,854 --> 00:01:58,502
query, how to use the client libraries, how to use additional tools

28
00:01:58,598 --> 00:02:01,950
and protocols. So yeah, if you

29
00:02:01,982 --> 00:02:04,782
have a time series use case and you want to learn more about how to

30
00:02:04,798 --> 00:02:07,990
use influencedB, highly recommend that you check out this resource. It's all

31
00:02:08,022 --> 00:02:11,630
entirely free and it allows you to earn digital badges so that you can share

32
00:02:11,662 --> 00:02:14,714
your achievements for, you know, specific classes that offer that.

33
00:02:15,474 --> 00:02:18,562
And before I go into how we are going to

34
00:02:18,578 --> 00:02:22,242
use the go client library to query and write data to influxdb, I wanted to

35
00:02:22,258 --> 00:02:25,346
take a step back and just kind of talk about time series data in general

36
00:02:25,410 --> 00:02:29,122
because it's kind of necessary to give us

37
00:02:29,138 --> 00:02:32,882
some context for why it is that you need a time series database. So what

38
00:02:32,898 --> 00:02:36,410
is time series data? Well, time series data is any data

39
00:02:36,442 --> 00:02:39,586
that has a time stamp associated with it. So probably one of

40
00:02:39,610 --> 00:02:43,344
the most classic examples of time series data is stock market data.

41
00:02:43,424 --> 00:02:47,360
And the other interesting thing about time series data is that individual points are

42
00:02:47,392 --> 00:02:50,720
not really that valuable. Right. When we think of, like, a single stock value.

43
00:02:50,792 --> 00:02:54,336
Not that interesting. What we're really focusing on is the trend of

44
00:02:54,360 --> 00:02:57,608
stock over time. That's interesting because it helps us determine whether or

45
00:02:57,616 --> 00:03:01,080
not we should buy or sell. So really, time series data

46
00:03:01,112 --> 00:03:04,280
is a sequence of data points. And that makes it quite different from data

47
00:03:04,312 --> 00:03:07,896
that you would store in a relational database, which usually, like, one point

48
00:03:08,000 --> 00:03:11,206
or one record is really one row is

49
00:03:11,230 --> 00:03:14,486
really everything that you need. And this is

50
00:03:14,510 --> 00:03:17,838
also true of any IoT data. You know,

51
00:03:17,926 --> 00:03:21,350
industrial IoT or IoT data can include anything that's

52
00:03:21,382 --> 00:03:24,422
coming from a sensor. So that can be vibration, temperature,

53
00:03:24,518 --> 00:03:29,142
pressure, humidity, concentration, light velocity,

54
00:03:29,318 --> 00:03:33,342
etcetera. Anything that we are measuring about our physical

55
00:03:33,398 --> 00:03:36,798
environment with a sensor is likely time series data. We also get

56
00:03:36,846 --> 00:03:40,508
a lot of time series data coming out of the virtual

57
00:03:40,556 --> 00:03:44,316
world, whether that's DevOps monitoring, whether that's application

58
00:03:44,380 --> 00:03:47,796
monitoring or performance monitoring, network monitoring.

59
00:03:47,900 --> 00:03:51,420
That's also a huge source of all time series data. And time

60
00:03:51,452 --> 00:03:54,628
series data usually falls into two categories,

61
00:03:54,716 --> 00:03:58,244
metrics and events. So metrics are usually predictable,

62
00:03:58,284 --> 00:04:02,148
and they occur at the same time interval. So if we were to think of

63
00:04:02,276 --> 00:04:06,660
an IoT use case, maybe we have a sensor that's on

64
00:04:06,772 --> 00:04:10,048
a robot, and we are

65
00:04:10,096 --> 00:04:14,280
looking at the vibration of this robot. So we have a vibration sensor.

66
00:04:14,472 --> 00:04:18,032
So we might be pulling the vibration sensor, you know,

67
00:04:18,088 --> 00:04:22,048
at a rate, we're getting a reading per second from an accelerometer,

68
00:04:22,096 --> 00:04:25,640
let's say accelerator, for let's say so in that case,

69
00:04:25,712 --> 00:04:30,112
that's a metric. An event, however, is an unpredictable type

70
00:04:30,128 --> 00:04:33,440
of time series data. We cannot derive when an event will occur,

71
00:04:33,632 --> 00:04:37,340
but we can still store the event data. And in this case,

72
00:04:37,372 --> 00:04:40,372
we might be thinking of something like a machine fault alert. We don't know when

73
00:04:40,388 --> 00:04:43,444
that machine will next register for a fault, but we can store that

74
00:04:43,484 --> 00:04:47,012
event, that time series event, when it does. One interesting relation between

75
00:04:47,068 --> 00:04:51,492
events and metrics, though, is also how we can transform an

76
00:04:51,508 --> 00:04:55,004
event into a metric. So if we did a daily count of how many machine

77
00:04:55,044 --> 00:04:58,612
faults occurred, then we would be getting a

78
00:04:58,748 --> 00:05:01,932
regular count of zero or more, and we'd

79
00:05:01,948 --> 00:05:05,196
get that reading every single day. So then we'd be converting that event into a

80
00:05:05,220 --> 00:05:08,904
metric. So what is a time series database?

81
00:05:09,324 --> 00:05:12,904
Basically, a time series database has four components or pillars.

82
00:05:13,244 --> 00:05:16,924
The first is that it ingests timestamp data, time series data.

83
00:05:16,964 --> 00:05:20,636
So every data point in the time series database is associated with a timestamp,

84
00:05:20,780 --> 00:05:24,012
and it allows you to query in time order. So that's similar

85
00:05:24,068 --> 00:05:28,364
to a data historian. One big difference between data historians

86
00:05:28,404 --> 00:05:32,504
and legacy tooling like that in influxdB is the lack of vendor lock in,

87
00:05:32,974 --> 00:05:37,030
especially with influxdB v three, we're really focused on interoperability

88
00:05:37,222 --> 00:05:41,286
with other tools and eventually even being able to allow you to

89
00:05:41,470 --> 00:05:45,334
later this year to, you know, query parquet directly from influxdb

90
00:05:45,374 --> 00:05:49,022
so that you can use it with other machine learning tools, for example, and also

91
00:05:49,078 --> 00:05:52,830
put it into other data stores as you need. So anyway,

92
00:05:52,862 --> 00:05:56,390
so yeah, one component of a time series database is ingesting time

93
00:05:56,422 --> 00:06:00,054
series data. The other is accommodating for really high throughput.

94
00:06:00,174 --> 00:06:03,154
So in most cases, in time series use cases,

95
00:06:03,664 --> 00:06:07,512
we're going to get really high volumes of batch data

96
00:06:07,608 --> 00:06:10,960
or real time streams from multiple endpoints. So you're

97
00:06:10,992 --> 00:06:14,760
thinking thousands of sensors or sensors with high throughput. For example,

98
00:06:14,792 --> 00:06:18,404
if we think of specifically that vibration sensor that we were talking about earlier,

99
00:06:18,824 --> 00:06:23,272
an industrial vibration sensor on

100
00:06:23,328 --> 00:06:26,576
average can be up to 10

101
00:06:26,600 --> 00:06:30,480
khz/second that's 10,000 points every second that you'd be writing.

102
00:06:30,592 --> 00:06:34,330
And you'd want to write that not just for that one sensor presumably, but for

103
00:06:34,362 --> 00:06:38,018
multiple. So you can see how the throughput requirements for

104
00:06:38,066 --> 00:06:41,254
time series database has to be able to be really high.

105
00:06:41,834 --> 00:06:45,362
Additionally, makes sense that if you are writing a lot of data to a

106
00:06:45,378 --> 00:06:48,854
database, you also want to be able to query it with the same sort efficiency.

107
00:06:49,314 --> 00:06:53,658
And specifically with time series, you want to be able to do things like have

108
00:06:53,746 --> 00:06:57,554
access to performant aggregations over time, whether that's averages,

109
00:06:57,714 --> 00:07:01,282
sums, global minimums and maximums. So you need to be able to scan

110
00:07:01,338 --> 00:07:04,922
over large ranges of this data and return

111
00:07:05,098 --> 00:07:08,514
the results very quickly. And last but not least, you need to think about

112
00:07:08,594 --> 00:07:12,354
scalability and performance. You know, it should be designed to

113
00:07:12,394 --> 00:07:15,874
scale horizontally to handle the increased load that is

114
00:07:15,914 --> 00:07:18,694
often associated with time series use cases,

115
00:07:19,034 --> 00:07:22,174
often across distributed clusters of machines.

116
00:07:25,074 --> 00:07:28,282
So let's talk about influxdB 3.0 specifically. So this is

117
00:07:28,298 --> 00:07:31,610
what our architecture diagram looks like for the 3.0 build.

118
00:07:31,762 --> 00:07:35,178
Basically, we really believe in open architecture and open

119
00:07:35,226 --> 00:07:39,266
data format, which means that we are part of the Apache ecosystem and

120
00:07:39,290 --> 00:07:42,774
contributing to those upward projects, upstream projects.

121
00:07:43,154 --> 00:07:46,250
So influxdB is built on data fusion, and data fusion

122
00:07:46,282 --> 00:07:50,378
is the query execution framework that allows

123
00:07:50,426 --> 00:07:53,494
influxDB users to query influxDB with SQL.

124
00:07:53,994 --> 00:07:57,282
Then we have Apache Arrow and Arrow is what we

125
00:07:57,298 --> 00:08:00,850
use for our in memory columnar format. And columnar

126
00:08:00,882 --> 00:08:05,082
format is really well suited for time series use cases for multiple reasons.

127
00:08:05,138 --> 00:08:09,090
The first is that oftentimes, especially when we are monitoring

128
00:08:09,122 --> 00:08:12,586
our environment and we're looking at to monitor changes in our environment.

129
00:08:12,690 --> 00:08:16,092
That means that we'll also typically have a lot of repeated values. So if

130
00:08:16,108 --> 00:08:19,420
we think of monitoring, for example, the temperature of a room,

131
00:08:19,572 --> 00:08:22,068
most likely that room temperature is going to be the same. So we're going to

132
00:08:22,076 --> 00:08:25,540
have a lot of repeated values. And if we store things in a columnar format,

133
00:08:25,652 --> 00:08:27,784
then we get really cheap compression there.

134
00:08:28,124 --> 00:08:31,980
Additionally, you know, you're likely going to want to perform things like

135
00:08:32,012 --> 00:08:35,108
find the max and min values across, you know,

136
00:08:35,196 --> 00:08:37,464
millions and millions of points sometimes.

137
00:08:38,364 --> 00:08:42,028
And so not having to scan through every single

138
00:08:42,076 --> 00:08:45,788
row in order to find global

139
00:08:45,836 --> 00:08:49,244
mins and maxes make time series. Additionally really well suited

140
00:08:49,284 --> 00:08:52,404
to a columnar format, then our durable file

141
00:08:52,444 --> 00:08:55,980
format is parquet, and we're also branching out

142
00:08:56,012 --> 00:08:59,372
into leveraging iceberg as well. And for those of you

143
00:08:59,388 --> 00:09:02,932
who are familiar with Parquet, you'll know just how efficient it

144
00:09:02,948 --> 00:09:06,660
is and how much cheaper storage it

145
00:09:06,692 --> 00:09:10,220
is than something like CSV as its counterpart, for example.

146
00:09:10,332 --> 00:09:13,628
And being able to eventually have a feature come out where we'll be

147
00:09:13,636 --> 00:09:17,036
able to query parquet directly from influxdB and use

148
00:09:17,060 --> 00:09:20,852
those parquet files for further data processing or

149
00:09:20,948 --> 00:09:24,104
in conjunction with other machine learning tools just really

150
00:09:24,644 --> 00:09:28,224
increases the interoperability of influxdb.

151
00:09:30,244 --> 00:09:33,132
So today, though, we're going to focus on the go client library.

152
00:09:33,228 --> 00:09:36,564
We're going to talk about what it is, then the requirements to actually use it,

153
00:09:36,604 --> 00:09:39,692
and we'll learn about how we can write data to influxdb v three with the

154
00:09:39,708 --> 00:09:43,440
go client and how we can query. And last but not least, I'll share some

155
00:09:43,472 --> 00:09:47,048
resources and some places that you can get additional help if you have any questions

156
00:09:47,096 --> 00:09:50,672
about this presentation. So what

157
00:09:50,688 --> 00:09:53,920
is the influxdb go client library? Well, it's essentially

158
00:09:53,952 --> 00:09:57,920
a software package that provides a set of tools and functions that allow

159
00:09:58,112 --> 00:10:01,424
any influxdb user using go

160
00:10:01,504 --> 00:10:04,524
to write in query to influxdb v three.

161
00:10:05,824 --> 00:10:09,596
So some of the advantages to using the influxdb v

162
00:10:09,620 --> 00:10:13,204
three go client library is that first and foremost, it wraps

163
00:10:13,244 --> 00:10:16,588
the Apache arrow client in

164
00:10:16,596 --> 00:10:20,324
a convenient influxdb v three interface, so it allows you to execute

165
00:10:20,364 --> 00:10:24,420
SQL and influx QL queries. InfluxqL is a query language

166
00:10:24,492 --> 00:10:27,836
that is proprietary to influxdb,

167
00:10:27,900 --> 00:10:30,964
specific to influxdb. It's very SQL esque, but v

168
00:10:31,004 --> 00:10:34,660
one users will have been familiar with influxql, so we wanted

169
00:10:34,692 --> 00:10:36,944
to carry that over in influxdbv three.

170
00:10:38,364 --> 00:10:41,940
And essentially you get to leverage using Apache

171
00:10:41,972 --> 00:10:45,596
Arrow and Apache Aeroflight and that flight protocol with

172
00:10:45,620 --> 00:10:48,948
your pc. So it really enables you to

173
00:10:49,076 --> 00:10:52,884
transport really large datasets over network interface by leveraging

174
00:10:53,004 --> 00:10:56,108
arrow and also the flight

175
00:10:56,156 --> 00:11:00,028
protocol provides really efficient serialization and deserialization

176
00:11:00,196 --> 00:11:04,340
as well as bi directional streaming, which just makes it really efficient for

177
00:11:04,412 --> 00:11:07,860
querying really large datasets from influxdB. And kind of

178
00:11:07,972 --> 00:11:11,996
touching upon just handling that high throughput use case that we're

179
00:11:12,020 --> 00:11:15,948
talking about. So how does the go client library work?

180
00:11:15,996 --> 00:11:19,004
Well, writes are implemented via our own write API,

181
00:11:19,044 --> 00:11:22,596
endpoint. You write data with line protocol, and line protocol is

182
00:11:22,620 --> 00:11:26,396
the ingest format. With influxdB, of course, we have methods

183
00:11:26,420 --> 00:11:29,524
that will generate that format for you within the go client. So we'll talk about

184
00:11:29,564 --> 00:11:33,310
that and then queries are implemented,

185
00:11:33,342 --> 00:11:36,434
like I mentioned, via the Apache aeroflight client.

186
00:11:37,974 --> 00:11:41,714
So some requirements for getting started first but not

187
00:11:42,534 --> 00:11:46,262
last, is to actually have an influxDB cloud

188
00:11:46,318 --> 00:11:49,654
3.0 account. So if you sign up, you can get a free

189
00:11:49,694 --> 00:11:52,894
tier. And that's probably the easiest

190
00:11:52,934 --> 00:11:56,376
way to get a feel for whether or not you even like influxdB, because you

191
00:11:56,400 --> 00:12:00,088
don't have to download anything or install anything, you can just get started.

192
00:12:00,256 --> 00:12:03,392
Then you'll need to actually create a database. This is also referred to

193
00:12:03,408 --> 00:12:07,408
as a bucket. They're the same thing. The only difference

194
00:12:07,496 --> 00:12:11,084
between a bucket and a database in like SQL is that

195
00:12:11,424 --> 00:12:14,712
within the context of influxdB, a bucket or database also has a

196
00:12:14,728 --> 00:12:18,472
retention policy assigned to it or associated

197
00:12:18,528 --> 00:12:22,176
with it. And a retention policy just determines how long you'll keep the

198
00:12:22,200 --> 00:12:25,762
data before you automatically expire it. And that's just a handy,

199
00:12:25,858 --> 00:12:29,894
convenient part of having a time series specific database.

200
00:12:30,594 --> 00:12:34,130
Just because that's a common function that you're going to want to perform is automatically

201
00:12:34,162 --> 00:12:38,374
expiring old data. You'll also need an authentication token.

202
00:12:41,194 --> 00:12:44,922
So let's actually learn how we can create a bucket and

203
00:12:44,938 --> 00:12:48,002
get our authentication token. Essentially what we'll do,

204
00:12:48,058 --> 00:12:51,794
this is what the UI looks like. We'll go to the load data pages,

205
00:12:51,874 --> 00:12:55,160
we'll hit the button tab. We'll say create

206
00:12:55,192 --> 00:12:58,768
a bucket. We'll give our bucket a name. Maybe our bucket's going to be called

207
00:12:58,816 --> 00:13:02,048
my bucket. We'll assign a retention policy. The default is 30

208
00:13:02,096 --> 00:13:05,128
days. We'll just leave it that way. Now we can also create a token in

209
00:13:05,136 --> 00:13:08,224
the load data page. We'll say generate an API token.

210
00:13:08,344 --> 00:13:11,760
You can generate an all access token or customize it to read and write just

211
00:13:11,792 --> 00:13:15,504
from a specific bucket. And then you have your token there that you

212
00:13:15,624 --> 00:13:19,564
might need as well for actually initializing your go client.

213
00:13:20,654 --> 00:13:24,830
Okay, so now let's talk about installation. So basically

214
00:13:24,982 --> 00:13:28,350
you're going to want to add the latest version of the client package to your

215
00:13:28,382 --> 00:13:32,910
projects dependency with go get and

216
00:13:32,942 --> 00:13:36,686
then we're ready to actually write data. So we're going to want to import our

217
00:13:36,710 --> 00:13:40,190
packages and then instantiate the client to write in query influxdb v.

218
00:13:40,222 --> 00:13:43,726
Three by providing our credentials. So this is what instantiating the

219
00:13:43,750 --> 00:13:47,262
client looks like. This is just kind of, you know, all boilerplate.

220
00:13:47,398 --> 00:13:50,726
We import our package and then we get our

221
00:13:50,750 --> 00:13:54,494
environment variables, and that includes the URL that is

222
00:13:54,534 --> 00:13:57,774
associated with your influxdb cloud account, your influxdb

223
00:13:57,814 --> 00:14:01,554
token that we just generated, and the database that we want to write to.

224
00:14:02,374 --> 00:14:04,914
And then we can actually instantiate that client.

225
00:14:06,054 --> 00:14:09,486
And now we'll talk about writing data so we can

226
00:14:09,510 --> 00:14:12,674
write data with line protocol in the line protocol format.

227
00:14:12,974 --> 00:14:16,406
Line protocol is just the ingest format for influxdB, and it

228
00:14:16,430 --> 00:14:19,654
consists of the following components, measurements, tags,

229
00:14:19,694 --> 00:14:23,390
fields, and timestamps. And I'll show an example

230
00:14:23,422 --> 00:14:26,926
of that in just a second. And when we write a point

231
00:14:26,950 --> 00:14:30,726
to influxdB, the only difference between tags

232
00:14:30,750 --> 00:14:34,830
and fields is that tags are used to store metadata to your instance, and fields

233
00:14:34,862 --> 00:14:38,942
are where you contain your actual time series data. However, in practice,

234
00:14:39,078 --> 00:14:42,526
both fields and tags do get converted into columns within a

235
00:14:42,550 --> 00:14:45,710
table in influxdb. So in practice they're really

236
00:14:45,742 --> 00:14:49,190
identical once you query. This distinction is really only for

237
00:14:49,222 --> 00:14:52,670
organizational purposes when you're writing your line protocol. But if

238
00:14:52,702 --> 00:14:56,110
you get confused between what to make a tag

239
00:14:56,142 --> 00:14:59,638
or a field, it's not really a big deal. Measurements are just basically what you

240
00:14:59,646 --> 00:15:03,190
would think of as your table name. And your timestamp is going to be

241
00:15:03,342 --> 00:15:07,318
in Unix precision or Unix

242
00:15:07,366 --> 00:15:11,190
format, sorry. And you'll obviously need that with every data

243
00:15:11,222 --> 00:15:13,234
point because we are writing time series data.

244
00:15:14,654 --> 00:15:17,990
So this is an example of what line protocol looks like. We have our

245
00:15:18,022 --> 00:15:21,974
table, our measurement called stat. Then we separate our

246
00:15:22,014 --> 00:15:25,646
tags and tag keys and tag values with a

247
00:15:25,670 --> 00:15:29,606
comma. So in this case, we have one tag called unit with a tag

248
00:15:29,750 --> 00:15:33,540
value of temperature. And then we have two fields, average and max,

249
00:15:33,732 --> 00:15:37,500
with, you know, two actual time series values,

250
00:15:37,612 --> 00:15:41,412
23.5 and 45.0. If you don't provide

251
00:15:41,508 --> 00:15:44,836
a timestamp, one will be generated at the time of write.

252
00:15:44,900 --> 00:15:47,972
When you write it, we can also use the point

253
00:15:48,028 --> 00:15:50,812
method instead. And this is probably what you would do if you were using the

254
00:15:50,828 --> 00:15:54,476
go client. And so yeah,

255
00:15:54,500 --> 00:15:57,860
we'll use the write points method, or you can also use the new point method

256
00:15:57,892 --> 00:16:01,042
to create a point. And you can append points to an array to write an

257
00:16:01,058 --> 00:16:04,882
array of points and pass those into the write points

258
00:16:04,938 --> 00:16:08,466
method as well. And data is also written synchronously. So this is what this looks

259
00:16:08,490 --> 00:16:11,746
like if you wanted to create basically a point that's

260
00:16:11,770 --> 00:16:15,162
similar to the line protocol that we just wrote. You would use that new point

261
00:16:15,218 --> 00:16:18,494
method and include the measurement,

262
00:16:19,154 --> 00:16:23,146
any tags that you have and any fields and any timestamp that

263
00:16:23,170 --> 00:16:27,104
you have. And then you'd pass that into the write points method

264
00:16:27,524 --> 00:16:30,932
along with the database that you want to write it to and

265
00:16:30,948 --> 00:16:34,340
write your point that way. Now there's an important note about

266
00:16:34,372 --> 00:16:37,940
upsets. You can always upset a field, but not a tag.

267
00:16:38,012 --> 00:16:40,944
So for example, if you add a second point,

268
00:16:41,244 --> 00:16:44,308
we'll notice the addition of a two to the field value.

269
00:16:44,396 --> 00:16:48,380
For example, I add the second point in the line

270
00:16:48,412 --> 00:16:52,276
of code below. We would upset those field values and the

271
00:16:52,300 --> 00:16:55,412
previous values would be overwritten. So this first line here

272
00:16:55,428 --> 00:16:58,904
where I have an average of 23.5 and 45.0,

273
00:16:59,644 --> 00:17:03,020
that would be overwritten by the second point

274
00:17:03,052 --> 00:17:06,484
that I wrote of 23.52 and

275
00:17:06,604 --> 00:17:09,948
45.2. Notice that the timestamps are the same as well.

276
00:17:09,996 --> 00:17:13,024
So then that upsurt would work. However,

277
00:17:13,924 --> 00:17:18,068
if we do add a second point and

278
00:17:18,116 --> 00:17:21,276
we change the tags, then we're not upsetting those values. So that

279
00:17:21,300 --> 00:17:24,876
is another thing to think about when you are thinking about the difference between tags

280
00:17:24,900 --> 00:17:28,412
and fields, it's just upsets. So in

281
00:17:28,428 --> 00:17:32,132
this case, if I added a second point with unit equals temperature two,

282
00:17:32,188 --> 00:17:35,732
that's an entirely different tag value. You can think

283
00:17:35,748 --> 00:17:39,756
of that as a different time series, right? Maybe more concrete

284
00:17:39,780 --> 00:17:43,652
example for tag would be sensor. And if I had sensorid and then

285
00:17:43,788 --> 00:17:46,636
I had one value equal to one and one value equal to two, it makes

286
00:17:46,660 --> 00:17:49,764
sense that we don't want to upsert that because most likely that is in fact

287
00:17:49,804 --> 00:17:53,626
a true new time series. Um, let's talk about

288
00:17:53,650 --> 00:17:57,266
how we can query data with the go client library. Well, we can use SQL

289
00:17:57,290 --> 00:18:00,666
to query. So following the example that we've been

290
00:18:00,690 --> 00:18:04,202
using, I can provide a simple SQL query like select all

291
00:18:04,258 --> 00:18:08,002
from that stat table where time is between now and

292
00:18:08,018 --> 00:18:11,130
the last five minutes. And I want to include the

293
00:18:11,162 --> 00:18:13,814
tag, the temperature tag.

294
00:18:14,714 --> 00:18:18,052
We can do that and then pass that query into

295
00:18:18,148 --> 00:18:20,864
our query method and then print the values.

296
00:18:21,644 --> 00:18:24,504
There are some additional method parameters that you can use,

297
00:18:25,764 --> 00:18:29,544
the context for the request, the database

298
00:18:30,164 --> 00:18:33,292
that you want to use for the database operations and the query. So those are

299
00:18:33,308 --> 00:18:36,652
all the things that we included. And so here's how you

300
00:18:36,668 --> 00:18:40,980
can query explicitly in that instance and

301
00:18:41,012 --> 00:18:44,998
actually provide those options and also the query

302
00:18:45,046 --> 00:18:48,558
type that you want. So you can switch between querying within

303
00:18:48,646 --> 00:18:52,790
with SQL or not. So here's an example of how you can query with influxql

304
00:18:52,822 --> 00:18:56,390
if you wanted to. Instead, we're using this influxql command

305
00:18:56,422 --> 00:18:59,894
called show measurements, which would show all the measurements or tables within

306
00:18:59,934 --> 00:19:03,830
our database and we are specifying that query option to

307
00:19:03,862 --> 00:19:08,038
be influxql instead of flight SQL and

308
00:19:08,126 --> 00:19:10,874
able to return all of our measurements.

309
00:19:11,754 --> 00:19:14,774
And similarly, here are some method parameters as well.

310
00:19:15,754 --> 00:19:19,738
I encourage you to check out all full code examples at the influx

311
00:19:19,786 --> 00:19:23,842
community so the influx community on GitHub contains a variety

312
00:19:23,898 --> 00:19:28,334
of examples for how to use influxdb

313
00:19:28,874 --> 00:19:32,074
with a bunch of projects on everything from

314
00:19:32,114 --> 00:19:35,054
IoT to data science and machine learning.

315
00:19:35,954 --> 00:19:39,162
I highly encourage you to go there because if there's something that you're looking to

316
00:19:39,178 --> 00:19:43,208
do with influxdb, there's a high probability that that we've already created a sample project

317
00:19:43,256 --> 00:19:46,816
which is a great place for you to get started. Last but not least,

318
00:19:46,840 --> 00:19:50,960
I wanted to share some final resources for getting help. The first is

319
00:19:51,112 --> 00:19:54,360
within the influx community. That's where all of our client libraries for v three are

320
00:19:54,392 --> 00:19:58,200
maintained, so please go there and take a look at it.

321
00:19:58,312 --> 00:20:02,944
Also our go client library documentation at Docs dot in fluxdata.com.

322
00:20:03,064 --> 00:20:06,656
Just make sure that you are using the right documentation for the

323
00:20:06,680 --> 00:20:10,686
right product by selecting your product in the upper right hand

324
00:20:10,750 --> 00:20:13,634
before you continue using the docs.

325
00:20:14,654 --> 00:20:17,518
And last but not least, I want to encourage you to please join our community

326
00:20:17,606 --> 00:20:21,926
slack workspace and participate in all conversations about influxdb.

327
00:20:22,070 --> 00:20:25,234
Go Python, whatever language you're using

328
00:20:25,894 --> 00:20:28,954
and any questions that you have about time series in general.

329
00:20:29,334 --> 00:20:32,270
And last but not least, I also wanted to let you know about our forums.

330
00:20:32,302 --> 00:20:36,026
Those are a great resource. Well, I mentioned

331
00:20:36,050 --> 00:20:39,450
GitHub already, our docs and also our blogs

332
00:20:39,482 --> 00:20:42,826
where you can find all these examples and more for

333
00:20:42,850 --> 00:20:44,934
how to get started using influxdb.

334
00:20:46,234 --> 00:20:49,850
Last but not least, I want to encourage you to try the free trial and

335
00:20:49,882 --> 00:20:53,574
get started for yourself. You can scan that QR code

336
00:20:53,914 --> 00:20:56,114
to learn more about how to use influxdb.

