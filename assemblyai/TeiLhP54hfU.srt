1
00:00:21,200 --> 00:00:24,726
Welcome to LLM 2024 organized by

2
00:00:24,750 --> 00:00:28,098
conferred tour. My name is Indika Vimalasurier

3
00:00:28,186 --> 00:00:32,378
and I'll walk you through about how you can leverage observability

4
00:00:32,466 --> 00:00:36,082
maturity model improve end

5
00:00:36,098 --> 00:00:39,442
user experience of the apps you are going to develop

6
00:00:39,498 --> 00:00:43,370
using LL lens. So we will touch about how to start

7
00:00:43,442 --> 00:00:47,014
which is the foundation, and then probably take it up to

8
00:00:47,514 --> 00:00:51,050
around using AI to

9
00:00:51,082 --> 00:00:54,814
support your operations. As you might aware,

10
00:00:55,234 --> 00:00:57,174
by around 2022,

11
00:00:58,674 --> 00:01:02,674
the hype started with Chatgbt. ChatgBT was

12
00:01:02,714 --> 00:01:06,530
a hit, it was mainstream and it resulted

13
00:01:06,562 --> 00:01:10,842
in lot of people who are not into AI start creating

14
00:01:10,978 --> 00:01:15,074
generative AI apps. So now it's already has taken over

15
00:01:15,114 --> 00:01:18,704
the world. The world is looking at what are the use cases

16
00:01:18,874 --> 00:01:23,060
which we can use and

17
00:01:23,092 --> 00:01:26,236
leverage. It's already mainstream.

18
00:01:26,380 --> 00:01:30,436
There's lot of developers who are building apps

19
00:01:30,580 --> 00:01:33,864
connecting llms. So there's a need.

20
00:01:34,644 --> 00:01:37,676
Apps which we are going to develop has a capability of

21
00:01:37,700 --> 00:01:41,932
providing full end user experience because

22
00:01:42,028 --> 00:01:45,448
we all know how it can end. While generative

23
00:01:45,496 --> 00:01:48,688
AI is which is opening creating

24
00:01:48,736 --> 00:01:52,696
lot of new opportunities. We also want to ensure that the apps which

25
00:01:52,720 --> 00:01:55,816
is being developed, deployed properly

26
00:01:55,840 --> 00:01:59,776
in production environments and are being served to end users

27
00:01:59,960 --> 00:02:03,296
as per the expectation and we don't want to make it

28
00:02:03,440 --> 00:02:07,024
ops problem. So we want to ensure we build a solid

29
00:02:07,144 --> 00:02:10,284
observability into our llms as well.

30
00:02:10,673 --> 00:02:14,585
So as part of today's presentation, I'll provide you a quick intro,

31
00:02:14,689 --> 00:02:18,393
what is observability? And we'll discuss about what

32
00:02:18,433 --> 00:02:21,745
is observability mean for llms. So there are two kind

33
00:02:21,769 --> 00:02:25,097
of observability which we can discuss,

34
00:02:25,225 --> 00:02:28,417
so which we are going to discuss, so which is about a direct

35
00:02:28,465 --> 00:02:31,929
observability, and second one is about indirect observability.

36
00:02:32,081 --> 00:02:35,505
So I'll be focusing more on indirect observability

37
00:02:35,569 --> 00:02:39,450
when discussing about the maturity model, which I'm going to

38
00:02:39,522 --> 00:02:43,066
walk you through. Then I'll walk you through about some of the pillars,

39
00:02:43,210 --> 00:02:46,674
give a quick intro about what is the LLM

40
00:02:46,714 --> 00:02:50,546
look like, and then we'll jump into my main focus,

41
00:02:50,690 --> 00:02:54,050
a maturity model for LLM. So then

42
00:02:54,202 --> 00:02:58,066
we'll look at some of the implementation

43
00:02:58,130 --> 00:03:02,370
guidelines, the services which we can leverage, and then of course,

44
00:03:02,522 --> 00:03:06,386
like every other maturity model, this should

45
00:03:06,490 --> 00:03:11,018
not be just a maturity model where people will just follow blindly,

46
00:03:11,106 --> 00:03:14,906
but we want to ensure we tack into business outcomes

47
00:03:15,010 --> 00:03:18,210
so we have an ability to measure the

48
00:03:18,242 --> 00:03:21,346
progress. And then we'll wrap this up with some of the

49
00:03:21,370 --> 00:03:25,574
best practices and some of the pitfalls I think you should avoid.

50
00:03:26,114 --> 00:03:29,294
Before we start, a quick intro about myself.

51
00:03:29,774 --> 00:03:33,174
My name is Indigo Emilasuri. I'm based out of Colombo.

52
00:03:33,334 --> 00:03:37,806
I'm a serious reliability

53
00:03:37,870 --> 00:03:40,950
engineering advocate and a practitioner as

54
00:03:40,982 --> 00:03:45,406
well. I'm a solution architect with specialize in SRE observability,

55
00:03:45,550 --> 00:03:49,078
AI ops and generative AI working

56
00:03:49,126 --> 00:03:52,470
at Vergisa as a senior systems engineering manager,

57
00:03:52,622 --> 00:03:56,302
I'm a passionate technical trainer. I have trained hundreds

58
00:03:56,318 --> 00:04:00,118
of people when it comes to SRE observability

59
00:04:00,286 --> 00:04:04,294
aiops and I'm an energetic technical blogger

60
00:04:04,454 --> 00:04:07,590
and I'm very proud AWS community builder

61
00:04:07,702 --> 00:04:11,710
under cloud operations and also a very proud ambassador

62
00:04:11,742 --> 00:04:15,110
at DevOps Institute which is also known as PC

63
00:04:15,182 --> 00:04:18,822
CERT because they have acquired it. So that's about me.

64
00:04:18,838 --> 00:04:22,716
So I am very passionate about this topic, observability. So when

65
00:04:22,740 --> 00:04:25,964
it comes to the distributed systems and then llms,

66
00:04:26,084 --> 00:04:29,988
the end of the day I look at things from a customer experience

67
00:04:30,156 --> 00:04:33,948
and how we can provide better customer experience to our end users

68
00:04:34,076 --> 00:04:37,500
and then how we can make a better business

69
00:04:37,572 --> 00:04:41,452
outcomes part of the presentation I'm

70
00:04:41,508 --> 00:04:44,516
mainly focused on AWS. So I'm looking at

71
00:04:44,620 --> 00:04:47,920
llms especially deployed in and

72
00:04:47,952 --> 00:04:51,752
been accessed through AWS. So one

73
00:04:51,768 --> 00:04:55,704
of the fantastic service AWS has offered is Amazon

74
00:04:55,744 --> 00:04:59,648
Bedrock, which is a managed

75
00:04:59,776 --> 00:05:03,440
service where you are able to use APIs to

76
00:05:03,592 --> 00:05:06,912
access the foundational models. So it's

77
00:05:06,968 --> 00:05:10,320
really fast, it's really quick, you just have to ensure that

78
00:05:10,352 --> 00:05:14,120
you have the ability of connecting. So the key features

79
00:05:14,152 --> 00:05:17,374
are it's giving access to the foundation models and

80
00:05:18,394 --> 00:05:22,574
the use cases such as text generation, image generation and

81
00:05:23,234 --> 00:05:26,530
the use cases around those. So it's also providing

82
00:05:26,562 --> 00:05:30,290
this private customization with own data with the

83
00:05:30,402 --> 00:05:33,778
techniques like the retrieval augmented

84
00:05:33,826 --> 00:05:37,850
generation. We call it rack. And it's also providing the

85
00:05:37,962 --> 00:05:42,378
ability of building agents and executed tasks using

86
00:05:42,546 --> 00:05:46,204
the system, enterprise systems and other data sources.

87
00:05:46,944 --> 00:05:50,392
Obviously one good thing is that there's no infrastructure, so you

88
00:05:50,408 --> 00:05:53,912
don't have to worry about infrastructure. So AWS is taking

89
00:05:53,968 --> 00:05:57,896
care of the infrastructure. So that, that's why we call it fully managed.

90
00:05:58,040 --> 00:06:01,328
So it's a very secure and it's a, you know, it's a

91
00:06:01,376 --> 00:06:05,044
go to tool if you want to develop generative AI apps.

92
00:06:05,424 --> 00:06:09,044
It's already consist of, you know, some of the most

93
00:06:10,014 --> 00:06:14,158
widely used foundation models provided by a 21

94
00:06:14,206 --> 00:06:17,910
labs anthropic cohere, meta and stability

95
00:06:18,022 --> 00:06:21,430
AI, Amazon as well. So there are a lot of models and they are

96
00:06:21,462 --> 00:06:25,114
also continuously adding these models into their.

97
00:06:26,454 --> 00:06:30,794
So with that, our observability

98
00:06:31,334 --> 00:06:35,486
maturity model or the approach is mainly focused on application,

99
00:06:35,550 --> 00:06:38,274
which has been developed using Amazon bedrock.

100
00:06:38,644 --> 00:06:42,036
So moving on. I just want to give a quick idea like you know,

101
00:06:42,060 --> 00:06:45,436
so when we say generate AI apps, so what is kind of the

102
00:06:45,460 --> 00:06:49,508
use case? The use cases, a typical user

103
00:06:49,676 --> 00:06:53,204
can kind of like enter query. So it will

104
00:06:53,244 --> 00:06:56,716
come into our, the query interface like we can take it from

105
00:06:56,740 --> 00:06:59,972
my API or user interface. And then the,

106
00:07:00,028 --> 00:07:03,676
we will process, start processing this user query and

107
00:07:03,700 --> 00:07:07,100
then we will try to connect it with the vector encoding.

108
00:07:07,252 --> 00:07:11,812
So it's trying to find similar queries,

109
00:07:11,908 --> 00:07:14,980
similar patterns using in our vector database.

110
00:07:15,172 --> 00:07:18,588
And then we will kind of like looking at retrieving the top k

111
00:07:18,636 --> 00:07:22,436
most relevant context from the vector

112
00:07:22,460 --> 00:07:25,740
database and then we'll make it as input in,

113
00:07:25,932 --> 00:07:29,308
combine it with input when we are providing into llab.

114
00:07:29,476 --> 00:07:33,436
So why? So the key thing to notice that we generally combine

115
00:07:33,500 --> 00:07:38,106
the user input as well as the retrievals

116
00:07:38,170 --> 00:07:41,986
we receive from the vector database. With that we

117
00:07:42,010 --> 00:07:45,210
will start inferencing with the LLM, we will send

118
00:07:45,242 --> 00:07:48,826
the LLM the request input and

119
00:07:48,850 --> 00:07:53,114
then we will start updating the output as well,

120
00:07:53,234 --> 00:07:57,306
which we can combine with our rack integration and then finally

121
00:07:57,450 --> 00:08:01,286
we can send it to after customization to end user.

122
00:08:01,410 --> 00:08:04,950
So this is typically a workflow of

123
00:08:04,982 --> 00:08:08,534
generative AI and this is the way we want to kind of like

124
00:08:08,654 --> 00:08:12,286
enable observability. What is observability?

125
00:08:12,430 --> 00:08:15,822
I'm sure most of you are aware, but just to ensure

126
00:08:15,878 --> 00:08:20,022
that we are kind of in the same page, I just spend a quick

127
00:08:20,078 --> 00:08:23,574
short amount of time to give my perspective

128
00:08:23,614 --> 00:08:27,930
of observability. So observability is nothing but ability

129
00:08:28,002 --> 00:08:31,762
to intercept or understand the system's internal

130
00:08:31,818 --> 00:08:34,754
state by looking at its external output.

131
00:08:34,914 --> 00:08:38,466
So what are the external outputs? We are typically looking at locks,

132
00:08:38,530 --> 00:08:41,594
metrics and traces. So I like to think, you know,

133
00:08:41,634 --> 00:08:45,338
observability is like, you know, looking at the big picture entire this mountain,

134
00:08:45,506 --> 00:08:49,002
not only focusing on what's, you know, outside the water.

135
00:08:49,178 --> 00:08:52,732
So what we are trying to look at it, trying to ask the questions like

136
00:08:52,858 --> 00:08:57,004
what is happening in my system right now, how the systems are performing

137
00:08:57,304 --> 00:09:01,376
and what anomalies they are in my system, what are the different

138
00:09:01,440 --> 00:09:04,712
components interacting with each other, what causes

139
00:09:04,728 --> 00:09:08,040
a particular issue or failure? So when it comes

140
00:09:08,072 --> 00:09:11,168
to monitoring observability, obviously there are a

141
00:09:11,176 --> 00:09:13,704
lot of good things when it comes to observability,

142
00:09:13,864 --> 00:09:17,728
because observability is more of a proactive

143
00:09:17,776 --> 00:09:21,020
approach, it's active approach instead of a passive,

144
00:09:21,172 --> 00:09:25,172
and it's looking at the big picture and looking at more of a qualitative

145
00:09:25,228 --> 00:09:29,260
and quantitative data. We want to make a quick

146
00:09:29,412 --> 00:09:33,436
discussion and agree on something. And we want to agree when

147
00:09:33,460 --> 00:09:37,356
we say observability and llms, what that means. So when

148
00:09:37,380 --> 00:09:41,212
it comes to observability in llms or the

149
00:09:41,228 --> 00:09:44,484
apps being developed using llms, we can divide it into

150
00:09:44,524 --> 00:09:47,854
two parts. One is something we call direct LLM

151
00:09:47,894 --> 00:09:51,234
observability or observability of LLM itself.

152
00:09:51,574 --> 00:09:55,254
So what that means is that we in this scenario we will

153
00:09:55,294 --> 00:09:59,198
start monitor, evaluate and look at the large language

154
00:09:59,246 --> 00:10:02,702
model directly. So this is all about observability into

155
00:10:02,758 --> 00:10:06,934
large language model. But then there are other aspects like indirect

156
00:10:07,094 --> 00:10:10,806
LLM observability or observability of applications of

157
00:10:10,830 --> 00:10:14,252
the systems using LLM. Here we are not looking at

158
00:10:14,268 --> 00:10:18,684
the LLM directly, but we are looking at the applications

159
00:10:18,764 --> 00:10:22,524
or the systems connecting utilizing LLM.

160
00:10:22,684 --> 00:10:26,404
So this is just to ensure that, you know, we are able to both ways,

161
00:10:26,444 --> 00:10:30,028
we are able to provide some really good benefits to the end users.

162
00:10:30,196 --> 00:10:33,500
So both have its and

163
00:10:33,612 --> 00:10:37,132
the techniques we will use is pretty much the similar standard way.

164
00:10:37,188 --> 00:10:40,792
When it comes to observability, we will look at, you know, how we can look,

165
00:10:40,848 --> 00:10:44,644
leverage the logs, the metrics, the traces and other things.

166
00:10:45,224 --> 00:10:48,360
So now if we kind of quickly look at, you know, what,

167
00:10:48,432 --> 00:10:51,712
when we mean direct LLM observability, what that means.

168
00:10:51,848 --> 00:10:55,680
So here we will integrate observability capabilities during

169
00:10:55,832 --> 00:10:59,400
the training, the deployments of LLM and while it's been

170
00:10:59,432 --> 00:11:01,884
used, so it's at LLM itself.

171
00:11:02,384 --> 00:11:05,856
So the main objective is we want to gain insight

172
00:11:05,920 --> 00:11:09,628
into how the LLM is functioning, identify anomalies and other

173
00:11:09,676 --> 00:11:13,476
issues directly related to LLM, understand the decision making

174
00:11:13,540 --> 00:11:17,708
process of LLM here, how we approach this, we will activate

175
00:11:17,756 --> 00:11:21,244
logging and we will look at things like the attention,

176
00:11:21,324 --> 00:11:24,916
weights and other internal states of the LLMs when it's doing,

177
00:11:24,980 --> 00:11:28,820
when we are doing inferences, we will implement probes or

178
00:11:28,852 --> 00:11:32,636
instrumentation with the model architecture. So the observability

179
00:11:32,740 --> 00:11:36,152
is being implemented at LLM level and we

180
00:11:36,168 --> 00:11:39,376
will start tracking performance metrics such as latency and the

181
00:11:39,400 --> 00:11:43,352
memory usage, and also things like external techniques like

182
00:11:43,408 --> 00:11:47,604
attention visualization. So as I said, this is more of

183
00:11:48,504 --> 00:11:52,368
LLM level. So this is about fully fledged looking

184
00:11:52,416 --> 00:11:55,752
at how the LLM is performing. So when

185
00:11:55,768 --> 00:11:59,736
it comes to indirect LLM observability, we are mainly looking

186
00:11:59,800 --> 00:12:03,556
at the applications or the systems

187
00:12:03,680 --> 00:12:07,220
which we have developed connecting with LLM. So here

188
00:12:07,252 --> 00:12:10,572
we are not looking at LLM isolately,

189
00:12:10,708 --> 00:12:14,716
but we are fully focused on the application side. So this is

190
00:12:14,740 --> 00:12:18,532
to understand when it comes to our application, how is our application

191
00:12:18,588 --> 00:12:22,452
is behaving, what observability things which we can enable

192
00:12:22,588 --> 00:12:25,624
and how we can interpret the internal state.

193
00:12:26,084 --> 00:12:30,008
This makes sense because just like any other application

194
00:12:30,156 --> 00:12:33,544
for Genai also we want to understand how is our

195
00:12:33,584 --> 00:12:37,568
application performed like because there can be any number of issues coming in.

196
00:12:37,656 --> 00:12:41,656
And here again, you know, it's end of end user customer experience, it's the users

197
00:12:41,760 --> 00:12:45,360
who are using our solution here what

198
00:12:45,392 --> 00:12:48,160
we are looking at is again we will look at the logging,

199
00:12:48,312 --> 00:12:51,496
the other inputs and outputs related to LLM.

200
00:12:51,680 --> 00:12:54,324
We will looking at the monitoring metrics,

201
00:12:54,864 --> 00:12:57,960
we will look at enabling anomaly detections

202
00:12:57,992 --> 00:13:01,470
on some of the LLM outputs. Obviously we need

203
00:13:01,502 --> 00:13:05,166
the human feedback loops as well. And then you know,

204
00:13:05,190 --> 00:13:08,822
we will look at lot of metrics such as error rate, latency.

205
00:13:08,998 --> 00:13:12,870
And the key objective as you would have already guessed is to understand

206
00:13:13,022 --> 00:13:16,830
how is our application is behaving and how is our application

207
00:13:16,942 --> 00:13:20,134
is leveraging LLM and how good kind

208
00:13:20,174 --> 00:13:23,634
of output we are providing into our end users.

209
00:13:24,094 --> 00:13:28,670
So when it comes to the LLM observability in

210
00:13:28,702 --> 00:13:32,566
this presentation, when I say LLM observability, I am looking at indirect

211
00:13:32,630 --> 00:13:36,246
LLM observability. So I am looking at coming up with

212
00:13:36,270 --> 00:13:40,510
the maturity model which is catering

213
00:13:40,542 --> 00:13:43,714
to applications develop using

214
00:13:44,214 --> 00:13:47,870
application develops connecting to AWS bedrock because AWS is

215
00:13:47,902 --> 00:13:51,886
what I am focusing on and the other aspects of AWS

216
00:13:51,910 --> 00:13:55,502
is bedrock. So we are trying to see how we can integrate

217
00:13:55,558 --> 00:13:58,798
observability practice into generative AI applications.

218
00:13:58,966 --> 00:14:02,070
So we are looking at, you know, how we can identify

219
00:14:02,222 --> 00:14:05,558
these applications internally. States also focus on some

220
00:14:05,606 --> 00:14:08,674
aspects of LLM and the prompt engineering.

221
00:14:08,974 --> 00:14:12,990
So we will look at the indirect oversight of LLM functionalities

222
00:14:13,182 --> 00:14:16,310
and we try to make sure that the generative AI applications are

223
00:14:16,342 --> 00:14:19,746
reliable and they are providing

224
00:14:19,910 --> 00:14:23,666
what is it's been designed and the end users are happy with

225
00:14:23,690 --> 00:14:27,674
the performance. So we want to answer this question,

226
00:14:27,754 --> 00:14:31,738
why observe build for llms. So just like any other application

227
00:14:31,826 --> 00:14:35,626
llms also that generative apps being developed

228
00:14:35,650 --> 00:14:39,218
using llms also require observability because we need

229
00:14:39,266 --> 00:14:42,890
observability, you know, when it comes to ensuring we kind

230
00:14:42,922 --> 00:14:46,330
of like make sure that our generative applications

231
00:14:46,402 --> 00:14:49,386
are correct, it's provide the correctness,

232
00:14:49,450 --> 00:14:53,386
the accuracy and it's about the, the performance,

233
00:14:53,530 --> 00:14:56,962
it's about providing great customer experience. But when it

234
00:14:56,978 --> 00:15:00,666
comes to llms, llms have its own challenge. It's sometimes

235
00:15:00,690 --> 00:15:03,802
it's complex. We might have to look at,

236
00:15:03,818 --> 00:15:07,770
you know, what kind of anomalies, you know, or the model bias it's having

237
00:15:07,922 --> 00:15:11,114
or the model drift. So when we say model

238
00:15:11,154 --> 00:15:15,316
drift is the model can be working fine when

239
00:15:15,340 --> 00:15:18,724
we are doing testing for considerable period of time but it

240
00:15:18,804 --> 00:15:22,844
started, you know, it start failing. So this can have a adverse

241
00:15:22,964 --> 00:15:25,384
impact on our end user performance.

242
00:15:26,284 --> 00:15:30,116
And sometimes the models can create some biasness,

243
00:15:30,180 --> 00:15:34,468
you know, which is again, you know, bad, which can create some bad customer experiences.

244
00:15:34,636 --> 00:15:38,020
Then we will look at the pretty much the other standard things like debugging,

245
00:15:38,052 --> 00:15:41,864
troubleshooting, how best we are using our resources

246
00:15:42,024 --> 00:15:44,872
and the ethics, the data privacy, security.

247
00:15:45,048 --> 00:15:48,136
So implementing kind of like looking at these things

248
00:15:48,200 --> 00:15:52,144
again, observability for LLM is very important because

249
00:15:52,224 --> 00:15:55,696
that is again allowing us to provide and

250
00:15:55,720 --> 00:15:59,104
you know, kind of like give generate

251
00:15:59,224 --> 00:16:01,484
great customer end user experiences.

252
00:16:01,984 --> 00:16:06,032
So now we'll focus on trying to understand what are the pillar shaping

253
00:16:06,088 --> 00:16:09,658
llms. What are the pillar shaping or

254
00:16:09,706 --> 00:16:13,466
LLM observability. So one of the key things is

255
00:16:13,570 --> 00:16:17,554
I'd like to split into few parts. One is that LLM

256
00:16:17,634 --> 00:16:21,194
specific metrics. So one is that LLM inference

257
00:16:21,234 --> 00:16:24,874
latency. Here we track the LL latency of llms,

258
00:16:24,954 --> 00:16:28,254
the request, you know, which is coming to bedrock application.

259
00:16:28,594 --> 00:16:31,610
We will start monitoring the latency at different stages

260
00:16:31,682 --> 00:16:35,592
of the request, such as like when they coming from the

261
00:16:35,768 --> 00:16:38,928
API gateways and lambda functions, LLM itself.

262
00:16:38,976 --> 00:16:42,776
So where however we have defined, we'll try to look at the

263
00:16:42,880 --> 00:16:46,764
potential bottlenecks and how we can improve or optimize the performance.

264
00:16:47,584 --> 00:16:51,264
And then we will look at LLM inference success rate. So we will start

265
00:16:51,304 --> 00:16:55,640
monitoring the success rate of, you know, the request going and coming from LLM.

266
00:16:55,792 --> 00:16:59,228
And then we will start, you know, looking at what are the errors

267
00:16:59,276 --> 00:17:02,524
and whether there's increase in errors, what is the reason for errors,

268
00:17:02,564 --> 00:17:06,220
all the troubleshooting aspects as well. And we have

269
00:17:06,252 --> 00:17:09,844
this LLM quality, output quality where, you know,

270
00:17:09,884 --> 00:17:13,820
we will like trying to understand the quality

271
00:17:13,852 --> 00:17:16,892
of the LLM outputs. So which is again important.

272
00:17:17,068 --> 00:17:20,548
So that kind of gives us the ability to kind of like, you know,

273
00:17:20,596 --> 00:17:24,444
improving those areas. And one other important thing is LLM prompt

274
00:17:24,484 --> 00:17:27,955
effectiveness. So it tracks the effectiveness of the prompts

275
00:17:27,979 --> 00:17:31,531
which we are kind of like sending to LLM. So this again,

276
00:17:31,587 --> 00:17:35,403
you know, we will start monitoring the quality of LLM outputs

277
00:17:35,483 --> 00:17:39,419
based on those prompts and based on various different kind of prompts and

278
00:17:39,451 --> 00:17:43,139
how these are getting deviated and then start continuously

279
00:17:43,171 --> 00:17:46,843
refining this and moving on. Some of the other things

280
00:17:46,883 --> 00:17:50,507
are, you know, about LLM model drift. So we will

281
00:17:50,555 --> 00:17:54,565
start monitor the distribution of, you know, LLM outputs with the application,

282
00:17:54,739 --> 00:17:58,454
understand over period of time whether there's any significant

283
00:17:59,434 --> 00:18:02,854
output distributions. And then we'll start tracking the performance.

284
00:18:03,354 --> 00:18:07,066
And of course we will have to start looking at the cost and then when

285
00:18:07,090 --> 00:18:10,642
we are integrating with llms, whether there are integration issues,

286
00:18:10,738 --> 00:18:15,042
especially because, you know, we are integrating with the

287
00:18:15,058 --> 00:18:18,226
AWS, the bedrock, and then we will look at

288
00:18:18,250 --> 00:18:22,450
some of the ethical consideration as well. So we will start monitor llms output

289
00:18:22,562 --> 00:18:25,762
with the bedrock itself for potential

290
00:18:25,858 --> 00:18:29,258
ethical things, violations and other things. So we'll have to ensure

291
00:18:29,306 --> 00:18:33,266
that, you know, our generative AI apps which we have developed

292
00:18:33,290 --> 00:18:37,338
are 100% safe, there's no harm, illegal or discriminatory

293
00:18:37,426 --> 00:18:40,562
content, and llms are, and the

294
00:18:40,618 --> 00:18:44,186
generative AI apps are safe to use. So with

295
00:18:44,210 --> 00:18:47,274
that, you know, we are looking, we kind of like, those are the key things,

296
00:18:47,314 --> 00:18:50,754
you know, when it comes to the LLM specific metrics.

297
00:18:50,874 --> 00:18:54,386
And then when it comes to the prompt engineering properties, we will look at the

298
00:18:54,410 --> 00:18:58,442
temperature, we will start, you know, see how we can control randomness in

299
00:18:58,458 --> 00:19:02,054
the model, because the more higher the temperature,

300
00:19:02,834 --> 00:19:05,842
the diverse the outputs are. And you know,

301
00:19:05,858 --> 00:19:09,466
if you can lower the temperature, the more focused the outputs are. And then

302
00:19:09,490 --> 00:19:12,906
we will look at the top P sampling so that we know we can control

303
00:19:12,970 --> 00:19:16,578
the output diversity. And then we will look at the top k

304
00:19:16,626 --> 00:19:19,686
sampling and things like Max token

305
00:19:19,830 --> 00:19:23,622
and the stop tokens, you know, which is about signals to model to step

306
00:19:23,678 --> 00:19:26,502
generating text when, you know, this encountered.

307
00:19:26,638 --> 00:19:30,854
We will look at the repetition penalties, present penalties, batch sizes as well.

308
00:19:30,974 --> 00:19:34,286
So all of these things, you know, we can extract via logs and then send

309
00:19:34,310 --> 00:19:37,822
it to cloud lots, the cloudbot. And then, you know, we can

310
00:19:37,878 --> 00:19:40,994
create custom metrics and then start visualizing.

311
00:19:41,494 --> 00:19:44,774
And then two other thing is we can look at the, you know, in the

312
00:19:44,814 --> 00:19:48,310
inference latency, we can check whether the time taken for

313
00:19:48,342 --> 00:19:51,358
model to generate output for the given inputs.

314
00:19:51,486 --> 00:19:55,366
And then we look at the model accuracy and the matrix as well.

315
00:19:55,470 --> 00:19:59,190
So these things, you know, probably we are using AWS X ray and

316
00:19:59,222 --> 00:20:02,686
then, you know, start publishing these things into cloud work and

317
00:20:02,710 --> 00:20:06,702
then we can bring and create the alarms and wrappers around

318
00:20:06,758 --> 00:20:10,766
that. And few other things are other specific

319
00:20:10,830 --> 00:20:13,974
things. One thing we have to look at it that, you know, when it comes

320
00:20:14,014 --> 00:20:17,392
to the rag models, so what are the key things?

321
00:20:17,448 --> 00:20:21,624
So when it comes to rags, you know, we again have metrics like query latency.

322
00:20:21,704 --> 00:20:24,928
We want to understand the time it takes for the rack models

323
00:20:24,976 --> 00:20:28,336
to process a query and generate the response. And then we will look

324
00:20:28,360 --> 00:20:31,720
at the success rate, how successful are these queries and how

325
00:20:31,752 --> 00:20:35,696
often it's getting failed. We will look at the resource utilization and

326
00:20:35,720 --> 00:20:38,976
you know, in case if you are using caching, we look, we can look at

327
00:20:39,000 --> 00:20:42,140
the cache, it's as well. And when it comes to logs,

328
00:20:42,172 --> 00:20:45,276
we look at the query logs, error logs and the audit logs, you know,

329
00:20:45,300 --> 00:20:48,860
which will probably, probably give us a comprehensive way of, you know,

330
00:20:48,972 --> 00:20:52,676
auditing, troubleshooting. And then we'll try to enable traces,

331
00:20:52,820 --> 00:20:56,196
x ray, you know, which will provide us the end to end tracing so that

332
00:20:56,220 --> 00:20:59,852
way that we can have a complete

333
00:20:59,908 --> 00:21:04,304
observability into the data store or data retriever

334
00:21:05,084 --> 00:21:08,412
and other pillars are the tracing. So we have, we will use x ray,

335
00:21:08,428 --> 00:21:12,112
you know, so that will enable us to get integrate the traces and

336
00:21:12,128 --> 00:21:15,408
we will look at, you know, other integration AWS services as well.

337
00:21:15,576 --> 00:21:19,064
And then we will use Cloudwatch as a visualization tool. We can

338
00:21:19,104 --> 00:21:22,952
also use the Grafana, the AWS managed Grafana

339
00:21:23,048 --> 00:21:25,724
or any other things as well.

340
00:21:26,224 --> 00:21:29,912
So one other key thing is be mindful about alerting and incident

341
00:21:29,968 --> 00:21:33,512
management. So we can use the cloud virtual arms and we

342
00:21:33,528 --> 00:21:36,164
can leverage AWS system manager as well.

343
00:21:36,824 --> 00:21:40,848
So one important thing is the security. So we will leverage AWS cloud

344
00:21:40,936 --> 00:21:44,536
trail to audit and monitor the API calls and

345
00:21:44,560 --> 00:21:48,072
we'll ensure that the compliance with security and regulatory requirements

346
00:21:48,128 --> 00:21:52,144
are being tracked. I know we can integrate crowd logs with cloud

347
00:21:52,184 --> 00:21:55,752
work logs for centralization and then we will use

348
00:21:55,808 --> 00:21:59,560
AWS config so that we can continuously monitor and

349
00:21:59,592 --> 00:22:03,496
assess the configuration of our systems, AWS resources and

350
00:22:03,520 --> 00:22:07,120
we can ensure that we have compliance and best practices with

351
00:22:07,152 --> 00:22:10,590
the compliance standard with that.

352
00:22:10,782 --> 00:22:14,294
One key aspect is cost as well. So the more we are using our

353
00:22:14,334 --> 00:22:17,446
llms, you know, the more the cost factor comes in. So we

354
00:22:17,470 --> 00:22:21,354
can leverage AWs cost explorer and AWS budgets.

355
00:22:21,894 --> 00:22:25,870
And finally, one other important thing is that, you know, AI upscale building.

356
00:22:25,982 --> 00:22:29,438
So we will have to ensure that all the metrics, you know, whether it's

357
00:22:29,606 --> 00:22:33,742
the LLM specific, application specific or the RaG is specific,

358
00:22:33,878 --> 00:22:37,714
we will kind of like enable anomaly detection. And then for

359
00:22:37,754 --> 00:22:41,130
all the logs which we are putting into cloud work, we are

360
00:22:41,162 --> 00:22:45,290
able to enable the log anomaly detection as well. So we can also use

361
00:22:45,362 --> 00:22:49,442
Aws, the DevOps guru. So it's a machine learning

362
00:22:49,498 --> 00:22:52,826
service provided by AWS. So it,

363
00:22:52,970 --> 00:22:56,842
the DevOps guru will help us to detect and resolve issues

364
00:22:56,898 --> 00:23:00,258
in our system, especially identifying anomalies and other

365
00:23:00,386 --> 00:23:04,334
issues which probably we might not be able to uncover manually.

366
00:23:04,954 --> 00:23:08,402
And then we will look at leveraging AWS code guru as

367
00:23:08,418 --> 00:23:12,290
well because this allow us to integrate with the application so that

368
00:23:12,362 --> 00:23:16,778
we can do profiling and we can do the understand the

369
00:23:16,826 --> 00:23:20,826
resource utilizations usage based on our applications.

370
00:23:21,010 --> 00:23:23,922
Another very important thing is use AWS forecasting.

371
00:23:24,018 --> 00:23:27,586
So all the metrics and other things which we are bringing into the

372
00:23:27,610 --> 00:23:31,274
table, we can use the forecasting that will able to

373
00:23:31,314 --> 00:23:34,850
understand things in advance so that we can make better decisions

374
00:23:34,882 --> 00:23:37,730
and we can plan things ahead with that.

375
00:23:37,762 --> 00:23:41,338
Probably you can ask the question why we need a maturity model. So I

376
00:23:41,346 --> 00:23:45,226
am a big fan of maturity model because I think maturity models act as

377
00:23:45,250 --> 00:23:49,562
a north star. So we all want to start someplace and then take

378
00:23:49,698 --> 00:23:52,810
our systems into observability journey. So if you do

379
00:23:52,842 --> 00:23:56,504
that without kind of a maturity model or framework, then it's are,

380
00:23:56,624 --> 00:24:00,208
you know, you, you may ended up with any place, but by

381
00:24:00,256 --> 00:24:03,880
using a maturity model you can guarantee that, you know, you start with the basic

382
00:24:03,952 --> 00:24:07,776
steps and then you can finish with it some of the advanced things

383
00:24:07,880 --> 00:24:11,564
and you have better control of how you go there.

384
00:24:12,024 --> 00:24:15,704
So the LLM, the indirect

385
00:24:15,824 --> 00:24:19,328
observability maturity model, I have three pillars. One is,

386
00:24:19,456 --> 00:24:23,076
I call it level one, which is about foundational observability.

387
00:24:23,220 --> 00:24:26,596
And level two is the proactive observability. At level three

388
00:24:26,660 --> 00:24:30,204
we are looking at advanced LLM observability with AI Ops.

389
00:24:30,364 --> 00:24:33,580
So in the level one we will start, you know, capturing some of the basic

390
00:24:33,612 --> 00:24:37,108
LLM metrics. We will start getting the logs and start

391
00:24:37,156 --> 00:24:41,212
monitor the basic from properties, and we will implement basic

392
00:24:41,268 --> 00:24:45,180
logging and other distributed tracing. And then we will put up the visualization

393
00:24:45,292 --> 00:24:48,564
and other basic alerts as well. So this

394
00:24:48,604 --> 00:24:52,484
will kind of give you a foundational observability into your generative AI

395
00:24:52,524 --> 00:24:55,964
application. The next step is, you know, taking system more

396
00:24:56,004 --> 00:24:59,300
proactive, like be proactive. So here we will start,

397
00:24:59,332 --> 00:25:02,828
you know, capture and analyze the advanced LLM metrics and you know,

398
00:25:02,876 --> 00:25:06,540
start, you know, start leveraging the logs, then the

399
00:25:06,572 --> 00:25:09,796
other advanced prompt properties. And then we

400
00:25:09,820 --> 00:25:13,316
will enhance alerts and other incident management workflow so that

401
00:25:13,340 --> 00:25:16,260
we can identify things much faster and you know,

402
00:25:16,292 --> 00:25:19,772
resolve things much faster as well. So we will bring in the security

403
00:25:19,868 --> 00:25:23,196
aspect, the security compliance. We will start generating,

404
00:25:23,340 --> 00:25:26,964
leveraging, AWS forecasting so that we can start focusing

405
00:25:27,004 --> 00:25:31,224
about some of the LlMe specific matrix, matrix and the

406
00:25:32,004 --> 00:25:35,340
prompt properties as well. And for the logs

407
00:25:35,372 --> 00:25:38,104
we can also set up log anomaly detection.

408
00:25:38,684 --> 00:25:41,828
And when it comes to level three, which is kind of like the advanced level,

409
00:25:41,876 --> 00:25:45,742
which is the kind of place where you all want to be in, but you

410
00:25:45,758 --> 00:25:48,550
have to be mindful that it's a journey. Like you will have to start with

411
00:25:48,582 --> 00:25:52,142
level one, go to level two, and then we can be into level three.

412
00:25:52,278 --> 00:25:56,094
So at level three we start with integrating with DevOps guru

413
00:25:56,214 --> 00:25:59,830
and the code guru, so that with DevOps Guru will provide the AI

414
00:25:59,862 --> 00:26:03,390
and ML capabilities code guru will provide our quality

415
00:26:03,422 --> 00:26:07,190
of the code and then we will start implementing AIOps

416
00:26:07,302 --> 00:26:11,234
capabilities like other things like the noise reduction,

417
00:26:11,644 --> 00:26:15,300
smart intelligent root causes and then kind

418
00:26:15,332 --> 00:26:18,612
of like business impact assessments. So the forecasting feature

419
00:26:18,668 --> 00:26:22,156
will kind of like allow us to understand, if at all, if the

420
00:26:22,180 --> 00:26:25,732
models can drift, when that can happen, if at all,

421
00:26:25,868 --> 00:26:29,252
the models can start having a bias, when that can happen,

422
00:26:29,428 --> 00:26:32,788
the response time predictions and all those things. So the

423
00:26:32,836 --> 00:26:35,764
AI kind of thing is, can give you a full control of, you know,

424
00:26:35,804 --> 00:26:38,704
predictability of your generative AI application.

425
00:26:39,234 --> 00:26:42,834
So now I am kind of like look at more focus on

426
00:26:42,874 --> 00:26:46,610
implementation angle. So in the foundation model, like, you know, we can

427
00:26:46,642 --> 00:26:50,034
use cloud work metrics, like so that we can capture the

428
00:26:50,074 --> 00:26:53,714
basic LLM metrics, like, you know, the inference time, model size,

429
00:26:53,834 --> 00:26:57,546
prompt length and those things, the prompt properties. Again, we can,

430
00:26:57,570 --> 00:27:01,010
you know, leverage the sender, those logs into cloud work logs

431
00:27:01,082 --> 00:27:04,362
so that, you know, we can start monitoring basic properties like from

432
00:27:04,418 --> 00:27:07,856
content prompt sources and those things, any other logs,

433
00:27:07,880 --> 00:27:10,608
you know, we will be shipping into cloud work so that we can start,

434
00:27:10,656 --> 00:27:14,184
you know, getting the basic, the detail.

435
00:27:14,344 --> 00:27:17,888
And then we will integrate AWS x ray based on the technology

436
00:27:17,976 --> 00:27:21,584
we are using to develop our LLM to generate

437
00:27:21,624 --> 00:27:25,304
a app so that we can have ability to start looking

438
00:27:25,344 --> 00:27:28,608
at the traces and then visualization the dashboards.

439
00:27:28,656 --> 00:27:32,110
We can use AWS Cloudword dashboards and if required, you know,

440
00:27:32,142 --> 00:27:35,478
we can go into AWS managed Grafana dashboards

441
00:27:35,526 --> 00:27:39,470
as well. Alert and incident management. We are leveraging Cloudwatch

442
00:27:39,582 --> 00:27:42,874
and that will help us to understand some of the

443
00:27:44,534 --> 00:27:49,062
more the basic to a medium complex

444
00:27:49,198 --> 00:27:52,366
some of these monitors so that we can have a good control of

445
00:27:52,390 --> 00:27:55,510
our, how the llms are behaving and like how,

446
00:27:55,622 --> 00:27:59,270
how is our, the prompt is successful

447
00:27:59,382 --> 00:28:03,710
and overall how is our generative application is behaving and probably

448
00:28:03,902 --> 00:28:07,270
not probably, but how our end users are feeling about it.

449
00:28:07,382 --> 00:28:11,958
And then we will wrap this up with the cost like we using AWS Explorer.

450
00:28:12,046 --> 00:28:15,446
Because llms are sometimes costly, we'll have to ensure the usage and

451
00:28:15,470 --> 00:28:18,822
we start monitoring that as well. So level

452
00:28:18,878 --> 00:28:21,910
two, like, you know, we will go a little bit advanced for the metrics.

453
00:28:21,942 --> 00:28:25,946
We will start, you know, looking at advanced metrics like model performance and

454
00:28:26,070 --> 00:28:29,418
output quality. And again, prompt the properties.

455
00:28:29,466 --> 00:28:32,922
We will look at the advanced properties like the prompt performance,

456
00:28:33,058 --> 00:28:36,850
prompt versioning. And then, you know, we will start advancing,

457
00:28:36,922 --> 00:28:40,522
improving the incident workflows. We will look at the

458
00:28:40,618 --> 00:28:44,194
security compliance, we will look at more into the uplifting

459
00:28:44,234 --> 00:28:47,962
and like improving the cost factor as well. And one

460
00:28:47,978 --> 00:28:51,986
of the key thing, you know, here we will bring in is AWS forecasting.

461
00:28:52,090 --> 00:28:55,490
So using forecasting we want to ensure that we have the ability of

462
00:28:55,522 --> 00:28:59,938
forecasting all the key, the complex or even every

463
00:29:00,066 --> 00:29:04,218
metric related to LLM performance, LLM the

464
00:29:04,306 --> 00:29:07,650
inference, the accuracy and the prompt properties related

465
00:29:07,722 --> 00:29:11,610
things as well. So and then we will also look at

466
00:29:11,762 --> 00:29:15,266
enabling metric anomalies, log anomalies so that, you know, we start

467
00:29:15,330 --> 00:29:19,610
using some of the capabilities of anomalies this and

468
00:29:19,762 --> 00:29:23,746
finally we will bringing in AWS DevOps guru,

469
00:29:23,850 --> 00:29:27,494
so and the code guru and that will allow us to bringing in

470
00:29:27,874 --> 00:29:31,882
the AI capabilities into here the AI ML capability so

471
00:29:31,898 --> 00:29:35,706
that we can look at things from holistic ways. DevOps guru is a

472
00:29:35,730 --> 00:29:39,658
perfect tool. And then we will, you know, bringing in AI of practices

473
00:29:39,826 --> 00:29:43,518
and then kind of like, you know, bring ensuring that our incident workflow

474
00:29:43,586 --> 00:29:47,206
are more into self healing and there are a

475
00:29:47,230 --> 00:29:51,206
lot of other improvements and AI kind of

476
00:29:51,230 --> 00:29:55,646
things which we can bring. So what

477
00:29:55,670 --> 00:30:00,094
are the other things like, you know, so we will look at bringing in while

478
00:30:00,134 --> 00:30:03,590
we do this, we want to ensure that we measure the progress.

479
00:30:03,702 --> 00:30:07,358
So once we enable observability. So we want to ensure

480
00:30:07,406 --> 00:30:10,980
that we look at LLM. So, so how the LLM output quality

481
00:30:11,012 --> 00:30:14,244
is getting improved, how we are improving, optimizing our

482
00:30:14,284 --> 00:30:17,900
LLM prompt engineering area and then like

483
00:30:17,972 --> 00:30:21,636
ensuring that, you know, we can able to detect our model drifts in advance

484
00:30:21,700 --> 00:30:24,972
and then we can take necessary actions. We look at, you know, what are the

485
00:30:24,988 --> 00:30:28,564
ethical things, you know, our models based

486
00:30:28,604 --> 00:30:31,836
on that, how our models are behaving and then, you know, we look at

487
00:30:31,860 --> 00:30:35,708
the interpredictability, extendability and start

488
00:30:35,756 --> 00:30:39,240
keep a close eye on those things and generally like,

489
00:30:39,272 --> 00:30:42,432
you know, we will start kind of looking at end user experience

490
00:30:42,528 --> 00:30:46,200
as well. We will clearly define some end user specific

491
00:30:46,352 --> 00:30:50,280
service level objectives. We will start, you know, track the

492
00:30:50,312 --> 00:30:53,936
metrics and the improvements and we will start looking at

493
00:30:53,960 --> 00:30:57,360
the customer experience, ensure that, you know, whatever we

494
00:30:57,392 --> 00:31:00,560
do is align and correlate with customer

495
00:31:00,592 --> 00:31:04,240
experience. We see increasing customer experience as well.

496
00:31:04,352 --> 00:31:07,792
So and like overall that,

497
00:31:07,808 --> 00:31:11,640
you know, we develop and provide a better world class services into for our

498
00:31:11,712 --> 00:31:15,264
end users. And then some of the best practices is

499
00:31:15,304 --> 00:31:18,688
like, you know, so we will have to use a structured log and you know,

500
00:31:18,776 --> 00:31:22,112
if case you are heavily using lambda, probably go to power tools,

501
00:31:22,208 --> 00:31:25,480
you'll have to instrument the code, ensure that, you know, you get all the,

502
00:31:25,592 --> 00:31:29,258
the critical, the LLM specific metrics.

503
00:31:29,416 --> 00:31:32,990
Then you obviously use x ray to enable the traces as well.

504
00:31:33,102 --> 00:31:36,934
So the metrics which we are extracting, it has to be meaningful

505
00:31:36,974 --> 00:31:40,646
it has to add value. So it should be aligned with our business objectives

506
00:31:40,710 --> 00:31:44,478
as well. And to wrap up like, you know, some of the

507
00:31:44,646 --> 00:31:48,834
pitfall is that, you know, ensure that, you know, we kind of have a security

508
00:31:49,174 --> 00:31:52,674
we plan in advance and the compliance as well

509
00:31:52,974 --> 00:31:56,502
because that's again a key thing, you know, modern day when we are

510
00:31:56,518 --> 00:31:59,838
using generative applications and clearly define

511
00:31:59,886 --> 00:32:03,726
the roles like whatever objectives, you know, we are going

512
00:32:03,750 --> 00:32:06,814
to achieve with this. And probably you can have some numbers,

513
00:32:06,854 --> 00:32:10,254
you can have some measurable things so that you can start, you know,

514
00:32:10,294 --> 00:32:13,594
performing and kind of like try to get the benefit.

515
00:32:13,974 --> 00:32:17,214
So with this like, you know I'm, we are at the close so

516
00:32:17,254 --> 00:32:20,974
thank you very much. So here I have taken AWS as

517
00:32:21,014 --> 00:32:24,770
example, especially AWS bedrock. From here we have look

518
00:32:24,802 --> 00:32:28,698
at what is a general architecture of workflow of

519
00:32:28,826 --> 00:32:32,694
generator application and what are the key pillars of

520
00:32:33,154 --> 00:32:36,778
the observability LLM related observability pillars

521
00:32:36,826 --> 00:32:40,174
which we have to enable and then we will look at the three,

522
00:32:40,714 --> 00:32:44,570
the levels, the foundation, the proactive observability

523
00:32:44,682 --> 00:32:48,146
and advanced observability is aiops. And then we

524
00:32:48,170 --> 00:32:52,200
have look at some of the best practices and the pitfalls and

525
00:32:52,232 --> 00:32:56,288
more importantly how we can look at this from ROI perspective.

526
00:32:56,456 --> 00:32:59,872
So with this, thank you very much for taking time. I hope, you know,

527
00:32:59,928 --> 00:33:04,232
you kind of like enjoy this and then you have understood

528
00:33:04,288 --> 00:33:08,144
or you have taken few things which you can take into your

529
00:33:08,264 --> 00:33:11,920
generative application and make it observable and leverage into to

530
00:33:11,952 --> 00:33:14,444
deliver great customer experiences.

531
00:33:14,944 --> 00:33:16,344
So with this, thank you very much.

