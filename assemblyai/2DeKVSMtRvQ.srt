1
00:00:27,454 --> 00:00:30,734
Today I am talking about how do you scale

2
00:00:30,774 --> 00:00:34,614
open telemetry collectors using Kafka? To introduce myself

3
00:00:34,694 --> 00:00:38,434
I am Pranay. I am one of the founders and maintainers at SigNos.

4
00:00:39,174 --> 00:00:42,790
I have been working as product manager in the past. I was product

5
00:00:42,902 --> 00:00:46,462
at Microsoft and multiple other startups in

6
00:00:46,478 --> 00:00:50,358
my pastime. I love reading and taking

7
00:00:50,406 --> 00:00:53,976
a walk in the nature just to set context.

8
00:00:54,150 --> 00:00:58,476
Why I'm talking about open telemetry collector so

9
00:00:58,620 --> 00:01:02,492
as I mentioned, I am maintainer at signals. It is an open source observatory

10
00:01:02,548 --> 00:01:06,148
platform. We are have been there around for

11
00:01:06,196 --> 00:01:10,196
three years. We have around 16,000 70,000 GitHub stars,

12
00:01:10,380 --> 00:01:14,704
4000 plus members of slack community, 130 plus contributors

13
00:01:15,604 --> 00:01:19,684
and it's open telemetry native single

14
00:01:19,724 --> 00:01:23,846
pane in glass for auxiliary. So we have support for different

15
00:01:24,030 --> 00:01:27,286
traces, metrics and logs and you can see all

16
00:01:27,310 --> 00:01:30,194
of them in a single pane and correlate across them.

17
00:01:30,494 --> 00:01:34,430
And today I'll be talking about our experience with

18
00:01:34,622 --> 00:01:38,270
signos cloud and underneath we use open telemetry

19
00:01:38,302 --> 00:01:41,878
collector and how we scaled it and our experience

20
00:01:41,966 --> 00:01:45,434
regarding that. So that's where I'm coming from.

21
00:01:46,214 --> 00:01:50,010
Just to set context for people who are not aware what is

22
00:01:50,042 --> 00:01:53,682
open telemetry. So open telemetry is a

23
00:01:53,698 --> 00:01:57,866
CNCF projects, it's a vendor neutral standard for

24
00:01:58,050 --> 00:02:01,414
sending telemetry data from your applications

25
00:02:01,834 --> 00:02:04,850
and it supports all the signals.

26
00:02:04,922 --> 00:02:08,906
So you can send metrics using it, you can send traces

27
00:02:08,970 --> 00:02:11,514
using it, you can send logs using it.

28
00:02:11,594 --> 00:02:15,708
And this is sort of becoming now the default standard

29
00:02:15,826 --> 00:02:19,248
how people do observe. This has helped

30
00:02:19,296 --> 00:02:21,244
and so why is it important?

31
00:02:21,944 --> 00:02:25,712
So open telemetry is the second fastest growing project in

32
00:02:25,728 --> 00:02:29,040
CNCF and this is just after kubernetes, so you can

33
00:02:29,072 --> 00:02:32,880
understand how popular it is. What it enables is it

34
00:02:32,912 --> 00:02:36,920
enables sending data to any

35
00:02:36,952 --> 00:02:40,816
backend. So it standardizes on that elementary protocol

36
00:02:41,000 --> 00:02:44,160
through which you send data or applications can send data,

37
00:02:44,232 --> 00:02:47,054
infrastructure can send data to a backend.

38
00:02:47,514 --> 00:02:51,426
This is now becoming the default standard for instrumentation

39
00:02:51,530 --> 00:02:55,290
and people can use any backend which supports

40
00:02:55,362 --> 00:02:58,934
open telemetry for their use cases.

41
00:02:59,234 --> 00:03:03,002
The big advantage it gives is that you are not users are now not getting

42
00:03:03,058 --> 00:03:06,874
vendor locked in to a particular ecosystem and hence have

43
00:03:06,914 --> 00:03:10,130
more flexibility. So this has promoted a

44
00:03:10,162 --> 00:03:14,456
lot of innovation in the ecosystem. The other key advantage is that

45
00:03:14,600 --> 00:03:17,752
it's, it has been a unique standard

46
00:03:17,848 --> 00:03:21,192
in the sense that it supports all the three signals, metrics,

47
00:03:21,208 --> 00:03:25,224
traces and logs from get go. And also new signals like profiling

48
00:03:25,264 --> 00:03:28,880
is in progress. And at Signos we are

49
00:03:28,912 --> 00:03:33,324
natively based on open telemetry. When we started the project in 2021.

50
00:03:33,824 --> 00:03:37,128
This we took a bet on just

51
00:03:37,176 --> 00:03:40,358
being natively based on open telemetry. That's the only SDK support.

52
00:03:40,536 --> 00:03:44,530
And most of the things we do, we try to rely on open telemetry

53
00:03:44,562 --> 00:03:48,338
as much as possible. So today in our talk, we'll talk about if

54
00:03:48,346 --> 00:03:53,690
you have used open telemetry collectors or have tried it for setting

55
00:03:53,722 --> 00:03:57,774
up your ability, how you can scale it and,

56
00:03:58,474 --> 00:04:01,334
and use it for like huge scale.

57
00:04:01,634 --> 00:04:05,538
And I'll talk about our context on

58
00:04:05,626 --> 00:04:08,654
how we leverage is for a signos cloud product.

59
00:04:09,284 --> 00:04:12,620
And hopefully it will be helpful for you to

60
00:04:12,772 --> 00:04:16,852
get a sense of where things are. Cool. So now

61
00:04:16,908 --> 00:04:20,424
we understand what open telemetry is.

62
00:04:20,964 --> 00:04:24,028
One of the key components of open telemetry is the,

63
00:04:24,156 --> 00:04:27,524
is the open telemetry collector. You can think of it

64
00:04:27,564 --> 00:04:31,484
essentially as a pipeline through

65
00:04:31,524 --> 00:04:34,804
which you can send data, process data and send to

66
00:04:34,844 --> 00:04:38,010
different destinations, right? So there are three key components

67
00:04:38,042 --> 00:04:40,854
of open telemetry collector. There are receivers,

68
00:04:41,514 --> 00:04:45,674
there is a processor, and then there are exporters. So through receivers

69
00:04:45,754 --> 00:04:49,106
you can receive data from different formats. For example, you have

70
00:04:49,130 --> 00:04:53,226
host metric receiver, which can receive data from

71
00:04:53,370 --> 00:04:57,650
machines and infrastructure metrics like

72
00:04:57,682 --> 00:05:01,074
cpu's memory res. There's a Kafka metric

73
00:05:01,114 --> 00:05:04,014
siever where you can get metrics from Kafka.

74
00:05:05,014 --> 00:05:08,446
There are like 90 plus such receivers which enable you to

75
00:05:08,470 --> 00:05:12,222
receive data from different sources. The next is processors,

76
00:05:12,278 --> 00:05:16,142
which basically helps you do different type of processing. So you

77
00:05:16,158 --> 00:05:20,206
can do change attributes. You can do filtering of particular type

78
00:05:20,230 --> 00:05:22,990
of logs, traces or metrics.

79
00:05:23,102 --> 00:05:26,622
And then you export it to new destinations. So you

80
00:05:26,638 --> 00:05:30,038
can export it to destinations like click house.

81
00:05:30,206 --> 00:05:34,754
You can export it to back end providers like signals.

82
00:05:35,414 --> 00:05:39,286
Or you can use any other exporter like Kafka

83
00:05:39,310 --> 00:05:42,678
exporter tools to send data across by

84
00:05:42,726 --> 00:05:46,754
Kafka. So open telemetry collectors are really

85
00:05:47,574 --> 00:05:51,150
the key components in open telemetry. And we'll

86
00:05:51,182 --> 00:05:55,230
focus on how we have used open telemetry collectors

87
00:05:55,302 --> 00:05:59,604
to, to provide this, our signals cloud service.

88
00:06:00,464 --> 00:06:05,288
Just to take you a bit deeper into our

89
00:06:05,336 --> 00:06:09,152
signals cloud for the singleton architecture. So without,

90
00:06:09,208 --> 00:06:12,672
and this is without Kafka, we are

91
00:06:12,688 --> 00:06:16,176
both multi tenant and single tenant architecture. In this talk, I'll primarily focus on

92
00:06:16,200 --> 00:06:19,936
the single tenant architecture because that's where we

93
00:06:19,960 --> 00:06:23,400
have used Kafka a lot to scale

94
00:06:23,592 --> 00:06:27,248
this architecture. So imagine

95
00:06:27,336 --> 00:06:31,600
there's a customer who is using otill collector as an agent and

96
00:06:31,632 --> 00:06:34,604
sending data to signals backend,

97
00:06:35,104 --> 00:06:38,496
or they're sending directly data through applications,

98
00:06:38,560 --> 00:06:42,480
right? So in architecture which

99
00:06:42,592 --> 00:06:46,976
doesn't involve Kafka, it will, they will send data directly to a load balancer

100
00:06:47,160 --> 00:06:51,654
and then there will be then directed to the individual

101
00:06:52,434 --> 00:06:56,454
tenant total collectors. So in this architecture,

102
00:06:57,114 --> 00:06:59,654
signos tenant has their own hotel collector.

103
00:07:00,234 --> 00:07:04,094
The load balancer points to like sends data

104
00:07:04,514 --> 00:07:07,970
from a particular customer to their specific signals hotel

105
00:07:08,002 --> 00:07:11,826
collector. Right. The problem with this architecture is

106
00:07:11,850 --> 00:07:15,378
that if the tenants go, go down

107
00:07:15,506 --> 00:07:18,882
or the dbs in this tenant have issues,

108
00:07:19,058 --> 00:07:22,974
then the agents start. The hotel collector here starts

109
00:07:23,274 --> 00:07:26,610
giving five xx that leads to loss

110
00:07:26,642 --> 00:07:30,034
of data. Also if the tenant has

111
00:07:30,074 --> 00:07:33,714
to, like if a customer suddenly has spiked

112
00:07:33,754 --> 00:07:38,154
their ingestion rate, so say they have made

113
00:07:38,194 --> 00:07:42,240
it ten x it in say few minutes. This leads

114
00:07:42,272 --> 00:07:46,528
to like if it is directly collected to a single hotel collector,

115
00:07:46,696 --> 00:07:50,640
maybe the hotel collector doesn't scale as quickly and

116
00:07:50,752 --> 00:07:53,920
that will lead to loss of data for the tenants.

117
00:07:53,992 --> 00:07:57,568
Right. So there's always some time which the tenant

118
00:07:57,656 --> 00:08:01,120
DB takes to scale up and during that time there

119
00:08:01,152 --> 00:08:04,600
will be loss of data for the tenant.

120
00:08:04,752 --> 00:08:08,522
So effectively customers may see some data getting dropped,

121
00:08:08,608 --> 00:08:11,354
which is obviously not a good thing. Right?

122
00:08:11,934 --> 00:08:15,310
So this was some of the problems which we immediately

123
00:08:15,342 --> 00:08:17,834
identified with a single tenant architecture.

124
00:08:18,294 --> 00:08:22,134
It seemed imperative that there should be a queuing system in front of

125
00:08:22,174 --> 00:08:25,718
the after the gateway or kettle collectors,

126
00:08:25,766 --> 00:08:29,070
right? So in this example, the,

127
00:08:29,222 --> 00:08:33,074
here is the load balancer. So the load balancers. So the signals

128
00:08:33,534 --> 00:08:36,486
customers send data to the signals load balancer.

129
00:08:36,670 --> 00:08:40,094
And then that talks to a gateway of collectors.

130
00:08:40,134 --> 00:08:43,394
So this is basically a fleet of urgently scaling

131
00:08:43,974 --> 00:08:46,754
which received data from the customers.

132
00:08:47,334 --> 00:08:50,918
We use something called a Kafka exporter. So as I mentioned earlier,

133
00:08:51,006 --> 00:08:54,614
hotel collector has something called exporter

134
00:08:54,694 --> 00:08:57,894
and receiver and processor. So in this

135
00:08:57,934 --> 00:09:01,094
example we are showing hotel

136
00:09:01,134 --> 00:09:04,386
collectors, gateway hotel collectors, they,

137
00:09:04,450 --> 00:09:06,894
they receive the data from the.

138
00:09:07,554 --> 00:09:10,850
And then we use Kafka exporter and the Sotel collector

139
00:09:10,882 --> 00:09:14,666
to send data to Kafka. So we have Kafka

140
00:09:14,690 --> 00:09:18,170
setup here. I'll get into more details on what is the setup and

141
00:09:18,202 --> 00:09:21,694
the configuration for that. And then in the signals

142
00:09:22,194 --> 00:09:26,186
tenant, which also has auto collector in it, we enable the Kafka

143
00:09:26,210 --> 00:09:29,254
receiver and get data from Kafka there.

144
00:09:29,734 --> 00:09:32,990
So as you can see here, sort of,

145
00:09:33,062 --> 00:09:36,794
even if there is a huge spike in

146
00:09:37,214 --> 00:09:40,942
load from a particular customer, the Kafka acts

147
00:09:40,998 --> 00:09:44,350
as the queing system in between to absorb

148
00:09:44,382 --> 00:09:47,926
that spike. And then as the tenant system,

149
00:09:48,110 --> 00:09:51,574
hotel collector gets scaled up, it can start consuming

150
00:09:51,614 --> 00:09:54,878
at a higher rate. So there's no loss

151
00:09:54,926 --> 00:09:59,046
of uh, packet draws which we had earlier where

152
00:09:59,070 --> 00:10:02,246
there was no Kafka in place. Yeah. So these are some

153
00:10:02,270 --> 00:10:04,874
of the use cases where Kafka can be helpful.

154
00:10:05,254 --> 00:10:09,374
So this enables a highly available ingestion we

155
00:10:09,414 --> 00:10:12,982
have. So this is our current Kafka setup. We have 6 hours retention

156
00:10:13,038 --> 00:10:17,086
period, depletion factor of three, and then a ten

157
00:10:17,110 --> 00:10:21,234
mb max message, right? So Kafka,

158
00:10:22,324 --> 00:10:26,044
as you know is higher, is we can, we have configured

159
00:10:26,084 --> 00:10:29,740
it to be highly available. So with a reflection factor

160
00:10:29,772 --> 00:10:33,244
of three. So Avka acts as a buffer for 6

161
00:10:33,284 --> 00:10:36,412
hours. It's, it has much higher availability,

162
00:10:36,588 --> 00:10:39,908
it can handle busty traffic, tenant can

163
00:10:39,956 --> 00:10:43,184
continue consuming at their rate and

164
00:10:43,564 --> 00:10:47,116
have some time to scale up as they need. While the Kafka acts as sort

165
00:10:47,140 --> 00:10:51,074
of the single line of defense against a very high spike

166
00:10:51,114 --> 00:10:54,394
in loads. These are the two key main factors.

167
00:10:54,514 --> 00:10:58,522
But one of the advantages, parallel advantage to this is that

168
00:10:58,658 --> 00:11:02,354
this can also enable lot of additional processing,

169
00:11:02,474 --> 00:11:05,442
which we can do at Kafka. For example,

170
00:11:05,618 --> 00:11:09,614
especially in the case of database sampling for traces.

171
00:11:10,354 --> 00:11:14,002
Sometimes people want to filter traces

172
00:11:14,058 --> 00:11:17,964
by trace id and accumulate all traces at one place and

173
00:11:18,004 --> 00:11:21,140
then do some processing on it. For example, if you

174
00:11:21,172 --> 00:11:24,860
want to reject or like not store traces which has

175
00:11:24,892 --> 00:11:29,100
particular attribute in it. For example say if you want to reject all

176
00:11:29,252 --> 00:11:33,036
traces which have health check endpoints, right? Which essentially don't add

177
00:11:33,060 --> 00:11:36,796
value in case, if there are just hotel collectors

178
00:11:36,980 --> 00:11:41,804
you would have to challenge. You'll have a challenge of like having

179
00:11:41,884 --> 00:11:45,490
all hotel collectors in the case where they're like multiple

180
00:11:45,522 --> 00:11:48,850
order collectors to like all spans come to the same hotel collector,

181
00:11:48,882 --> 00:11:52,754
right? Because auto collector by natal is horizontally

182
00:11:52,794 --> 00:11:56,618
scalable, so you don't really control which hotel collector which span

183
00:11:56,666 --> 00:12:00,610
will go to and they are stateless. In Kafka,

184
00:12:00,762 --> 00:12:04,578
this problem can be solved much easily because you can use

185
00:12:04,626 --> 00:12:07,650
trace trace id as a partition key and that would

186
00:12:07,682 --> 00:12:11,768
enable all trace ids to go to a particular and

187
00:12:11,816 --> 00:12:15,440
hence you can do all those trace tail based

188
00:12:15,472 --> 00:12:18,952
sampling much easily there. So having a

189
00:12:18,968 --> 00:12:22,376
queue system in between helps you doing a

190
00:12:22,400 --> 00:12:26,248
lot of additional processing, especially if you want to do tail based sample

191
00:12:26,296 --> 00:12:30,104
tail sampling at the auto collector level.

192
00:12:30,264 --> 00:12:34,040
Much easier, right? So as I said, this is

193
00:12:34,072 --> 00:12:37,540
our Kafka setup. So just to give

194
00:12:37,572 --> 00:12:42,004
an example, these are some of the typical things

195
00:12:42,044 --> 00:12:45,212
we monitor. So we like if you have set up Kafka,

196
00:12:45,388 --> 00:12:48,628
you will also need to monitor like how many records are getting

197
00:12:48,676 --> 00:12:51,024
produced, what's the,

198
00:12:51,884 --> 00:12:55,228
for each tenant, what's the sort of difference

199
00:12:55,356 --> 00:12:59,464
how much records are getting consumed? So the difference between

200
00:13:00,124 --> 00:13:04,040
the number of records produce and consume basically tells you like what

201
00:13:04,072 --> 00:13:07,312
you need to do. Is that a lag in between if

202
00:13:07,328 --> 00:13:09,764
you need to scale your system or not, right?

203
00:13:10,144 --> 00:13:13,856
So if, so, when you set up our system

204
00:13:13,920 --> 00:13:17,444
here with Kafka we actively monitor

205
00:13:17,984 --> 00:13:21,192
all this, all these metrics and

206
00:13:21,288 --> 00:13:24,944
as you can see we use signals to monitor signals so this is a case

207
00:13:24,984 --> 00:13:29,144
of the monitor getting monitored itself and

208
00:13:29,224 --> 00:13:33,400
we use signals internally heavily to monitor all our SAS

209
00:13:33,472 --> 00:13:37,320
customers and use cases so you can monitor

210
00:13:37,392 --> 00:13:41,368
all different types of kafka queues here

211
00:13:41,416 --> 00:13:45,284
how many records are being consumed, how many records are being generated

212
00:13:46,744 --> 00:13:50,032
monitoring consumer lag is very important so this gives you

213
00:13:50,048 --> 00:13:53,280
a sense of like hey like how much ingestion is

214
00:13:53,312 --> 00:13:56,664
there and how much is the individual hotel collector is

215
00:13:56,704 --> 00:14:00,052
consuming right? For example in this case as you can see like for

216
00:14:00,068 --> 00:14:04,228
example a particular customer starts sending in data

217
00:14:04,276 --> 00:14:08,188
at a very high rate and this signal,

218
00:14:08,236 --> 00:14:12,476
this like the tenant for that is consuming a particular rate, right? So if

219
00:14:12,500 --> 00:14:15,524
the initial reads rate suddenly spikes then

220
00:14:15,684 --> 00:14:19,100
this the consumer lag here will

221
00:14:19,132 --> 00:14:22,932
increase and that we monitor actively to

222
00:14:23,108 --> 00:14:26,674
throw alerts and so the act of the signal let hey like

223
00:14:26,764 --> 00:14:30,486
maybe the signals tenant needs to be automatically scaled

224
00:14:30,630 --> 00:14:33,838
and like we need to more add more resources there so this

225
00:14:33,966 --> 00:14:37,942
scaling happens automatically but consumer lag is like one of the pointers

226
00:14:37,998 --> 00:14:42,110
which helps you decide what to and when to

227
00:14:42,262 --> 00:14:45,966
scale the tenants, right? So we monitor

228
00:14:46,030 --> 00:14:50,234
all this active like consumer lag actively

229
00:14:50,614 --> 00:14:54,294
and what are the number of kafka

230
00:14:54,334 --> 00:14:58,582
cluster. So we use red panda internally to monitor all this so how many

231
00:14:58,718 --> 00:15:02,190
are the red panda brokers? What is the consumer

232
00:15:02,222 --> 00:15:05,750
lag which is there in between? One of the key factor which

233
00:15:05,782 --> 00:15:09,062
we use for scaling is to scale

234
00:15:09,118 --> 00:15:14,286
based on consumer lag so how many partitions

235
00:15:14,350 --> 00:15:17,622
to increase or not and then this can also be used

236
00:15:17,678 --> 00:15:20,954
as a metric to scale up your consumer group,

237
00:15:20,994 --> 00:15:25,226
right? So you and tenant so if you see the number of

238
00:15:25,410 --> 00:15:28,754
like the consumer lag is increasing a lot we write scripts

239
00:15:28,834 --> 00:15:33,186
to automatically scale this up basically

240
00:15:33,250 --> 00:15:36,714
enable higher ingestion like higher consumptions from the hotel

241
00:15:36,754 --> 00:15:40,494
collector tenant tenant altar collectors and that

242
00:15:41,154 --> 00:15:44,418
that reduces the consumer lag and basically the systems get stable

243
00:15:44,466 --> 00:15:48,326
again so kafka acts as a buffer in between to

244
00:15:48,350 --> 00:15:51,958
handle workloads in the scenario if there was no kafka

245
00:15:52,006 --> 00:15:56,062
as as we had discussed earlier then

246
00:15:56,118 --> 00:15:59,734
suddenly this hotel collector will get overlapped and there

247
00:15:59,774 --> 00:16:03,430
will and if it doesn't scale at the same rate then

248
00:16:03,462 --> 00:16:07,234
it will start dropping packets if Kafka is present then that

249
00:16:07,574 --> 00:16:11,366
doesn't happen so consumer lag is

250
00:16:11,390 --> 00:16:14,440
an important thing to monitor that

251
00:16:14,472 --> 00:16:17,816
helps you scale kafka also one thing

252
00:16:17,840 --> 00:16:21,072
which accumulated monitor is consumer latency so what is the

253
00:16:21,168 --> 00:16:24,608
producer consumer latency so here if you see we have plotted

254
00:16:24,656 --> 00:16:28,184
how much time it takes for the producer to produce and get into

255
00:16:28,224 --> 00:16:32,560
Kafka. And what is the latency which like the consumer has.

256
00:16:32,712 --> 00:16:36,224
And if this latency increases by a particular amount we

257
00:16:36,304 --> 00:16:39,976
throw out alerts and signals and that basically indicates that hey, there is

258
00:16:40,000 --> 00:16:43,874
something wrong and then maybe some steps needs to be taken to fix

259
00:16:43,914 --> 00:16:47,466
it, right? So till now the

260
00:16:47,490 --> 00:16:50,994
kafka base architecture is working quite well for us. It enables very

261
00:16:51,034 --> 00:16:54,770
fast ingestion, it can handle spikes

262
00:16:54,802 --> 00:16:59,058
in loads from workload customers and data

263
00:16:59,106 --> 00:17:02,934
is retained for 6 hours and that works.

264
00:17:03,754 --> 00:17:07,374
We even get a huge compression factor of ten to 15 because

265
00:17:08,124 --> 00:17:12,732
hotel collector by default works in a batch model. So it

266
00:17:12,748 --> 00:17:16,464
sends data in batches and before ingestion into Kafka

267
00:17:17,324 --> 00:17:20,860
we are able to get a huge compression before sending that.

268
00:17:20,892 --> 00:17:25,024
So in that sense also it works quite well and

269
00:17:25,324 --> 00:17:29,184
it handles spikes, as I mentioned earlier, also very well.

270
00:17:29,684 --> 00:17:33,428
There are few areas where I think there could be improvements

271
00:17:33,556 --> 00:17:37,772
which we are working on currently. So can we make this

272
00:17:37,828 --> 00:17:41,220
automatically increase of based on just on the

273
00:17:41,252 --> 00:17:45,036
scale of ingestion of a topic, if a partition gets stuck

274
00:17:45,100 --> 00:17:49,076
for a tenant total collector, then can we

275
00:17:49,180 --> 00:17:53,172
use some methods like dead letter queue to drop

276
00:17:53,228 --> 00:17:56,884
after a few retries so that it doesn't get stuck in a permanent

277
00:17:56,924 --> 00:18:01,036
failure. Also making the whole tenant

278
00:18:01,100 --> 00:18:04,894
hotel collector which is like Kafka receiver to processors to exporter

279
00:18:05,004 --> 00:18:08,338
a synchronous module so that consumer commits

280
00:18:08,386 --> 00:18:11,914
an offset only after the message is successfully returned to a DB.

281
00:18:12,074 --> 00:18:15,746
So there's some guarantee on when the

282
00:18:15,770 --> 00:18:19,194
message is being written and the other

283
00:18:19,234 --> 00:18:22,498
key pieces like hey, can we make this exactly one delivery?

284
00:18:22,546 --> 00:18:26,466
Which as you know in our queuing processes are not easy

285
00:18:26,490 --> 00:18:30,618
to do right? So these are some of the improvements which we

286
00:18:30,706 --> 00:18:35,068
foresee in the future and which you're working on to solve. But overall,

287
00:18:35,196 --> 00:18:38,892
just adding Kafka on the Autel character has been

288
00:18:38,908 --> 00:18:42,964
a step improvement for us and hopefully for

289
00:18:43,004 --> 00:18:46,744
other people, other teams which are running the set scale,

290
00:18:48,204 --> 00:18:50,984
they can take this as a guide.

291
00:18:51,884 --> 00:18:55,104
So that's all I had for my talk.

292
00:18:56,244 --> 00:18:59,864
Just want to give a shout out for signals. So we are actively

293
00:18:59,904 --> 00:19:03,488
growing community in using open

294
00:19:03,536 --> 00:19:07,312
telemetry and we are a complete authority stack so you

295
00:19:07,328 --> 00:19:10,944
can check out all signals repo if you try

296
00:19:10,984 --> 00:19:14,080
it out create feel free to create an issue or participate

297
00:19:14,112 --> 00:19:17,848
in our slack community. As I mentioned, we are quite

298
00:19:17,896 --> 00:19:22,376
active slack community where you can come and get your questions answered and

299
00:19:22,400 --> 00:19:26,288
if you have any follow on thoughts with me, feel free to email me

300
00:19:26,336 --> 00:19:29,394
or that's all from my side.

301
00:19:29,694 --> 00:19:32,686
Looking forward to hear more from you and feel free to reach out to us

302
00:19:32,710 --> 00:19:34,774
if you have any feedback. Thank you.

