1
00:00:20,680 --> 00:00:24,102
Hiya. So, today we're going to be talking about llms and whether or not

2
00:00:24,158 --> 00:00:27,286
they're good anomaly detectors. And if they're not good anomaly

3
00:00:27,310 --> 00:00:30,554
detectors, then what are the alternatives? What is out there?

4
00:00:30,914 --> 00:00:34,538
So, I'm going to keep this super short, but my name is Chloe, I'm currently

5
00:00:34,626 --> 00:00:38,138
a tech lead and a data engineer at Theo dot UK law.

6
00:00:38,186 --> 00:00:41,922
Fun fact is that I've lived in six countries and moved eight times before

7
00:00:41,978 --> 00:00:45,530
turning 18. No, my parents are not diplomats. That's usually the

8
00:00:45,562 --> 00:00:48,666
question I always get. So why are we

9
00:00:48,690 --> 00:00:52,034
looking at data anomalies? Why do we care? There are different

10
00:00:52,074 --> 00:00:55,506
types of data anomalies out there. There's those that kind

11
00:00:55,530 --> 00:00:58,986
of come along quite regularly and that completely normal. So if you think

12
00:00:59,010 --> 00:01:02,494
of, for example, energy generation, particular things like solar

13
00:01:02,534 --> 00:01:06,246
energy, you might have a day where it's particularly cloudy

14
00:01:06,270 --> 00:01:09,590
and there's no light that gets through and so you'll have a drop in that

15
00:01:09,622 --> 00:01:12,662
solar, you know, solar energy generation, it's a

16
00:01:12,678 --> 00:01:15,702
data anomaly, but it's not, it's not a bad thing, it's kind of just something

17
00:01:15,758 --> 00:01:19,670
that happens. On the other hand, you can also have data anomalies

18
00:01:19,702 --> 00:01:23,246
which are caused by bad data quality, and so that's what we're

19
00:01:23,270 --> 00:01:26,582
going to focus on for this talk. So why is

20
00:01:26,598 --> 00:01:30,222
that so bad? To give you some statistics about

21
00:01:30,398 --> 00:01:33,734
this 12% average revenue loss by

22
00:01:33,774 --> 00:01:36,834
us companies due to bad data.

23
00:01:37,294 --> 00:01:39,594
So why does that happen? Actually,

24
00:01:40,134 --> 00:01:43,718
data quality is so important for lots and lots of reasons,

25
00:01:43,766 --> 00:01:47,174
and people probably will give you different reasons depending on who you ask.

26
00:01:47,294 --> 00:01:50,950
But the ones I like to highlight is, firstly, data is used

27
00:01:51,022 --> 00:01:54,390
in such a big part of decision making.

28
00:01:54,502 --> 00:01:57,462
So when you get data, that's how you know, you know what decisions to make

29
00:01:57,478 --> 00:02:00,706
on your business, on your product, on your users,

30
00:02:00,770 --> 00:02:04,026
all of those things. So if your data quality is bad, it can have

31
00:02:04,050 --> 00:02:08,034
a really big impact on your decision making. You might be making incorrect

32
00:02:08,074 --> 00:02:11,402
decisions, you might be making the non optimal decisions,

33
00:02:11,458 --> 00:02:15,234
etcetera. So that's the first thing to think of. The second one, it definitely affects

34
00:02:15,274 --> 00:02:18,858
trust. So it will affect trust between your data

35
00:02:18,986 --> 00:02:22,330
engineers and the people who lead the business,

36
00:02:22,402 --> 00:02:25,490
because if you're producing incorrect statistics,

37
00:02:25,562 --> 00:02:28,992
it's really going to be damage that trust between the tech team and the

38
00:02:29,008 --> 00:02:32,616
product side, and also it affects trust with your users

39
00:02:32,640 --> 00:02:36,384
as well. If you're providing them wrong data, if you're not cleaning

40
00:02:36,424 --> 00:02:39,728
out that, making sure that quality stays high, it's also

41
00:02:39,776 --> 00:02:43,704
going to impact that trust. The third point is security.

42
00:02:43,864 --> 00:02:47,592
You'll be handling sensitive data a lot of the time either

43
00:02:47,648 --> 00:02:51,240
to do with user data, things behind GDPR,

44
00:02:51,392 --> 00:02:54,920
or even more sensitive data than that. And if you have

45
00:02:54,992 --> 00:02:59,144
bad data quality, that's going to also affect the security of your data.

46
00:02:59,304 --> 00:03:03,776
So that's only three examples, but the list can go on and on. So be

47
00:03:03,800 --> 00:03:07,200
sure to find out why data quality is so important to you and your use

48
00:03:07,232 --> 00:03:10,512
case. It will likely fit within these, but I'm sure that there

49
00:03:10,528 --> 00:03:14,368
are many more. So what causes bad

50
00:03:14,416 --> 00:03:18,176
quality data? Again, there's a long list of causes, but here are a

51
00:03:18,200 --> 00:03:21,912
few that you might think of. First. One could be missing data,

52
00:03:22,008 --> 00:03:25,496
incorrect data. Those are the two most common ones. But there's also

53
00:03:25,560 --> 00:03:29,610
things like outdated data, there's inconsistent formatting

54
00:03:29,762 --> 00:03:33,082
standards. You've also got incompatible systems, you've got data

55
00:03:33,138 --> 00:03:36,354
complexity. All of these kind of feed into bad

56
00:03:36,394 --> 00:03:39,574
data quality. And of course, there's many more than these as well.

57
00:03:40,074 --> 00:03:42,706
So the question is, what can we actually do about it?

58
00:03:42,890 --> 00:03:46,082
So, first off, before jumping into llms, I do want to

59
00:03:46,098 --> 00:03:49,938
say that there are existing tools that help you with data observability,

60
00:03:50,026 --> 00:03:53,386
that help you try and detect that data quality that you have.

61
00:03:53,490 --> 00:03:56,866
So syphilit and elementary are two of those that

62
00:03:56,890 --> 00:04:00,210
are already out in the market and that can be used and they really help

63
00:04:00,242 --> 00:04:03,826
you to pick out, to kind of observe literally the

64
00:04:03,850 --> 00:04:07,554
quality of your data. Or, you know, you could spice

65
00:04:07,594 --> 00:04:10,634
things up a little, which is what we're going to be doing in our case.

66
00:04:10,714 --> 00:04:14,146
And we're going to be using OpenAI and seeing

67
00:04:14,210 --> 00:04:17,426
if it's good as anomaly detector. So we'll

68
00:04:17,450 --> 00:04:21,194
be going through this in four stages. The first one we'll be asking ourselves

69
00:04:21,234 --> 00:04:24,666
the question, why? Why are we doing this? Why do we even want to try

70
00:04:24,690 --> 00:04:27,776
and apply OpenAI to anomaly detection?

71
00:04:27,970 --> 00:04:31,236
The second thing we're going to do is a basic test to just get it

72
00:04:31,260 --> 00:04:34,324
to work. The third thing is we're going to talk about

73
00:04:34,404 --> 00:04:37,948
prompt engineering and how that's going to affect the results

74
00:04:38,036 --> 00:04:41,604
from your anomaly detector. And lastly, we're going to be looking at data

75
00:04:41,644 --> 00:04:45,220
types as well. How does the data type impact how

76
00:04:45,252 --> 00:04:48,104
well OpenAI does as an anomaly detector?

77
00:04:49,324 --> 00:04:52,844
First off, why? The first thing we need to know

78
00:04:52,884 --> 00:04:56,428
is that the data that we can have is very varied.

79
00:04:56,516 --> 00:04:59,644
You can have structured data, you can have unstructured data, you can have semi structured

80
00:05:00,144 --> 00:05:03,556
data, structured data. There are so many different data formats out there

81
00:05:03,660 --> 00:05:07,972
that not all the anomaly detectors that we currently have,

82
00:05:08,108 --> 00:05:11,436
like the ones we saw before, don't really fit all of

83
00:05:11,460 --> 00:05:15,228
these different formattings. So being able to use OpenAI gives us

84
00:05:15,276 --> 00:05:18,844
a bit more flexibility in kind of the data we're handling and

85
00:05:18,884 --> 00:05:22,380
how we're detecting all those anomalies. If we're

86
00:05:22,412 --> 00:05:25,932
using, you know, structured data that we don't have

87
00:05:25,948 --> 00:05:29,972
that much uncertainty in the data formatting, there's a lot more classic methods that we

88
00:05:29,988 --> 00:05:32,852
can use for anomaly detecting, for example,

89
00:05:32,948 --> 00:05:36,852
autoencoders. And we don't have to go through as far as OpenAI in

90
00:05:36,868 --> 00:05:40,636
these cases. So that's one of the reasons why we might want to use OpenAI

91
00:05:40,700 --> 00:05:44,020
in anomaly detection. Now, the second thing is

92
00:05:44,092 --> 00:05:47,764
curiosity. We're all curious as developers, it's kind of

93
00:05:47,804 --> 00:05:51,172
one of the main things that define us. And so of course we

94
00:05:51,188 --> 00:05:54,644
want to try throwing open AI at something and seeing what

95
00:05:54,684 --> 00:05:58,704
happens in this case. So that's what we're going to do for anomaly detection.

96
00:05:59,934 --> 00:06:03,614
So let's start off with a basic test. This. If you haven't

97
00:06:03,654 --> 00:06:07,606
used OpenAI before, this is kind of what it looks like. You've got

98
00:06:07,630 --> 00:06:11,422
a client. In this case, we're using GPT 3.5 as the model

99
00:06:11,558 --> 00:06:14,806
and we're going to send it a sequence of messages. So in this case there's

100
00:06:14,830 --> 00:06:17,954
only two. You've got the system message where we're telling,

101
00:06:18,294 --> 00:06:21,590
we're telling the model here that it's a

102
00:06:21,622 --> 00:06:25,590
data analyzer, it's going to pick up any anomalies that are received in the data.

103
00:06:25,782 --> 00:06:29,763
And then as a user, I'm giving it some examples of data to

104
00:06:30,263 --> 00:06:33,946
here going, we can see that we've got a repetition of the id two that

105
00:06:33,970 --> 00:06:37,458
happens twice. And we've also got a negative cost on that last

106
00:06:37,506 --> 00:06:41,282
one as well. So when you try and put this, when you run this,

107
00:06:41,338 --> 00:06:44,786
you notice that. Yep. It actually picks up the anomalies pretty well. It's not

108
00:06:44,810 --> 00:06:47,614
too complex. It does fine with this basic test.

109
00:06:48,634 --> 00:06:52,122
Let's up the level a little bit. So we already mentioned

110
00:06:52,218 --> 00:06:55,994
electricity generation with solar energy, so why don't we use

111
00:06:56,034 --> 00:06:59,954
the actual data behind this here we're using electricity

112
00:07:00,074 --> 00:07:03,618
consumption data from the UK. There's a

113
00:07:03,666 --> 00:07:07,002
huge amount of data on Kaggle that you can use for this. Right now,

114
00:07:07,058 --> 00:07:09,994
I'm just using a little bit of an extract, so a few lines from it

115
00:07:10,034 --> 00:07:13,610
and it looks a bit like this. There's lots of columns to do with demand,

116
00:07:13,682 --> 00:07:16,874
to do with generation capacity, etcetera, that you can

117
00:07:16,914 --> 00:07:20,666
have from this database. So that's the sort of data we're giving it.

118
00:07:20,850 --> 00:07:24,330
Sending that through to GPT 3.5 to try to

119
00:07:24,362 --> 00:07:27,822
text the anomaly and I don't know if you can pick up from

120
00:07:27,838 --> 00:07:31,566
the images, but there's some places, you know, where I've put a minus one when

121
00:07:31,590 --> 00:07:34,886
there's only positive numbers, where I've put a huge deviation in numbers.

122
00:07:34,910 --> 00:07:38,526
So there's, there's a ten somewhere in there where it should be like 9000,

123
00:07:38,670 --> 00:07:42,134
etc, etcetera. So I manually added some anomalies in that

124
00:07:42,174 --> 00:07:45,462
data and I've given it to the

125
00:07:45,478 --> 00:07:46,114
model.

126
00:07:47,254 --> 00:07:50,422
Unsurprisingly, with such complex data,

127
00:07:50,558 --> 00:07:53,558
I could actually find none of the anomalies I gave it.

128
00:07:53,606 --> 00:07:57,022
In most of the cases, there's a few cases where it did pick up

129
00:07:57,038 --> 00:07:59,902
on some anomalies. Those cases are, for example,

130
00:08:00,078 --> 00:08:03,278
GPT four performed better than GPT 3.5.

131
00:08:03,406 --> 00:08:06,614
It tended to detect the anomalies more. The second

132
00:08:06,654 --> 00:08:10,182
thing I noticed was the more anomalies I gave it, the more difficult it

133
00:08:10,198 --> 00:08:13,566
was to find them. And then the third thing, which was actually kind of

134
00:08:13,590 --> 00:08:17,554
surprising, is that the more the number of lines I gave it,

135
00:08:18,094 --> 00:08:21,478
it didn't necessarily do better. So the number of lines of test data

136
00:08:21,566 --> 00:08:24,154
didn't have a significant impact on its performance.

137
00:08:25,434 --> 00:08:28,522
Right. So after running all of these tests, what's kind of

138
00:08:28,538 --> 00:08:32,210
the conclusion of OpenAI being used as anomaly

139
00:08:32,242 --> 00:08:36,226
detector? So on average, for GPT four,

140
00:08:36,330 --> 00:08:40,314
it detected about 32% of intended anomalies that we

141
00:08:40,354 --> 00:08:43,738
had. This is based off of two anomalies with

142
00:08:43,786 --> 00:08:47,634
20 lines of data. Of course, your results are going to vary

143
00:08:47,754 --> 00:08:50,946
depending on the type of data that you're giving it, the number of anomalies,

144
00:08:51,010 --> 00:08:54,202
et cetera, et cetera. But for our particular case, we're going to use this

145
00:08:54,218 --> 00:08:57,262
as a baseline. Two anomalies, 20 lines of data,

146
00:08:57,358 --> 00:09:01,086
GPT four. In this case, the basic test showed

147
00:09:01,150 --> 00:09:04,914
32% of intended anomalies detected on average.

148
00:09:05,494 --> 00:09:08,934
Great. Basic test done. Let's apply some prompt

149
00:09:08,974 --> 00:09:12,862
engineering. So there's lots of techniques around prompt engineering.

150
00:09:12,998 --> 00:09:15,886
I'm just going to focus on the few. So the first thing we're going to

151
00:09:15,910 --> 00:09:19,590
look at is chain of thought. So with chain of thought, you're really encouraging

152
00:09:19,622 --> 00:09:23,758
the model to break down a complex process into smaller

153
00:09:23,806 --> 00:09:27,256
steps before giving a response. So you're forcing

154
00:09:27,280 --> 00:09:31,320
it to give, you know, its thoughts, break down its thoughts into smaller

155
00:09:31,352 --> 00:09:34,920
bricks, um, so it can really process what it's telling you.

156
00:09:35,072 --> 00:09:38,104
And that's the key behind this form of prompt is,

157
00:09:38,184 --> 00:09:42,216
uh, we're going to do it very simply by giving it a line which

158
00:09:42,240 --> 00:09:45,084
simply says, let's think step by step.

159
00:09:45,624 --> 00:09:49,488
So chain of thought can be done in various ways. You can literally

160
00:09:49,536 --> 00:09:52,944
show it to the breakdown of the thinking it has to give. But the simplest

161
00:09:52,984 --> 00:09:56,434
application of it, it's simply telling it. Let's think step

162
00:09:56,474 --> 00:09:59,898
by step. And so that's what we've done here.

163
00:09:59,986 --> 00:10:02,894
You can see I've indicated a few steps that it should take.

164
00:10:03,554 --> 00:10:06,906
Well, it actually increases the percentage of intended

165
00:10:06,970 --> 00:10:10,362
anomalies detected by about 8% for GPT four.

166
00:10:10,498 --> 00:10:13,930
So that's quite good. Can we push it further?

167
00:10:14,082 --> 00:10:17,290
So if we try and apply few shot learning. Few shot learning is

168
00:10:17,322 --> 00:10:21,472
a different form of prompt engineering. And in

169
00:10:21,488 --> 00:10:25,192
the case of few shot learning, what you're doing is that you're providing an example

170
00:10:25,288 --> 00:10:28,804
of how the model should be responding to the prompts that we give it.

171
00:10:29,184 --> 00:10:32,640
Concretely, what does this mean? You can see

172
00:10:32,672 --> 00:10:36,432
here, you've got kind of got an array of messages. The system message remains the

173
00:10:36,448 --> 00:10:40,304
same, but you can see that there's multiple interactions

174
00:10:40,344 --> 00:10:44,120
between a user and assistant. So what's happening is, I'm telling it, okay, if the

175
00:10:44,152 --> 00:10:48,120
user tells you this. So in this case, if the user gives you this

176
00:10:48,152 --> 00:10:51,744
data of CSV, the CSV data, I'm expecting you

177
00:10:51,784 --> 00:10:55,632
to give this answer, which you can see also in

178
00:10:55,648 --> 00:10:59,624
the image right here. So I've done that only twice.

179
00:10:59,704 --> 00:11:02,792
So I've kind of given it an example of some bad data and an

180
00:11:02,808 --> 00:11:06,376
example of data with no anomaly. And at the end, I'm going to give it

181
00:11:06,400 --> 00:11:09,592
that final prompt of, okay, this is the data I want you

182
00:11:09,608 --> 00:11:12,976
to analyze. What happens in this case,

183
00:11:13,080 --> 00:11:17,424
we up the number of intended anomalies detected by

184
00:11:17,464 --> 00:11:21,104
about 24% for GPT four. So that's a massive

185
00:11:21,144 --> 00:11:24,480
increase. It really does a lot better when you've given it some

186
00:11:24,512 --> 00:11:27,604
examples. Now, can we take this further?

187
00:11:27,944 --> 00:11:31,416
The last one that I tried that had a successful,

188
00:11:31,560 --> 00:11:35,088
let's say successful outcome to it was self reflection and

189
00:11:35,136 --> 00:11:39,072
multi step. So what do I mean by self reflection and multi steps?

190
00:11:39,248 --> 00:11:42,960
The aspect of self reflection is getting a model

191
00:11:42,992 --> 00:11:46,734
to question whether or not the answer it's given you. It's sure

192
00:11:46,774 --> 00:11:50,142
about that answer. They're essentially asking the question, are you sure?

193
00:11:50,238 --> 00:11:53,510
Take your time. And multi step is trying

194
00:11:53,542 --> 00:11:57,158
to break down the amount of things that the model has to do

195
00:11:57,326 --> 00:12:00,470
into several models, then it does. It's not

196
00:12:00,502 --> 00:12:03,662
overloaded by the amount of work it has to do. What do

197
00:12:03,678 --> 00:12:06,830
I mean by this is firstly, as the input, I'm giving it the

198
00:12:06,862 --> 00:12:10,222
data as a CSV with the anomalies. The model is going to

199
00:12:10,238 --> 00:12:14,030
give me the anomalies in the data that it thinks are there.

200
00:12:14,222 --> 00:12:18,046
The second step is I'm going to ask you, are you sure? Take your time.

201
00:12:18,230 --> 00:12:21,934
That's going to make it think, okay, have I given the correct answer?

202
00:12:22,054 --> 00:12:26,054
Here are the new anomalies that I think are there. And then finally

203
00:12:26,134 --> 00:12:29,646
we've got this convert your response to JSON. So converting

204
00:12:29,670 --> 00:12:33,294
the response to JSON just makes our lives so much easier in the long

205
00:12:33,334 --> 00:12:36,798
term because we'll be able to use that elsewhere, we'll be able to

206
00:12:36,846 --> 00:12:40,534
pass that information on. And so really that JSON conversion is to make our lives

207
00:12:40,574 --> 00:12:44,730
easier further down the pipeline. So when we break these steps down

208
00:12:44,802 --> 00:12:47,802
and we give it that self reflection, what happens?

209
00:12:47,938 --> 00:12:51,394
Well, compared to the baseline, we have plus 28% of

210
00:12:51,434 --> 00:12:54,058
intended anomalies detected for GPT four.

211
00:12:54,226 --> 00:12:57,546
So that's amazing. Prompt engineering really has

212
00:12:57,610 --> 00:13:01,330
helped us detect more of the intended anomalies

213
00:13:01,442 --> 00:13:05,170
using OpenAI. Now with all these percentages,

214
00:13:05,322 --> 00:13:08,946
got to make sure that I clarify they don't add up altogether. It's kind

215
00:13:08,970 --> 00:13:13,114
of just how it compares to the baseline. So when you combine them all,

216
00:13:13,534 --> 00:13:17,166
you go from 32% of intended anomalies detected for the

217
00:13:17,190 --> 00:13:20,958
basic test up to about 68% when you use

218
00:13:21,046 --> 00:13:24,670
prompt engineering. So that's a really good increase. We're doing pretty

219
00:13:24,702 --> 00:13:27,990
well, but 68% is still far from

220
00:13:28,022 --> 00:13:31,502
ideal. It's not that accurate in the real world. It's going to, you know,

221
00:13:31,518 --> 00:13:34,814
it's going to require some checking in the background because you, you're not going

222
00:13:35,314 --> 00:13:38,858
to be able to guarantee the results are, are correct. So 68%

223
00:13:38,906 --> 00:13:42,682
is good, but it's not amazing. Now the last stage

224
00:13:42,778 --> 00:13:47,698
is the data types. How does the data type affects the

225
00:13:47,826 --> 00:13:50,986
percentage of anomalies that OpenAI is

226
00:13:51,010 --> 00:13:52,014
able to detect?

227
00:13:53,754 --> 00:13:56,858
So for those of you who don't really know

228
00:13:56,946 --> 00:14:00,970
about OpenAI and how it does with numerics, this might come as a surprise.

229
00:14:01,122 --> 00:14:04,962
For those who have heard about how notorious OpenAI was,

230
00:14:05,058 --> 00:14:08,054
is with numerical data, this isn't going to be so surprising.

231
00:14:08,514 --> 00:14:11,970
So throughout all of these examples, we've been using numerical

232
00:14:12,002 --> 00:14:15,722
data. Now what happens when we apply textual data?

233
00:14:15,858 --> 00:14:19,850
So in this case, the textual data we're using are movie reviews.

234
00:14:19,922 --> 00:14:23,818
So in this case I've given it a sequence of

235
00:14:23,986 --> 00:14:27,546
movie reviews that you can have, and I've inputted some ads and

236
00:14:27,570 --> 00:14:30,850
I've inputted some phrases that don't

237
00:14:30,882 --> 00:14:33,814
make sense as well inside it. And I want to see,

238
00:14:34,154 --> 00:14:37,690
is OpenAI able to detect those anomalies better

239
00:14:37,842 --> 00:14:41,202
than with numerical data? And this is

240
00:14:41,218 --> 00:14:44,666
what you find. So in this case, what I mean by accuracy

241
00:14:44,770 --> 00:14:48,850
is I probably should have renamed that when you mean by accuracies is more the

242
00:14:48,962 --> 00:14:51,746
percentage of intended anomalies detected.

243
00:14:51,850 --> 00:14:56,130
So that's what we've been seeing so far. In the case of numerical data,

244
00:14:56,322 --> 00:15:00,130
GPT 3.5 really stayed at 16%. It didn't

245
00:15:00,162 --> 00:15:05,216
get that much better with prompt engineering. On the other hand, GPT 468

246
00:15:05,240 --> 00:15:08,800
percent, like we saw, it's good, but not amazing.

247
00:15:08,992 --> 00:15:12,896
Now, as soon as you switch over to text based data, to those movie reviews,

248
00:15:13,000 --> 00:15:17,144
you see that with GPT 3.5, the percentage of

249
00:15:17,264 --> 00:15:20,680
intended anomalies detected is. Jumps up to

250
00:15:20,752 --> 00:15:25,088
78%. And for GPT four, it's nearly 100%.

251
00:15:25,216 --> 00:15:28,024
Now, I do say nearly, because this will, you know,

252
00:15:28,064 --> 00:15:31,520
depend on the data you're testing it on. It will depend on your number of

253
00:15:31,552 --> 00:15:35,544
anomalies you have, etcetera. But. But its accuracy is absolutely amazing

254
00:15:35,584 --> 00:15:39,832
with text based data. So why

255
00:15:39,848 --> 00:15:43,336
is this the case? Why is OpenAI so bad with numerical data?

256
00:15:43,400 --> 00:15:47,536
But it's great with text based data? It's kind of in the name. So it's

257
00:15:47,560 --> 00:15:51,016
an LLM, it's a large language model. It hasn't been trained to

258
00:15:51,040 --> 00:15:54,704
understand mathematical concepts. It will even struggle with the basic,

259
00:15:54,744 --> 00:15:58,016
you know, which number is bigger than the other.

260
00:15:58,160 --> 00:16:01,876
So open air hasn't been GPT 3.5.

261
00:16:01,900 --> 00:16:05,580
And GPT four aren't built to handle maths. They're more

262
00:16:05,732 --> 00:16:09,156
built to handle text and to handle languages.

263
00:16:09,220 --> 00:16:11,584
So that's why it does so much better in those cases.

264
00:16:12,484 --> 00:16:15,612
Okay, great. So, you know, we've seemed

265
00:16:15,628 --> 00:16:19,260
to have resolved that all. OpenAI is a great anomaly detector. It can go

266
00:16:19,292 --> 00:16:23,132
up to 100%. That's amazing. But is

267
00:16:23,148 --> 00:16:27,076
it really? So, we've said that this is in the case of textual data.

268
00:16:27,220 --> 00:16:30,480
Now it's going to be. It's not that far of a stretch to say that

269
00:16:30,612 --> 00:16:33,952
most of the data we handle on a day to day basis is

270
00:16:34,048 --> 00:16:37,432
numerical. You know, it's in the financial industry, in healthcare,

271
00:16:37,488 --> 00:16:41,056
in so many industries, most of our data is numerical.

272
00:16:41,160 --> 00:16:44,784
So even though OpenAI is really good at detecting anomalies in text,

273
00:16:44,944 --> 00:16:48,684
we can't forget all the other data that exists out there.

274
00:16:49,144 --> 00:16:53,048
So what do we do in the case of numerical data? There's lots of options

275
00:16:53,096 --> 00:16:56,702
out there. But one of the ones I tried out was actually bigquery.

276
00:16:56,848 --> 00:17:01,114
So why did I choose Bigquery? Bigquery actually has an inbuilt anomaly

277
00:17:01,154 --> 00:17:04,762
detector that you can use, so there's three steps to applying it.

278
00:17:04,858 --> 00:17:08,282
The first one is you have to choose the model that best fits your data.

279
00:17:08,418 --> 00:17:11,946
So in this case, I used an Arima plus model, mostly because we're in a

280
00:17:11,970 --> 00:17:15,586
time series. The second thing you have to do is then create

281
00:17:15,650 --> 00:17:19,642
a model for each of the data columns. So if you remember when

282
00:17:19,658 --> 00:17:22,623
we were looking at that energy generation CSV,

283
00:17:22,793 --> 00:17:26,659
we had multiple columns to do with generation, to do with consumption,

284
00:17:26,691 --> 00:17:30,083
to do with capacity. All of that existed within

285
00:17:30,123 --> 00:17:33,555
the CSV. Now we're going to have to create a model for each of the

286
00:17:33,579 --> 00:17:37,731
data columns using this inbuilt anomaly detector.

287
00:17:37,867 --> 00:17:41,251
Once you built those models, you can run your anomaly detector for each of the

288
00:17:41,267 --> 00:17:45,027
data columns to see. Was there any anomalies

289
00:17:45,075 --> 00:17:48,235
within energy generation in Wales, et cetera,

290
00:17:48,259 --> 00:17:51,790
et cetera. Now, bigquery does allow you

291
00:17:51,822 --> 00:17:55,726
to have interdependence between your columns, especially when you're building your model.

292
00:17:55,910 --> 00:17:59,478
For the case of this experiment, we kind of assumed more

293
00:17:59,606 --> 00:18:02,918
that the columns were independent to each other. But of course,

294
00:18:02,966 --> 00:18:06,526
that will depend on your application and how

295
00:18:06,550 --> 00:18:10,246
you want to get your model to be trained. So there's

296
00:18:10,270 --> 00:18:13,422
a lot of code. I'll have the links at the end for you

297
00:18:13,438 --> 00:18:16,662
to kind of see the article which has all of

298
00:18:16,678 --> 00:18:20,050
the code inside it, but this is the key part. So you have something

299
00:18:20,122 --> 00:18:23,546
which is going to be ML detect anomalies. You're going to give

300
00:18:23,570 --> 00:18:26,666
it the name of your model, and you're going to give it this threshold

301
00:18:26,730 --> 00:18:30,026
that you see at the end. Why do we have a threshold?

302
00:18:30,170 --> 00:18:33,906
So the way that Bigquery's anomaly detector works

303
00:18:34,010 --> 00:18:37,498
is it's going to give you a percentage of certainty that

304
00:18:37,546 --> 00:18:40,746
something is an anomaly. So if it's not very short, it might

305
00:18:40,770 --> 00:18:44,292
tell you, you know, I think this is, you know, zero point, 110 percent

306
00:18:44,348 --> 00:18:47,612
chance of this being an anomaly fit. Sure. It probably is going to go up

307
00:18:47,668 --> 00:18:50,820
to 99%, being like, this is definitely an anomaly.

308
00:18:50,932 --> 00:18:55,104
And so it gives you this probability of something

309
00:18:56,284 --> 00:19:00,364
is an anomaly alongside the boolean true false.

310
00:19:00,524 --> 00:19:04,316
So that really helps us understand what's going on under the hood

311
00:19:04,420 --> 00:19:06,184
when it's detecting these anomalies.

312
00:19:07,324 --> 00:19:10,172
Okay. It does super well.

313
00:19:10,228 --> 00:19:13,370
It detects 100% of all of the intended anomalies.

314
00:19:13,482 --> 00:19:16,814
This is amazing, but closed poignant.

315
00:19:17,594 --> 00:19:20,986
Not quite. What we actually find is that

316
00:19:21,050 --> 00:19:25,146
there are 21 false positives when you look at only about 28

317
00:19:25,170 --> 00:19:28,426
lines of data where there's meant to only be five intended

318
00:19:28,490 --> 00:19:32,066
anomalies here. So when you look at it a bit deeper,

319
00:19:32,090 --> 00:19:35,090
there's some. Some of these are genuine false positives.

320
00:19:35,122 --> 00:19:38,752
When you think, okay, this should definitely not be enough, this should definitely not be

321
00:19:38,768 --> 00:19:42,256
an anomaly. There's nothing that kind of indicates that it is but

322
00:19:42,320 --> 00:19:45,432
when you look a bit deeper, there are some things which do seem as if

323
00:19:45,448 --> 00:19:49,456
they could be anomalies. So if you remember at the start, I mentioned

324
00:19:49,480 --> 00:19:52,624
the example of solar data, you know, when there's a cloudy

325
00:19:52,664 --> 00:19:56,328
day, it drops down to zero or near zero

326
00:19:56,376 --> 00:20:00,144
before coming back up. And so that's an anomaly. That's not too

327
00:20:00,184 --> 00:20:03,840
surprising. So we've got to deal with two cases.

328
00:20:03,912 --> 00:20:07,982
We've got to deal with the false positives, which are genuine false positives.

329
00:20:08,078 --> 00:20:11,406
This isn't anomaly at all. And you also have to deal with more

330
00:20:11,430 --> 00:20:14,254
of the normal anomalies that you can have,

331
00:20:14,374 --> 00:20:17,686
um, within your data. The first thing

332
00:20:17,710 --> 00:20:21,350
we can do is we can increase the threshold. So, like I mentioned before,

333
00:20:21,462 --> 00:20:24,822
there's a level of certainty that bigquery will give you about

334
00:20:24,878 --> 00:20:28,702
whether or not it thinks that something is an anomaly. So we can increase

335
00:20:28,758 --> 00:20:32,198
that threshold being saying, yeah, instead of being 95%

336
00:20:32,246 --> 00:20:36,706
sure, I want you to be 99% sure that this

337
00:20:36,730 --> 00:20:40,466
is an anomaly before classing it as such. So when

338
00:20:40,490 --> 00:20:44,186
we make that change, the internal anomalies

339
00:20:44,250 --> 00:20:47,174
are still detected, as they should. So that's great.

340
00:20:47,594 --> 00:20:51,082
We still have all five interned anomalies detected, and we

341
00:20:51,098 --> 00:20:54,562
have a drop in the number of false positives just because we want to

342
00:20:54,578 --> 00:20:58,306
be sure. We want the model to be more sure that something

343
00:20:58,370 --> 00:21:01,786
is an anomaly before it classes it that

344
00:21:01,810 --> 00:21:05,496
way. So that's all great. This kind of works. So what about the

345
00:21:05,520 --> 00:21:09,024
next one? Adding separate trainee data? So when we're.

346
00:21:09,104 --> 00:21:13,120
When we're creating that model on the data, what happens if we throw even

347
00:21:13,192 --> 00:21:17,152
more data at it? And what I actually found is that

348
00:21:17,248 --> 00:21:20,616
this isn't helpful. So if you just throw more data at it,

349
00:21:20,720 --> 00:21:24,024
what it often does is that it overfits it

350
00:21:24,064 --> 00:21:27,004
overfits it and it isn't able to handle,

351
00:21:27,304 --> 00:21:31,390
let's say, for example, I trained it on a whole year of 2016 data,

352
00:21:31,502 --> 00:21:34,114
and then I wanted to figure out, okay, there's,

353
00:21:34,894 --> 00:21:38,134
like, two days worth of data in 2017.

354
00:21:38,254 --> 00:21:41,514
Can you pick up an anomaly? And it actually performed worse.

355
00:21:42,094 --> 00:21:45,974
So this was quite surprising. And then after looking into

356
00:21:46,054 --> 00:21:49,102
it a little bit, there's one thing that we haven't done yet is that we

357
00:21:49,118 --> 00:21:52,654
haven't tuned our model. So tuning our model is that last thing that

358
00:21:52,694 --> 00:21:55,918
happens. So this might bring back some

359
00:21:55,966 --> 00:21:57,634
memories from university.

360
00:22:00,014 --> 00:22:03,318
Um, but you can tune the non seasonal order

361
00:22:03,366 --> 00:22:07,086
terms that you have. I'm not going to go into too much detail

362
00:22:07,150 --> 00:22:10,958
on this, but feel free to check out Bigquery's

363
00:22:11,086 --> 00:22:14,430
ML anomaly detector, because it's got some more details

364
00:22:14,462 --> 00:22:17,662
about it. But there's three parameters. There's three terms that you

365
00:22:17,678 --> 00:22:21,198
can fine tune. There's the term, which we call the term p,

366
00:22:21,286 --> 00:22:25,134
which is the auto, the order of autoregressive part

367
00:22:25,174 --> 00:22:28,714
of it. So it's a value that can vary between zero and five.

368
00:22:29,524 --> 00:22:32,660
The second term that you can tune is the term d,

369
00:22:32,772 --> 00:22:36,276
which is the degree of differencing. So that's also a value between

370
00:22:36,380 --> 00:22:39,724
zero and two. And finally, the last term that you can tune

371
00:22:39,804 --> 00:22:43,180
is q, and that is the order of moving part and

372
00:22:43,212 --> 00:22:47,076
moving average part of the equation. So you

373
00:22:47,100 --> 00:22:51,596
kind of see that you either move more towards an autoregressive

374
00:22:51,620 --> 00:22:54,424
model or more towards a moving average.

375
00:22:55,304 --> 00:22:58,664
And so with this diagram, what you can see is that the color

376
00:22:58,704 --> 00:23:01,640
coding is going to show you the number of false positive.

377
00:23:01,792 --> 00:23:05,144
So you see that the more you move towards the left

378
00:23:05,184 --> 00:23:08,240
at the moment, the less false positives you have.

379
00:23:08,352 --> 00:23:11,400
So it's really that p term dropping down,

380
00:23:11,472 --> 00:23:15,304
that D term dropping down, and you end up with more of a moving

381
00:23:15,384 --> 00:23:19,000
average model. So this is where you've realized that energy generation in

382
00:23:19,032 --> 00:23:21,644
this particular case is more of a moving average.

383
00:23:22,074 --> 00:23:25,842
So that's super interesting and it's really going to depend on your application.

384
00:23:25,898 --> 00:23:29,330
So I highly encourage you to fine tune

385
00:23:29,442 --> 00:23:33,138
with these different non seasonal order terms, see what kind of fits

386
00:23:33,226 --> 00:23:36,522
your particular application. But this is super

387
00:23:36,578 --> 00:23:39,666
important to do because that's where you realize that you really need to tune for

388
00:23:39,690 --> 00:23:42,690
your particular application. Okay, great.

389
00:23:42,802 --> 00:23:46,294
So we've tuned our model, we've increased the threshold.

390
00:23:46,914 --> 00:23:49,734
What's happened? What's happened with our false positives?

391
00:23:50,494 --> 00:23:53,894
Amazingly, we dropped down to about one false positive for

392
00:23:53,934 --> 00:23:58,110
28 lines of data and five and ten little anomalies, which is amazing.

393
00:23:58,142 --> 00:24:00,474
You know, we've really helped improve the performance.

394
00:24:01,494 --> 00:24:05,334
So this is great. We've looked at with OpenAI is great

395
00:24:05,374 --> 00:24:09,166
with textual data. Now do remember that there is a cost

396
00:24:09,230 --> 00:24:13,014
associated to it. I'm going to show you a comparison with other models

397
00:24:13,094 --> 00:24:17,022
just after. But there is a cost associated to using OpenAI

398
00:24:17,198 --> 00:24:19,114
and for numerical data.

399
00:24:20,174 --> 00:24:23,606
Bigquery ML has this inbuilt anomaly

400
00:24:23,630 --> 00:24:27,046
detector which performs very, very well and you just need to

401
00:24:27,070 --> 00:24:31,062
fine tune it for your application. So amazing.

402
00:24:31,118 --> 00:24:34,114
We've managed to build our own anomaly detectors.

403
00:24:34,614 --> 00:24:38,086
Now, I did mention that OpenAI is quite expensive. So what

404
00:24:38,110 --> 00:24:41,966
about all the open source model? So when I was running this experiment,

405
00:24:42,070 --> 00:24:45,694
I kind of ran it against two of Mistral's model,

406
00:24:45,734 --> 00:24:49,874
which you can see here. And you can see that although

407
00:24:50,374 --> 00:24:54,158
OpenAI's GPT 3.5 and GPT four does

408
00:24:54,206 --> 00:24:57,478
perform better. Mistral is catching up, so it's

409
00:24:57,566 --> 00:25:00,670
not that far behind. There are these open source models which

410
00:25:00,702 --> 00:25:03,514
are doing much, much better now. Coming up.

411
00:25:04,654 --> 00:25:08,086
Great. So that's it for the talk. We've managed to

412
00:25:08,110 --> 00:25:11,646
build two different types of anomaly detectors. If you

413
00:25:11,670 --> 00:25:15,190
want to try things out yourself. I published some things

414
00:25:15,262 --> 00:25:18,926
on Twitter and LinkedIn, including the two articles

415
00:25:18,990 --> 00:25:23,206
that kind of mention everything that I've done here with the code

416
00:25:23,230 --> 00:25:27,054
extract. So please check those out and I hope that you enjoyed it.

