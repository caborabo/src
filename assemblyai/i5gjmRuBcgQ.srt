1
00:00:24,170 --> 00:00:27,320
Hello everybody, and thank you for choosing this talk.

2
00:00:27,770 --> 00:00:30,914
Today we are going to talk about an interesting subject.

3
00:00:30,962 --> 00:00:35,010
We are going to talk about how to push your streaming platform to the limit.

4
00:00:35,170 --> 00:00:38,934
We're going to talk about performance, benchmarking and

5
00:00:38,972 --> 00:00:42,006
how to measure it. Now, before we begin,

6
00:00:42,108 --> 00:00:46,002
let me introduce myself - My name is Elad Leev,

7
00:00:46,146 --> 00:00:50,010
and I'm a big advocate of everything related to

8
00:00:50,050 --> 00:00:53,566
distributed systems, data streaming data as a whole,

9
00:00:53,668 --> 00:00:57,198
the whole concept of data mesh and products

10
00:00:57,284 --> 00:01:00,766
like Kafka, Flink, Pinot and so on. So if

11
00:01:00,788 --> 00:01:04,346
you want to hear my opinions about those subjects, go ahead to my Twitter

12
00:01:04,378 --> 00:01:07,634
account and follow me. And all the links from this

13
00:01:07,672 --> 00:01:11,938
talk will be later on posted as a thread in my account. So definitely

14
00:01:12,024 --> 00:01:15,474
check it out. Now, before we begin with the

15
00:01:15,512 --> 00:01:19,074
actual content, let's start with a little bit of marketing,

16
00:01:19,122 --> 00:01:22,694
because, that's life, and we have to do it ;) So what

17
00:01:22,732 --> 00:01:26,514
is Dojo? Dojo is one of the fastest growing

18
00:01:26,562 --> 00:01:30,486
fintech in Europe. We power mostly everything related

19
00:01:30,518 --> 00:01:33,670
to the face to face economy,

20
00:01:33,830 --> 00:01:37,978
whether it's bars, pubs, restaurants and so on.

21
00:01:38,144 --> 00:01:41,998
So if you are a UK best based, you probably saw

22
00:01:42,084 --> 00:01:44,830
our devices around those areas.

23
00:01:45,810 --> 00:01:49,870
And as you can imagine, we are dealing with tons of

24
00:01:49,940 --> 00:01:53,614
data, which is quite awesome.

25
00:01:53,812 --> 00:01:57,282
I want to start with a quote from a book by this

26
00:01:57,336 --> 00:02:01,234
guy, a computer scientist named Jim Gray. Now,

27
00:02:01,272 --> 00:02:05,426
I'm a big sucker of those kind of books from the 80's, 90's

28
00:02:05,448 --> 00:02:08,758
that are still relevant even today. I mean, almost, what,

29
00:02:08,844 --> 00:02:12,040
40 years later on? It's kind of amazing. Now,

30
00:02:13,370 --> 00:02:17,170
the book itself, or the handbook, contains a lot of gems regarding

31
00:02:17,250 --> 00:02:20,310
everything from system performance,

32
00:02:20,390 --> 00:02:24,250
how to measure it, and so on. And those gems are actually

33
00:02:24,400 --> 00:02:28,182
relevant even today in the "cloud native" area of Kubernetes

34
00:02:28,246 --> 00:02:30,380
and so on. Now,

35
00:02:31,090 --> 00:02:35,406
we need to understand that measuring system is *hard*.

36
00:02:35,588 --> 00:02:38,702
There is no magic bullets. We can't have a single

37
00:02:38,756 --> 00:02:42,702
metric that will tell us whether our system is behaving well

38
00:02:42,756 --> 00:02:47,170
or not, right? It's a really hard task. So even

39
00:02:47,240 --> 00:02:51,026
if, for example, we will take the biggest streaming platform

40
00:02:51,128 --> 00:02:54,180
in the world, which is of course, Kafka, you all know it,

41
00:02:54,630 --> 00:02:57,954
the use cases may vary

42
00:02:58,002 --> 00:03:02,534
because we might have a different message size,

43
00:03:02,652 --> 00:03:06,374
we might have a different traffic pattern, we might

44
00:03:06,412 --> 00:03:10,074
have different configurations between the different services and

45
00:03:10,112 --> 00:03:14,358
the different consumers and producers in our systems.

46
00:03:14,534 --> 00:03:19,158
And as a result of these different aspects,

47
00:03:19,254 --> 00:03:23,930
we might get a completely different performance

48
00:03:24,010 --> 00:03:27,818
from the same machines, from the same cluster. So it's

49
00:03:27,994 --> 00:03:32,000
crucial to understand it, and crucial to understand that

50
00:03:32,610 --> 00:03:35,746
when we evaluate a system, usually most

51
00:03:35,768 --> 00:03:40,082
of the people just look on the TCO, which is the total cost

52
00:03:40,136 --> 00:03:43,762
of ownership. And it's okay. It makes sense in a way,

53
00:03:43,816 --> 00:03:47,762
but we can't just rely on the performance benchmark

54
00:03:47,826 --> 00:03:51,378
that the vendors are given. Because eventually,

55
00:03:51,474 --> 00:03:54,946
if you think about it, those benchmarks

56
00:03:54,978 --> 00:03:58,742
are a marketing jobs, right? There are almost no engineers

57
00:03:58,806 --> 00:04:01,980
involved in those processes.

58
00:04:02,350 --> 00:04:07,046
Now, it's okay, but as data driven professionals,

59
00:04:07,238 --> 00:04:10,314
this is not something we can trust, right? We need to

60
00:04:10,352 --> 00:04:13,774
actually understand and run it on our own to understand those

61
00:04:13,812 --> 00:04:17,102
limits. We can just believe that the system will

62
00:04:17,156 --> 00:04:20,558
scale as we grow our business and so on.

63
00:04:20,644 --> 00:04:23,954
This is something that we will actually need to test and to see

64
00:04:23,992 --> 00:04:26,500
with our bare eyes. Now,

65
00:04:27,110 --> 00:04:30,414
we need to know those limits. We need to know the limits

66
00:04:30,462 --> 00:04:33,842
of our system, because knowing how much our system

67
00:04:33,896 --> 00:04:37,142
can handle, whether if it's, I don't know,

68
00:04:37,196 --> 00:04:40,326
the biggest message size we can process,

69
00:04:40,428 --> 00:04:44,162
or how many RPS our database can handle,

70
00:04:44,306 --> 00:04:48,202
how many messages a second and so on, and especially

71
00:04:48,336 --> 00:04:52,234
knowing what is our limiting factor,

72
00:04:52,352 --> 00:04:55,766
might later on assist us in different aspects.

73
00:04:55,798 --> 00:04:59,786
for example, in preventing maintenance in a better and

74
00:04:59,808 --> 00:05:03,834
more accurate capacity planning, and even in eliminating toil,

75
00:05:03,882 --> 00:05:07,550
because eventually eliminating toil is a cost, right?

76
00:05:07,700 --> 00:05:11,262
So we need to know the limit of our system.

77
00:05:11,396 --> 00:05:15,330
Now, when you look on those systems

78
00:05:16,470 --> 00:05:20,882
in general, on servers and computers and

79
00:05:20,936 --> 00:05:24,450
performance, sorry, we can actually understand that

80
00:05:24,600 --> 00:05:27,250
we only have four pillars of failures,

81
00:05:27,330 --> 00:05:31,298
right? It's either going to be the disk, the memory,

82
00:05:31,394 --> 00:05:34,566
the CPU, or the network, nothing else.

83
00:05:34,668 --> 00:05:38,346
usually. So we need to actually understand what is

84
00:05:38,368 --> 00:05:41,690
the limiting factor while running those benchmarks.

85
00:05:42,350 --> 00:05:45,974
Now, understanding those limits

86
00:05:46,102 --> 00:05:49,782
is nice, but what are the key criteria

87
00:05:49,926 --> 00:05:53,438
for our benchmarks? Before we actually run it,

88
00:05:53,524 --> 00:05:57,166
we need to build it properly. Now, first and

89
00:05:57,188 --> 00:06:00,894
foremost, it might sound obvious, but when running those

90
00:06:00,932 --> 00:06:04,354
benchmarks, we can't do any tricks. We can't use,

91
00:06:04,392 --> 00:06:08,462
I don't know - faster machines, we can't use a different JVM,

92
00:06:08,606 --> 00:06:11,394
we can't use anything like that

93
00:06:11,432 --> 00:06:15,134
right? Even if it costs us a bit more

94
00:06:15,192 --> 00:06:18,630
to run those benchmarks using

95
00:06:18,700 --> 00:06:22,294
the same hardware and the same systems as we have in production (or any

96
00:06:22,332 --> 00:06:26,482
other environments that you are using) it is crucial

97
00:06:26,546 --> 00:06:29,980
for benchmark, because remember - it's not a game.

98
00:06:30,590 --> 00:06:34,266
We are not trying to aim for a

99
00:06:34,288 --> 00:06:38,474
better result or getting better numbers or

100
00:06:38,592 --> 00:06:42,394
anything like that. We actually aim for an accurate

101
00:06:42,442 --> 00:06:46,350
result in the benchmarks, right? So even if it costs us more,

102
00:06:46,420 --> 00:06:49,242
it's better to mimic our production,

103
00:06:49,306 --> 00:06:52,734
let's say environment, in our benchmark, and use

104
00:06:52,772 --> 00:06:56,430
the same machine types and

105
00:06:56,500 --> 00:06:59,682
amount of clusters and amount of nodes and so on.

106
00:06:59,736 --> 00:07:03,726
Now the second bit is that we should aim

107
00:07:03,758 --> 00:07:06,934
for the peak, right? Because if you think about it,

108
00:07:06,972 --> 00:07:10,194
metrics are crucial for the success of the benchmark.

109
00:07:10,242 --> 00:07:14,018
So to begin with, if you don't collect those metrics,

110
00:07:14,114 --> 00:07:18,238
whether it's the system performance metric, and anything else, the application metrics of course,

111
00:07:18,274 --> 00:07:22,342
you should start by doing that because it will be crucial

112
00:07:22,406 --> 00:07:26,650
later on. But also you need to understand what are your

113
00:07:26,720 --> 00:07:29,290
SLA, what are your SLOs,

114
00:07:29,790 --> 00:07:33,466
for example, what is

115
00:07:33,488 --> 00:07:36,814
the acceptable, I don't know, end-to-end latency from your

116
00:07:36,852 --> 00:07:40,174
system, because it might change between different clusters, right?

117
00:07:40,212 --> 00:07:43,474
Because you have different services and different use cases and so

118
00:07:43,512 --> 00:07:45,620
on. And also,

119
00:07:46,470 --> 00:07:51,074
what do we consider as a downtime? If that

120
00:07:51,192 --> 00:07:54,254
latency is spiking, is it considered to be a downtime?

121
00:07:54,302 --> 00:07:57,542
A downtime is when the system is not performing well

122
00:07:57,596 --> 00:08:01,366
and so on. And one of

123
00:08:01,388 --> 00:08:04,614
the most crucial metric to find is

124
00:08:04,652 --> 00:08:07,914
your peak traffic, because you should aim to

125
00:08:07,952 --> 00:08:11,690
that traffic, right? You should aim to your peak traffic

126
00:08:13,070 --> 00:08:16,394
and better yet, add some buffer because you need

127
00:08:16,432 --> 00:08:21,214
to expect the unexpected. There is a great post by

128
00:08:21,412 --> 00:08:25,082
AWS CTO Werner Vogels,

129
00:08:25,146 --> 00:08:29,310
where he mentions that eventually

130
00:08:29,650 --> 00:08:33,598
failures are given, everything will fail over time.

131
00:08:33,684 --> 00:08:36,980
again, whether it's the disk, the CPU or anything else.

132
00:08:37,590 --> 00:08:41,742
But on those cases, we will still need to serve our users

133
00:08:41,886 --> 00:08:44,978
during peak time. So we actually need to understand what is the

134
00:08:44,984 --> 00:08:48,662
peak time, what is the peak traffic that we have. And maybe

135
00:08:48,716 --> 00:08:52,150
we should aim for N-1, right? Because we still

136
00:08:52,220 --> 00:08:55,814
want to serve successfully even when

137
00:08:55,852 --> 00:08:59,434
we lost one or two of the machines. So on

138
00:08:59,472 --> 00:09:01,820
your benchmarks, aim for that point.

139
00:09:02,510 --> 00:09:06,620
Now, the benchmarks should be scalable and

140
00:09:07,230 --> 00:09:10,662
portable, because the benchmark

141
00:09:10,726 --> 00:09:14,602
itself, for now, we might run it on

142
00:09:14,656 --> 00:09:18,078
system, let's say Kafka or RedPanda or anything else. But in

143
00:09:18,084 --> 00:09:20,798
the future we might decide to move to a different system,

144
00:09:20,884 --> 00:09:24,306
right? We might decide to use RabbitMQ or

145
00:09:24,328 --> 00:09:27,554
Pulsar or NATS or anything that the future will

146
00:09:27,592 --> 00:09:31,474
bring. So the benchmark should apply to

147
00:09:31,672 --> 00:09:34,798
every other system that we use. It should be portable.

148
00:09:34,974 --> 00:09:38,342
And also we want to test our benchmark in different

149
00:09:38,396 --> 00:09:42,374
use cases. So our benchmark should be able to scale up

150
00:09:42,412 --> 00:09:45,878
and scale down the same as our services,

151
00:09:45,964 --> 00:09:49,066
right? The same idea. So it should be possible to

152
00:09:49,088 --> 00:09:52,406
scale up and down our benchmarks and our worker nodes,

153
00:09:52,438 --> 00:09:55,994
which we later on we'll see as a reflect of our

154
00:09:56,032 --> 00:09:59,210
actual performance of the cluster.

155
00:10:01,490 --> 00:10:05,162
The next bit is that simplicity,

156
00:10:05,226 --> 00:10:08,878
is the key. Don't try to overcomplicate things.

157
00:10:09,044 --> 00:10:12,802
Don't try to do any of those stuff. The benchmark must

158
00:10:12,856 --> 00:10:17,150
be understandable, and the benchmark must be reproducible,

159
00:10:17,230 --> 00:10:22,910
because otherwise it will kind of lose the credibility

160
00:10:22,990 --> 00:10:26,878
of the test. Because you want to document everything in the process and

161
00:10:26,904 --> 00:10:30,710
you want to document the key result. And you want your

162
00:10:30,780 --> 00:10:34,518
users, whether it's internal user or external user, depend on

163
00:10:34,524 --> 00:10:37,782
your cases. But you want your users to be able

164
00:10:37,836 --> 00:10:41,402
to mimic and to reproduce the same test

165
00:10:41,456 --> 00:10:45,114
that you saw, because later on they might test it on

166
00:10:45,152 --> 00:10:48,682
their end, if you know what I mean. So it's crucial that your

167
00:10:48,736 --> 00:10:50,880
test will be as simple as possible.

168
00:10:51,730 --> 00:10:55,550
Now we understand why it's super important

169
00:10:55,620 --> 00:10:59,534
to run those tests and how

170
00:10:59,572 --> 00:11:03,306
should we build those kind of tests. But what should

171
00:11:03,348 --> 00:11:07,202
we look for when doing those tests, right? Because this is something

172
00:11:07,256 --> 00:11:08,980
that it's important to understand.

173
00:11:09,910 --> 00:11:13,154
So one of the best methods that

174
00:11:13,192 --> 00:11:16,742
I know of defining a system performance is the USE

175
00:11:16,796 --> 00:11:20,454
method. It was created originally by an

176
00:11:20,492 --> 00:11:24,082
engineer called Brendan Gregg. Amazing. Check his blog.

177
00:11:24,146 --> 00:11:28,214
Definitely. it's an amazing blog. So Brendan

178
00:11:28,262 --> 00:11:31,850
actually created a method to solve 80%,

179
00:11:31,920 --> 00:11:36,170
to identify 80% of the server's issues with

180
00:11:36,240 --> 00:11:40,270
5% of the effort. So the same as for example,

181
00:11:40,340 --> 00:11:42,590
you have a flight attendant,

182
00:11:44,130 --> 00:11:48,250
they have some kind of manual or tiny emergency

183
00:11:48,330 --> 00:11:52,206
checklist that it was like idiot prone to what

184
00:11:52,228 --> 00:11:55,954
to do when there is an emergency. So the same thing we have with

185
00:11:55,992 --> 00:11:59,698
the USE method, we have a straightforward, complete

186
00:11:59,864 --> 00:12:03,698
and fast manual how to test our system.

187
00:12:03,864 --> 00:12:07,522
So for each one of our resources that we already mentioned, the CPU,

188
00:12:07,586 --> 00:12:11,830
disk, network and so on, we want to look on the

189
00:12:11,980 --> 00:12:15,218
utilization, on the saturation and the errors

190
00:12:15,314 --> 00:12:19,186
in the logs or whatsoever. This will help

191
00:12:19,228 --> 00:12:22,714
us to identify what is our problem as fast as

192
00:12:22,752 --> 00:12:26,106
possible. Now for example, if you look on

193
00:12:26,128 --> 00:12:29,814
the same pillars, so if we are looking on the CPU,

194
00:12:29,862 --> 00:12:32,702
for example, we can look on our system time,

195
00:12:32,756 --> 00:12:36,670
the user, the idle and so on. We can check the load average,

196
00:12:37,010 --> 00:12:40,570
if we are talking about our memory.

197
00:12:40,650 --> 00:12:43,934
So we might want to use the metrics related

198
00:12:43,982 --> 00:12:46,930
to the buffers, to the cache,

199
00:12:47,270 --> 00:12:50,450
to see the JVM heap. If it's a JVM

200
00:12:51,190 --> 00:12:54,900
based system, we might want to see the GC time

201
00:12:55,990 --> 00:12:59,686
when we talk about networks. So we might want to use to

202
00:12:59,708 --> 00:13:03,030
look on the bytes in and out, to see how many package, if any,

203
00:13:03,100 --> 00:13:06,962
dropped and so on. So how do we benchmark

204
00:13:07,026 --> 00:13:10,154
those systems? Now like I mentioned, today,

205
00:13:10,272 --> 00:13:13,180
in the streaming platform area,

206
00:13:13,630 --> 00:13:17,146
there's like billions of products already, right? And we have more

207
00:13:17,168 --> 00:13:21,086
and more products launching every day. Now if

208
00:13:21,108 --> 00:13:24,560
it's a JVM product, maybe we can use the old,

209
00:13:25,170 --> 00:13:29,018
I don't want to say rusty, but the old, nice JMeter

210
00:13:29,114 --> 00:13:32,510
that everyone used to run in the past.

211
00:13:32,580 --> 00:13:36,242
But again we are in an area where not all of the data

212
00:13:36,296 --> 00:13:39,730
system are JVM based. So we want to use something

213
00:13:39,800 --> 00:13:43,154
else and we

214
00:13:43,192 --> 00:13:46,680
could use the system specific benchmark tools. For example,

215
00:13:47,130 --> 00:13:51,362
Kafka is packed with its own performance

216
00:13:51,426 --> 00:13:55,474
test shell scripts that you can run against your cluster.

217
00:13:55,602 --> 00:13:59,418
Same goes for RabbitMQ, and NATS for example

218
00:13:59,504 --> 00:14:02,330
has its own "bench" utility.

219
00:14:03,150 --> 00:14:06,374
But as I mentioned in the past, our goal is to seek

220
00:14:06,422 --> 00:14:10,318
a system that will be easy to move between

221
00:14:10,404 --> 00:14:13,754
different systems, right? So we don't

222
00:14:13,802 --> 00:14:17,600
want to use a system specific benchmark tool.

223
00:14:18,130 --> 00:14:21,486
So exactly for that use case we have

224
00:14:21,588 --> 00:14:24,910
a project, a nice project, from the Linux Foundation

225
00:14:25,070 --> 00:14:28,642
which is called the Open Messaging Benchmark or

226
00:14:28,696 --> 00:14:32,562
in short, OMB. This system

227
00:14:32,696 --> 00:14:35,830
is a cloud native, vendor neutral,

228
00:14:36,410 --> 00:14:40,386
open source distributed messaging benchmark.

229
00:14:40,578 --> 00:14:44,278
A lot of buzzwords. Yeah, and it basically

230
00:14:44,364 --> 00:14:49,510
allows us to run benchmark on most of those common

231
00:14:49,580 --> 00:14:52,380
systems that we all use in a simple way.

232
00:14:52,750 --> 00:14:56,858
Now the system itself is built out of two components, which is

233
00:14:56,944 --> 00:15:00,874
easy to understand. You have the drivers and you have the OMB workers.

234
00:15:00,922 --> 00:15:04,554
Again, OMB, open messaging benchmark, the driver

235
00:15:04,602 --> 00:15:08,014
itself is responsible for assigning the task to the

236
00:15:08,052 --> 00:15:11,694
workers. It's also

237
00:15:11,732 --> 00:15:14,562
responsible for everything related to the metadata itself.

238
00:15:14,696 --> 00:15:18,306
So for example it is the one who actually

239
00:15:18,408 --> 00:15:21,938
creates the topics in Kafka, creating the

240
00:15:21,944 --> 00:15:25,442
consumers and so on. And we have the OMB workers

241
00:15:25,506 --> 00:15:28,470
which is the benchmarks executor.

242
00:15:29,050 --> 00:15:33,046
So the driver is communicating over network with

243
00:15:33,068 --> 00:15:36,760
the worker and assigning task and

244
00:15:37,530 --> 00:15:41,386
the worker, sorry, actually execute the test again the

245
00:15:41,408 --> 00:15:44,986
cluster. Now it's super easy to

246
00:15:45,008 --> 00:15:48,646
use it. You can just install using the provided helm chart.

247
00:15:48,678 --> 00:15:52,830
You can easily deploy OMB on every Kubernetes cluster

248
00:15:53,250 --> 00:15:56,960
or even deploy it outside of Kubernetes. Of course if you want

249
00:15:58,370 --> 00:16:02,574
in Kubernetes case our driver will read

250
00:16:02,772 --> 00:16:06,386
all the configuration from a config file for example and will distribute the

251
00:16:06,408 --> 00:16:09,854
load as I send to the workers, which are pods

252
00:16:09,902 --> 00:16:13,550
eventually. So you might want to spawn

253
00:16:13,710 --> 00:16:16,494
the same amount of worker as your most,

254
00:16:16,632 --> 00:16:19,560
let's say your biggest service or something like that,

255
00:16:21,530 --> 00:16:25,186
like I mentioned in the criteria. So it's super easy to scale

256
00:16:25,218 --> 00:16:29,094
the system. You can scale up and down the amount

257
00:16:29,132 --> 00:16:32,620
of workers to match any number that you want.

258
00:16:33,070 --> 00:16:36,826
And like I said, it's a good practice to run them against the

259
00:16:36,848 --> 00:16:40,310
same number of pods that you have in your most crucial

260
00:16:40,390 --> 00:16:43,806
system or service. Of course again I will

261
00:16:43,828 --> 00:16:47,322
say it, use the same Kubernetes machine, use the same instance

262
00:16:47,386 --> 00:16:50,400
types and so on because it's crucial for the test.

263
00:16:51,730 --> 00:16:55,194
Now here you can see an example of the configuration.

264
00:16:55,322 --> 00:16:58,514
On the left side you have the Kafka configuration, on the right

265
00:16:58,552 --> 00:17:02,130
side you have the Pulsar configuration. You can see that we start

266
00:17:02,200 --> 00:17:05,910
with the name of the test and a driver class.

267
00:17:05,980 --> 00:17:09,778
So again makes sense. Kafka is Kafka, Pulsar is Pulsar.

268
00:17:09,954 --> 00:17:14,322
Next, we assign the basic configurations

269
00:17:14,386 --> 00:17:17,794
for the test. For example, we have the connection

270
00:17:17,842 --> 00:17:21,782
string to pulsar or to Kafka the port, we have the amount of

271
00:17:21,836 --> 00:17:26,454
I/O threads, we have the required request,

272
00:17:26,502 --> 00:17:29,546
timeout and so on. Next we have

273
00:17:29,568 --> 00:17:33,822
the producer configurations. So we

274
00:17:33,876 --> 00:17:37,806
might want to use different producer configuration like I mentioned, based on

275
00:17:37,828 --> 00:17:41,294
our services, but it allows you in Kafka use case,

276
00:17:41,332 --> 00:17:44,858
for example, you can set the arc to be all

277
00:17:44,964 --> 00:17:48,306
minus one or whatever you want. You can change the

278
00:17:48,328 --> 00:17:51,986
linger, match bed size and so on. Same goes to pulsar, you can

279
00:17:52,008 --> 00:17:56,230
change the producer. And lastly you define your consumers.

280
00:17:56,730 --> 00:18:00,002
Now in the project repo itself, in GitHub

281
00:18:00,066 --> 00:18:04,280
of course. And later on I will post a link. Like I mentioned,

282
00:18:05,770 --> 00:18:09,242
you have many examples. So make sure to do

283
00:18:09,296 --> 00:18:13,002
the market search, make sure to identify your

284
00:18:13,056 --> 00:18:17,146
biggest producers and consumers and

285
00:18:17,248 --> 00:18:21,374
what are the configurations that they are using. Because later on you might

286
00:18:21,412 --> 00:18:25,370
want to use those configurations in your tests.

287
00:18:25,530 --> 00:18:29,386
Now based on this information, make sure to generate

288
00:18:29,498 --> 00:18:32,714
different use cases and these edge cases and test

289
00:18:32,772 --> 00:18:36,466
it against your cluster. This is an

290
00:18:36,488 --> 00:18:40,370
example for a workload file. Eventually the workload,

291
00:18:40,950 --> 00:18:44,146
the message that we send is just like a binary file that

292
00:18:44,168 --> 00:18:47,734
we send, but the workload definition looks as

293
00:18:47,772 --> 00:18:52,482
you can see in here you have the message size, you have some randomized

294
00:18:52,626 --> 00:18:56,086
payloads configuration. So on things that you

295
00:18:56,108 --> 00:19:00,010
can change, you have the rate of the producer

296
00:19:00,430 --> 00:19:04,122
and many other stuff. So running

297
00:19:04,176 --> 00:19:07,978
the test, it's quite easy. With OMB you

298
00:19:07,984 --> 00:19:11,642
can test it the same test, you can test it again, different systems.

299
00:19:11,786 --> 00:19:15,998
And after you run this test, the test result

300
00:19:16,084 --> 00:19:19,662
will be printed to your screen, but also

301
00:19:19,716 --> 00:19:23,026
it will be saved as a file which later on you can

302
00:19:23,048 --> 00:19:26,366
share. And the project itself is also containing

303
00:19:26,398 --> 00:19:32,386
a nice Python script that allows you to generate a

304
00:19:32,408 --> 00:19:36,590
pie chart from the result and then maybe put it in your documentation

305
00:19:36,670 --> 00:19:40,440
so everyone could see what the result of the system.

306
00:19:41,690 --> 00:19:44,850
Now, after we understand how to do it, and that it's

307
00:19:44,930 --> 00:19:48,220
kind of better to do it with this kind of system,

308
00:19:49,390 --> 00:19:53,482
what are the kind of insights that we might get

309
00:19:53,536 --> 00:19:57,306
from these kind of tests? First of

310
00:19:57,328 --> 00:20:00,554
all, we could potentially find our average

311
00:20:00,602 --> 00:20:04,320
latency, right, the end to end latency, which is important.

312
00:20:07,010 --> 00:20:11,290
We can identify that latency

313
00:20:11,370 --> 00:20:15,586
to find any kind of problems, because maybe if

314
00:20:15,608 --> 00:20:19,054
we have services that the latency is crucial

315
00:20:19,102 --> 00:20:22,594
to them, maybe we could play a little bit

316
00:20:22,632 --> 00:20:26,706
with the configuration, lower the linger.ms , lower the

317
00:20:26,728 --> 00:20:29,750
batch size (again in the world of Kafka),

318
00:20:31,370 --> 00:20:34,630
and then get a better end-to-end latency.

319
00:20:34,970 --> 00:20:38,314
Maybe we can find a better fine tune for

320
00:20:38,352 --> 00:20:41,834
our services and for our configuration, right? So it's really

321
00:20:41,872 --> 00:20:46,266
important to find it. The next bit is we

322
00:20:46,288 --> 00:20:50,810
have a unique opportunity to stress test our

323
00:20:50,880 --> 00:20:54,238
monitoring service and dashboards because we usually,

324
00:20:54,324 --> 00:20:58,346
I hope so. We don't have a lot of those cases, extreme cases,

325
00:20:58,378 --> 00:21:01,946
where the system is super overloaded. So we have a unique

326
00:21:01,978 --> 00:21:05,106
opportunity to see that our Grafana dashboards or

327
00:21:05,128 --> 00:21:09,826
Datadog, or whatever you use, can actually handle these

328
00:21:09,848 --> 00:21:14,754
kind of values and these kinds of extreme cases. Because one

329
00:21:14,792 --> 00:21:18,166
of the things that you don't want is your monitoring service to

330
00:21:18,188 --> 00:21:21,830
be broken when shit hit the fan. Sorry for my

331
00:21:21,900 --> 00:21:26,694
language. Next, you might find

332
00:21:26,812 --> 00:21:30,346
the potential bottlenecks because as we mentioned,

333
00:21:30,448 --> 00:21:34,022
it's super important to identify our limiting

334
00:21:34,086 --> 00:21:38,458
factor because then we could actually address

335
00:21:38,544 --> 00:21:42,794
it in advance, right? We can, I don't know, put more alerts on our systems.

336
00:21:42,922 --> 00:21:46,446
If, for example, it's the disk type, maybe we can lower the

337
00:21:46,468 --> 00:21:50,798
threshold of the alert so we will be notified before

338
00:21:50,884 --> 00:21:54,418
the disk get into I/O starvation or something like that.

339
00:21:54,584 --> 00:21:58,046
Maybe we can change the type of the disk and yada yada yada.

340
00:21:58,078 --> 00:22:00,020
So it's super important to do it.

341
00:22:01,750 --> 00:22:04,914
Next, we might have some kind

342
00:22:04,952 --> 00:22:09,942
of, I call it scale up ballpark. Because when

343
00:22:09,996 --> 00:22:13,382
running those systems, you can actually

344
00:22:13,516 --> 00:22:18,474
add more brokers to your Kafka cluster and

345
00:22:18,512 --> 00:22:21,338
to see the impact of the latency during that time,

346
00:22:21,424 --> 00:22:24,780
but also the impact of

347
00:22:25,150 --> 00:22:29,306
the overall cluster performance after you added that node. So you

348
00:22:29,328 --> 00:22:32,806
will have some kind of ballpark to know how many nodes

349
00:22:32,838 --> 00:22:36,046
in the future. You have some kind of ballpark to know how many nodes you

350
00:22:36,068 --> 00:22:39,806
need to add to your system to your cluster in order to

351
00:22:39,908 --> 00:22:43,634
sustain the growth of the system. Last but not

352
00:22:43,672 --> 00:22:46,894
least, if you run enough benchmark,

353
00:22:46,942 --> 00:22:50,130
like I said, different use cases, extreme cases, and so on,

354
00:22:50,280 --> 00:22:54,858
you can easily create some kind of system of recommendations.

355
00:22:54,974 --> 00:22:58,994
Because if you have enough stress

356
00:22:59,042 --> 00:23:02,920
test cases, and if you test all your producers and

357
00:23:03,850 --> 00:23:07,806
consumer configurations, you might be able to collect

358
00:23:07,858 --> 00:23:12,182
them to put it on some kind of backend with a nice UI

359
00:23:12,326 --> 00:23:15,850
where your developers can actually

360
00:23:16,000 --> 00:23:20,010
change the different parameters and so the result based on your test.

361
00:23:20,160 --> 00:23:23,914
So maybe as a developer, I want to see if

362
00:23:23,952 --> 00:23:27,758
I can raise my batch size

363
00:23:27,844 --> 00:23:31,710
or to lower my batch size, but I want to see the impact

364
00:23:32,210 --> 00:23:35,886
on the latency. So based on your configurations, you might be

365
00:23:35,908 --> 00:23:39,646
able to give your developers the abilities to see

366
00:23:39,668 --> 00:23:43,198
the result in advance without them spending time

367
00:23:43,284 --> 00:23:46,182
and money testing it. Yeah.

368
00:23:46,236 --> 00:23:49,766
And this is it! First of all, I hope

369
00:23:49,788 --> 00:23:53,094
that you like it. And second of all, I hope that this talk give you

370
00:23:53,132 --> 00:23:57,030
enough reason to run those kind of benchmarks.

371
00:23:58,250 --> 00:24:01,442
If you have any questions, feel free to reach out to me on LinkedIn

372
00:24:01,506 --> 00:24:05,222
and Twitter or whatever, and I hope you enjoy

373
00:24:05,276 --> 00:24:07,700
the conference. Thank you again and see you later.

