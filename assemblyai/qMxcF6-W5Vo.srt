1
00:00:27,160 --> 00:00:30,320
Hi everyone, and thank you for watching our session. The topic of

2
00:00:30,352 --> 00:00:34,056
our talk today is journey to next gen AI ops powered by EBPF and

3
00:00:34,080 --> 00:00:37,432
Genai. My name is Michele, and today I will be presenting

4
00:00:37,488 --> 00:00:41,496
together with my colleague Nastia. I am a site reliability engineer

5
00:00:41,560 --> 00:00:44,872
at Accenture. In the past years, I've been focusing on various

6
00:00:44,928 --> 00:00:47,720
s related topics such as aiops,

7
00:00:47,792 --> 00:00:51,520
observability and chaos engineering. Those are also topics which

8
00:00:51,552 --> 00:00:55,804
I also presented on various other occasions as a public speaker.

9
00:00:56,344 --> 00:00:59,352
Over to you, Nasya. Hello everyone.

10
00:00:59,488 --> 00:01:02,992
My name is Nostesiar Hangriska, and I'm working as

11
00:01:03,008 --> 00:01:07,032
a cloud advisory manager at Accenture and helping our

12
00:01:07,168 --> 00:01:11,204
customers with the topics on aiops and observability.

13
00:01:12,224 --> 00:01:15,616
Thank you, Nastia. So let's start by taking a quick look on what's

14
00:01:15,640 --> 00:01:19,976
on our agenda today. We start with the motivation, why aiops?

15
00:01:20,080 --> 00:01:23,954
What solve the fast with aiops? And why do site reliability engineers need

16
00:01:23,994 --> 00:01:28,202
it? Nasya will then take you on a voyage and explain the various steps

17
00:01:28,218 --> 00:01:31,474
of the aiops journey. After having a short intro

18
00:01:31,514 --> 00:01:34,938
into EBPF, we will then jump into a few cool aiops use

19
00:01:34,986 --> 00:01:38,562
cases, followed by a custom demo before wrapping

20
00:01:38,618 --> 00:01:42,374
everything up. Okay, I suggest we get started.

21
00:01:43,314 --> 00:01:46,810
Why do sres need aiops? Here we see

22
00:01:46,842 --> 00:01:50,786
multiple challenges that we have identified in operations across different

23
00:01:50,850 --> 00:01:54,890
industries, so let's maybe pick a few. A very common one

24
00:01:54,922 --> 00:01:59,274
with big organizations is complex and siloed it environments,

25
00:01:59,394 --> 00:02:02,938
especially if talking about multi cloud environments,

26
00:02:02,986 --> 00:02:06,426
that aim to resolve by providing a unified

27
00:02:06,490 --> 00:02:09,374
view of the entire it landscape through observability.

28
00:02:09,994 --> 00:02:14,010
Then we have incident resolutions, time. So with traditional

29
00:02:14,042 --> 00:02:17,496
operations, it simply takes too much time to identify

30
00:02:17,690 --> 00:02:21,116
the issue and to manually fix it. And by the time we

31
00:02:21,140 --> 00:02:24,700
fix it, by the time it's resolved, the impact of the failure might

32
00:02:24,732 --> 00:02:28,180
have already spread it. We don't have alert fatigue.

33
00:02:28,252 --> 00:02:32,308
Perhaps an organization has already set up a level of monitoring,

34
00:02:32,436 --> 00:02:35,524
but if it doesn't smartly correlate and analyze

35
00:02:35,564 --> 00:02:39,244
failures, errors, problems, the team will end up

36
00:02:39,284 --> 00:02:42,804
being spammed with many relevant alerts. So the idea is

37
00:02:42,844 --> 00:02:46,822
to let AI opposite the correlation, highlight critical incidents for faster

38
00:02:46,878 --> 00:02:51,114
resolution, and basically reduce the overall noise.

39
00:02:51,854 --> 00:02:55,526
As you can see, there are many challenges ahead. The reason for that

40
00:02:55,590 --> 00:02:58,742
is that systems today are becoming more and more complex and more

41
00:02:58,758 --> 00:03:02,374
and more critical. So the figure to the right illustrates exactly

42
00:03:02,414 --> 00:03:05,158
this. So due to heavy complexity,

43
00:03:05,286 --> 00:03:09,262
it's impossible to see what's going on with our system because we are lacking

44
00:03:09,318 --> 00:03:13,714
a level of visibility. Okay, why sres

45
00:03:13,794 --> 00:03:17,114
need aiops? In the beginning, I already

46
00:03:17,194 --> 00:03:20,346
mentioned that I am foremost a site reliability engineer.

47
00:03:20,490 --> 00:03:23,746
And when dealing with the type of challenges that I

48
00:03:23,770 --> 00:03:26,954
mentioned just now, or when architecting solutions

49
00:03:27,034 --> 00:03:30,186
to these challenges, I always tend to frame everything from

50
00:03:30,210 --> 00:03:33,410
an SRE perspective. So I always refer to

51
00:03:33,562 --> 00:03:37,442
SRE best practices such as measure everything,

52
00:03:37,618 --> 00:03:41,554
learn from failure, and most importantly, automate everything,

53
00:03:41,674 --> 00:03:45,774
or at least automate what makes sense. So the question here

54
00:03:46,114 --> 00:03:50,466
why do we need aiops and observability for site reliability engineering?

55
00:03:50,610 --> 00:03:53,906
So, as mentioned, as sres, we tend to automate

56
00:03:53,970 --> 00:03:57,866
as much as possible, since the goal is always to reduce, toil and

57
00:03:57,890 --> 00:04:01,770
improve key KPI's. So talking about meantime to detect mean time

58
00:04:01,802 --> 00:04:06,056
to resolve and so on. But in order to achieve that,

59
00:04:06,160 --> 00:04:09,416
we first need to access end to end insights.

60
00:04:09,520 --> 00:04:13,248
And this is why, as a prerequisite for aiops, we also

61
00:04:13,296 --> 00:04:17,364
need to enable observability, which Nasya will soon explain in detail.

62
00:04:18,144 --> 00:04:22,056
So this diagram that you're seeing right now is illustrating

63
00:04:22,120 --> 00:04:25,360
the big picture. So how do we as Sres

64
00:04:25,512 --> 00:04:29,284
position aiops and observability in the greater context?

65
00:04:30,084 --> 00:04:34,060
And today we're gonna talk to you about the IOP's journey

66
00:04:34,132 --> 00:04:37,984
from reactive monitoring to the zero touch operations.

67
00:04:38,924 --> 00:04:42,588
So imagine yourself on a ship, and your ship is sailing

68
00:04:42,636 --> 00:04:45,264
through the sea of a tee.

69
00:04:45,644 --> 00:04:49,404
And on the ship you have the data that you

70
00:04:49,444 --> 00:04:53,420
gather from your main sensors, the data about the speed,

71
00:04:53,492 --> 00:04:55,984
about fuel, about engine health.

72
00:04:56,544 --> 00:05:00,504
Those things are metrics. And those metrics are

73
00:05:00,544 --> 00:05:04,744
gathered by the vigilant clickout team who is

74
00:05:04,864 --> 00:05:08,324
reacting and informing the responsible team

75
00:05:08,984 --> 00:05:10,804
when something goes wrong.

76
00:05:11,704 --> 00:05:15,684
Here we're talking about reactive monitoring.

77
00:05:16,224 --> 00:05:18,804
The next stage would be observability.

78
00:05:20,064 --> 00:05:24,040
At the observability stage, in addition to the metrics

79
00:05:24,072 --> 00:05:27,240
that we are gathering, the lookout team will also

80
00:05:27,312 --> 00:05:30,164
look into the journal logs of the captain,

81
00:05:30,504 --> 00:05:33,884
and they will also follow the crew members,

82
00:05:34,864 --> 00:05:38,384
see what they are doing, and to document what they are

83
00:05:38,424 --> 00:05:41,520
doing. That would be traces and the journal

84
00:05:41,592 --> 00:05:45,208
of the captain would be the logs. And the combination of all

85
00:05:45,256 --> 00:05:49,044
three, like speed like metrics,

86
00:05:49,744 --> 00:05:53,992
traces and logs, is the magic triangle

87
00:05:54,088 --> 00:05:57,952
of the observability. This triangle

88
00:05:58,048 --> 00:06:01,568
gives you an understanding and the behavior of the system,

89
00:06:01,736 --> 00:06:05,844
so it helps you to understand not only

90
00:06:06,264 --> 00:06:10,284
what is happening, but also why is it happening.

91
00:06:10,784 --> 00:06:13,884
And this is very important if you want to understand

92
00:06:14,304 --> 00:06:17,776
the root cause, not only what went wrong,

93
00:06:17,880 --> 00:06:20,324
but why exactly it went wrong,

94
00:06:21,204 --> 00:06:24,644
which is very important when you address and want

95
00:06:24,684 --> 00:06:28,324
to know how to fix it. So the next stage

96
00:06:28,404 --> 00:06:31,224
would be full stack contextual observability.

97
00:06:31,884 --> 00:06:35,964
At this stage, we are not looking only at the ship itself

98
00:06:36,084 --> 00:06:38,884
and gathering information from the ship itself.

99
00:06:39,044 --> 00:06:42,652
But we are looking into the wider context, we are looking

100
00:06:42,708 --> 00:06:46,004
into the business context, the purpose of the ship,

101
00:06:46,084 --> 00:06:49,916
where we are sailing, why we are sailing, the depth of

102
00:06:49,940 --> 00:06:53,708
the sea, we are looking at our route.

103
00:06:53,796 --> 00:06:57,344
And if we are traveling from Italy to Spain,

104
00:06:58,364 --> 00:07:02,324
then we probably need one amount of supply. But if you are traveling from

105
00:07:02,364 --> 00:07:05,596
Italy to Spain via Alexandria, then we need

106
00:07:05,740 --> 00:07:08,544
another amount of fuel and supply.

107
00:07:09,044 --> 00:07:12,884
So we gather and correlate all the information that

108
00:07:12,924 --> 00:07:16,770
we gather from the context in

109
00:07:16,802 --> 00:07:20,074
which ship is sailing and

110
00:07:20,114 --> 00:07:23,474
from the ship itself. We combine it to get

111
00:07:23,514 --> 00:07:26,834
a bigger picture and better understanding of

112
00:07:26,914 --> 00:07:30,434
the behavior of our system. In the

113
00:07:30,474 --> 00:07:34,194
next step, we are talking about intelligent or predictive

114
00:07:34,234 --> 00:07:37,970
observability. So based on all the

115
00:07:38,002 --> 00:07:41,746
data that we are gathering, and we are looking into the context,

116
00:07:41,850 --> 00:07:45,486
and we are looking into the ship itself, and based on this historical

117
00:07:45,550 --> 00:07:48,514
data, we can predict what will be happening,

118
00:07:49,214 --> 00:07:52,966
understanding and seeing the failure before it happened.

119
00:07:53,030 --> 00:07:56,798
That's our goal here. And this proactive approach

120
00:07:56,886 --> 00:08:00,926
allows us to have strategic planning,

121
00:08:01,030 --> 00:08:04,550
minimize disruptions, and optimize the ship's

122
00:08:04,582 --> 00:08:08,830
performance. However, all the resolution

123
00:08:08,902 --> 00:08:11,074
steps would be still manual,

124
00:08:11,794 --> 00:08:14,534
and thus we go into the next stage,

125
00:08:14,994 --> 00:08:18,802
which will be autonomous or zero

126
00:08:18,858 --> 00:08:22,594
touch AI Ops. Here we have already

127
00:08:22,714 --> 00:08:26,074
our autonomous voyager. Here,

128
00:08:26,114 --> 00:08:29,442
automated systems powered by AI take

129
00:08:29,498 --> 00:08:33,290
over routine tasks and decision making processes.

130
00:08:33,482 --> 00:08:37,312
The automated navigation system, for example.

131
00:08:37,498 --> 00:08:41,396
No one needs to steer the boat, and the ship

132
00:08:41,460 --> 00:08:44,784
becomes self aware and self healing,

133
00:08:45,084 --> 00:08:49,144
responding to challenges without human intervention.

134
00:08:51,004 --> 00:08:54,544
This is why we call it zero touch operations.

135
00:08:55,524 --> 00:08:58,852
Here you look at the full journey from reactive

136
00:08:58,908 --> 00:09:02,224
monitoring through awareness of observability.

137
00:09:02,804 --> 00:09:08,498
Proactive context, preventive predictive

138
00:09:08,546 --> 00:09:12,170
observability, moving towards the

139
00:09:12,202 --> 00:09:14,974
zero touch powered by automation.

140
00:09:15,674 --> 00:09:19,602
Of course, this is a journey. This is not an overnight

141
00:09:19,658 --> 00:09:23,554
process, and this is an iterative process, as different

142
00:09:23,634 --> 00:09:28,162
parts of organization might be a different stage of maturity

143
00:09:28,338 --> 00:09:31,722
on this journey. Also,

144
00:09:31,778 --> 00:09:36,194
there is no one size fits at all. That means that

145
00:09:36,274 --> 00:09:40,290
not for every organization requires to

146
00:09:40,442 --> 00:09:44,002
reach the same level of

147
00:09:44,098 --> 00:09:47,614
automation and maturity as the other one.

148
00:09:48,114 --> 00:09:51,586
But it is good to know where you are and it

149
00:09:51,610 --> 00:09:55,122
is good to make an assessment and to see where

150
00:09:55,178 --> 00:09:57,814
you on this part of the journey.

151
00:09:58,844 --> 00:10:02,244
Before we jump into aiops use cases, let's have a quick brush

152
00:10:02,284 --> 00:10:06,756
up on EBPF. So what is EBPF?

153
00:10:06,940 --> 00:10:10,564
As we can see here, EBPF stands for extended Berkeley packet

154
00:10:10,604 --> 00:10:14,292
filter. And EBPF is a technology

155
00:10:14,468 --> 00:10:17,932
that is able to run sandbox programs in a privileged context.

156
00:10:18,068 --> 00:10:21,796
So basically, EBPF extends the OS kernel

157
00:10:21,900 --> 00:10:25,528
without really changing the kernel source, without requiring a reboot,

158
00:10:25,636 --> 00:10:29,408
and without causing any crashes. So how does EBPF

159
00:10:29,456 --> 00:10:32,920
actually work. So let's look at the figure on the

160
00:10:32,952 --> 00:10:36,480
left side. Let's start from the user space where we attached our application,

161
00:10:36,552 --> 00:10:40,564
microservices, networking and basically various processes.

162
00:10:41,104 --> 00:10:44,376
On the other side, we have the kernel space at this point,

163
00:10:44,480 --> 00:10:48,244
currently entirely decoupled from the application.

164
00:10:48,704 --> 00:10:52,216
Then the application process will at some point have a system call.

165
00:10:52,360 --> 00:10:55,600
Execv is often used, but there are also others, as we can see from the

166
00:10:55,632 --> 00:10:59,296
figure. And the system call will create an event which

167
00:10:59,320 --> 00:11:02,564
is then calling the EBPF program that we have injected.

168
00:11:02,904 --> 00:11:06,520
This way, every time that the process executes

169
00:11:06,552 --> 00:11:09,816
something on the kernel side, it will run the EBPF program.

170
00:11:09,960 --> 00:11:14,456
And this is why we say that EBPF programs are event driven. So basically

171
00:11:14,600 --> 00:11:17,920
every time that we have something, anything happening in our

172
00:11:17,952 --> 00:11:21,502
program, it will run the system call inside the kernel, which then calls

173
00:11:21,518 --> 00:11:25,814
the BPF program, which will run on the schedule and will start our application.

174
00:11:25,974 --> 00:11:29,998
So in this way we strengthen the coupling and context awareness

175
00:11:30,046 --> 00:11:33,998
between the kernel and our application sitting in the user space,

176
00:11:34,126 --> 00:11:37,582
and the kernel becomes a sort of big brother as it is able to see

177
00:11:37,638 --> 00:11:41,542
everything. Nastia previously covered the various enablement

178
00:11:41,598 --> 00:11:45,182
steps towards AI Ops. So now we want to dive deep

179
00:11:45,238 --> 00:11:48,514
into some concrete use cases. The idea of this

180
00:11:48,554 --> 00:11:52,306
part is to give you a flavor of the art of the possible. So we

181
00:11:52,330 --> 00:11:55,554
will tackle some challenges and pain points, see what the current

182
00:11:55,594 --> 00:11:58,770
trends on the market are, as well as explore solutions that

183
00:11:58,802 --> 00:12:02,098
reflect recent state of the art AI ops by leveraging technologies

184
00:12:02,146 --> 00:12:05,054
such as GenaI and EBPF.

185
00:12:05,874 --> 00:12:09,738
Okay, now that we have refreshed relevant concepts,

186
00:12:09,826 --> 00:12:13,786
let's look at some SRE challenges that are currently very predominant

187
00:12:13,810 --> 00:12:17,630
in different industries. There are obviously numerous challenges,

188
00:12:17,702 --> 00:12:21,194
but for today we cherry picked a couple of exemplary ones.

189
00:12:21,654 --> 00:12:24,314
So let's start with the first one.

190
00:12:24,854 --> 00:12:27,934
This is in the context of cloud native and kubernetes.

191
00:12:28,014 --> 00:12:32,234
So we are talking about deploying containerized applications.

192
00:12:33,054 --> 00:12:36,334
For those who have already worked with kubernetes and containers,

193
00:12:36,454 --> 00:12:41,126
they know that it's often a bit of a struggle because it

194
00:12:41,150 --> 00:12:44,876
often happens that it's a black box approach and

195
00:12:44,900 --> 00:12:48,300
it's very hard to debug what's happening really inside of container.

196
00:12:48,452 --> 00:12:51,916
So how do we do risk assessment inside containers?

197
00:12:51,980 --> 00:12:55,812
How can we be sure that there are no vulnerabilities

198
00:12:55,908 --> 00:12:59,020
before we deploy this into production?

199
00:12:59,172 --> 00:13:02,636
So the question here is, how can we

200
00:13:02,660 --> 00:13:06,220
apply aiops to immediate such risks? So we want to do

201
00:13:06,252 --> 00:13:10,094
ships left, but how do we do it? When, what is the effort?

202
00:13:10,174 --> 00:13:15,190
Does it scale? These are all questions that we need to consider staying

203
00:13:15,222 --> 00:13:19,198
close to the concept of deployments. Imagine now several developer

204
00:13:19,246 --> 00:13:22,430
teams or sres, deploying various features on a

205
00:13:22,462 --> 00:13:25,782
system in production. Now imagine that a

206
00:13:25,798 --> 00:13:29,334
new feature has passed all tests and is about to be deployed.

207
00:13:29,454 --> 00:13:33,278
However, in the meantime, an issue occurred in production and the system

208
00:13:33,326 --> 00:13:37,094
is unstable. So just taking a step back, is it

209
00:13:37,134 --> 00:13:41,314
really smart to deploy a new feature on an unstable production environment?

210
00:13:41,734 --> 00:13:45,038
Especially if you consider that we have multiple

211
00:13:45,086 --> 00:13:48,502
sres deploying features concurrently, meaning we

212
00:13:48,518 --> 00:13:52,366
need a mechanism to prevent these situations. And the final

213
00:13:52,430 --> 00:13:55,854
challenge, when deploying something to production,

214
00:13:55,974 --> 00:13:59,478
we often forget to enable context aware reliability

215
00:13:59,646 --> 00:14:02,988
since the very beginning of the software development lifecycle.

216
00:14:03,126 --> 00:14:06,768
So not just in production, but also in

217
00:14:06,776 --> 00:14:09,896
the previous stages. Okay, let's start

218
00:14:09,920 --> 00:14:13,488
with the first use case, classic deployment structure

219
00:14:13,616 --> 00:14:17,524
devsecops pipeline running on kubernetes and or cloud native.

220
00:14:18,064 --> 00:14:21,400
Then we have our application that we want to deploy.

221
00:14:21,592 --> 00:14:25,344
As mentioned earlier, the application is running on a pod with possibly

222
00:14:25,384 --> 00:14:29,244
many containers, so we don't know what's happening inside the container.

223
00:14:29,794 --> 00:14:34,294
So the question obviously arises, what about security vulnerabilities?

224
00:14:35,114 --> 00:14:39,002
Obviously with EBPF we have visibility into

225
00:14:39,058 --> 00:14:42,514
what is happening and what the vulnerabilities might be.

226
00:14:42,674 --> 00:14:46,242
So on the left hand side, we have just one singular

227
00:14:46,298 --> 00:14:49,426
application that we want to deploy,

228
00:14:49,530 --> 00:14:52,898
one user space and one pod. So that is something that

229
00:14:52,986 --> 00:14:56,450
we can monitor relatively easily thanks to BPF, which feeds

230
00:14:56,482 --> 00:15:00,386
this information to my end to end SRE emission control, which can

231
00:15:00,410 --> 00:15:02,854
trigger actions depending on detected issues.

232
00:15:03,354 --> 00:15:06,714
However, what happens if I have a much,

233
00:15:06,834 --> 00:15:11,010
if I have much more than one deployment? So imagine

234
00:15:11,122 --> 00:15:14,694
a huge and complex microservice deployment.

235
00:15:15,354 --> 00:15:19,882
Our situation doesn't change much. So when creating

236
00:15:20,018 --> 00:15:22,954
a container, accessing any file or network,

237
00:15:23,034 --> 00:15:26,292
so keep track of the b icon underneath.

238
00:15:26,428 --> 00:15:30,188
EBPF is able to scale and see all of this. So EBPF

239
00:15:30,236 --> 00:15:33,620
is aware of everything that is going on in the node, and that is why

240
00:15:33,652 --> 00:15:37,276
we say that EBPF enables context awareness on a more

241
00:15:37,300 --> 00:15:41,052
cloud native level. So this is an example of how

242
00:15:41,108 --> 00:15:44,468
we can trigger specific remediation actions

243
00:15:44,516 --> 00:15:47,464
based on enriched insights coming from EBPF.

244
00:15:48,044 --> 00:15:51,980
Now that we know how to use EBPF to obtain actionable

245
00:15:52,052 --> 00:15:55,822
insights on a more cloud native level, the question now arises,

246
00:15:55,998 --> 00:15:59,182
how do we prevent, for instance, a feature deployment to

247
00:15:59,198 --> 00:16:01,354
an unstable or faulty production environment?

248
00:16:02,694 --> 00:16:05,750
So we have our site reliability engineer who is

249
00:16:05,782 --> 00:16:09,766
about to deploy a new feature onto production. Let's consider

250
00:16:09,830 --> 00:16:13,598
our devsecops pipeline as part of the various pipeline stages.

251
00:16:13,686 --> 00:16:18,038
We have a step where we filter and analyze relevant EBPF events,

252
00:16:18,166 --> 00:16:21,572
and based on this data we can create proactive alerts,

253
00:16:21,668 --> 00:16:25,220
but as mentioned before, if we want to reach zero touch

254
00:16:25,252 --> 00:16:28,868
operations, we need more on that. This is where AI

255
00:16:28,916 --> 00:16:32,020
comes into play. There are obviously various benefits

256
00:16:32,052 --> 00:16:36,412
for placing AI here, but most importantly, based on context

257
00:16:36,508 --> 00:16:40,676
and topology aware data from numerous sources, including EBPF,

258
00:16:40,820 --> 00:16:43,824
we can predict anomalies much, much more efficiently.

259
00:16:44,324 --> 00:16:48,460
Such AI should also be capable of detecting an unstable prod environment

260
00:16:48,612 --> 00:16:52,284
and based on this information, trigger an action that

261
00:16:52,324 --> 00:16:55,636
blocks any deployment to production until the environment is

262
00:16:55,660 --> 00:16:58,828
stable again. And this is a good example of zero

263
00:16:58,876 --> 00:17:02,172
touch automation, if you remember the last pillar in the

264
00:17:02,188 --> 00:17:05,860
AIops maturity curve that Nastia presented because

265
00:17:05,932 --> 00:17:09,388
we are moving from reactive to predictive and our system

266
00:17:09,436 --> 00:17:13,424
is now self healing, meaning no manual operations are required.

267
00:17:14,144 --> 00:17:17,928
Genai is another technology that has realistically

268
00:17:17,976 --> 00:17:22,272
flooded the market, and that is an important part of our AIOP story.

269
00:17:22,448 --> 00:17:26,008
So we will now take a look at an SRE copilot

270
00:17:26,096 --> 00:17:27,924
powered by Genai.

271
00:17:28,904 --> 00:17:32,560
So imagine that your observability platform

272
00:17:32,672 --> 00:17:36,080
on the top left detects an increased response

273
00:17:36,112 --> 00:17:40,084
time for a specific service. As a direct consequence,

274
00:17:40,384 --> 00:17:43,640
the error budget is burned and the SLO is breached.

275
00:17:43,752 --> 00:17:47,208
So these are two KPI's. So we are keeping

276
00:17:47,256 --> 00:17:50,976
measure of both of these KPI's. So the

277
00:17:51,000 --> 00:17:54,456
moment that the error budget is burned, we trigger two processes.

278
00:17:54,640 --> 00:17:58,080
As you can see, one of the trigger processes proactively fires

279
00:17:58,112 --> 00:18:02,084
an alert to our SRE teams so that they know what's going on,

280
00:18:02,424 --> 00:18:05,840
so that they know that the error budget has been burned. But since

281
00:18:05,872 --> 00:18:09,660
we have no time to wait, the second one in parallel launches

282
00:18:09,692 --> 00:18:13,236
the OpenAI feature. Our SRE copilot, as we

283
00:18:13,260 --> 00:18:17,804
want to call it, which generates a post mortem and suggests a problem resolution

284
00:18:17,964 --> 00:18:21,364
which will display on our SRE mission control dashboard

285
00:18:21,404 --> 00:18:25,424
so that the SRE can check the insights suggested by the copilot.

286
00:18:26,164 --> 00:18:29,940
So this is an example of how we can leverage an

287
00:18:29,972 --> 00:18:34,276
AI in order to obtain much more rich insights

288
00:18:34,420 --> 00:18:37,932
into our issues. Now that we have an idea

289
00:18:37,948 --> 00:18:41,500
of what kind of use cases we are dealing with, let's jump straight into

290
00:18:41,532 --> 00:18:45,380
the demo. This demo is based on a devsecops use case.

291
00:18:45,532 --> 00:18:48,740
We have our devsecops ci CD pipeline which we have

292
00:18:48,772 --> 00:18:52,516
implemented in GitLab. The goal of the pipeline is to deploy a

293
00:18:52,540 --> 00:18:56,172
containerized application, so we are in a cloud native context

294
00:18:56,228 --> 00:19:00,020
here. We also deploy our honeypot

295
00:19:00,092 --> 00:19:03,166
where we will execute our EBPF experiment,

296
00:19:03,300 --> 00:19:07,538
after which we will use this EBPF data with our genai copilot,

297
00:19:07,626 --> 00:19:11,570
which will provide suggestions in case any vulnerabilities

298
00:19:11,642 --> 00:19:15,442
are fine. So this entire end to end process is

299
00:19:15,458 --> 00:19:19,154
then visualized in our end to end devsecops mission control. And for

300
00:19:19,194 --> 00:19:22,354
this demo we have used dynatrace. Okay,

301
00:19:22,434 --> 00:19:23,694
let's start the demo.

302
00:19:25,554 --> 00:19:29,652
So this is the devsecops pipeline that we implemented in GitLab.

303
00:19:29,818 --> 00:19:33,200
Imagine that we are trying to deploy a containerized application,

304
00:19:33,352 --> 00:19:36,324
and here you can see all of the pipeline stages.

305
00:19:37,184 --> 00:19:41,064
So in the first stage check, we start by deploying the application

306
00:19:41,104 --> 00:19:44,768
to a Kubernetes cluster. Then we have the deploy phase. Here I

307
00:19:44,776 --> 00:19:47,444
am deploying my honeypot with Kubernetes,

308
00:19:47,864 --> 00:19:51,304
then the experiment EBPF phase. This is where

309
00:19:51,384 --> 00:19:55,646
I execute my EBPF experiment by deploying Silium's tetragon,

310
00:19:55,760 --> 00:19:58,974
which is an EBPF based security observability tool.

311
00:19:59,274 --> 00:20:02,930
And finally AI check. This is the point in

312
00:20:02,962 --> 00:20:06,778
which I feed my EBPF data collected in the previous step

313
00:20:06,866 --> 00:20:10,554
to my OpenAI SRE copilot, which based on this

314
00:20:10,594 --> 00:20:13,134
data will provide relevant suggestions.

315
00:20:13,914 --> 00:20:17,834
And then finally I have the cleaner phase which simply cleans up my

316
00:20:17,914 --> 00:20:21,458
environment. Okay, I suggest

317
00:20:21,506 --> 00:20:24,964
we run the pipeline. So as you can see, the first

318
00:20:25,004 --> 00:20:28,508
stage already completed. I prepared the deployment of my containerized

319
00:20:28,556 --> 00:20:31,820
application. So now we can launch the deploy

320
00:20:31,892 --> 00:20:35,604
stage. And I can see here

321
00:20:35,644 --> 00:20:39,444
that all pods have been deployed

322
00:20:39,564 --> 00:20:42,940
as well as the honeypot, which we can see here

323
00:20:42,972 --> 00:20:47,012
printed out. Okay, let's move to the EBPF

324
00:20:47,068 --> 00:20:49,684
phase. Let's launch it.

325
00:20:50,424 --> 00:20:53,404
If we look here closely in the outputs.

326
00:20:54,904 --> 00:20:58,616
Yes, exactly. Here you

327
00:20:58,640 --> 00:21:02,264
will notice that we are seeing here inside the tetragon

328
00:21:02,304 --> 00:21:06,352
logs, that etc. Password has been exposed in

329
00:21:06,368 --> 00:21:10,160
the container. And this is obviously a vulnerability

330
00:21:10,272 --> 00:21:13,752
list which in the next stage. So if I

331
00:21:13,768 --> 00:21:17,962
run the Genai stage. So now I'm feeding the EBPF

332
00:21:18,018 --> 00:21:22,002
data to my copilot. So we

333
00:21:22,018 --> 00:21:25,834
can notice here that the copilot

334
00:21:25,954 --> 00:21:29,826
identified that this vulnerability in our EBPF data

335
00:21:29,970 --> 00:21:33,082
and is warning us by providing suggestions. For example, as you

336
00:21:33,098 --> 00:21:36,466
can read, reading, etc. Password can pose a security

337
00:21:36,570 --> 00:21:40,482
risk as it contains sensitive information, so it may lead to

338
00:21:40,578 --> 00:21:44,306
password tracking and other vulnerabilities and

339
00:21:44,410 --> 00:21:47,866
problems. So this is the part in which my

340
00:21:47,890 --> 00:21:51,538
copilot is telling me, hey, be careful, you are trying to

341
00:21:51,586 --> 00:21:54,810
execute a deployment to production, but your etc.

342
00:21:54,922 --> 00:21:58,562
Password is exposed. So this is the data that we have collected in

343
00:21:58,578 --> 00:22:02,442
the previous stage via EBPF, and we are now feeding it to our

344
00:22:02,498 --> 00:22:05,930
Genai I copilot, which is alarming us and

345
00:22:05,962 --> 00:22:09,046
alerting us and telling us careful here, you don't want

346
00:22:09,070 --> 00:22:12,966
to deploy this to production. And this is obviously extremely

347
00:22:12,990 --> 00:22:16,314
valuable information for our SRE teams.

348
00:22:19,334 --> 00:22:21,794
Finally, we have the cleanup stage,

349
00:22:22,334 --> 00:22:26,074
which simply cleans up my environment after

350
00:22:26,454 --> 00:22:28,954
the pipeline has been executed.

351
00:22:29,574 --> 00:22:33,110
Okay, now we have built,

352
00:22:33,142 --> 00:22:36,802
and we've seen how we built our devsecops pipeline and

353
00:22:36,818 --> 00:22:40,386
how we do the deployment. But the question now comes, how do we

354
00:22:40,410 --> 00:22:43,754
monitor this? So, as already mentioned in the beginning,

355
00:22:43,914 --> 00:22:47,618
we have built the dessert cops mission control demo with

356
00:22:47,706 --> 00:22:51,554
Dynatrace. So what you're seeing here is a dynatrace

357
00:22:51,594 --> 00:22:55,266
dashboard which shows EBPF events which have been collected.

358
00:22:55,370 --> 00:22:59,146
So the honeypot heartbeat as well as the trend of EBPF

359
00:22:59,250 --> 00:23:02,710
attack events. Now, this has been implemented

360
00:23:02,742 --> 00:23:05,926
with a classic dynatrace dashboard. However, if you want a

361
00:23:05,950 --> 00:23:09,870
more custom feel with Dynatrace app engine, you can also

362
00:23:09,942 --> 00:23:13,514
build your own web application, which is exactly what we did.

363
00:23:14,614 --> 00:23:18,046
So this is another version of our

364
00:23:18,110 --> 00:23:21,710
mission control. As you can see here, we are mapping and

365
00:23:21,742 --> 00:23:25,366
tracking the various stages of our GitLab pipeline in real time

366
00:23:25,470 --> 00:23:28,696
and reporting relevant analytics, pipeline status,

367
00:23:28,800 --> 00:23:32,376
failure ratio, heartbeat. But the most interesting piece

368
00:23:32,400 --> 00:23:35,928
of data is exactly the copilot suggestions

369
00:23:35,976 --> 00:23:39,624
that we're seeing here. So this copilot is warning us

370
00:23:39,664 --> 00:23:43,416
about the eTC password vulnerability, which I just

371
00:23:43,440 --> 00:23:46,728
previously explained. So this is, in summary

372
00:23:46,816 --> 00:23:50,480
a great example of how we can leverage an AI and a BPF

373
00:23:50,592 --> 00:23:53,804
to enrich our overall end to end insights.

374
00:23:54,304 --> 00:23:57,744
Okay, time to wrap things up. First thing

375
00:23:57,784 --> 00:24:01,392
we have seen on this session are the different AIops enablement

376
00:24:01,448 --> 00:24:05,080
steps, starting from reactive monitoring by slowly

377
00:24:05,112 --> 00:24:09,136
enhancing observability, contextualization and automation.

378
00:24:09,280 --> 00:24:13,256
The North Star is represented by zero touch operations, where systems

379
00:24:13,280 --> 00:24:16,584
are able to automatically resolve the issue before the failure

380
00:24:16,624 --> 00:24:20,176
occurs. After that, we have looked into

381
00:24:20,280 --> 00:24:23,678
several aiops use cases in a devsecops demo,

382
00:24:23,776 --> 00:24:27,066
through which we saw how we can leverage an AI and

383
00:24:27,090 --> 00:24:30,602
EBPF to significantly enrich end to end insights,

384
00:24:30,698 --> 00:24:34,254
which can be of crucial answers to site reliability engineers.

385
00:24:34,954 --> 00:24:38,626
And the final takeaway I would like to point out from today's session,

386
00:24:38,770 --> 00:24:42,354
start simple and scale fast. Thank you for watching.

