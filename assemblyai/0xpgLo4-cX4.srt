1
00:00:20,720 --> 00:00:24,246
Hi there. My name is Bolu, and today I'm going to talk about superposition

2
00:00:24,350 --> 00:00:28,216
in neural network representations. So I guess it motivates that

3
00:00:28,280 --> 00:00:31,936
a bit. I'll share some context about where this hypothesis comes

4
00:00:31,960 --> 00:00:35,284
from or the field of neural network research.

5
00:00:35,624 --> 00:00:40,688
This field is called mechanistic interpretability, and basically it

6
00:00:40,696 --> 00:00:43,984
follows from the following reasoning. Okay, so we all understand that neural

7
00:00:44,024 --> 00:00:47,324
networks solve an increasing number of important tasks really well,

8
00:00:47,864 --> 00:00:51,448
and it would be at least interesting and probably important to understand

9
00:00:51,536 --> 00:00:55,660
how they do that. So mechanrap is basically a subfield that tackled

10
00:00:55,692 --> 00:00:58,980
this problem by seeking granular mechanistic

11
00:00:59,052 --> 00:01:03,356
explanations for different observed behaviors in neural networks.

12
00:01:03,460 --> 00:01:06,844
It's basically pushing back on the idea that neural networks are just these black

13
00:01:06,884 --> 00:01:10,548
boxes that are completely inscrutable and just do magic

14
00:01:10,596 --> 00:01:14,436
with linear algebra. So it pairs into a given network

15
00:01:14,500 --> 00:01:18,744
at a granular level to investigate some very isolated behavior.

16
00:01:19,354 --> 00:01:23,186
At the same time, it also has very broad hypotheses and

17
00:01:23,210 --> 00:01:26,578
theories about how neural networks do things. And one of these

18
00:01:26,626 --> 00:01:29,994
is about representation learning.

19
00:01:30,074 --> 00:01:34,546
That is, how do neural networks learn which representations to use for inputs,

20
00:01:34,610 --> 00:01:38,362
and how are these inputs, how these representations passed around in

21
00:01:38,378 --> 00:01:41,826
the computation? That's what this

22
00:01:41,850 --> 00:01:45,048
is basically understanding what a model sees and how it does.

23
00:01:45,146 --> 00:01:48,500
So what informations have model found important to look for?

24
00:01:48,572 --> 00:01:51,980
And how is information propagated and I guess represented

25
00:01:52,012 --> 00:01:55,788
and propagated internally in the network? I guess to

26
00:01:55,916 --> 00:01:59,244
paint a picture of what we mean by representations and propagation.

27
00:01:59,364 --> 00:02:02,784
So basically, at the bottom left, on the bottom left here, we have,

28
00:02:03,484 --> 00:02:07,124
let's just say this is like a simple tokenized version of

29
00:02:07,164 --> 00:02:10,412
input you're going to pass on to based on a

30
00:02:10,428 --> 00:02:13,604
transformer, right? So you have like on colon, off wet colon,

31
00:02:13,644 --> 00:02:17,068
dry old colon. And I guess, as any of us would,

32
00:02:17,156 --> 00:02:19,784
would attest to from using something like chat JPT,

33
00:02:20,204 --> 00:02:24,124
these neural networks are definitely able to predict that old. I figure

34
00:02:24,204 --> 00:02:27,868
that the next thing is going to be old colon, new, right,

35
00:02:27,916 --> 00:02:30,864
since you can figure out what you were in opposite. So the idea is,

36
00:02:31,484 --> 00:02:34,996
between this entry of our text and the prediction

37
00:02:35,020 --> 00:02:38,940
on the other side, entire network has to have

38
00:02:38,972 --> 00:02:42,332
encoded certain information and done computations

39
00:02:42,428 --> 00:02:46,026
in the process to get this output of, oh, the next thing to

40
00:02:46,050 --> 00:02:49,370
come after this column is new. So basically

41
00:02:49,522 --> 00:02:52,626
what we're trying to ask is, okay, just what do we know and what can

42
00:02:52,650 --> 00:02:56,106
we investigate about how this information

43
00:02:56,210 --> 00:02:58,962
is encoded? Right? So, because in the beginning, all it knows is that I'm a

44
00:02:58,978 --> 00:03:02,514
column again. After going through just the embedded network of mapping the column

45
00:03:02,554 --> 00:03:07,346
character to a collection

46
00:03:07,370 --> 00:03:10,218
of numbers are just called vectors of ordered vectors, as you can see there in

47
00:03:10,226 --> 00:03:13,906
the column. So the question is summer between entry on

48
00:03:13,930 --> 00:03:17,578
I'm a column to exiting on, my next is new,

49
00:03:17,666 --> 00:03:21,090
which is again the result of

50
00:03:21,282 --> 00:03:25,266
this vector going through an unembedding layer and the softmax, and again the

51
00:03:25,290 --> 00:03:28,570
highest probability weight being attributed to as.

52
00:03:28,602 --> 00:03:32,306
Again, just for simplicity, let's assume the word new is

53
00:03:32,330 --> 00:03:36,654
its own token, because as we know, prediction is done on a token basis.

54
00:03:37,294 --> 00:03:40,734
So, right, so somewhere between signing with M and Colin and my next is new

55
00:03:40,894 --> 00:03:44,998
is a bunch of stuff. So what do we know about

56
00:03:45,126 --> 00:03:48,982
what these representations look like internally? All right,

57
00:03:49,078 --> 00:03:52,774
so here are a couple qualities of these representations

58
00:03:52,854 --> 00:03:56,102
that the starting school of mechanistic

59
00:03:56,118 --> 00:03:59,542
interpretability posits. Basically, it says

60
00:03:59,718 --> 00:04:03,688
that there are discrete features that a model

61
00:04:03,736 --> 00:04:07,216
has learned to look for in an input, and these discrete features

62
00:04:07,280 --> 00:04:10,744
basically compose into giving any given representation, right?

63
00:04:10,784 --> 00:04:15,484
So if we looked at any layer or at any component in the architecture,

64
00:04:16,224 --> 00:04:20,016
all the information the model has at that stage is basically going to be

65
00:04:20,120 --> 00:04:22,884
some composition of discrete things.

66
00:04:23,584 --> 00:04:27,232
And the second is linearity. And so this

67
00:04:27,288 --> 00:04:31,256
basically takes the composite, the decomposability statement, a bit further to say

68
00:04:31,280 --> 00:04:35,102
that not only are these discrete components composed

69
00:04:35,158 --> 00:04:38,694
together, they're composed linearly. And again, we'll discuss a bit

70
00:04:38,774 --> 00:04:42,754
later what exactly that means. And the third basically just says

71
00:04:43,374 --> 00:04:47,118
we can think of these discrete qualities as something called features. And again,

72
00:04:47,166 --> 00:04:50,390
the precise time code definition for what features are comes

73
00:04:50,422 --> 00:04:53,934
later. So I guess maybe to summarize, maybe this, like one

74
00:04:53,974 --> 00:04:57,230
single line or tagline, it probably, like summarizes the school

75
00:04:57,262 --> 00:05:00,718
of thought that says language model. Again, you can replace your neural network

76
00:05:00,766 --> 00:05:03,914
representation that basically have similar architectures.

77
00:05:04,694 --> 00:05:09,526
Language model representations are linearly decomposable

78
00:05:09,630 --> 00:05:13,078
in two features, right? So we're going to pick apart each one

79
00:05:13,086 --> 00:05:17,274
of those, of those items in the course of this talk.

80
00:05:18,694 --> 00:05:22,486
The first, again, is this is kind of a weak statement.

81
00:05:22,550 --> 00:05:25,798
It's not that strong. And I'll explain why. In isolation,

82
00:05:25,926 --> 00:05:29,734
decomposability just basically means that, well, we assume

83
00:05:29,814 --> 00:05:33,830
that neural networks learn different things. That is,

84
00:05:33,982 --> 00:05:37,726
giving a neural network doesn't just basically

85
00:05:37,790 --> 00:05:41,182
memorize every simple potential input.

86
00:05:41,318 --> 00:05:45,694
It learns to abstract certain features

87
00:05:45,734 --> 00:05:48,982
like blueness or redness, or perhaps even more general,

88
00:05:49,038 --> 00:05:52,834
to color. Right? If it has a general abstraction for

89
00:05:53,134 --> 00:05:56,552
color representations. Again, in this simple case,

90
00:05:56,608 --> 00:06:00,904
we have a neural network that may be trained to identify colors

91
00:06:00,944 --> 00:06:05,364
and shapes like. So let's just say maybe this is like some classification

92
00:06:05,664 --> 00:06:09,504
neural network, right? And the idea is that, okay, somewhere in, you know,

93
00:06:09,584 --> 00:06:14,120
all linear network weights are

94
00:06:14,312 --> 00:06:19,688
basically transformations that are able to extract certain

95
00:06:19,736 --> 00:06:23,516
discrete qualities such as the center shape or

96
00:06:23,540 --> 00:06:27,396
the background color. Right here simplify this. We just look for blueness or

97
00:06:27,420 --> 00:06:30,692
redness on the left. But the interesting thing is, well, this is kind

98
00:06:30,708 --> 00:06:33,988
of just saying sometimes neural networks don't overfit, which is why

99
00:06:33,996 --> 00:06:36,324
I say this is kind of a weak statement because like, sure, like it's pretty

100
00:06:36,364 --> 00:06:39,540
obvious that, yes, I mean, or at least like anyone

101
00:06:39,572 --> 00:06:43,064
that has training neural network can demonstrate in

102
00:06:43,684 --> 00:06:46,684
with like a test set to show that yes, indeed,

103
00:06:46,724 --> 00:06:50,244
these neural networks can generalize and not everything is overfitting or memorizing.

104
00:06:50,324 --> 00:06:54,860
So on the right there, if we have something like a purple triangle

105
00:06:54,972 --> 00:06:58,356
that supposedly this neural network has not seen in training before,

106
00:06:58,500 --> 00:07:02,356
it could depend on its previous, previously learned features to

107
00:07:02,380 --> 00:07:05,812
say that, well, even though it doesn't quite have the conception of purple as a

108
00:07:05,828 --> 00:07:09,904
distinct thing, it could compose of the color purple as being

109
00:07:11,364 --> 00:07:15,384
perhaps reading the RGB values, being equally composed of

110
00:07:16,284 --> 00:07:19,464
red and blue values.

111
00:07:20,174 --> 00:07:24,126
So. Right, so at this day, like all, that's all decomposability is saying,

112
00:07:24,270 --> 00:07:27,774
there's certain things in this diagram that are quite strong assumptions.

113
00:07:27,814 --> 00:07:30,990
Again, this whole idea that, oh, there's one thing called a blue neuron,

114
00:07:31,102 --> 00:07:34,246
as we'll see, that's a pretty strong thing to say. And it's not obvious at

115
00:07:34,270 --> 00:07:38,006
all that this is how things play out in reality. But again, at this stage,

116
00:07:38,070 --> 00:07:41,526
decomposability just says irreproentation is composed

117
00:07:41,550 --> 00:07:44,494
of a bunch of little stuff. Because in your own work,

118
00:07:44,654 --> 00:07:48,038
again, demonstrably does not just generalize all the time. Right?

119
00:07:48,126 --> 00:07:52,474
Again, for sufficient size for spatially small problem set.

120
00:07:52,894 --> 00:07:56,714
The second is linearity. So this again takes the decomposability

121
00:07:57,654 --> 00:08:00,894
property a bit further. You say, okay, cool. Not only are these different

122
00:08:00,934 --> 00:08:05,514
properties, well, distinct or different, they combine

123
00:08:06,454 --> 00:08:09,414
linear sums quite simply,

124
00:08:09,494 --> 00:08:13,862
which basically just says, if you can imagine a vector

125
00:08:13,998 --> 00:08:17,934
representing a certain vector direction representing

126
00:08:17,974 --> 00:08:20,594
some feature. And again,

127
00:08:22,214 --> 00:08:25,518
this is an as contrived. Remember looking at this

128
00:08:25,566 --> 00:08:28,726
diagram, the inputs are

129
00:08:28,750 --> 00:08:33,154
already ordered collectible numbers. So again, everything that's for a colon is already inherently

130
00:08:33,494 --> 00:08:37,550
having this vector format. I should mention though, just because a thing

131
00:08:37,702 --> 00:08:40,838
is an order to collection of numbers, it doesn't mean it has to be linear,

132
00:08:40,886 --> 00:08:44,654
right? So it's a bit confusing, obviously, because it has this like

133
00:08:45,954 --> 00:08:49,458
vector formatting of, again, an order to collection of numbers

134
00:08:49,546 --> 00:08:53,234
relating to one entity, then surely it was obviously mini direction.

135
00:08:53,274 --> 00:08:57,534
That is not obvious. And again, I'll show you examples of what that

136
00:08:57,874 --> 00:09:01,402
looks like when it's not the case. Okay, so again, the larynx

137
00:09:01,458 --> 00:09:05,482
says these decomposable sub vectors

138
00:09:05,538 --> 00:09:09,146
basically, literally just add together to give you

139
00:09:09,250 --> 00:09:13,184
the representation for something, right? So here we have how the,

140
00:09:13,644 --> 00:09:17,204
some other neural network that cares about size and

141
00:09:17,324 --> 00:09:21,388
redness in the abstract has

142
00:09:21,556 --> 00:09:24,956
two different directions. Again, given it only has two

143
00:09:25,100 --> 00:09:29,092
features or qualities it cares about, it can dedicate two different directions to

144
00:09:29,108 --> 00:09:33,084
it, right? And then these directions can simply combine to represent

145
00:09:33,164 --> 00:09:36,892
any one given input. And again, like, how do

146
00:09:36,908 --> 00:09:40,834
we have any evidence for this in practice? Is that. Yes, I guess

147
00:09:40,874 --> 00:09:43,994
this is a bit of a popular example

148
00:09:44,034 --> 00:09:47,786
by now, but there was a paper that came out a couple years ago that

149
00:09:47,810 --> 00:09:51,010
basically showed regularities in the differences

150
00:09:51,082 --> 00:09:54,810
between pairs of vectors. So the difference between the

151
00:09:54,842 --> 00:09:58,242
man and woman, again, this is like the man and woman, let's say

152
00:09:58,378 --> 00:10:02,494
word representation in certain language

153
00:10:02,954 --> 00:10:06,562
model architectures was consistent. So if you do something

154
00:10:06,658 --> 00:10:10,346
as silly as, let's say, subtracted the vector, again, just the ordered collection

155
00:10:10,370 --> 00:10:14,194
of numbers for, of uncle from the vector for

156
00:10:14,234 --> 00:10:17,746
aunt, and you simply just impose that on, let's say,

157
00:10:17,810 --> 00:10:22,054
some other pair on something like

158
00:10:22,794 --> 00:10:27,194
man, you would end up with precisely the

159
00:10:27,234 --> 00:10:30,514
vector called woman, right? And you have a bit of this like vector algebra

160
00:10:30,554 --> 00:10:33,850
here on the right with the card. Again, let's assume this is like another relationship

161
00:10:33,882 --> 00:10:37,572
of plurals, again of

162
00:10:37,628 --> 00:10:41,636
like cars, the vector recognition for cars. If you subtract the

163
00:10:41,820 --> 00:10:45,116
singular recognition for car and add to apple,

164
00:10:45,180 --> 00:10:48,276
you get something like apples, right? So this kind of behavior of literal,

165
00:10:48,380 --> 00:10:52,036
like ordered subtraction of

166
00:10:52,140 --> 00:10:56,068
values is what you would see in a linear

167
00:10:56,116 --> 00:11:00,504
system, right? A system where the masculine,

168
00:11:01,084 --> 00:11:04,472
you know, this abstract feature of this is referring

169
00:11:04,488 --> 00:11:08,152
to a masculine entity is encoded with all

170
00:11:08,168 --> 00:11:11,912
the other stuff that has to do with royalty in king, or has to do

171
00:11:12,048 --> 00:11:16,056
with relatives or relatives of siblings,

172
00:11:16,080 --> 00:11:19,760
of your parents, an uncle and aunt, or just again, in the literal

173
00:11:19,792 --> 00:11:23,392
world, man and woman. Effectively,

174
00:11:23,568 --> 00:11:27,280
if these two, if these multiple things are composed in a linear fashion,

175
00:11:27,352 --> 00:11:31,516
then you can get it. We'll be doing things like this, vector subtraction

176
00:11:31,620 --> 00:11:34,508
and arithmetic as we're seeing here, right? But again,

177
00:11:34,596 --> 00:11:38,228
this doesn't mean everything completely is indeed, right. So this is part,

178
00:11:38,276 --> 00:11:41,636
this is just for the embedding layer. And again, to remind us what the embedding

179
00:11:41,660 --> 00:11:44,464
layer is, it is the very bottom of this, right? It's,

180
00:11:45,804 --> 00:11:49,340
there's still a lot of uncertainty as to sure, if maybe for

181
00:11:49,372 --> 00:11:53,108
simple things like embedding a word, you get this vector

182
00:11:53,156 --> 00:11:56,716
algebra, does that mean like for everything? And all the layers in the network,

183
00:11:56,900 --> 00:12:01,276
all the information that it has to encode is in fact

184
00:12:01,420 --> 00:12:04,420
composed in this linear fashion. Right. So that is why there's still a mystery,

185
00:12:04,452 --> 00:12:08,036
even though we have seen some evidence. I guess, as I mentioned,

186
00:12:08,180 --> 00:12:12,324
it would be worth noting that again, just because a thing is an ordered

187
00:12:12,404 --> 00:12:15,772
collection of numbers, again, which is how neural networks tend to

188
00:12:15,788 --> 00:12:19,052
be like represented or simply just how they are, this kind of,

189
00:12:19,068 --> 00:12:23,146
this meme around how neural networks are just linear algebra

190
00:12:23,210 --> 00:12:26,970
scaled up. Well, just because things aren't auto collection numbers doesn't

191
00:12:27,002 --> 00:12:30,370
necessarily mean that they are linear, right? Linearity is a very particular

192
00:12:30,482 --> 00:12:33,650
statement about how different entities interact. Right?

193
00:12:33,682 --> 00:12:37,186
So here's an example of, again on, we can imagine a different

194
00:12:37,250 --> 00:12:42,162
regime where we had a neural network that again

195
00:12:42,218 --> 00:12:45,458
was able to extract a discrete component for redness and another

196
00:12:45,506 --> 00:12:49,014
for blueness, but then join

197
00:12:49,054 --> 00:12:52,674
them together. It did something like, well, maybe just exploited the

198
00:12:53,814 --> 00:12:57,150
simple precision

199
00:12:57,302 --> 00:13:00,606
decimal places. Again, how it does this,

200
00:13:00,630 --> 00:13:03,870
again, special edition, is by simply just taking the first

201
00:13:03,982 --> 00:13:07,606
thing on the left and then making it the value to one decimal

202
00:13:07,630 --> 00:13:11,678
place and taking the other thing. And here you have an algorithm to

203
00:13:11,686 --> 00:13:16,410
do this. Again, this is an example of a non web winner expression.

204
00:13:16,522 --> 00:13:19,530
Um, and again, like the component of this that makes it nonlinear is,

205
00:13:19,562 --> 00:13:23,506
you see, it relies on the floor operation. Again, this is like from,

206
00:13:23,690 --> 00:13:27,346
like from a python, except that like math or floor, it basically just

207
00:13:27,370 --> 00:13:30,650
like tries to do, do the rounding. That's basically how they exploit this like,

208
00:13:30,762 --> 00:13:34,614
um, precision and placement to basically like squish

209
00:13:35,154 --> 00:13:37,842
these two different values together. Right? So again, so this is just like one,

210
00:13:37,978 --> 00:13:41,442
I guess like dummy example of showing that, well, yes,

211
00:13:41,498 --> 00:13:44,922
things ordered collection of numbers can act in

212
00:13:44,938 --> 00:13:48,786
ways that are not quite vector like or don't quite simply just

213
00:13:48,810 --> 00:13:52,174
do addition. Right? You do have other compression schemes,

214
00:13:53,714 --> 00:13:57,534
right? And so what the linear representation is saying is that actually

215
00:13:58,914 --> 00:14:02,442
on the journey from, again, the input of I am a column,

216
00:14:02,498 --> 00:14:04,894
which is what like the embedding does to the output.

217
00:14:06,314 --> 00:14:09,850
All the information it has at that point. Again, all the information at this

218
00:14:09,962 --> 00:14:13,658
single um, column and the input has maintained as it

219
00:14:13,666 --> 00:14:16,802
went through all the layers were simply just

220
00:14:16,858 --> 00:14:20,698
added to each other. Right? There's one vector that represents, oh, there's something,

221
00:14:20,866 --> 00:14:24,418
there's a bit of like a word and opposite game going on. And there was

222
00:14:24,426 --> 00:14:27,658
an interesting paper that, that showed up to say that actually, yes,

223
00:14:27,746 --> 00:14:31,530
not just nouns or discrete informations about inputs

224
00:14:31,602 --> 00:14:35,490
can be encoded, but also abstract things like functions. Right? Again, this whole idea

225
00:14:35,522 --> 00:14:38,874
of, oh, this is a word and opposite game that's been played here between

226
00:14:38,954 --> 00:14:42,282
wet and dry, old and new, etc. That itself is

227
00:14:42,298 --> 00:14:45,386
one vector and yet another vector. Again, this,

228
00:14:45,570 --> 00:14:48,930
this is something that attention can give us, right? Being able

229
00:14:48,962 --> 00:14:52,642
to basically like, look at previous inputs. So again,

230
00:14:52,698 --> 00:14:55,722
the colon token is able to like, look behind,

231
00:14:55,898 --> 00:14:59,082
immediately behind it. See that all the thing that came before me is old and

232
00:14:59,098 --> 00:15:02,314
also is able to look maybe further back to other

233
00:15:02,354 --> 00:15:06,412
things to like glean the pattern of words and opposites, right? All these different

234
00:15:06,538 --> 00:15:10,128
bits of information are literally just different vectors or different

235
00:15:10,176 --> 00:15:13,352
directions that compose as

236
00:15:13,488 --> 00:15:16,800
simple additions to end with the conclusion

237
00:15:16,832 --> 00:15:19,244
of. Okay, surely my next thing is new.

238
00:15:20,184 --> 00:15:24,208
Again, the representations aren't really concerned with how the

239
00:15:24,336 --> 00:15:27,888
network is doing combination that is like, like, what are the mechanics inside of

240
00:15:27,896 --> 00:15:30,896
it that know how to do that? Okay, giving this vector for work opposite this

241
00:15:30,920 --> 00:15:33,920
vector for this. How does it do that? Like, again, there's another body of work

242
00:15:33,952 --> 00:15:36,992
that explores basically this, like, algorithmic interpretability.

243
00:15:37,128 --> 00:15:40,776
This is just saying, um, basically the variables that is being used for these

244
00:15:40,800 --> 00:15:44,364
computations, what do you look like and how are the variables that composed?

245
00:15:46,224 --> 00:15:49,736
Again, by how? I mean, like how in a sense of, like,

246
00:15:49,760 --> 00:15:53,432
are they saying, like, do you have weird stuff like this going on where

247
00:15:53,488 --> 00:15:57,832
it's like, in the c, in the space of all potential transformations,

248
00:15:57,888 --> 00:16:00,710
of taking redness and blueness together to get purpleness?

249
00:16:00,832 --> 00:16:04,082
Um, is it like some unknown arbitrary thing which would be messy

250
00:16:04,098 --> 00:16:07,386
and hard, or is it just literally symbol vector addition?

251
00:16:07,530 --> 00:16:10,850
Um, so that's what recommendation is as being like,

252
00:16:10,882 --> 00:16:14,654
distinct to like, algorithmic, um, interpretation, interpretability.

253
00:16:15,474 --> 00:16:18,538
Um, right. So again, as we see here, um, again,

254
00:16:18,586 --> 00:16:22,298
just think of the linear composition as just a compression scheme for

255
00:16:22,346 --> 00:16:25,770
how all this information is packed together. Okay? So linearity

256
00:16:25,802 --> 00:16:29,520
is great because it basically helps us narrow down, again, as I said, like one

257
00:16:29,552 --> 00:16:33,040
compression algorithm in a very large function space. So there are many things, again,

258
00:16:33,072 --> 00:16:37,608
these are the giant networks to be doing in their, like, typical inscrutable fashion.

259
00:16:37,776 --> 00:16:41,216
So linearity is pretty helpful in that actually kind of narrows it

260
00:16:41,240 --> 00:16:44,504
down to one like,

261
00:16:44,544 --> 00:16:46,004
well known, unstudied,

262
00:16:47,624 --> 00:16:52,104
basically like compression scheme, right? Which is the entire field of linear algebra.

263
00:16:52,184 --> 00:16:56,164
Right? If it happens to be linear. And also the other things that

264
00:16:56,204 --> 00:17:00,332
this gives us as well, which is, it aids in

265
00:17:00,428 --> 00:17:04,780
diagnostics and helps improve our

266
00:17:04,812 --> 00:17:07,908
understanding of the models in ways that, again, if there were some,

267
00:17:07,996 --> 00:17:11,748
if, for example, in every single representation or every single layer,

268
00:17:11,836 --> 00:17:15,116
use a different type of arbitrary algorithm, will be hard.

269
00:17:15,260 --> 00:17:20,168
So, yeah, it basically would be very convenient if

270
00:17:20,256 --> 00:17:22,936
this was the case. Right? And again, we have seen some evidence for it.

271
00:17:22,960 --> 00:17:27,360
Right? So this is just an important point to make where to

272
00:17:27,512 --> 00:17:31,128
state that this is a combination of having some evidence, but also there's a

273
00:17:31,136 --> 00:17:34,324
bit of a motivated inquiry into this. Right? Again,

274
00:17:34,944 --> 00:17:37,712
if this wasn't something we cared about, there are many things about neural networks that

275
00:17:37,728 --> 00:17:41,328
seem to be interesting, but people just haven't really dug

276
00:17:41,376 --> 00:17:45,548
into. But the fact that they seem to have inner behavior has

277
00:17:45,596 --> 00:17:49,324
drawn a very large community of researchers to study just

278
00:17:49,364 --> 00:17:52,868
why. Because, again, it makes the problem a lot more tractable

279
00:17:52,916 --> 00:17:55,980
than if it wasn't the case. Right? And yes,

280
00:17:56,012 --> 00:17:59,332
I guess I put here effectively mind control being like, they're a bit of like.

281
00:17:59,508 --> 00:18:02,796
As these tools become more mature to understand what's happening, we get

282
00:18:02,820 --> 00:18:06,532
to do different things, like everything from mind reading to mind control.

283
00:18:06,628 --> 00:18:10,164
That is like, again, if you get to run your strand

284
00:18:10,204 --> 00:18:12,988
brain on a computer and you have access to all the numbers and you understand

285
00:18:13,156 --> 00:18:16,676
the general, both the algorithms for how the information is

286
00:18:16,700 --> 00:18:19,844
represented and how the information is transformed, then you can eventually, like,

287
00:18:19,884 --> 00:18:23,540
intervene and or at least just like, you know,

288
00:18:23,612 --> 00:18:26,704
have a log stream on what's going on.

289
00:18:27,084 --> 00:18:30,628
Cool. So that's the motivation for why linear composability is great,

290
00:18:30,716 --> 00:18:34,660
right? Because again, it's a algorithm for

291
00:18:34,772 --> 00:18:37,544
these transformers to use for understanding.

292
00:18:38,004 --> 00:18:40,792
Okay, here is a bit of the downside,

293
00:18:40,888 --> 00:18:43,976
is that linearity is kind of demanding in

294
00:18:44,000 --> 00:18:47,928
that it basically says that to have

295
00:18:48,056 --> 00:18:52,320
a lossless compression scheme that composes linearly,

296
00:18:52,432 --> 00:18:55,808
it requires as many dimensions, that is, as many of those different

297
00:18:55,856 --> 00:18:59,712
boxes, as many, like, of the. As many

298
00:18:59,848 --> 00:19:04,744
distinct ordered numbers in the collected set as

299
00:19:04,784 --> 00:19:08,574
you have qualities you want to encode. Right. As you can see here,

300
00:19:09,834 --> 00:19:13,066
again, we have something for redness, for blueness, for squareness, for triangles. And then,

301
00:19:13,090 --> 00:19:17,154
like, as you see, basically, you have this like, one hot vector kind of

302
00:19:17,234 --> 00:19:20,778
situation going where the thing that makes one of these

303
00:19:20,906 --> 00:19:24,026
directions and code for the property

304
00:19:24,050 --> 00:19:27,722
of redness is that it is only the first cell

305
00:19:27,738 --> 00:19:31,002
that activates for it. Now, I do want to point out that there's

306
00:19:31,018 --> 00:19:34,336
a slight difference between just the dimensions and, uh,

307
00:19:34,480 --> 00:19:37,336
like, the number of dimensions is the requirement.

308
00:19:37,440 --> 00:19:41,816
It doesn't necessarily mean, though, that you would always have this perfect idea of,

309
00:19:41,960 --> 00:19:45,176
uh, one cell coordinating to

310
00:19:45,200 --> 00:19:48,984
one thing, right? You. You could basically have, like, there are,

311
00:19:49,024 --> 00:19:53,200
you know, infinite many, um, orthogonal bases

312
00:19:53,312 --> 00:19:56,536
that are able to achieve this. Basically, all you just want is that for,

313
00:19:56,600 --> 00:20:00,252
you want to have as many orthogonal, um, directions as

314
00:20:00,268 --> 00:20:03,740
you have features, right? Again, just for the sake of,

315
00:20:03,772 --> 00:20:07,044
like, easier understanding, we just focus on the

316
00:20:07,084 --> 00:20:10,620
one in this very large, this infinite set of orthogonal

317
00:20:10,652 --> 00:20:14,556
bases that happens to be one hot encoded,

318
00:20:14,620 --> 00:20:18,100
right? So just going for. Just imagine

319
00:20:18,132 --> 00:20:21,556
that every time I talk about a neuron

320
00:20:21,660 --> 00:20:25,144
or a dimension. I just literally mean one neuron, but that's not necessarily the case.

321
00:20:26,404 --> 00:20:29,224
Cool. So why is this a problem? Okay,

322
00:20:30,364 --> 00:20:33,412
so let's just like, again, to remind ourselves of, like, where we're at right now.

323
00:20:33,428 --> 00:20:37,412
Again, this certain hypothesis for how representation is done says that

324
00:20:37,588 --> 00:20:41,404
language model, large language model representations are linearly decomposable,

325
00:20:41,484 --> 00:20:45,476
composable into features. Okay, this brings

326
00:20:45,500 --> 00:20:48,624
us to the linear organizational puzzle. Why is this a puzzle?

327
00:20:48,924 --> 00:20:52,612
So basically, in a couple steps, first, we have

328
00:20:52,628 --> 00:20:55,772
some evidence that indeed we LLMs do represent stuff linearly.

329
00:20:55,828 --> 00:20:57,424
Right? Again, so, like, meaning this,

330
00:20:58,604 --> 00:21:02,140
this claim has, you know, some basis in reality,

331
00:21:02,212 --> 00:21:05,788
right? And again, there are several other arguments in research to suggest

332
00:21:05,836 --> 00:21:09,756
that, like, this behavior is more likely, or like,

333
00:21:09,940 --> 00:21:13,412
or has either from, or can be observed

334
00:21:13,468 --> 00:21:16,436
either from, like, looking at the number of flops that are dedicated to the transformations

335
00:21:16,500 --> 00:21:20,600
versus not, etc. Cool. So linear

336
00:21:20,632 --> 00:21:24,288
stuff is happening. Linear combinations

337
00:21:24,336 --> 00:21:27,304
require as many dimensions or neurons,

338
00:21:27,344 --> 00:21:30,408
again, which is, again, a subset of the case of

339
00:21:30,496 --> 00:21:34,448
orthogonal basis as features. If you want to encode for redness,

340
00:21:34,496 --> 00:21:38,032
blueness, triangle ness and squareness,

341
00:21:38,128 --> 00:21:41,944
you need literally four different things. Again, if you want to encode these things distinctly

342
00:21:42,024 --> 00:21:45,104
as being different things, you need four different

343
00:21:45,144 --> 00:21:48,652
directions. However, and this is

344
00:21:48,668 --> 00:21:53,020
where the puzzle comes in experience, it seems

345
00:21:53,092 --> 00:21:56,932
that these networks are able to represent way more stuff

346
00:21:57,068 --> 00:22:00,252
than they have neurons for. Right? And again, I have a bit of a back

347
00:22:00,268 --> 00:22:04,052
of the envelope calculation here, where GPT-2 has an order of hundreds of thousands

348
00:22:04,068 --> 00:22:08,412
of neurons. Again, the exact numbers might vary on the architecture,

349
00:22:08,548 --> 00:22:12,150
basically, but if you look at the number of attention heads, MLP layers,

350
00:22:12,292 --> 00:22:15,810
and the dimensions that these architecture

351
00:22:15,842 --> 00:22:19,858
components operate with, um, you, you're in the order of a couple

352
00:22:19,906 --> 00:22:23,810
hundred thousands of neurons. Uh, and the assumption

353
00:22:23,842 --> 00:22:27,098
is that these models encode a lot more than that. And if

354
00:22:27,106 --> 00:22:31,494
you're finding hard to wrap your hand around, what about how 100,000,

355
00:22:31,994 --> 00:22:35,658
a couple hundred thousand, um, features is not enough. Remember that,

356
00:22:35,706 --> 00:22:38,730
again, this is encoding all of the english language, right?

357
00:22:38,762 --> 00:22:42,786
Or all of language, if you recall how

358
00:22:42,810 --> 00:22:46,394
well GBD two was able to perform, it is plausible to that, yes, it probably

359
00:22:46,434 --> 00:22:50,178
encodes a lot more than a couple hundred thousand features,

360
00:22:50,306 --> 00:22:52,494
because again, this is all of language itself.

361
00:22:53,794 --> 00:22:57,650
The question is, how is that possible? Again, we know that linear.

362
00:22:57,842 --> 00:23:01,186
So basically, we seem to have conflicting or contradicting evidence when

363
00:23:01,210 --> 00:23:03,734
we have evidence of linearity, but at the same time,

364
00:23:04,034 --> 00:23:07,630
we have all needs, which is as many

365
00:23:07,762 --> 00:23:12,606
neurons as it has features. But at the same time, we have models in

366
00:23:12,630 --> 00:23:16,134
production, we have external models that seem to do quite

367
00:23:16,174 --> 00:23:20,302
well without having as many neurons

368
00:23:20,358 --> 00:23:22,634
as they seem to have features. Right. Again,

369
00:23:25,054 --> 00:23:28,270
to appreciate why this is a puzzle, you just have to depend on your

370
00:23:28,302 --> 00:23:32,302
gut feeling of there are probably more than 200,000 things that

371
00:23:32,318 --> 00:23:35,266
you need to be looking for in. In any given standards.

372
00:23:35,330 --> 00:23:38,414
Remember how open ended all of language is?

373
00:23:39,674 --> 00:23:43,306
And again, this is one of the difficulties in describing what exactly a

374
00:23:43,330 --> 00:23:46,034
feature is. A feature,

375
00:23:46,154 --> 00:23:50,834
I think, of a feature is one helpful definition. Feature is a

376
00:23:50,874 --> 00:23:54,378
thing that a neuron would be dedicated to in a sufficiently

377
00:23:54,426 --> 00:23:57,826
large language model. But again,

378
00:23:57,890 --> 00:24:01,330
we get to that shortly. Okay, cool. So this is our puzzle. How is this

379
00:24:01,362 --> 00:24:04,686
happening? There's a great paper that came from a team at

380
00:24:04,710 --> 00:24:08,254
Anthropoc that basically tries to. Basically building off of

381
00:24:08,294 --> 00:24:12,046
previous work. Exploring this puzzle suggests

382
00:24:12,070 --> 00:24:15,526
a way forward on tackling what exactly

383
00:24:15,550 --> 00:24:17,874
is going on and basically being able to.

384
00:24:19,094 --> 00:24:22,598
Being able to disentangle all the mess that seems to

385
00:24:22,606 --> 00:24:26,014
be happening, because, again, something strange. But I guess basically,

386
00:24:26,134 --> 00:24:29,638
before this paper came out, the same team introduced

387
00:24:29,686 --> 00:24:33,086
the idea of superposition.

388
00:24:33,190 --> 00:24:37,238
And superposition is basically a hypothesis that attempts

389
00:24:37,286 --> 00:24:41,390
to answer the puzzle the riddle of how can a model do more represent

390
00:24:41,462 --> 00:24:44,034
more features than it has neurons?

391
00:24:44,534 --> 00:24:47,982
Now, it effectively says that neural networks are

392
00:24:47,998 --> 00:24:51,366
able to do this because they exploit

393
00:24:51,470 --> 00:24:55,434
feature sparsity and the relative feature importance.

394
00:24:56,694 --> 00:25:00,422
It basically just says that, like, the model does not, in fact,

395
00:25:00,478 --> 00:25:03,670
do perfectly lossless compression, but it

396
00:25:03,702 --> 00:25:07,314
trades that off in exchange for representing more features,

397
00:25:08,534 --> 00:25:11,966
because of a property called like

398
00:25:11,990 --> 00:25:15,190
feature sparsity, which basically means that, again, even though the english language,

399
00:25:15,222 --> 00:25:18,446
or like any arbitrary text in english language, or like, in the set of all

400
00:25:18,470 --> 00:25:21,990
possible texts of coherent english language sentences,

401
00:25:22,062 --> 00:25:24,034
or perhaps not even coherent,

402
00:25:24,974 --> 00:25:28,394
even though those contain a very large number of features,

403
00:25:29,134 --> 00:25:32,314
it turns out not all those features are active at the same time.

404
00:25:32,854 --> 00:25:36,222
And there's a great. And this provides an opportunity for a

405
00:25:36,238 --> 00:25:40,062
trade off where we can say, okay, what if I choose a

406
00:25:40,118 --> 00:25:43,366
not perfectly orthogonal set of

407
00:25:43,390 --> 00:25:47,310
vectors to represent my features, which, again, is the requirement for

408
00:25:47,342 --> 00:25:50,070
that to be a lossless compression. What if,

409
00:25:50,102 --> 00:25:53,910
instead, I chose n plus

410
00:25:54,102 --> 00:25:57,294
m, actually a bit more than this ideal

411
00:25:57,334 --> 00:26:00,354
set of perfectly orthogonal vectors, and in exchange,

412
00:26:01,214 --> 00:26:05,222
basically, each additional direct feature direction that I

413
00:26:05,278 --> 00:26:08,582
add basically adds some noise.

414
00:26:08,678 --> 00:26:11,390
Again, using the compression analogy, add some noise.

415
00:26:11,502 --> 00:26:15,936
Basically, I trade off a little bit of noise for

416
00:26:16,120 --> 00:26:19,376
having a much wider set of features that can

417
00:26:19,400 --> 00:26:22,568
represent. And again, this only works if all features are not present

418
00:26:22,616 --> 00:26:25,992
together, because, again, if all features are present all the time,

419
00:26:26,168 --> 00:26:29,440
that, again, if you have no sparsity, you will

420
00:26:29,472 --> 00:26:32,404
have noise in all your outputs.

421
00:26:33,664 --> 00:26:36,404
And, yeah, this is actually what this is saying, right?

422
00:26:37,024 --> 00:26:40,696
So in the top and the top three boxes you have

423
00:26:40,720 --> 00:26:43,814
there is basically saying, okay, you have,

424
00:26:44,194 --> 00:26:47,986
again, all the different dots are meant to be different features

425
00:26:48,090 --> 00:26:50,578
that your model cares about. Again, maybe one of them is redness, one of them

426
00:26:50,586 --> 00:26:53,774
is blue nest, one of them is square nest, etc.

427
00:26:54,194 --> 00:26:57,642
And you can see you have like a two dimensional surface. You have

428
00:26:57,658 --> 00:27:01,202
a two dimensional surface. In the case where there's

429
00:27:01,218 --> 00:27:05,054
no sparsity, where every feature is as likely to be important

430
00:27:05,354 --> 00:27:08,302
as the other, you effectively have,

431
00:27:08,458 --> 00:27:12,046
um, what we, what we expect that is

432
00:27:12,070 --> 00:27:15,782
like the neural network indeed only has two directions to represent the

433
00:27:15,798 --> 00:27:19,134
two things. Um, most important thing it cares about. Okay? And again,

434
00:27:19,254 --> 00:27:22,270
let's just imagine right now that they're all equally important. So you just like,

435
00:27:22,302 --> 00:27:25,366
randomly chooses two features. Again, maybe it only cares about,

436
00:27:25,470 --> 00:27:29,154
um, having a distinction between redness and.

437
00:27:29,734 --> 00:27:33,846
And it's unable to have a distinction between squareness

438
00:27:33,910 --> 00:27:37,786
and triangle nest, for example. Right? It basically just chooses like, one pair.

439
00:27:37,890 --> 00:27:41,770
That is, you know, is the best it can do

440
00:27:41,802 --> 00:27:46,298
to like, get a good classification loss on

441
00:27:46,346 --> 00:27:49,586
the problem set. Okay? But it turns out that as

442
00:27:49,610 --> 00:27:55,810
you increase sparsity, right, as you maybe make redness

443
00:27:55,882 --> 00:27:59,250
and circleness not as common. For example,

444
00:27:59,362 --> 00:28:03,118
let's say you had a case whereby sometimes

445
00:28:03,266 --> 00:28:06,926
the image has no shape in the middle. That is, sometimes it's

446
00:28:06,950 --> 00:28:10,606
just color that matters, and sometimes you have a completely colorless input,

447
00:28:10,710 --> 00:28:13,558
but it's just the shape in the middle. Right? And that's what sparsity means,

448
00:28:13,606 --> 00:28:17,366
right? Where you have like, two different. In this

449
00:28:17,390 --> 00:28:21,414
case, you have like more than two different properties

450
00:28:21,494 --> 00:28:26,074
that don't always coincide together, right? It turns out that

451
00:28:26,494 --> 00:28:30,564
in the 80% sparsity example, as you see here, it actually

452
00:28:30,644 --> 00:28:33,852
chooses to represent more features than it should be

453
00:28:33,868 --> 00:28:37,244
able to, right? Again, this is consistent with what we see in the experience.

454
00:28:37,404 --> 00:28:41,172
And on the right here, you see 90% sparsity case. It basically

455
00:28:41,228 --> 00:28:44,704
has that direction showing that, okay, that's what spark, that's what indifference means.

456
00:28:45,404 --> 00:28:49,516
Because the whole point of having an orthogonal basis is that if

457
00:28:49,540 --> 00:28:53,236
you try to extract the component feature, that, again, you have some vector,

458
00:28:53,340 --> 00:28:56,804
and then you have only, again, two potential orthogonal,

459
00:28:56,924 --> 00:29:00,228
um, features. If you do a dot product against each

460
00:29:00,276 --> 00:29:03,784
of these, each value you get is, um,

461
00:29:04,444 --> 00:29:08,652
basically saying how much this giving vector

462
00:29:08,748 --> 00:29:12,292
is composed of

463
00:29:12,348 --> 00:29:16,428
each of those directions. So basically, if you have, again, some direction

464
00:29:16,476 --> 00:29:20,852
for some representation of a red

465
00:29:20,908 --> 00:29:24,956
square, basically if you take a dot product of that against the

466
00:29:24,980 --> 00:29:28,308
redness direction, it gives you, oh, this is either really red or

467
00:29:28,316 --> 00:29:33,780
not that much red. And take a dot product of that against the

468
00:29:33,812 --> 00:29:36,956
squared direction. It says, oh, how much of it does

469
00:29:36,980 --> 00:29:39,708
this look like? A square? Right. So basically, so that's why these needs to be

470
00:29:39,716 --> 00:29:43,380
orthogonal. Like they shouldn't interfere with each other. Like the quality of redness

471
00:29:43,412 --> 00:29:47,264
should only interfere with the quality of squareness, if that makes sense.

472
00:29:47,604 --> 00:29:51,068
So that's why orthogonality is important. However,

473
00:29:51,156 --> 00:29:54,228
again, if sometimes you basically just

474
00:29:54,276 --> 00:29:58,764
get a colorless square or, or you get a

475
00:29:58,804 --> 00:30:02,012
shapeless color, basically you just have any potential color. There's a

476
00:30:02,028 --> 00:30:06,716
shape at either sparsity where you don't always have cases where these two show

477
00:30:06,740 --> 00:30:10,572
up together. And again, for your problem set,

478
00:30:10,628 --> 00:30:13,772
you only need to care about these in isolation. That is, you only need to

479
00:30:13,788 --> 00:30:17,940
be really good at detecting color sometimes or detecting shape sometimes.

480
00:30:18,092 --> 00:30:21,804
It turns out that you actually be fine if

481
00:30:21,844 --> 00:30:24,940
you chose directions for squareness and redness

482
00:30:25,012 --> 00:30:28,556
that interfered with that is what the bottom example

483
00:30:28,580 --> 00:30:31,940
is trying to show. Okay? So on the bottom left square,

484
00:30:32,052 --> 00:30:36,140
you, our, let's say our orange vector is the thing that we actually

485
00:30:36,212 --> 00:30:39,500
want to observe. Again, it's our red square, right?

486
00:30:39,612 --> 00:30:43,108
It's a thing, it's the input is the true

487
00:30:43,156 --> 00:30:47,084
input vector, right? And you

488
00:30:47,124 --> 00:30:51,144
see that, again, we have five different directions, five different features.

489
00:30:52,304 --> 00:30:56,160
There's this, if you take a dot product of this value against all the different,

490
00:30:56,192 --> 00:30:59,840
five different vectors to see, like, how much it,

491
00:30:59,952 --> 00:31:03,600
it has a value with all of them. So you see it is along one

492
00:31:03,632 --> 00:31:07,112
direction, for example, right? So that direction would have is given value, right? Let's say

493
00:31:07,128 --> 00:31:09,840
that direction means how red it is. However,

494
00:31:09,912 --> 00:31:13,472
because there's interference. It has tiny little vectors that

495
00:31:13,488 --> 00:31:16,976
are going in along the other features that it actually

496
00:31:17,040 --> 00:31:20,462
doesn't have. Again, work. Let's imagine that in this case, we have

497
00:31:20,478 --> 00:31:24,086
a simple case of where, oh, this input is simply just, again,

498
00:31:24,150 --> 00:31:27,782
a very red input, right? It's just like a red blob or like a red

499
00:31:27,958 --> 00:31:31,454
square, right? Sorry, not a red square, like, I mean, just like a red image.

500
00:31:31,534 --> 00:31:35,358
There's nothing else in it. So indeed it is aligns perfectly with one of

501
00:31:35,366 --> 00:31:38,502
the features. But again, but because in this representation, again,

502
00:31:38,558 --> 00:31:41,902
you have only two dimensions, two pure dimensions, but then

503
00:31:41,918 --> 00:31:45,138
you're trying to squish five different things inside. Um,

504
00:31:45,226 --> 00:31:48,626
you would actually have a little bit of, it would pick up a

505
00:31:48,650 --> 00:31:52,338
little bit of a component along other features that

506
00:31:52,386 --> 00:31:55,374
it actually doesn't have. So that's what interference is.

507
00:31:56,034 --> 00:31:59,714
Um, and however, why this works is that neural

508
00:31:59,754 --> 00:32:03,754
networks have non linearities. Um, basically, like the activation

509
00:32:03,794 --> 00:32:08,362
functions, um, are non linearies that are able to basically

510
00:32:08,538 --> 00:32:12,224
turn off these, um, tiny,

511
00:32:12,524 --> 00:32:15,780
tiny bits of noise, right. If they didn't

512
00:32:15,812 --> 00:32:19,276
have that, then the bits of noise become quite annoying and

513
00:32:19,300 --> 00:32:22,612
would actually, like, count more towards your errors. But because again, in a

514
00:32:22,628 --> 00:32:27,708
case whereby there's actually is very little annoyance

515
00:32:27,756 --> 00:32:31,140
coming from the other values in the dot product, these are able

516
00:32:31,172 --> 00:32:35,012
to be tuned off effectively. Right, so now let's imagine

517
00:32:35,108 --> 00:32:38,326
now on the bottom right. In this case, let's imagine

518
00:32:38,350 --> 00:32:42,154
that the true input, again, like this thing we care about,

519
00:32:43,534 --> 00:32:47,062
the two blue vectors. Again, that is, you have something

520
00:32:47,118 --> 00:32:50,398
that is a really big square and

521
00:32:50,446 --> 00:32:54,078
also a really yellow background.

522
00:32:54,126 --> 00:32:57,514
Okay? So again, you have squareness and you have yellowness.

523
00:32:58,534 --> 00:33:02,038
If you can observe this vector

524
00:33:02,086 --> 00:33:05,724
addition of these two, again, remember, were operating in like a linear combination

525
00:33:05,804 --> 00:33:09,772
regime. This is exactly the

526
00:33:09,788 --> 00:33:13,212
same thing as we get on the left. This is why interference

527
00:33:13,268 --> 00:33:16,748
is important, because interference requires that, like, for it to work. There should be only

528
00:33:16,876 --> 00:33:20,076
very few number of things in this case, like, just say like, for interference to

529
00:33:20,100 --> 00:33:23,476
have no impact here, it should only be one feature

530
00:33:23,540 --> 00:33:27,052
that is truly trying to be detected at a time. But in a case whereby

531
00:33:27,108 --> 00:33:29,648
the two blue directions are trying to be,

532
00:33:29,756 --> 00:33:33,312
um, it would look to our system,

533
00:33:33,408 --> 00:33:36,664
our neural network, as if it was actually this case

534
00:33:36,704 --> 00:33:40,280
on the left. And in. And as we've seen, it is going to end

535
00:33:40,312 --> 00:33:43,640
up chipping away those two different values to nothing because

536
00:33:43,672 --> 00:33:47,256
of the non linearities. I'm going to think that, oh, actually, instead of

537
00:33:47,280 --> 00:33:50,444
seeing a square, a yellow square, it's just going to see

538
00:33:51,024 --> 00:33:54,240
a circle. Maybe that's what that third direction represents, which is

539
00:33:54,272 --> 00:33:57,702
complete noise, isn't it? Which is completely wrong, so to

540
00:33:57,718 --> 00:34:01,446
speak. Right. This is basically going to end up ignoring the components of this vector

541
00:34:01,550 --> 00:34:04,654
along those two as just being noise, which is really

542
00:34:04,694 --> 00:34:08,006
bad, which is really why, in the case where there's no sparsity, where, like,

543
00:34:08,030 --> 00:34:11,990
again, all the features are likely to be active, the neural network doesn't even

544
00:34:12,022 --> 00:34:15,126
bother trying to do anything funny. It doesn't try any funny

545
00:34:15,150 --> 00:34:18,434
business asset in the top left square. It simply just

546
00:34:19,734 --> 00:34:22,758
represents, you know, again, an arbitrary two features.

547
00:34:22,806 --> 00:34:26,484
Or in a case whereby it's able to have a sense of relative importance

548
00:34:27,504 --> 00:34:31,284
of features, maybe, like, one feature is way more important in

549
00:34:31,824 --> 00:34:35,336
determining. To give you an example, let's say

550
00:34:35,360 --> 00:34:38,576
one feature of language is, well, what language is it

551
00:34:38,600 --> 00:34:41,728
in? That is like, is it English or Spanish? Is it English or Chinese?

552
00:34:41,856 --> 00:34:46,000
Another feature is the sentence referring

553
00:34:46,032 --> 00:34:48,084
to in the past tense or present tense. Right.

554
00:34:49,133 --> 00:34:52,557
This past or present tense feature, you know, helps you avoid

555
00:34:52,605 --> 00:34:56,205
grammatical mistakes, but it's fair enough to assume that the

556
00:34:56,229 --> 00:34:59,749
feature of at least knowing what language the question or the

557
00:34:59,781 --> 00:35:03,085
query is in is probably way more

558
00:35:03,109 --> 00:35:07,101
important in terms of, like, having you avoid errors

559
00:35:07,197 --> 00:35:10,893
than detecting if it's past tense or present tense.

560
00:35:10,933 --> 00:35:14,593
Right? So again, that's just like one abstract idea of

561
00:35:15,694 --> 00:35:19,054
the model. So again, if the model only had like two different features, like one,

562
00:35:19,134 --> 00:35:22,238
let's like just one, like, or like on the margin.

563
00:35:22,326 --> 00:35:25,478
If the model has to represent one more thing and it has to choose between

564
00:35:25,606 --> 00:35:29,646
the language detection and the past or

565
00:35:29,670 --> 00:35:33,630
present tense, it will most likely prioritize choosing to represent, using that one extra

566
00:35:33,662 --> 00:35:36,726
feature to represent language and language type, that is this,

567
00:35:36,750 --> 00:35:40,814
English or French or Chinese, as that probably has way more predictive power.

568
00:35:40,894 --> 00:35:44,646
Like, we'll have it have way less error than if it was instead

569
00:35:44,670 --> 00:35:47,462
of trying to predict the correct tense,

570
00:35:47,518 --> 00:35:51,262
but in the wrong language. Again, this bit

571
00:35:51,278 --> 00:35:54,814
of a toy example. Okay, so how do we solve this?

572
00:35:54,854 --> 00:35:58,686
Right. Again, giving this, we suspect this is what models are doing. Or again,

573
00:35:58,710 --> 00:36:01,958
we suspect this is why they're able to do it. This is why they're

574
00:36:01,966 --> 00:36:05,486
able to get away with it. Again, they're trying to exploit sparsity by compressing

575
00:36:05,510 --> 00:36:08,862
stuff. So the paper I shared basically tries to

576
00:36:08,878 --> 00:36:12,720
do this, um, by tackling a smaller

577
00:36:12,752 --> 00:36:16,648
model, right? So the tackle a one layer transformer, and they pick out one

578
00:36:16,696 --> 00:36:20,632
component of the architecture. Again, as I explained in

579
00:36:20,648 --> 00:36:25,016
this, in our typical large transformer, yeah. Every single component

580
00:36:25,120 --> 00:36:28,832
is doing some version of this, right? Since this information is

581
00:36:28,848 --> 00:36:32,760
flowing through our entire network, each discrete component is going to need to

582
00:36:32,792 --> 00:36:36,608
have to do some version of this, right? So they focus on the MLP

583
00:36:36,656 --> 00:36:40,848
layer, which is what comes the attention heads. And in

584
00:36:40,896 --> 00:36:44,328
the model, they use the dimensions of that,

585
00:36:44,376 --> 00:36:47,608
basically, like how many vectors or how many neurons it has.

586
00:36:47,776 --> 00:36:51,644
So how many dimensions of each vector has or how many neuron,

587
00:36:52,144 --> 00:36:55,616
how many neurons that layer has is 512. And what they do

588
00:36:55,640 --> 00:36:58,896
is, as seen here on the right, they use something called a

589
00:36:58,920 --> 00:37:02,016
sparse overcomplete order encoder. Obviously,

590
00:37:02,040 --> 00:37:05,216
I'll describe what that means, starting from the right. Okay, so what does that mean?

591
00:37:05,280 --> 00:37:09,136
And autoencoder and a BSN. Autoencoder is basically a neural

592
00:37:09,160 --> 00:37:12,536
network whose primary purpose is reconstruction. So basically

593
00:37:12,680 --> 00:37:16,128
you have some input, you have something in

594
00:37:16,136 --> 00:37:19,592
the middle, which is like, again, your network. And the job of

595
00:37:19,608 --> 00:37:23,524
that network is to try to replicate, to reproduce the output.

596
00:37:24,024 --> 00:37:27,464
That seems kind of silly. Like why just bother with this density transform?

597
00:37:27,584 --> 00:37:31,212
Because, well, in some cases, you might want to do something like,

598
00:37:31,328 --> 00:37:34,708
okay, compressed dimensions, right? So let's say you have this very like,

599
00:37:34,756 --> 00:37:39,172
large input you want to find interesting.

600
00:37:39,228 --> 00:37:42,612
You want to find the most important critical features by compressing it

601
00:37:42,628 --> 00:37:45,932
in the middle and seeing how. Well, okay, like,

602
00:37:45,988 --> 00:37:49,644
again, let's say something needs five dimensions to this

603
00:37:49,684 --> 00:37:53,276
input is five dimensions. What are the two most important

604
00:37:53,340 --> 00:37:56,620
dimensions of this or two most important, like representations of these five

605
00:37:56,652 --> 00:38:00,532
dimensions I can take that would have me still

606
00:38:00,588 --> 00:38:05,380
do well on reconstructing with

607
00:38:05,412 --> 00:38:08,804
this. That basically has this property

608
00:38:08,844 --> 00:38:12,196
of feature discovery by compression.

609
00:38:12,380 --> 00:38:13,944
That's what auto encoders do.

610
00:38:14,564 --> 00:38:18,436
Overcomplete. Again, starting from the right to left, describe overcomplete,

611
00:38:18,460 --> 00:38:21,500
basically does the slightly opposite version of that,

612
00:38:21,572 --> 00:38:24,526
which is, instead of compressing, you're basically trying to expand.

613
00:38:24,660 --> 00:38:27,970
You basically try to give the

614
00:38:28,002 --> 00:38:31,618
order encoder in the middle again between the input and your

615
00:38:31,666 --> 00:38:35,234
construction of the input is much larger than as you're saying. If this

616
00:38:35,274 --> 00:38:39,034
neural network representation had way more room

617
00:38:39,154 --> 00:38:42,330
to represent stuff, what would it look

618
00:38:42,362 --> 00:38:46,018
like? Right? And remember, the whole

619
00:38:46,066 --> 00:38:49,202
point of our store

620
00:38:49,218 --> 00:38:52,830
position is that we're assuming that the model we see is actually trying

621
00:38:52,862 --> 00:38:55,942
to simulate a much larger model. Again, remember, that's the

622
00:38:55,958 --> 00:38:59,830
whole point of superposition, right? So by using this

623
00:38:59,982 --> 00:39:03,726
overcompleted encoder, we're trying to say that, cool, whatever representation

624
00:39:03,830 --> 00:39:07,594
this MLP node has for some input,

625
00:39:08,214 --> 00:39:12,154
what if we gave it way more neurons to work with?

626
00:39:12,494 --> 00:39:16,190
What would you do with it? That's the overcomplete section.

627
00:39:16,302 --> 00:39:19,772
Then the sparse component of that description is

628
00:39:19,908 --> 00:39:23,452
saying, like, sure, what if we just go from like, you know,

629
00:39:23,548 --> 00:39:26,724
a five dimensional,

630
00:39:26,844 --> 00:39:30,052
inscrutable, compressed complex thing to

631
00:39:30,108 --> 00:39:33,244
a hundred dimension inscrutable complex thing, right?

632
00:39:33,324 --> 00:39:36,332
We're not much better than we started, right? And again, neural networks, just like,

633
00:39:36,428 --> 00:39:40,036
don't really have any incentives to just make things explainable to us. So the sparse

634
00:39:40,060 --> 00:39:43,476
component says, okay, in addition to giving you the

635
00:39:43,540 --> 00:39:46,810
network, more room to work with, to like, expand, to like, see what you learned,

636
00:39:46,932 --> 00:39:50,622
we want to force you to narrow your

637
00:39:50,758 --> 00:39:55,278
learnings, your features, to being

638
00:39:55,366 --> 00:39:59,102
active in one node at a time. Right. I think I

639
00:39:59,118 --> 00:40:02,638
explained before how just because I

640
00:40:02,646 --> 00:40:06,314
think it was, in this example, just going to jump quickly. So just because

641
00:40:06,654 --> 00:40:10,190
I say that, oh, like, linearity says you must have as

642
00:40:10,222 --> 00:40:13,318
many dimensions as you want features, it doesn't necessarily mean that it

643
00:40:13,326 --> 00:40:16,644
will always be one. Hot, right? You might have a case whereby there's several,

644
00:40:16,724 --> 00:40:19,664
you know, there are infinite, many orthogonal,

645
00:40:21,004 --> 00:40:24,708
four orthogonal vectors, like four vectors that form an orthogonal basis

646
00:40:24,796 --> 00:40:28,108
that aren't one. Hot. Right. It's kind of like smeared between all the different values.

647
00:40:28,196 --> 00:40:31,204
But again, from our, for our convenience to like say that,

648
00:40:31,324 --> 00:40:34,424
that neuron is like firing a lot when it sees redness,

649
00:40:35,684 --> 00:40:39,372
we want to impose an extra basic

650
00:40:39,428 --> 00:40:42,690
constraint on the auto encoder to say that,

651
00:40:42,722 --> 00:40:46,522
cool. Don't just try to find representations

652
00:40:46,578 --> 00:40:50,178
with more nodes to work with when you do this,

653
00:40:50,306 --> 00:40:53,826
narrow down your learnings or try to isolate your learnings

654
00:40:53,930 --> 00:40:56,818
for one feature to like, one node at a time, right?

655
00:40:56,906 --> 00:41:00,214
That's basically just for our interpretability benefit.

656
00:41:00,754 --> 00:41:03,906
And yet that is what a sparse overcomplete order code is.

657
00:41:03,930 --> 00:41:07,450
Or usually they usually just ignore the overcomplete

658
00:41:07,482 --> 00:41:10,686
part part and just call it a sparse or encoder and basically

659
00:41:10,710 --> 00:41:15,510
just says, cool. We want to give neural

660
00:41:15,542 --> 00:41:19,638
networks opportunity to want to extract what they've learned by

661
00:41:19,766 --> 00:41:23,194
trying to reconstruct representations using more dimensions.

662
00:41:24,334 --> 00:41:27,678
And we want this new like extraction

663
00:41:27,726 --> 00:41:31,502
to be sparse in such a way that only one node

664
00:41:31,518 --> 00:41:36,844
is activated at a time for a given feature that is applied and

665
00:41:37,264 --> 00:41:40,680
effectively that looks like this. So they ran

666
00:41:40,712 --> 00:41:44,448
this, this training, this training

667
00:41:44,496 --> 00:41:47,912
process for the sparse auto encoder on their one layer network for the

668
00:41:47,928 --> 00:41:51,912
MLP layer. And again, if you see here, they describe on

669
00:41:51,928 --> 00:41:55,800
the left there you see this like the act 512,

670
00:41:55,912 --> 00:41:59,280
which is like the activation of MLP layer, typically should have 500,

671
00:41:59,312 --> 00:42:03,300
1212 dimensions. Again, that would just mean instead of 1

672
00:42:03,332 --> 00:42:06,748
second, instead of like four different blocks here,

673
00:42:06,836 --> 00:42:09,744
that'll be 512, right? That's like how big the vector is.

674
00:42:10,404 --> 00:42:13,892
So they went from 512 and expanded all the RAN

675
00:42:13,908 --> 00:42:19,584
different versions, but the largest ones went up to 131k.

676
00:42:20,524 --> 00:42:23,892
So basically that would mean if again, with the

677
00:42:23,948 --> 00:42:27,652
network of the quarter on the right, if the,

678
00:42:27,828 --> 00:42:31,344
on the input and output was 512 different like

679
00:42:31,764 --> 00:42:35,544
neurons. And in the middle you had this giant

680
00:42:36,004 --> 00:42:39,316
130K node network,

681
00:42:39,460 --> 00:42:42,820
or a node like layer, basically that was trying

682
00:42:42,852 --> 00:42:46,144
to reconstruct the input into the output.

683
00:42:47,124 --> 00:42:50,492
And they learned a bunch of stuff. They have a very

684
00:42:50,548 --> 00:42:54,104
nice interactive, um, application that

685
00:42:54,144 --> 00:42:58,400
I encourage you all to check out that basically shows the

686
00:42:58,552 --> 00:43:00,824
model learning really interesting things. So one of the,

687
00:43:00,904 --> 00:43:04,360
um, the neurons I discovered, again, neuron simply means like,

688
00:43:04,472 --> 00:43:07,664
because of this like, um, constraint of sparsity, the,

689
00:43:07,744 --> 00:43:11,744
the model learns like isolate some abstract

690
00:43:11,824 --> 00:43:15,544
feature. So literally one of these 130K nodes,

691
00:43:15,624 --> 00:43:19,136
basically, like once this feature is present an input, it just like fires

692
00:43:19,160 --> 00:43:22,872
and screams a lot. You see like this input is really here. And these features

693
00:43:22,888 --> 00:43:26,552
are like so like wide and varied. When,

694
00:43:26,608 --> 00:43:29,404
for example, detects, the,

695
00:43:30,064 --> 00:43:33,400
detects arabic characters in input,

696
00:43:33,472 --> 00:43:37,640
another of them, as you see here, detects if a sequence

697
00:43:37,672 --> 00:43:41,256
of text is probably like a DNA

698
00:43:41,280 --> 00:43:44,680
sequence which I think was pretty wild because, like, this could also be gibberish,

699
00:43:44,712 --> 00:43:48,256
but there are certain patterns and

700
00:43:48,280 --> 00:43:51,888
I guess the actual letters, for example, that is used for DNA encoding.

701
00:43:52,016 --> 00:43:54,536
That seems like such an arbitrary thing that a model will learn. But it did

702
00:43:54,560 --> 00:43:58,564
learn this. And you can check out other esoteric ones that you learned

703
00:43:59,984 --> 00:44:03,888
in this reconstruction. And again, feature was present in

704
00:44:03,936 --> 00:44:07,856
the 512 mlp.

705
00:44:07,960 --> 00:44:11,212
But because it was coked up and

706
00:44:11,308 --> 00:44:15,264
all cooped up together with the superposition,

707
00:44:16,164 --> 00:44:19,788
in the superposition phase, it was hard to basically discern. The whole point of

708
00:44:19,796 --> 00:44:22,412
the recorder is basically to extract these features out there.

709
00:44:22,548 --> 00:44:27,052
So now they become one isolated thing, which kind

710
00:44:27,068 --> 00:44:30,644
of brings us full circle to the definition of what a feature is. So again,

711
00:44:30,684 --> 00:44:34,516
I've been throwing around the idea of feature as just a distinct

712
00:44:34,580 --> 00:44:39,480
thing. Model find to be interesting, right. As basically

713
00:44:39,592 --> 00:44:43,680
one perhaps more narrow

714
00:44:43,752 --> 00:44:46,920
definition of it. I guess I don't want to say formal, but just like,

715
00:44:46,952 --> 00:44:50,288
one more particular definition of it, based on this paradigm that

716
00:44:50,296 --> 00:44:54,024
we've described, is like a feature is basically a property that

717
00:44:54,064 --> 00:44:56,444
a model would encode.

718
00:44:57,744 --> 00:45:00,678
Would dedicate an entire neuron to. Would encode using an.

719
00:45:00,776 --> 00:45:03,666
Using one neuron if it had enough neurons, right?

720
00:45:03,730 --> 00:45:08,034
So basically, if there's such a thing that if a model was sufficiently

721
00:45:08,074 --> 00:45:11,498
large, would it get one neuron to it? That thing is a feature,

722
00:45:11,586 --> 00:45:15,034
right? But if there's a thing that no matter how many neurons

723
00:45:15,114 --> 00:45:18,650
it had, this thing wouldn't have a neuron,

724
00:45:18,682 --> 00:45:21,778
it perhaps would be like a part of some other neuron, then that thing's not

725
00:45:21,786 --> 00:45:25,898
a feature, right. It seems kind of circular, but it

726
00:45:25,906 --> 00:45:28,802
turns about. Yeah. The precise definition of, like, futures can be kind of gnarly.

727
00:45:28,938 --> 00:45:32,442
Um, but, like, for all practical purposes, you know, think of, again, features in the

728
00:45:32,498 --> 00:45:35,722
colloquial sense of just like, you know, a thing that the model finds

729
00:45:35,738 --> 00:45:39,134
to be interesting, like squareness or bonus or whatever. Um,

730
00:45:39,474 --> 00:45:42,418
and but the interesting thing, I guess, is that, again, like,

731
00:45:42,546 --> 00:45:45,866
part of the things. Part of the ways, like, a more powerful model is more

732
00:45:45,890 --> 00:45:49,250
powerful is because it can indeed encode for more

733
00:45:49,282 --> 00:45:53,354
stuff than a small one can. And again, as a proposition suggest

734
00:45:53,474 --> 00:45:57,066
the smaller ones actually encode a lot more than they might than

735
00:45:57,090 --> 00:46:01,922
their size alone might suggest, right? Because, again, the whole point of this is if

736
00:46:02,058 --> 00:46:05,434
indeed there was no superposition, or like, if indeed there was nothing

737
00:46:05,474 --> 00:46:09,210
weird happening, then this MLP layer would actually only have 512,

738
00:46:09,322 --> 00:46:13,254
but they were able to extract way over 100,000

739
00:46:13,794 --> 00:46:17,346
reasonable features. So something for sure where

740
00:46:17,370 --> 00:46:21,066
it is happening. And, like. And these features, like, were consistent with

741
00:46:21,130 --> 00:46:24,214
experimental validation for.

742
00:46:24,254 --> 00:46:27,710
Again like they had different evaluation methods that you can check out in the paper

743
00:46:27,822 --> 00:46:30,510
to show like how much confidence they have for it. But basically the features like

744
00:46:30,542 --> 00:46:34,254
very confidence or incoherence or at least in like explainability

745
00:46:34,334 --> 00:46:37,622
to like it's a real human being human. But I feel like the

746
00:46:37,638 --> 00:46:41,062
number of explainable features, high quality feature definitely exceeds 512.

747
00:46:41,158 --> 00:46:44,494
So indeed this compression is happening for sure and

748
00:46:44,534 --> 00:46:47,942
this is like the proof for it. And yeah the future of this

749
00:46:47,958 --> 00:46:51,006
work could basically look like scaling up this auto encoders to work

750
00:46:51,030 --> 00:46:54,594
on much larger models and uncover more useful features going forward.

751
00:46:55,334 --> 00:46:58,902
Awesome and that is the talk. Thanks for

752
00:46:58,918 --> 00:47:02,582
joining here and I encourage you to read more of the

753
00:47:02,598 --> 00:47:07,334
papers out there. I think the anthropic blog

754
00:47:07,374 --> 00:47:10,742
posts and paper, informal papers and formal papers are a great place to

755
00:47:10,758 --> 00:47:14,142
start as that basically represents

756
00:47:14,158 --> 00:47:16,354
where the frontiers right now. Awesome,

757
00:47:17,294 --> 00:47:19,014
thank you for the time and see you later.

