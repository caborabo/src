1
00:00:20,720 --> 00:00:24,302
Hi, everyone. My name is Ben. I'm the co founder of Symante AI. And in

2
00:00:24,318 --> 00:00:27,834
this talk, we're going to dive deep into semantic search and

3
00:00:27,954 --> 00:00:32,026
how to set that up, what the technical difficulties are, and most importantly,

4
00:00:32,210 --> 00:00:36,026
where you would use that in a business sense. So without

5
00:00:36,090 --> 00:00:39,570
further ado, let's just jump straight into it, and we

6
00:00:39,602 --> 00:00:43,034
have to give a little bit of technical detail first. And that is

7
00:00:43,154 --> 00:00:47,174
what the building block of semantic search is, and that is vector databases.

8
00:00:47,514 --> 00:00:50,858
This is actually a technology that's been around for some time. In fact, it dates

9
00:00:50,906 --> 00:00:54,402
back to sometime around 2005, but with the

10
00:00:54,458 --> 00:00:57,874
emergence of large language models, particularly, obviously, chat GPT,

11
00:00:57,954 --> 00:01:01,534
but many others as well, we've seen a revamp of this technology,

12
00:01:01,874 --> 00:01:05,898
and most importantly, we've seen a huge technological breakthrough. So now

13
00:01:05,946 --> 00:01:09,154
semanticsearch is technically so much easier to

14
00:01:09,194 --> 00:01:12,434
implement than it was a couple of years ago without large language

15
00:01:12,474 --> 00:01:16,530
models. Actually, Gartner predicts that by 2026

16
00:01:16,562 --> 00:01:20,190
or so, roughly 30% of all businesses will have implemented

17
00:01:20,222 --> 00:01:24,070
this vector database, which is the thing that powers

18
00:01:24,102 --> 00:01:27,822
semantic search. So what are vector databases? Let's just take a

19
00:01:27,838 --> 00:01:30,670
look at the example on the right, on the bottom right, where you have a

20
00:01:30,702 --> 00:01:34,198
matrix which consists of four words, man, woman,

21
00:01:34,286 --> 00:01:37,790
king and queen. Now, obviously, those are words that are somehow connected.

22
00:01:37,862 --> 00:01:41,862
Man and woman are genders. King and queen are both types of royalty,

23
00:01:42,038 --> 00:01:46,234
but you know that they're polar opposite between each other in that subcategory.

24
00:01:46,624 --> 00:01:50,272
And so if you were to assign them coordinates, so king would be

25
00:01:50,288 --> 00:01:53,144
one and one, queen would be two and two, man is one, one and a

26
00:01:53,644 --> 00:01:56,936
half. Woman is two, two and a half. You can see on that matrix that

27
00:01:57,080 --> 00:02:01,056
man and king are much closer to each other than, for example, woman and king.

28
00:02:01,160 --> 00:02:04,952
And conversely, woman and queen are very close, but queen

29
00:02:05,008 --> 00:02:08,568
and man are a bit further apart, and that is because they are

30
00:02:08,616 --> 00:02:11,824
semantically linked. They're closer to each other. So what

31
00:02:11,864 --> 00:02:16,284
happened in this example is that these words were stored as vector representations,

32
00:02:16,644 --> 00:02:20,300
then they had to be somehow compared, and that comparison

33
00:02:20,452 --> 00:02:24,268
was done using one of any number of methods. There are multiple,

34
00:02:24,436 --> 00:02:28,748
just to name a few, cosine, euclidean or Jakkar distance.

35
00:02:28,876 --> 00:02:32,444
So you basically measure the distance between those coordinates, in a sense,

36
00:02:32,484 --> 00:02:35,996
or the vectors, and then you have to use an algorithm which

37
00:02:36,020 --> 00:02:39,388
is going to somehow find the next nearest

38
00:02:39,436 --> 00:02:43,706
neighbor when you have a query. So that's kind of the, the way that's

39
00:02:43,900 --> 00:02:47,390
vector databases. Power semantic search as the backbone,

40
00:02:47,542 --> 00:02:51,030
there are multiple solutions out there, and new ones are sprouting just about every day

41
00:02:51,062 --> 00:02:55,254
now. But what we found is that they usually only give you a

42
00:02:55,294 --> 00:02:58,838
part of the process. The most typical part that they give you

43
00:02:58,886 --> 00:03:01,686
is this vector database, which basically says,

44
00:03:01,750 --> 00:03:05,030
okay, you have a bit of data here, it can be documents,

45
00:03:05,102 --> 00:03:08,566
structured data, audio, images. And what

46
00:03:08,590 --> 00:03:11,778
we're going to do for you is create vectors out of that data.

47
00:03:11,946 --> 00:03:15,314
So you put that data in via rest API or whatever

48
00:03:15,354 --> 00:03:19,106
they have at their disposal and we'll spit out some vectors for you.

49
00:03:19,290 --> 00:03:22,690
And you might be sitting there saying, okay, well, that does not seem like the

50
00:03:22,722 --> 00:03:26,362
full search solution, so we need a little bit more. And that

51
00:03:26,418 --> 00:03:29,818
more is now offered by large language models. And the large

52
00:03:29,866 --> 00:03:32,970
language model basically allows you to do two things.

53
00:03:33,122 --> 00:03:36,582
First and foremost, represent a user's query

54
00:03:36,778 --> 00:03:39,754
in a vector state as well,

55
00:03:40,094 --> 00:03:43,514
and also generate an answer once you retrieve some information.

56
00:03:44,014 --> 00:03:47,318
But again, you might be saying, okay, so I have on one

57
00:03:47,366 --> 00:03:51,238
side the vectors from my data, on the other side I

58
00:03:51,246 --> 00:03:54,822
have the vectors from what the user types in. So I still need

59
00:03:54,838 --> 00:03:58,398
to do a little bit of work here. And so what we decided to do

60
00:03:58,446 --> 00:04:01,598
is package up that little bit of work, which is actually

61
00:04:01,646 --> 00:04:04,598
a lot of work as you're going to see in just a second, and just

62
00:04:04,646 --> 00:04:07,718
offer it as a solution. And that's what Symante basically is.

63
00:04:07,846 --> 00:04:11,374
And the key difference here is, well, first and foremost, there's similarity

64
00:04:11,414 --> 00:04:14,646
search already built in. Everything is configured for you

65
00:04:14,670 --> 00:04:17,934
and packaged up, and you have everything

66
00:04:18,014 --> 00:04:21,830
accessible as a set of rest APIs. So to use our technology, you just

67
00:04:21,902 --> 00:04:24,514
configure API endpoints and that's it.

68
00:04:24,894 --> 00:04:28,622
And the next slide is going to dive very deep into all kinds

69
00:04:28,638 --> 00:04:32,494
of problems we ran into. And let me tell you, there are

70
00:04:32,534 --> 00:04:35,074
a lot of pitfalls when you set this stuff up.

71
00:04:35,654 --> 00:04:39,094
The first pitfall, or the kind of first question you have to ask yourself

72
00:04:39,134 --> 00:04:42,966
is which LLM do you want to use? Somebody would typically say, all right,

73
00:04:42,990 --> 00:04:46,782
let's just use GPT four, it doesn't matter. But there are a lot of considerations

74
00:04:46,838 --> 00:04:50,366
to make. GPT is not necessarily the cheapest model,

75
00:04:50,510 --> 00:04:53,814
and in fact it's not the best model for some use cases.

76
00:04:53,974 --> 00:04:57,354
Yes, there are some use cases where a much,

77
00:04:57,394 --> 00:05:00,610
much less powerful model will actually be more optimal for that

78
00:05:00,642 --> 00:05:04,522
use case. Just to name an example, GPT four will

79
00:05:04,578 --> 00:05:07,834
translate text to English. So it does work

80
00:05:07,874 --> 00:05:11,538
in a multilingual fashion. But because everything is kind of translated

81
00:05:11,586 --> 00:05:15,250
to English, the results may be different if you ask in

82
00:05:15,282 --> 00:05:18,466
different languages. And so if you want to have a

83
00:05:18,490 --> 00:05:22,314
truly multilingual solution, you're going to have to use a model that is language

84
00:05:22,354 --> 00:05:26,314
agnostic, meaning that it doesn't actually matter which language the data

85
00:05:26,354 --> 00:05:30,002
is in and the query is in, because everything is just represented as

86
00:05:30,138 --> 00:05:33,850
vectors and it's completely irrelevant which language

87
00:05:33,882 --> 00:05:37,314
it ultimately is. The second aspect is obviously

88
00:05:37,354 --> 00:05:40,522
choosing the right similarity algorithm. There are many of them,

89
00:05:40,698 --> 00:05:43,642
and you have to choose the one that fits your use case best. Some of

90
00:05:43,658 --> 00:05:47,162
them have pros, some of them have other pros and cons, so you

91
00:05:47,178 --> 00:05:50,670
know you're going to have to make a selection here. But the most important aspect

92
00:05:50,702 --> 00:05:54,630
of them all by far is how to handle large datasets.

93
00:05:54,782 --> 00:05:58,886
The first implementation that we did had 1.5 million

94
00:05:58,950 --> 00:06:03,034
records, so that one was really a test for us. That's a very large database

95
00:06:03,734 --> 00:06:07,030
to give you a benchmark. 1.5 million records using GPT

96
00:06:07,102 --> 00:06:11,238
four would have taken several years just to get vectors from,

97
00:06:11,406 --> 00:06:14,900
and it would have cost hundreds of thousands of dollars to set everything up.

98
00:06:15,022 --> 00:06:18,440
That's not something we wanted to invest in. So with our solution,

99
00:06:18,512 --> 00:06:21,672
we had to somehow optimize that whole vector

100
00:06:21,728 --> 00:06:25,584
generation and the whole process of creating

101
00:06:25,624 --> 00:06:29,312
vector embeddings and also the cost associated with it.

102
00:06:29,448 --> 00:06:32,888
And so that's something that we really tweaked and played around

103
00:06:32,936 --> 00:06:36,044
with very significantly. Secondly,

104
00:06:36,544 --> 00:06:39,752
you're going to want to curb the cost. So some things are very

105
00:06:39,808 --> 00:06:43,738
easily handled by much less powerful models than GPT four.

106
00:06:43,896 --> 00:06:47,046
On the generation aspect, you might use GPT four. On the

107
00:06:47,070 --> 00:06:50,262
search aspect, you might use another language model.

108
00:06:50,438 --> 00:06:53,926
It's all your choice, but you'll have to make that choice if you want to

109
00:06:53,950 --> 00:06:57,438
make something up and running. Now, in the real

110
00:06:57,486 --> 00:07:00,662
world, your database very rarely remains static over long periods

111
00:07:00,678 --> 00:07:04,214
of time, right? Entries change, new entries are

112
00:07:04,254 --> 00:07:07,542
added. So every single time something changes, you're going to

113
00:07:07,558 --> 00:07:10,110
have to re index your database and you're going to have to go through that

114
00:07:10,142 --> 00:07:13,568
process again. So we've taken through this re indexing

115
00:07:13,696 --> 00:07:17,352
and we've really made sure that that's something that is not

116
00:07:17,528 --> 00:07:21,200
anything you have to consider when you use our solution and then

117
00:07:21,232 --> 00:07:24,792
finally implementing it in practice. So you still have to integrate it

118
00:07:24,808 --> 00:07:28,120
into the customer's application landscape. And so

119
00:07:28,152 --> 00:07:31,936
this is why we just make sure that everything is accessible as an endpoint.

120
00:07:32,000 --> 00:07:35,928
Essentially it's just an API because we wanted to make sure it's very easily

121
00:07:36,016 --> 00:07:38,774
integrated into your customers landscape.

122
00:07:39,234 --> 00:07:42,694
Finally, you're going to sometimes run into very complex queries.

123
00:07:43,034 --> 00:07:46,074
A user might ask something like, I want to have a

124
00:07:46,114 --> 00:07:49,514
red house which is no bigger than 400 m²,

125
00:07:49,554 --> 00:07:53,374
but also not smaller than 200. It should have a garage.

126
00:07:53,954 --> 00:07:57,146
Those are very complicated queries. I guarantee you virtually

127
00:07:57,210 --> 00:08:00,490
no large language model other than GPT four will actually

128
00:08:00,562 --> 00:08:04,212
understand that query properly. But more importantly,

129
00:08:04,338 --> 00:08:08,384
some parts of that query can actually be resorted

130
00:08:08,424 --> 00:08:11,696
back to keyword search. Right? You have a bunch of parameters

131
00:08:11,720 --> 00:08:13,056
in there. A red house,

132
00:08:13,200 --> 00:08:16,764
200 garage, yes or no.

133
00:08:17,104 --> 00:08:20,792
Those things can actually be kind of done in a

134
00:08:20,808 --> 00:08:24,056
hybrid fashion where some parts are going to be searched

135
00:08:24,080 --> 00:08:27,288
using keywords and other parts are going to be searched using

136
00:08:27,336 --> 00:08:30,576
semantic search. Another example is when a user just

137
00:08:30,600 --> 00:08:33,880
types in red car. Probably overkill to

138
00:08:33,912 --> 00:08:37,016
use semantic search for that. So you're gonna have to make a decision on when

139
00:08:37,040 --> 00:08:39,924
to fall back to your regular keyword search.

140
00:08:40,264 --> 00:08:44,224
And finally, when you have an eshop or a product catalog,

141
00:08:44,304 --> 00:08:48,152
they're almost always gonna be an underlying SQL database there.

142
00:08:48,328 --> 00:08:51,384
And so you're gonna have to create some kind of

143
00:08:51,424 --> 00:08:55,184
mechanism to sort of translate AI search to

144
00:08:55,224 --> 00:08:58,736
SQL search. And this is something that we've really tweaked and

145
00:08:58,760 --> 00:09:01,294
played around with quite a bit. But you're gonna have to do this if you,

146
00:09:01,304 --> 00:09:04,402
you want to use semanticsearch on your eShop, for example.

147
00:09:04,578 --> 00:09:07,546
I could actually go on and on and on about all the things that we

148
00:09:07,570 --> 00:09:11,442
ran into, but these are just some of the more important technical

149
00:09:11,498 --> 00:09:15,354
intricacies that you have to deal with. Now, on top

150
00:09:15,394 --> 00:09:19,034
of all of that, we've actually done something a little bit extra. This is

151
00:09:19,074 --> 00:09:22,242
obviously not a must have, but we decided to also visualize

152
00:09:22,298 --> 00:09:26,210
data using this neat 3d map where you can see the

153
00:09:26,282 --> 00:09:30,216
semantic proximity of your words or, or queries, let's say.

154
00:09:30,360 --> 00:09:33,760
And you can browse through this, you can click through it

155
00:09:33,792 --> 00:09:37,456
and it's interactive. So that's just another feature that we decided to

156
00:09:37,520 --> 00:09:40,976
implement into semantic search. So we

157
00:09:41,000 --> 00:09:44,112
have the technology down. And so what's kind of the way that you

158
00:09:44,128 --> 00:09:47,884
would use it in real life? So the most obvious is obviously search

159
00:09:48,184 --> 00:09:52,040
in an eshop or within your knowledge base. You can just

160
00:09:52,152 --> 00:09:55,824
search using human language and you don't have to use keyword

161
00:09:55,864 --> 00:09:59,198
search. This one is pretty easy to understand and it's one

162
00:09:59,206 --> 00:10:02,774
of the most common ways that semanticsearch is used.

163
00:10:02,934 --> 00:10:06,862
But there are other ways. For example, you could use categorization.

164
00:10:07,038 --> 00:10:10,510
So an entire text can very easily be placed into one

165
00:10:10,542 --> 00:10:13,702
or other categories. Or you can use a generative

166
00:10:13,758 --> 00:10:16,870
model to suggest categories for that text,

167
00:10:17,022 --> 00:10:20,934
which is very useful in, for example, incident management or when you're working with customer

168
00:10:20,974 --> 00:10:24,690
service, that immediately the request is routed

169
00:10:24,722 --> 00:10:28,210
to the proper department because it deals with a

170
00:10:28,242 --> 00:10:31,810
certain type of request. Other things you can do is

171
00:10:31,922 --> 00:10:35,394
similar image search. So you can kind of paste an image

172
00:10:35,434 --> 00:10:39,374
into an eshop and it will spit out things that look similarly.

173
00:10:40,354 --> 00:10:43,530
Chatbots is another example where you have the

174
00:10:43,562 --> 00:10:46,882
generative part where the customer is actually talking to that chatbot,

175
00:10:46,978 --> 00:10:50,322
but then you have that retrieval aspect where you can actually. Okay, so what is

176
00:10:50,338 --> 00:10:53,632
the customer actually asking about? You can sort

177
00:10:53,648 --> 00:10:57,128
of look in your knowledge base. Is there something that's been answered

178
00:10:57,176 --> 00:11:00,408
before? Yes. Okay, let's generate that to the customer.

179
00:11:00,496 --> 00:11:03,888
That's a very powerful chatbot right there. And then other things you

180
00:11:03,896 --> 00:11:07,848
can do is recommendations. So for example, rather than using the typical recommendation

181
00:11:07,936 --> 00:11:11,896
engine, you can actually say, okay, if a customer is using kind

182
00:11:11,920 --> 00:11:15,776
of, let's say, nose drop as a search, you can

183
00:11:15,800 --> 00:11:19,134
also offer them tissues and you can offer them allergy relief.

184
00:11:19,264 --> 00:11:22,734
So kind of things that are similar in terms of meaning.

185
00:11:23,034 --> 00:11:26,626
And anomaly detection is an interesting one because we're dealing

186
00:11:26,650 --> 00:11:30,834
with patterns here. So banks are often searching

187
00:11:30,874 --> 00:11:34,986
for things that are kind of out of pattern. And interestingly, semanticsearch offers

188
00:11:35,010 --> 00:11:36,854
a way of doing this very scalably.

189
00:11:38,234 --> 00:11:41,986
Matching engines are another thing. So if you have a long, long text,

190
00:11:42,130 --> 00:11:45,362
one of the things that you can actually do is match it to

191
00:11:45,418 --> 00:11:48,812
other long texts that are similar in terms of meaningful. If you

192
00:11:48,828 --> 00:11:51,424
have a use case for that, you can use semanticsearch.

193
00:11:52,044 --> 00:11:55,796
And this is all nice, but let's see a

194
00:11:55,820 --> 00:11:59,004
real world application. So because I had to stick to PowerPoint,

195
00:11:59,044 --> 00:12:01,844
I had to create a GiF. But we're going to go through this gif many

196
00:12:01,884 --> 00:12:05,564
times. So what you have here is an actual demo of a real world

197
00:12:05,604 --> 00:12:09,956
application. It's a database that has roughly 100 bmws,

198
00:12:10,100 --> 00:12:13,452
and you can search in it using human language.

199
00:12:13,588 --> 00:12:16,806
So, for example, here, cheap car is suitable for my dog,

200
00:12:16,980 --> 00:12:20,170
and what we're looking for is basically two things.

201
00:12:20,242 --> 00:12:23,466
It has the cheap aspect and it has the suitable for

202
00:12:23,490 --> 00:12:26,698
my dog aspect. Cheap obviously refers to a low price,

203
00:12:26,786 --> 00:12:30,066
very easy to do. So we have to spit out cars that are

204
00:12:30,090 --> 00:12:33,482
cheap. But the suitable for my dog is a kind of semantic

205
00:12:33,538 --> 00:12:36,962
aspect, which means we're going to have to probably have a larger trunk. So you

206
00:12:36,978 --> 00:12:40,250
can see all three of these cars have kind of that extended trunk.

207
00:12:40,282 --> 00:12:44,234
They're active tours. What about extravagant cars for successful managers?

208
00:12:44,354 --> 00:12:48,250
So again, there's two aspects here. One would be that they're

209
00:12:48,282 --> 00:12:52,010
supposed to be probably expensive because they're designed

210
00:12:52,042 --> 00:12:55,882
to be for successful managers. And extravagant is kind of somehow

211
00:12:55,938 --> 00:12:59,306
unusual, right? So you're going to want cars that are kind

212
00:12:59,330 --> 00:13:02,826
of flashy, maybe very expensive, maybe just spec to

213
00:13:02,850 --> 00:13:05,922
the maximum. So what we do when we type this in here, we have

214
00:13:05,938 --> 00:13:09,290
the XM, which is the most expensive suv that you have,

215
00:13:09,402 --> 00:13:12,834
and then the other two are kind of flashy colors. They're spec

216
00:13:12,874 --> 00:13:16,286
to the max. They're very expensive. Expensive. So that's the idea behind

217
00:13:16,350 --> 00:13:19,782
semantic search when you use it in the context of a product

218
00:13:19,878 --> 00:13:23,710
catalog like this. And that's actually it for this

219
00:13:23,742 --> 00:13:27,166
talk. Guys, I'm very thankful that I was able to be here. Thanks so

220
00:13:27,190 --> 00:13:30,630
much for paying attention. If you have any questions whatsoever, just ping

221
00:13:30,662 --> 00:13:32,934
me a message and I'll be very happy to answer.

