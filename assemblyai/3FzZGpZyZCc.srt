1
00:00:24,650 --> 00:00:27,970
Thank you for taking your time for the presentation

2
00:00:28,050 --> 00:00:31,526
of enriching the data versus filtering the data in

3
00:00:31,548 --> 00:00:35,254
Apache's part. I'm Gokul Prabagaren, engineering manager and

4
00:00:35,292 --> 00:00:38,422
Carl loyalty organization Capital. So before

5
00:00:38,476 --> 00:00:41,734
we really dive into today's topic of

6
00:00:41,772 --> 00:00:45,766
our interest, enriching versus filtering, I would like

7
00:00:45,788 --> 00:00:49,714
to give some details about capital loan to capital

8
00:00:49,762 --> 00:00:52,970
loan is the first US bank to exit out of legacy

9
00:00:53,130 --> 00:00:56,414
on premise data centers to go all in cloud.

10
00:00:56,612 --> 00:01:00,670
You can imagine what kind of a tech transformation

11
00:01:01,170 --> 00:01:04,574
a public company would have gone through to really achieve

12
00:01:04,622 --> 00:01:08,514
such a feat. So that is why we are

13
00:01:08,552 --> 00:01:11,986
a tech company currently which happened to be in a banking business.

14
00:01:12,088 --> 00:01:15,594
We have invested heavily into our tech capabilities

15
00:01:15,742 --> 00:01:19,494
and we pretty much operate now as a tech company.

16
00:01:19,692 --> 00:01:23,062
And these are all things possible mainly because

17
00:01:23,116 --> 00:01:26,870
we are a founder led company till date, staying true

18
00:01:26,940 --> 00:01:30,602
to its mission of change banking for good. How we

19
00:01:30,656 --> 00:01:33,942
really stay focused to that mission,

20
00:01:34,086 --> 00:01:37,754
there are many ways we do. One of that is we give back to our

21
00:01:37,792 --> 00:01:41,822
community and in that also we do multiple things.

22
00:01:41,956 --> 00:01:45,678
This being a tech conference and also we are a tech focus

23
00:01:45,844 --> 00:01:49,774
organization. I would like to start off with we

24
00:01:49,812 --> 00:01:52,970
not only operate as an open source first company,

25
00:01:53,140 --> 00:01:56,722
we also contribute really into open

26
00:01:56,776 --> 00:02:00,820
source projects as well as from our

27
00:02:01,510 --> 00:02:05,606
implementations within our enterprise organization for

28
00:02:05,628 --> 00:02:08,854
our financial services company. There are many

29
00:02:08,892 --> 00:02:12,920
things we do in the regulated industry which

30
00:02:13,690 --> 00:02:17,986
can benefit others. So we also give

31
00:02:18,028 --> 00:02:21,574
back lot of those things as open source projects.

32
00:02:21,622 --> 00:02:25,642
And there are many things we formed as open

33
00:02:25,696 --> 00:02:29,434
source project which came from our organization and I have

34
00:02:29,472 --> 00:02:32,346
called out few which is like critical Stack,

35
00:02:32,458 --> 00:02:36,042
Rubicon, Data profiler, data compi, cloud custodian.

36
00:02:36,186 --> 00:02:39,854
They all play in various spaces like

37
00:02:39,892 --> 00:02:44,038
DevOps, Kubernetes, data yaml,

38
00:02:44,154 --> 00:02:47,662
data cleansing and lot of things. So that's

39
00:02:47,726 --> 00:02:51,198
open source for you guys. The next one is coders.

40
00:02:51,294 --> 00:02:55,302
Coders is a program we run in middle schools across United

41
00:02:55,356 --> 00:02:59,094
States where our work with middle

42
00:02:59,132 --> 00:03:02,262
school students and provide them

43
00:03:02,396 --> 00:03:06,694
opportunities to envision a tech career in their future and

44
00:03:06,732 --> 00:03:10,586
also get really hands on experience for them while they

45
00:03:10,608 --> 00:03:14,890
are in middle school itself. The next one is Quoda.

46
00:03:15,390 --> 00:03:18,842
This is the program which paves way for non tech

47
00:03:18,896 --> 00:03:22,494
folks to get into tech stream and tech

48
00:03:22,532 --> 00:03:26,286
as their career. And we provide and empower them with

49
00:03:26,388 --> 00:03:30,318
the opportunities for them to be really successful with

50
00:03:30,404 --> 00:03:34,794
tech. So first we will start off in our agenda.

51
00:03:34,842 --> 00:03:38,606
First we are going to really see the loyalty use causes in capital.

52
00:03:38,798 --> 00:03:42,418
When we really get into the details, you will understand why

53
00:03:42,504 --> 00:03:45,686
this is what is our starting point,

54
00:03:45,788 --> 00:03:49,350
right? Because this is where this

55
00:03:49,420 --> 00:03:53,042
whole design pattern evolved

56
00:03:53,106 --> 00:03:56,470
and that design pattern first starts with filtering the

57
00:03:56,540 --> 00:04:00,234
data approach in Apache Spark. And there were

58
00:04:00,272 --> 00:04:03,914
some challenges we faced with that approach and

59
00:04:04,112 --> 00:04:07,514
that is what led us to a different approach which we

60
00:04:07,552 --> 00:04:11,534
tried out and now we are sharing this for all

61
00:04:11,572 --> 00:04:15,402
our folks which is enriching the data approach

62
00:04:15,466 --> 00:04:19,166
and how those challenges were fixed using enriching the

63
00:04:19,188 --> 00:04:23,086
data approach. First we will start off with the theory on what

64
00:04:23,108 --> 00:04:26,170
this really means using our use case.

65
00:04:26,340 --> 00:04:29,758
Then we will go through a databased

66
00:04:29,934 --> 00:04:33,394
approach for the same which enables us

67
00:04:33,432 --> 00:04:37,334
to compare two different approaches and why we

68
00:04:37,372 --> 00:04:41,334
pivoted towards one over the other. And we believe that

69
00:04:41,452 --> 00:04:44,680
that probably something which will help lot of.

70
00:04:45,050 --> 00:04:48,986
Finally we will conclude and leave some time for the Q A.

71
00:04:49,168 --> 00:04:53,382
So first we are going to start off with loyalty

72
00:04:53,526 --> 00:04:57,546
use case in Capitalone. So loyalty use case

73
00:04:57,728 --> 00:05:00,814
if you have used any of capitalone product,

74
00:05:00,932 --> 00:05:05,102
Capitalone being a pioneer in credit card and this

75
00:05:05,156 --> 00:05:08,880
platform is the one which pretty much any of our credit card products,

76
00:05:09,490 --> 00:05:12,866
be it Venturex or Venture, focusing on

77
00:05:12,888 --> 00:05:17,310
travel or saver one, dining and entertainment,

78
00:05:17,470 --> 00:05:20,978
all those transactions gets processed through this platform.

79
00:05:21,144 --> 00:05:24,994
And this is the platform which is one of the core credit card

80
00:05:25,032 --> 00:05:29,086
rewards based out of Spark. I have abstracted the details

81
00:05:29,118 --> 00:05:33,126
for brewty, but if you see we

82
00:05:33,148 --> 00:05:36,786
receive all those credit card transactions and we process them by reading

83
00:05:36,818 --> 00:05:40,154
them and then we apply a lot of account specific

84
00:05:40,272 --> 00:05:44,374
eligibility rules. And then transaction specific eligibility

85
00:05:44,422 --> 00:05:48,726
rules. Finally we compute earn and then we persist them in the database.

86
00:05:48,838 --> 00:05:52,970
So you can imagine like account specific business rules

87
00:05:53,390 --> 00:05:56,686
probably maybe to the simplest term, like hey, is this account is

88
00:05:56,708 --> 00:06:00,526
really a valid account to get rewards at this

89
00:06:00,548 --> 00:06:04,282
point in time? And the same goes for transaction too,

90
00:06:04,356 --> 00:06:08,482
right? There are various things we do, but this is the simplest one where

91
00:06:08,536 --> 00:06:11,970
a transaction is this really eligible transaction

92
00:06:12,630 --> 00:06:16,180
to earn rewards. So those are

93
00:06:16,630 --> 00:06:20,086
business rules we apply and then we compute the

94
00:06:20,108 --> 00:06:23,126
earn and that's what gets persisted in the data store.

95
00:06:23,228 --> 00:06:26,934
The reason now you can imagine, right, why you are starting off with

96
00:06:26,972 --> 00:06:30,666
this. This is the platform which is built on top of

97
00:06:30,688 --> 00:06:34,518
Apache Spark which processes millions of transactions.

98
00:06:34,614 --> 00:06:38,602
And this is our brewing ground for comparing these two

99
00:06:38,736 --> 00:06:42,640
design patterns in Apache Spark. Right? So let's start.

100
00:06:43,250 --> 00:06:46,858
So first we are going to start off with filtering

101
00:06:46,874 --> 00:06:50,954
the data approach. So just keep the previous picture

102
00:06:51,002 --> 00:06:54,882
in your mind and we are going to stretch that same example

103
00:06:55,016 --> 00:06:58,738
into a little bit deeper on

104
00:06:58,904 --> 00:07:01,534
using filtering the data approach lens,

105
00:07:01,662 --> 00:07:04,290
the same use case.

106
00:07:04,440 --> 00:07:07,606
Now what we are really doing with filtering the

107
00:07:07,628 --> 00:07:11,282
data approach in this we are receiving some transactions

108
00:07:11,346 --> 00:07:14,738
when we are applying some account specific business rules,

109
00:07:14,834 --> 00:07:18,506
then transaction specific business rules, how are we doing this,

110
00:07:18,608 --> 00:07:22,026
the key thing to remember when it comes to filtering the

111
00:07:22,048 --> 00:07:26,534
data approaches, it is done using Spark's

112
00:07:26,582 --> 00:07:30,638
inner join. And Spark being one of the

113
00:07:30,804 --> 00:07:34,426
big data framework specialist using in memory.

114
00:07:34,538 --> 00:07:37,982
So filtering the data approaches also do the same,

115
00:07:38,116 --> 00:07:40,640
which is inner join using.

116
00:07:41,570 --> 00:07:45,714
So with that context, let's see this. So even

117
00:07:45,752 --> 00:07:48,894
though we do operate with millions of transactions,

118
00:07:48,942 --> 00:07:52,626
just for establishing the fact, like let's take

119
00:07:52,728 --> 00:07:56,214
ten transaction is what we are dealing with in this example. So when

120
00:07:56,252 --> 00:08:00,120
we are reading those transactions and then we are applying some

121
00:08:00,570 --> 00:08:04,114
account specific business rules using Sparks inner join,

122
00:08:04,242 --> 00:08:08,102
what happens naturally is it is actually

123
00:08:08,156 --> 00:08:11,446
going to filtered out the ones which is not matching,

124
00:08:11,558 --> 00:08:15,450
right? So transactions which are got matching in this example,

125
00:08:15,520 --> 00:08:19,146
phi are gone. So Phi goes to the next stage. And same

126
00:08:19,248 --> 00:08:23,390
we are doing with the transaction specific business rules where we are

127
00:08:23,460 --> 00:08:27,120
applying sparks inner join in memory which

128
00:08:27,570 --> 00:08:31,166
filters out three of the transaction which is

129
00:08:31,188 --> 00:08:35,162
not matching. So then finally only two are making it towards

130
00:08:35,226 --> 00:08:38,898
computation. And let's assume true are really

131
00:08:39,064 --> 00:08:42,510
eligible transactions and we are computing earn

132
00:08:42,590 --> 00:08:46,134
for those. So the key takeaway from

133
00:08:46,172 --> 00:08:49,538
this is we are using sparks inner join

134
00:08:49,634 --> 00:08:52,578
in memory at each stage of data pipeline.

135
00:08:52,674 --> 00:08:56,466
What happens is the non matching ones naturally getting filtered

136
00:08:56,498 --> 00:09:00,186
out at each stage and only the ones which

137
00:09:00,208 --> 00:09:03,962
are eligible or which are matching is what really making

138
00:09:04,016 --> 00:09:10,800
it to the end. So if you see this now

139
00:09:11,410 --> 00:09:14,654
filtering data approach may imagine, hey,

140
00:09:14,692 --> 00:09:17,342
this sounds like really what spark does,

141
00:09:17,476 --> 00:09:20,880
right? Let's stretch this same example,

142
00:09:21,730 --> 00:09:25,778
establish the fact using data the same.

143
00:09:25,864 --> 00:09:29,202
Now we are having, we have three

144
00:09:29,256 --> 00:09:33,054
transactions and we are trying to really do spark

145
00:09:33,102 --> 00:09:36,478
inner join to find the matching ones out of this.

146
00:09:36,584 --> 00:09:40,434
So what are the things which are matching? So we naturally

147
00:09:40,482 --> 00:09:43,494
are getting only two transactions which are matching when

148
00:09:43,532 --> 00:09:47,590
we are trying to apply the account status of good,

149
00:09:47,740 --> 00:09:51,242
which is which indirectly means that they're good

150
00:09:51,296 --> 00:09:54,998
accounts, right? So the same if we do with the transaction eligibility,

151
00:09:55,094 --> 00:09:58,554
assume that it's the category is what we are trying to see,

152
00:09:58,592 --> 00:10:01,838
which is we don't want to deal with any other payment.

153
00:10:01,924 --> 00:10:05,998
So in that case, we are trying to filter out the payment

154
00:10:06,084 --> 00:10:10,286
and we are only processing the accounts which have made the

155
00:10:10,388 --> 00:10:14,322
purchase. So that is leaving us with one

156
00:10:14,376 --> 00:10:18,578
transaction in this example. So that is what

157
00:10:18,744 --> 00:10:22,354
goes to our earned competition. And then we really give

158
00:10:22,392 --> 00:10:25,602
them all the rewards, whatever they are entitled to.

159
00:10:25,736 --> 00:10:29,246
So if you see this flow, it's the same spark inner

160
00:10:29,278 --> 00:10:32,520
join in memory which causes one in this case.

161
00:10:33,130 --> 00:10:36,614
So what's the problem with this approach? When we

162
00:10:36,652 --> 00:10:40,006
put this in production, there has been some challenges we really faced.

163
00:10:40,118 --> 00:10:44,214
First thing is after having the application deployed production,

164
00:10:44,342 --> 00:10:47,434
it was really hard to debug this. The main thing

165
00:10:47,472 --> 00:10:51,762
is, hey, what has happened to those transaction

166
00:10:51,846 --> 00:10:55,454
which got filtered in memory? Because it's something which

167
00:10:55,492 --> 00:10:58,906
happens at that moment and everything is dealt

168
00:10:58,938 --> 00:11:03,054
in memory for the fastness of Apache spark

169
00:11:03,102 --> 00:11:06,370
framework. But if we have to really

170
00:11:06,440 --> 00:11:10,226
backtracing all those transactions which has happened in

171
00:11:10,248 --> 00:11:13,534
memory, that's where the real challenge starts.

172
00:11:13,662 --> 00:11:16,310
And being in a regulated industry,

173
00:11:16,890 --> 00:11:20,294
we really need to know mainly what

174
00:11:20,332 --> 00:11:24,120
has happened to the transactions which we really did not

175
00:11:24,490 --> 00:11:27,862
provide earned or we should be in a production for anyone for that matter.

176
00:11:27,916 --> 00:11:30,714
Right? You should be in a position to know that what has happened to each

177
00:11:30,752 --> 00:11:34,234
one of your records which

178
00:11:34,272 --> 00:11:38,006
if something getting filtered out in memory. In some use cases,

179
00:11:38,038 --> 00:11:41,580
it probably fits. In our use case, it did not.

180
00:11:42,110 --> 00:11:45,486
People who are familiar with apologies park probably may argue that

181
00:11:45,508 --> 00:11:49,440
hey, you can do counts at each stage. Yes, that's possible.

182
00:11:50,050 --> 00:11:53,906
But there are two issues with that approach too, which is a

183
00:11:53,928 --> 00:11:58,162
costly operation in Apache Spark. Data pipelines probably

184
00:11:58,216 --> 00:12:02,100
can live with doing counts. But if your processing is

185
00:12:02,550 --> 00:12:06,046
huge enough, then you probably may not be afforded to do counts

186
00:12:06,078 --> 00:12:09,126
at each stage when you are dealing with millions and billions of

187
00:12:09,148 --> 00:12:12,920
rows. And the second problem with the count approach is

188
00:12:13,530 --> 00:12:17,446
you will again get to know how many records really

189
00:12:17,548 --> 00:12:20,874
got filtered out or made it to the next stage. You will

190
00:12:20,912 --> 00:12:24,266
never get to know why unless you really know the

191
00:12:24,288 --> 00:12:27,734
context. So these are all the challenges we faced

192
00:12:27,782 --> 00:12:31,694
with the filtering the data approach. How did we really overcome this

193
00:12:31,732 --> 00:12:36,458
problem? Right, that's where we pivoted.

194
00:12:36,634 --> 00:12:40,894
After doing some research, we pivoted towards our

195
00:12:40,932 --> 00:12:44,370
next design pattern which is enriching the data

196
00:12:44,440 --> 00:12:48,450
approach. The same example, if you see here the same example

197
00:12:48,600 --> 00:12:52,082
of dealing with ten transaction, the key

198
00:12:52,136 --> 00:12:55,170
difference is instead of sparks inner join,

199
00:12:55,690 --> 00:12:59,160
we are going to use sparks left outer join. In this case,

200
00:12:59,690 --> 00:13:03,510
the main thing is we are not really filtering out

201
00:13:03,580 --> 00:13:07,602
any data at any stage. So in previous

202
00:13:07,666 --> 00:13:10,954
case, you started off with ten. You filter out the information,

203
00:13:11,072 --> 00:13:14,278
so which means that your number of rows decreases.

204
00:13:14,454 --> 00:13:17,802
But in this case, we are got filtering out. We are really

205
00:13:17,856 --> 00:13:21,242
enriching the data with all the contextual

206
00:13:21,306 --> 00:13:24,606
information from your left data set.

207
00:13:24,788 --> 00:13:28,414
Keep enhancing your right data set so that you have all

208
00:13:28,452 --> 00:13:32,342
the information so the rows are not changing.

209
00:13:32,426 --> 00:13:35,762
Instead, your columns are growing. So the same

210
00:13:35,816 --> 00:13:39,490
example, ten transactions. We are applying five account

211
00:13:39,560 --> 00:13:43,122
specific rules. Nothing changes. But we have really

212
00:13:43,176 --> 00:13:46,934
picked up some columns which are required for us

213
00:13:46,972 --> 00:13:50,486
to determine later. So ten rows, again making it to the

214
00:13:50,508 --> 00:13:54,210
transaction stage, we are applying transaction

215
00:13:54,370 --> 00:13:58,086
rules. The same ten transaction stays. We picked

216
00:13:58,118 --> 00:14:01,402
up some transaction specific business rules. Now we

217
00:14:01,456 --> 00:14:04,842
apply all the business logic. Then we are actually

218
00:14:04,896 --> 00:14:08,474
arranging with the same result good accounts which

219
00:14:08,512 --> 00:14:12,454
may purchase right two transactions.

220
00:14:12,582 --> 00:14:16,254
Then that's what really is pushed to the next stage and

221
00:14:16,292 --> 00:14:19,886
they are getting whatever they are entitled to. So the key difference in

222
00:14:19,908 --> 00:14:23,474
this is the rows are not changing because we

223
00:14:23,512 --> 00:14:26,850
are enriching the columns using left outer join.

224
00:14:27,190 --> 00:14:30,962
Let's drive home the fact using a data example for

225
00:14:31,016 --> 00:14:34,610
enriching as well, similar to what we did with the filtered,

226
00:14:34,970 --> 00:14:39,094
we are going to see the same example. So here

227
00:14:39,292 --> 00:14:42,550
the same accounts and transactions,

228
00:14:42,890 --> 00:14:46,306
the left outer join number of rows

229
00:14:46,338 --> 00:14:50,150
are not changing. Instead, we really enriched

230
00:14:50,230 --> 00:14:53,738
the data set with the status which is what we need to make a

231
00:14:53,744 --> 00:14:56,858
determination whether that was a good

232
00:14:56,944 --> 00:15:00,254
account. Right same we are doing

233
00:15:00,292 --> 00:15:03,738
with the transaction left outer join.

234
00:15:03,914 --> 00:15:06,986
So our column got increased where we picked

235
00:15:07,018 --> 00:15:10,080
up the category into our data set.

236
00:15:10,770 --> 00:15:14,402
Now we have same three transactions with few more

237
00:15:14,456 --> 00:15:17,890
columns. And what we are doing is we are using

238
00:15:17,960 --> 00:15:21,490
this information and trying to apply some business logic.

239
00:15:21,910 --> 00:15:25,314
Then we can enhance with some more columns as

240
00:15:25,352 --> 00:15:28,514
well to make really your computation more

241
00:15:28,552 --> 00:15:32,434
easier. So what we are doing is, hey, is this really eligible

242
00:15:32,482 --> 00:15:35,846
for next stage? So just have them as true. Then you can

243
00:15:35,868 --> 00:15:39,466
go and pick whatever that particular column is true for your

244
00:15:39,568 --> 00:15:42,730
next stage of processing. So in this, that's the only

245
00:15:42,800 --> 00:15:46,154
one which has good account and

246
00:15:46,192 --> 00:15:49,414
also they have made the purchase, right? So that's

247
00:15:49,462 --> 00:15:53,214
what really we are getting the same result

248
00:15:53,412 --> 00:15:57,242
similar to our previous example too. What's the advantage

249
00:15:57,306 --> 00:15:59,950
of enriching over the filtering?

250
00:16:00,290 --> 00:16:03,854
So if you compare the problems we faced with the filtering,

251
00:16:03,982 --> 00:16:07,410
with enriching here the data

252
00:16:07,480 --> 00:16:11,250
is not changing. Instead we are really enriching the original data

253
00:16:11,320 --> 00:16:14,850
state which captures the state information which makes it easy

254
00:16:14,920 --> 00:16:18,902
for us to debug and analyze later. And also

255
00:16:19,036 --> 00:16:22,470
we have the data columns and flags captured at each

256
00:16:22,540 --> 00:16:26,466
stage. Gives us more granular details to debug

257
00:16:26,578 --> 00:16:30,330
as well as backtracing. Hey, what has happened to the other two

258
00:16:30,400 --> 00:16:34,010
transaction in our example? Why they did not make it to the next

259
00:16:34,080 --> 00:16:37,766
stage? This naturally enables

260
00:16:37,798 --> 00:16:42,030
us no need of having counts at any stage because

261
00:16:42,100 --> 00:16:45,454
we have all the required information for us to really do.

262
00:16:45,652 --> 00:16:50,394
Cool. You probably all got some comparison

263
00:16:50,442 --> 00:16:53,742
details between two Apache Spark

264
00:16:53,806 --> 00:16:57,202
based design patterns and you

265
00:16:57,256 --> 00:17:00,050
probably may be able to make some informed decisions.

266
00:17:00,470 --> 00:17:04,210
Which one fits your use case, right? We really

267
00:17:04,280 --> 00:17:07,734
made the switch to enriching the data approach in our

268
00:17:07,932 --> 00:17:11,426
production after we went live with the first version

269
00:17:11,458 --> 00:17:15,014
using filtering in initial days itself. And that

270
00:17:15,052 --> 00:17:19,222
filtering approach is what really is successfully

271
00:17:19,286 --> 00:17:22,566
running in production and processing millions

272
00:17:22,598 --> 00:17:26,282
of credit card transactions daily and that's what

273
00:17:26,336 --> 00:17:29,766
really provides all our customers with millions

274
00:17:29,798 --> 00:17:33,294
of cash card and miles. Hope you all

275
00:17:33,332 --> 00:17:37,070
had some details about me. I'm a capital

276
00:17:37,730 --> 00:17:41,854
engineering manager. I have been building software application

277
00:17:41,972 --> 00:17:45,630
from its initial version of Java so as Apache Spark

278
00:17:46,050 --> 00:17:49,998
I regularly give presentations based on

279
00:17:50,084 --> 00:17:53,674
big data NoSQL as well as contribute

280
00:17:53,722 --> 00:17:57,346
to capital tech blogs. I have provided

281
00:17:57,378 --> 00:18:01,110
my social handles for you guys to thank you for the opportunity

282
00:18:01,260 --> 00:18:04,946
watching your valet. Hope you all have great conference.

283
00:18:05,058 --> 00:18:05,380
Thank you.

