1
00:00:27,814 --> 00:00:31,878
Hello. Welcome. Today we are going to discuss about Netata,

2
00:00:31,966 --> 00:00:34,634
the open source observability platform.

3
00:00:36,054 --> 00:00:39,182
I am the founder of Netdata, and to tell you

4
00:00:39,198 --> 00:00:42,598
the truth, I started Netdata out of frustration.

5
00:00:42,726 --> 00:00:45,434
So I was facing some issues.

6
00:00:47,014 --> 00:00:51,114
I couldn't identify them actually with any of the existing

7
00:00:51,934 --> 00:00:55,802
solutions, and I decided

8
00:00:55,898 --> 00:00:59,266
to do something about it. So what's the problem? Why we

9
00:00:59,290 --> 00:01:02,954
need another tool? So the

10
00:01:02,994 --> 00:01:07,162
key problems are this. The first is that most

11
00:01:07,218 --> 00:01:11,346
monitoring solutions today provide very low fidelity

12
00:01:11,410 --> 00:01:15,338
insights. Why is that? The key problem is that

13
00:01:15,386 --> 00:01:18,610
they force us to select the data sources,

14
00:01:18,682 --> 00:01:22,514
to select a few data sources, and then instead

15
00:01:22,554 --> 00:01:26,934
of collecting everything per second and going everywhere high resolution,

16
00:01:28,714 --> 00:01:32,394
they force us to lower the granularity,

17
00:01:32,434 --> 00:01:36,226
the resolution of the insights. The second is

18
00:01:36,250 --> 00:01:39,682
inefficiency. So most monitoring tools

19
00:01:39,738 --> 00:01:43,626
assume that we're going to build some kind of dashboards ourselves and

20
00:01:43,650 --> 00:01:46,374
we're going to do everything by hand, etcetera.

21
00:01:47,714 --> 00:01:51,746
For my taste, this was very problematic. So I don't want to build custom

22
00:01:51,810 --> 00:01:54,922
dashboards by hand. Of course I want to have the ability to do

23
00:01:54,938 --> 00:01:58,934
it, but it should not be the main means for

24
00:01:59,234 --> 00:02:01,934
troubleshooting or exploring an infrastructure.

25
00:02:02,994 --> 00:02:05,974
The next is that there is no AI.

26
00:02:06,474 --> 00:02:10,130
So even the observability solutions

27
00:02:10,202 --> 00:02:13,854
that claim that they do some kind of machine learning,

28
00:02:14,624 --> 00:02:18,632
they don't do true machine learning. It's more like an expert system where

29
00:02:18,688 --> 00:02:22,408
they have some rules, let's say that

30
00:02:22,456 --> 00:02:25,032
they call them collectively AI,

31
00:02:25,088 --> 00:02:28,856
but it's not really. And the last

32
00:02:28,920 --> 00:02:32,272
one is that all of them are expensive.

33
00:02:32,408 --> 00:02:36,448
Even the open source ones are

34
00:02:36,536 --> 00:02:40,750
really expensive to run. So this was my

35
00:02:40,912 --> 00:02:44,562
driving force. This is why I wanted to have

36
00:02:44,658 --> 00:02:49,410
a new monitoring tool that will simplify the

37
00:02:49,442 --> 00:02:53,374
lives of engineers instead of complicating them.

38
00:02:54,194 --> 00:02:58,066
Now, the current state, as I see it

39
00:02:58,250 --> 00:03:01,578
from my experience, the tools that exist

40
00:03:01,706 --> 00:03:04,754
out there, and I respect all of them.

41
00:03:04,914 --> 00:03:08,656
So each of these tools contributes

42
00:03:08,720 --> 00:03:13,096
significantly, or has contributed significantly to

43
00:03:13,120 --> 00:03:16,764
the evolution, let's say, of the observability solutions.

44
00:03:17,264 --> 00:03:20,884
So there is a world of the too little observability.

45
00:03:21,424 --> 00:03:24,364
These are the traditional check based systems,

46
00:03:25,744 --> 00:03:29,424
the systems that do some check, and then you have something

47
00:03:29,504 --> 00:03:32,896
like traffic lights monitoring. You have green,

48
00:03:33,000 --> 00:03:36,844
yellow, red to indicate that something

49
00:03:36,964 --> 00:03:40,276
needs attention or is problematic. The problem with this,

50
00:03:40,300 --> 00:03:43,704
this is a robust solution. There are excellent tools in this area.

51
00:03:44,364 --> 00:03:49,556
The problem with this philosophy is that they

52
00:03:49,580 --> 00:03:52,580
have a lot of blind spots. They don't monitor much.

53
00:03:52,652 --> 00:03:55,784
They monitor only the things that they have checks for.

54
00:03:56,524 --> 00:03:58,904
Then there is the two complex observability,

55
00:04:00,004 --> 00:04:03,756
like the Grafana work, the Grafana world is a very

56
00:04:03,820 --> 00:04:07,484
powerful and an excellent visualizer

57
00:04:07,644 --> 00:04:10,996
with excellent databases and other

58
00:04:11,060 --> 00:04:14,940
tools in this ecosystem. But the problem, the biggest

59
00:04:14,972 --> 00:04:18,904
problem is that it has a very steep learning curve

60
00:04:19,364 --> 00:04:22,524
and it very quickly becomes

61
00:04:22,684 --> 00:04:26,516
overcomplicated to maintain and scale.

62
00:04:26,700 --> 00:04:30,642
And then, of course, there are the commercial solutions, like the

63
00:04:30,738 --> 00:04:34,266
data doc, Dynatrace, et cetera, which of course,

64
00:04:34,330 --> 00:04:38,538
are very nice integrated systems. However, they are

65
00:04:38,706 --> 00:04:42,010
very expensive, and to my understanding,

66
00:04:42,042 --> 00:04:45,094
they cannot do otherwise. It's expensive what they do.

67
00:04:45,434 --> 00:04:47,054
So the evolution,

68
00:04:48,514 --> 00:04:51,586
as I see it, is like this. So initially,

69
00:04:51,650 --> 00:04:55,138
we had all the check based systems. That was the first generation

70
00:04:55,186 --> 00:04:59,284
of monitoring tools. Then we had the

71
00:04:59,324 --> 00:05:03,020
metrics based systems, then the logs based systems.

72
00:05:03,172 --> 00:05:06,580
Now, okay, this is more or less in parallel,

73
00:05:06,652 --> 00:05:10,196
they evolved. The fourth generation

74
00:05:10,300 --> 00:05:12,784
is the integrated ones,

75
00:05:14,724 --> 00:05:17,860
like the commercial providers and Grafana, of course.

76
00:05:18,052 --> 00:05:21,540
And the fifth is what I believe that netdata

77
00:05:21,612 --> 00:05:25,184
introduces, that is, the all in one,

78
00:05:26,124 --> 00:05:29,476
like the integrated ones, all in

79
00:05:29,500 --> 00:05:33,264
one, integrated, but at the same time, real time, high fidelity,

80
00:05:34,364 --> 00:05:38,224
AI powered and extremely cost efficient.

81
00:05:39,044 --> 00:05:43,300
Now, in order for to understand my thinking how

82
00:05:43,332 --> 00:05:47,620
I started the thing, this is what I believe are the biggest,

83
00:05:47,732 --> 00:05:51,478
let's say bad practices. They call it best,

84
00:05:51,526 --> 00:05:54,794
but in my mind, they are bad practices.

85
00:05:55,374 --> 00:05:59,254
So the first bad practice

86
00:05:59,374 --> 00:06:03,062
is this myth that says that

87
00:06:03,158 --> 00:06:05,754
you should monitor only what you need and understand.

88
00:06:06,934 --> 00:06:11,078
This is no, we should

89
00:06:11,126 --> 00:06:15,182
monitor everything. Everything that is available should be monitored,

90
00:06:15,238 --> 00:06:19,164
no matter what, no exceptions. Why? We need this because

91
00:06:19,284 --> 00:06:23,624
we need a holistic view of the system and the infrastructure

92
00:06:24,204 --> 00:06:28,332
of even our applications. So we need more

93
00:06:28,388 --> 00:06:31,144
data to make our lives easier.

94
00:06:31,844 --> 00:06:34,584
When we do root cause analysis,

95
00:06:35,284 --> 00:06:38,660
we need enough data in order to

96
00:06:38,692 --> 00:06:42,300
feed the detection mechanism that

97
00:06:42,332 --> 00:06:45,594
can proactively detect detect issues. And of

98
00:06:45,634 --> 00:06:49,814
course, since we collect everything,

99
00:06:50,474 --> 00:06:54,338
this means that we are adaptable to changing

100
00:06:54,386 --> 00:06:58,614
environments. So as the infrastructure changes or the application changes or

101
00:07:00,954 --> 00:07:04,626
additional things are introduced, we still

102
00:07:04,730 --> 00:07:08,106
monitor everything, so we have all the information that is

103
00:07:08,130 --> 00:07:11,940
there, no need to maintain it. The second is

104
00:07:12,052 --> 00:07:16,572
that we need real time per second data

105
00:07:16,748 --> 00:07:21,076
and extremely low latency. So the

106
00:07:21,180 --> 00:07:25,092
bad practice is that, okay, if you monitor

107
00:07:25,148 --> 00:07:28,444
every 1015 or 60 seconds, that's enough.

108
00:07:28,604 --> 00:07:32,796
To my understanding, this is not enough, because the

109
00:07:32,820 --> 00:07:36,204
first thing is that the environment we live today, with all

110
00:07:36,244 --> 00:07:39,666
this virtualization and all these

111
00:07:39,730 --> 00:07:43,934
different providers that are involved in our infrastructure,

112
00:07:44,954 --> 00:07:48,970
monitoring every 1015, or even monitoring every two,

113
00:07:49,122 --> 00:07:52,614
is not enough in many cases, to understand

114
00:07:52,994 --> 00:07:57,250
what is really happening at the infrastructure or the application

115
00:07:57,322 --> 00:08:00,826
level. The next is that when

116
00:08:00,850 --> 00:08:02,214
you monitor every second,

117
00:08:03,874 --> 00:08:07,394
it's easy to see the correlations.

118
00:08:07,774 --> 00:08:11,078
It's easy to see when this application did

119
00:08:11,126 --> 00:08:14,062
something, and when that application did something,

120
00:08:14,238 --> 00:08:17,914
so spikes and dives or different errors

121
00:08:18,414 --> 00:08:21,754
can be easily put in sequence.

122
00:08:22,214 --> 00:08:26,166
And of course, when you have a low latency system and

123
00:08:26,190 --> 00:08:29,662
you know that the moment that you hit enter on the keyboard to apply

124
00:08:29,718 --> 00:08:32,896
a command, the next second the, the monitoring system

125
00:08:32,960 --> 00:08:37,976
will be updated. Of course, this improves response

126
00:08:38,000 --> 00:08:41,960
to issues, even makes

127
00:08:41,992 --> 00:08:45,776
it extremely easy to identify issues during

128
00:08:45,840 --> 00:08:49,204
deployments or during changes in the infrastructure, et cetera.

129
00:08:50,584 --> 00:08:53,904
The next is about dashboards.

130
00:08:54,024 --> 00:08:59,206
So most of the tools, most of the monitoring tools have

131
00:08:59,230 --> 00:09:02,310
a design that allow people and

132
00:09:02,342 --> 00:09:06,742
forces, actually people, to create the custom dashboards,

133
00:09:06,918 --> 00:09:10,174
the custom dashboards that they need beforehand.

134
00:09:10,254 --> 00:09:14,190
So that's the idea. Create enough custom dashboards

135
00:09:14,342 --> 00:09:18,830
so that you are going to have them available when the

136
00:09:18,862 --> 00:09:22,910
time comes, especially under crisis. The problem

137
00:09:22,982 --> 00:09:26,620
with this is that our data have

138
00:09:26,692 --> 00:09:30,220
infinite correlations. So okay, you can

139
00:09:30,252 --> 00:09:33,740
put the database and the queries and this and that, they are

140
00:09:33,812 --> 00:09:37,452
in the same dashboard, but how this is correlated

141
00:09:37,508 --> 00:09:40,668
with the storage, the network, the web servers,

142
00:09:40,716 --> 00:09:44,140
etcetera. So it's very hard to build

143
00:09:44,212 --> 00:09:47,684
by hand, infinite correlations. So I

144
00:09:47,724 --> 00:09:51,404
prefer generally a system that

145
00:09:51,524 --> 00:09:55,144
provides a powerful, fully automated

146
00:09:56,484 --> 00:09:59,452
dynamic dashboards. So it's a tool,

147
00:09:59,628 --> 00:10:02,996
it's not just static, a few dashboards,

148
00:10:03,060 --> 00:10:06,784
it's a tool to explore, it's a tool to correlate,

149
00:10:08,124 --> 00:10:11,884
and it's a tool that it's consistent no matter

150
00:10:11,924 --> 00:10:15,652
what we want to do. So this fully automated

151
00:10:15,708 --> 00:10:19,940
dashboard, it's again, one of the things

152
00:10:19,972 --> 00:10:22,544
that they believe are ideal,

153
00:10:23,964 --> 00:10:27,060
one of the ideal attributes of monitoring of

154
00:10:27,092 --> 00:10:30,956
a modern observability solution. And the last one is

155
00:10:30,980 --> 00:10:34,788
of course, that we should engage,

156
00:10:34,876 --> 00:10:38,464
we should use AI and machine learning

157
00:10:38,764 --> 00:10:42,748
to help us in observability. There are many,

158
00:10:42,916 --> 00:10:46,446
many presentations in the Internet, and there is a hype

159
00:10:46,540 --> 00:10:49,574
that machine learning cannot help in observability.

160
00:10:50,274 --> 00:10:53,546
Unfortunately, this is not true. Fortunately,

161
00:10:53,610 --> 00:10:57,066
actually this is not true. So ML can learn the

162
00:10:57,090 --> 00:11:00,922
behavior of matrix, so given a time series, it can

163
00:11:00,978 --> 00:11:04,226
learn how it behaves over time. ML can be

164
00:11:04,250 --> 00:11:07,754
trained at the edge, so there is no need to train models

165
00:11:07,874 --> 00:11:11,442
and then publish models and use. And actually this

166
00:11:11,498 --> 00:11:15,296
does not work for observability, because even if you have the same servers,

167
00:11:15,360 --> 00:11:19,744
exactly the same data, exactly the same things, the workload defines

168
00:11:19,864 --> 00:11:23,448
what the metrics will do. So each

169
00:11:23,536 --> 00:11:27,244
time series should be trained individually,

170
00:11:28,744 --> 00:11:32,152
then ML machine learning can detect

171
00:11:32,208 --> 00:11:36,128
outliers, reliably, can reveal hidden

172
00:11:36,216 --> 00:11:39,912
correlations. So you can see, for example, in anomalies

173
00:11:39,968 --> 00:11:43,532
that where the moment the anomaly of this thing and that thing happened at

174
00:11:43,548 --> 00:11:46,584
the same time. And this is consistent

175
00:11:46,964 --> 00:11:50,980
across time. So this means that these metrics

176
00:11:51,012 --> 00:11:55,024
somehow, even if you don't realize how they are correlated.

177
00:11:55,404 --> 00:11:59,244
And the most interesting part is that when you have a system that

178
00:11:59,364 --> 00:12:02,188
monitors everything, has everything high fidelity,

179
00:12:02,356 --> 00:12:05,292
and trains machine learning models for everything,

180
00:12:05,468 --> 00:12:09,200
is that the density of outliers and

181
00:12:09,232 --> 00:12:13,392
the number of anomalous, of concurrently anomalous

182
00:12:13,448 --> 00:12:16,712
metrics provide key insights. So you

183
00:12:16,728 --> 00:12:20,144
can see clusters of anomalies within

184
00:12:20,184 --> 00:12:21,964
a server or across servers,

185
00:12:23,624 --> 00:12:26,872
and you can see how the anomalies spread

186
00:12:27,048 --> 00:12:30,324
across different components of the systems and the services.

187
00:12:31,344 --> 00:12:35,686
Now in

188
00:12:35,710 --> 00:12:39,534
order to overcome the first problem, to monitor everything

189
00:12:39,614 --> 00:12:44,726
problem and the real time, of course the per second and

190
00:12:44,830 --> 00:12:48,950
low latency visualization, the data comes

191
00:12:49,022 --> 00:12:52,714
up with a distributed design.

192
00:12:54,614 --> 00:12:58,414
But in order for this, in order to understand it, let's see what

193
00:12:58,454 --> 00:13:02,218
affects fidelity. Granularity is the resolution

194
00:13:02,306 --> 00:13:06,694
of the data, how frequently you collect data.

195
00:13:07,514 --> 00:13:11,026
If you have a low granularity, the data are blurry,

196
00:13:11,090 --> 00:13:14,754
they are averaged over time. So you have a point here, a point

197
00:13:14,794 --> 00:13:17,094
there, just averaged in between.

198
00:13:18,114 --> 00:13:21,842
Low cardinality means that you have fewer

199
00:13:21,898 --> 00:13:25,162
sources. So you don't collect everything that is there.

200
00:13:25,218 --> 00:13:28,174
You collect some of the data that are there.

201
00:13:28,554 --> 00:13:32,514
If you have both of them, both low granularity and low

202
00:13:32,554 --> 00:13:35,898
cardinality, then you are lacking both

203
00:13:35,946 --> 00:13:38,778
detail and coverage. So it's an abstract view.

204
00:13:38,866 --> 00:13:42,930
You have monitoring, you can see the overall thing you can see,

205
00:13:43,082 --> 00:13:46,802
but it's a very high level view of what

206
00:13:46,978 --> 00:13:50,614
is really happening in your systems and applications.

207
00:13:51,554 --> 00:13:55,228
Why not having high fidelity? What's the problem? The key problem

208
00:13:55,316 --> 00:13:58,864
for all monitoring systems is the centralized design.

209
00:13:59,164 --> 00:14:02,940
So the centralized design means that you push everything from your entire

210
00:14:02,972 --> 00:14:06,956
infrastructure to a database server. It can be a time series database

211
00:14:06,980 --> 00:14:10,204
or whatever it is, doesn't matter, but it goes, the whole infrastructure

212
00:14:10,244 --> 00:14:13,500
goes to one database server. This means

213
00:14:13,612 --> 00:14:16,652
that in order to scale the monitoring solution,

214
00:14:16,708 --> 00:14:21,228
to scale this database server and also to control its

215
00:14:21,276 --> 00:14:24,636
costs. So for commercial providers

216
00:14:24,660 --> 00:14:28,754
this means costs, this means in bound samples that

217
00:14:28,794 --> 00:14:32,494
need to be processed, stored, indexed, et cetera.

218
00:14:32,834 --> 00:14:37,034
So in order to scale

219
00:14:37,154 --> 00:14:41,978
the centralized design, they have to lower granularity

220
00:14:42,066 --> 00:14:45,434
and cardinality and of course the output.

221
00:14:45,514 --> 00:14:48,674
The outcome is always expensive because no matter how much

222
00:14:48,714 --> 00:14:52,282
you lower it, you have to have some data there, you have

223
00:14:52,298 --> 00:14:56,008
to have enough data to actually

224
00:14:56,056 --> 00:14:59,944
provide some insights. Now the data

225
00:14:59,984 --> 00:15:03,256
for example, collects everything per second. And as we

226
00:15:03,280 --> 00:15:06,976
will see later on an empty VM, it has about 3000

227
00:15:07,040 --> 00:15:10,352
metrics per second. So if you install the data on an empty VM, you are

228
00:15:10,368 --> 00:15:12,444
going to get 3000 metrics per second,

229
00:15:13,344 --> 00:15:17,284
3000 unique time series per second.

230
00:15:18,984 --> 00:15:22,520
This is compared to most other. Most other monitoring solutions collect

231
00:15:22,552 --> 00:15:26,074
about 100 metrics every 15 seconds or something like this.

232
00:15:26,374 --> 00:15:31,834
Compared to this to it, the data manages

233
00:15:32,734 --> 00:15:36,154
450 times more data.

234
00:15:37,614 --> 00:15:41,382
That's the big deal. This is why all the others

235
00:15:41,478 --> 00:15:45,006
are forced to lower select the

236
00:15:45,030 --> 00:15:47,674
cherry pick sources, lower the frequency.

237
00:15:48,134 --> 00:15:51,390
Now, we used

238
00:15:51,502 --> 00:15:54,818
a decentralized design, a distributed design,

239
00:15:55,006 --> 00:15:58,602
so we keep the data at the edge. The data keeps the data at the

240
00:15:58,618 --> 00:16:02,546
edge. When we keep the data at the edge, then you

241
00:16:02,570 --> 00:16:06,442
understand that we have a few benefits. The first is that each

242
00:16:06,498 --> 00:16:10,418
of these servers has its own data. It's a small data

243
00:16:10,466 --> 00:16:14,482
set. It may be a few thousand metrics, but it's small data

244
00:16:14,538 --> 00:16:17,986
set, easily manageable. The second is that the

245
00:16:18,010 --> 00:16:21,686
resources required to do so are already available

246
00:16:21,750 --> 00:16:25,054
in spare. So metadata is very efficient in both cpu

247
00:16:25,094 --> 00:16:28,310
and memory and storage. So you can expect,

248
00:16:28,382 --> 00:16:32,022
for example, a couple percent cpu of a single

249
00:16:32,078 --> 00:16:35,870
core, two 2% of a single core,

250
00:16:36,022 --> 00:16:39,630
and 200 megabytes of ram and

251
00:16:39,662 --> 00:16:42,798
3gb of disk space.

252
00:16:42,886 --> 00:16:46,430
That's it. I o is almost idle disk

253
00:16:46,462 --> 00:16:50,974
IO. So this allows

254
00:16:51,014 --> 00:16:55,030
nadeta to be a very polite citizen for production

255
00:16:55,062 --> 00:16:58,798
applications. So despite the fact that it's a full blown monitoring solution

256
00:16:58,966 --> 00:17:02,606
in a box, in one application, and it does

257
00:17:02,670 --> 00:17:06,274
everything in there, the agent that you install,

258
00:17:06,934 --> 00:17:10,870
it still is one of the most

259
00:17:11,062 --> 00:17:14,080
efficient applications. Actually we did.

260
00:17:14,112 --> 00:17:17,328
Also, if you search our site, we have a blog where

261
00:17:17,376 --> 00:17:21,136
we evaluated the performance of all the agents and

262
00:17:21,160 --> 00:17:25,464
you can verify there. That data is one of the lightest monitoring

263
00:17:25,504 --> 00:17:29,044
agents available, despite the fact that it's a full monitoring

264
00:17:29,584 --> 00:17:33,568
solution in a box. The second is that

265
00:17:33,696 --> 00:17:36,888
if you have ephemeral nodes, or if you have other operational

266
00:17:36,976 --> 00:17:40,596
needs to not having the

267
00:17:40,660 --> 00:17:44,516
data inside the edge, the production systems,

268
00:17:44,660 --> 00:17:48,020
then the data, the same software, the agent can

269
00:17:48,052 --> 00:17:52,904
be used as centralization point. So you can build with

270
00:17:53,364 --> 00:17:56,532
data centralization points across your infrastructure.

271
00:17:56,668 --> 00:17:59,464
But decentralization points do not need to be,

272
00:18:00,284 --> 00:18:04,064
it does not need to be one. You can have as many as you want.

273
00:18:04,864 --> 00:18:08,320
And in order to provide unified views

274
00:18:08,392 --> 00:18:12,320
across the infrastructure, we merge the data at

275
00:18:12,352 --> 00:18:14,804
query time. So for us,

276
00:18:15,344 --> 00:18:18,640
the biggest trick, the hardest part,

277
00:18:18,832 --> 00:18:23,056
was actually to come up with a query engine that can

278
00:18:23,120 --> 00:18:25,728
do all these complex queries that are required,

279
00:18:25,896 --> 00:18:30,856
but it can execute them in a distributed

280
00:18:30,880 --> 00:18:34,264
way. So parts of the queries are running all over the place.

281
00:18:34,384 --> 00:18:36,924
And then the final thing is aggregated.

282
00:18:39,304 --> 00:18:42,848
One of the common concerns about decentralized design is

283
00:18:42,936 --> 00:18:46,488
the agent will be heavy. We discussed this already.

284
00:18:46,656 --> 00:18:50,724
It will not. We have verified this. Actually, it's one of the lightest

285
00:18:51,904 --> 00:18:54,672
queries will influence production systems?

286
00:18:54,808 --> 00:18:57,576
No, because the data set is very small.

287
00:18:57,760 --> 00:19:01,760
So there is really no

288
00:19:01,912 --> 00:19:05,716
effect, zero effect on production systems even when

289
00:19:05,780 --> 00:19:09,544
queries are run. But what is more important is that

290
00:19:10,764 --> 00:19:13,924
we give all the tooling to

291
00:19:13,964 --> 00:19:17,660
allow offloading very sensitive production systems.

292
00:19:17,692 --> 00:19:21,036
So if you have a database server and you really don't want queries to run

293
00:19:21,060 --> 00:19:24,668
there by net data observability

294
00:19:24,756 --> 00:19:28,380
queries, I mean then you can easily stream

295
00:19:28,412 --> 00:19:31,932
the data to another metadata server next to it. And this

296
00:19:31,988 --> 00:19:34,664
one will be used for all queries.

297
00:19:35,764 --> 00:19:39,572
Queries will be slower? No, it will not be slower.

298
00:19:39,628 --> 00:19:43,036
Actually they are faster. Now imagine this. If you have a thousand

299
00:19:43,100 --> 00:19:46,804
servers out there and you have installed metadata on all of them

300
00:19:46,924 --> 00:19:50,636
and you want to aggregate one chart with a total

301
00:19:50,700 --> 00:19:54,268
bandwidth of everything, all the total cpu utilization, let's say

302
00:19:54,316 --> 00:19:56,784
of all your servers, a thousand servers.

303
00:19:57,144 --> 00:20:00,544
Then the moment you hit that button

304
00:20:00,704 --> 00:20:04,296
for the query to be executed, tiny queries

305
00:20:04,360 --> 00:20:07,084
are executed on a thousand servers.

306
00:20:07,904 --> 00:20:11,704
The horsepower that you have is

307
00:20:11,784 --> 00:20:15,720
amazingly big. Each server is doing

308
00:20:15,752 --> 00:20:19,304
a very tiny job but the overall

309
00:20:19,384 --> 00:20:22,726
performance is much much much bigger,

310
00:20:22,760 --> 00:20:28,010
better. And another

311
00:20:28,082 --> 00:20:31,442
concern is that it will require more bandwidth. No,

312
00:20:31,538 --> 00:20:35,234
because in observability most of the bandwidth is in

313
00:20:35,274 --> 00:20:38,818
streaming the collected data. That's all the

314
00:20:38,826 --> 00:20:42,346
bandwidth goes there. It's magnitudes bigger than

315
00:20:42,370 --> 00:20:47,002
the bandwidth required to query something or view a page with

316
00:20:47,018 --> 00:20:52,166
a chart, etcetera. So actually another

317
00:20:52,230 --> 00:20:56,382
aspect is that there are times that you

318
00:20:56,478 --> 00:21:00,046
use the monitoring system a lot and you are all day

319
00:21:00,070 --> 00:21:03,214
on it because you need to troubleshoot something. But most

320
00:21:03,254 --> 00:21:06,494
of the time most of the data are never

321
00:21:06,534 --> 00:21:09,194
queried. So most of the time,

322
00:21:09,854 --> 00:21:14,862
come on. They are just collected, stored, indexed but they

323
00:21:14,878 --> 00:21:18,460
are not queried. So you have them there as an ability to

324
00:21:18,492 --> 00:21:22,348
troubleshoot. You don't need to go and see them all

325
00:21:22,396 --> 00:21:23,264
every day.

326
00:21:25,404 --> 00:21:28,620
Now this is what I told you before, what the data

327
00:21:28,732 --> 00:21:32,276
collects. How if you install metadata on

328
00:21:32,300 --> 00:21:36,004
a purely empty vm, so nothing, just buy

329
00:21:36,044 --> 00:21:39,220
an AWS VM or GCP or azure or whatever,

330
00:21:39,372 --> 00:21:42,768
install the data on it. This is what you're going

331
00:21:42,776 --> 00:21:46,856
to get. This is as

332
00:21:46,880 --> 00:21:50,416
you see, 150 more than 150 charts,

333
00:21:50,560 --> 00:21:53,484
more than 2000 unique time series.

334
00:21:54,264 --> 00:21:57,680
More than 50 alerts will be running monitoring components.

335
00:21:57,832 --> 00:22:01,504
You're going to have a system D logs explorer and network

336
00:22:01,584 --> 00:22:04,960
explorer. So all the circuits in out, whatever it is, even the local

337
00:22:05,032 --> 00:22:08,400
ones, you're going to have unsupervised anomaly

338
00:22:08,432 --> 00:22:12,262
detection for all metrics. Two years of retention

339
00:22:12,358 --> 00:22:15,742
using 3GB of disk space and it's going to

340
00:22:15,758 --> 00:22:19,942
use about 1% CPU of single core 120

341
00:22:19,998 --> 00:22:23,914
megabytes RAM and almost zero disk I O.

342
00:22:25,854 --> 00:22:29,830
This includes machine learning and this includes the metrics,

343
00:22:29,862 --> 00:22:33,630
retention, storage, everything. Now if you

344
00:22:33,662 --> 00:22:37,102
see what any data agent is internally is this thing.

345
00:22:37,158 --> 00:22:40,584
So you have a discovery process

346
00:22:40,664 --> 00:22:44,688
where it auto discovers all the metrics. It starts collecting

347
00:22:44,736 --> 00:22:47,816
them, it detects anomaly. But this is a

348
00:22:47,840 --> 00:22:52,048
feedback loop, we will go there. So after

349
00:22:52,096 --> 00:22:55,592
collection, usually it stores them. So it stores in

350
00:22:55,608 --> 00:22:58,432
its own database, time series database.

351
00:22:58,568 --> 00:23:02,056
And this is a time series that we have specially crafted to

352
00:23:02,080 --> 00:23:05,546
achieve this. Half a byte

353
00:23:05,640 --> 00:23:08,554
per sample on disk on the high resolution tier.

354
00:23:09,814 --> 00:23:13,694
Now after the data are stored

355
00:23:13,734 --> 00:23:17,914
in the database, we have machine learning where

356
00:23:18,614 --> 00:23:22,750
multiple models are trained per metric. And this

357
00:23:22,782 --> 00:23:27,078
of course provides reliable anomaly detection

358
00:23:27,246 --> 00:23:30,034
during data collection. This is in real time.

359
00:23:31,054 --> 00:23:34,880
We have the health engine that we check for common

360
00:23:34,912 --> 00:23:39,004
error conditions or whatever alerts you have configured.

361
00:23:39,424 --> 00:23:43,320
We have here are the query engines. We're going to discuss about the

362
00:23:43,352 --> 00:23:47,208
scoring engine later. The query engine is

363
00:23:47,216 --> 00:23:50,644
the normal query engine that most monitoring solutions have.

364
00:23:51,424 --> 00:23:54,604
Net data can export metrics to third parties,

365
00:23:56,024 --> 00:23:59,744
prometheus or influx or whatever it is,

366
00:23:59,864 --> 00:24:03,576
where it can export its own metrics and actually it can

367
00:24:03,600 --> 00:24:07,384
also downsample them so that the other system will

368
00:24:07,424 --> 00:24:11,392
not be overloaded by the amount of information that

369
00:24:11,408 --> 00:24:14,416
the data sends. Actually it can filter them.

370
00:24:14,520 --> 00:24:17,960
So zap a few metrics or end

371
00:24:18,032 --> 00:24:21,216
or down sample instead of per second, do it

372
00:24:21,240 --> 00:24:24,792
every 10 seconds, every minute or whatever needed.

373
00:24:24,928 --> 00:24:28,536
And then there is a streaming functionality. This is the function that allows

374
00:24:28,560 --> 00:24:31,858
you to build parents. So the streaming part,

375
00:24:31,986 --> 00:24:35,322
centralization points, the streaming part of one data

376
00:24:35,418 --> 00:24:39,650
is connected to this point on

377
00:24:39,682 --> 00:24:43,114
the remote net data. So it's like building

378
00:24:43,194 --> 00:24:46,562
pipelines as legos. So you install metadata everywhere.

379
00:24:46,618 --> 00:24:50,354
There is no central component anywhere. You can have centralization points,

380
00:24:50,394 --> 00:24:53,754
but it's the same thing. So you install

381
00:24:53,794 --> 00:24:57,282
the data everywhere. If you want to create centralization points,

382
00:24:57,338 --> 00:25:01,226
you use the same software and you install it as a centralization point

383
00:25:01,330 --> 00:25:04,424
and you, you just appoint the others to push

384
00:25:04,464 --> 00:25:08,056
the matrix, their matrix to that. And that's

385
00:25:08,080 --> 00:25:11,848
it. That's everything actually about metadata.

386
00:25:11,976 --> 00:25:15,800
So if you see in this

387
00:25:15,832 --> 00:25:19,204
example, we have five servers, you install five

388
00:25:20,744 --> 00:25:23,960
agent on all five servers. You understand

389
00:25:24,032 --> 00:25:27,964
that every agent by default is isolated,

390
00:25:28,354 --> 00:25:32,114
is by itself alone, stand alone. So in this case

391
00:25:32,194 --> 00:25:35,314
you have in order to access metrics,

392
00:25:35,354 --> 00:25:38,650
logs, whatever dashboards you need

393
00:25:38,722 --> 00:25:42,866
to hit the IP or the host name of each server

394
00:25:43,050 --> 00:25:46,754
and alerts are dispatched from each server.

395
00:25:46,834 --> 00:25:50,770
So they are standalone. But what you can do there is

396
00:25:50,802 --> 00:25:55,222
that you want to use data cloud to actually all

397
00:25:55,278 --> 00:25:59,006
the agents are connected to the data cloud. But are not

398
00:25:59,070 --> 00:26:02,702
streaming data to the data cloud. So the data cloud only maintains,

399
00:26:02,798 --> 00:26:06,078
ok, this is the list of servers. The guy has five

400
00:26:06,126 --> 00:26:08,674
servers. They have these metrics,

401
00:26:09,614 --> 00:26:12,766
these are the alerts that have been configured, but that's

402
00:26:12,790 --> 00:26:16,794
it. Just metadata about what the agent is actually doing.

403
00:26:17,174 --> 00:26:21,070
And then when you go to view dashboards, the data cloud

404
00:26:21,142 --> 00:26:25,190
queries all the servers behind the scenes, aggregates the

405
00:26:25,222 --> 00:26:28,730
data, and presents unified dashboards

406
00:26:28,922 --> 00:26:32,970
similarly for alerting. So these systems sent

407
00:26:33,162 --> 00:26:37,258
trigger alerts. The agent evaluates

408
00:26:37,306 --> 00:26:41,186
the alerts. It sends to data cloud,

409
00:26:41,370 --> 00:26:44,554
a notification that says, hey, this alert has been triggered.

410
00:26:44,634 --> 00:26:48,034
And a data cloud dispatches email

411
00:26:48,114 --> 00:26:52,606
notifications or pages due to notifications, or whatever notifications

412
00:26:52,790 --> 00:26:56,470
you want in order for you to get notified. We have also

413
00:26:56,502 --> 00:27:00,526
a mobile app where you can get alert notifications

414
00:27:00,590 --> 00:27:04,270
to your mobile if you want to build centralization

415
00:27:04,382 --> 00:27:09,526
points. It works like this. So you

416
00:27:09,550 --> 00:27:13,174
appoint one in a data s six, in this case

417
00:27:13,294 --> 00:27:16,502
as a parent for all the others. And then

418
00:27:16,638 --> 00:27:20,470
all the others can be offloaded.

419
00:27:20,622 --> 00:27:24,356
They don't need to be queried, they don't need to store data, or they

420
00:27:24,380 --> 00:27:28,188
can store data only for maintenance work in

421
00:27:28,196 --> 00:27:31,412
the parent. So we have a replication

422
00:27:31,468 --> 00:27:34,356
feature where if the connection,

423
00:27:34,420 --> 00:27:37,516
for example, between s one and s six gets interrupted,

424
00:27:37,660 --> 00:27:41,740
then s one, the next time it connects to s six,

425
00:27:41,932 --> 00:27:45,148
they will negotiate and it will feel the

426
00:27:45,196 --> 00:27:48,404
missing points, the missing samples from as six,

427
00:27:48,444 --> 00:27:52,380
and then of course continue. And a

428
00:27:52,412 --> 00:27:56,304
hybrid setup looks like this. So you may have

429
00:27:57,004 --> 00:28:00,028
here we have two data centers, data center one, data center two,

430
00:28:00,076 --> 00:28:04,284
and one cloud provider. This means that you can have parents

431
00:28:04,444 --> 00:28:07,916
at each data center, optional, but if you want, you can have them,

432
00:28:08,020 --> 00:28:12,264
and then using the data cloud on top of all of them, to have infrastructure

433
00:28:12,684 --> 00:28:16,684
wide dashboards, even across

434
00:28:17,304 --> 00:28:20,964
different providers, different data centers. Now,

435
00:28:22,184 --> 00:28:26,112
we stress tested the data as a

436
00:28:26,168 --> 00:28:29,960
parent versus Prometheus to understand how

437
00:28:30,112 --> 00:28:34,096
better or worse it is. Actually, we beat Prometheus

438
00:28:34,160 --> 00:28:38,080
in every aspect. So we need one third less cpu, half the

439
00:28:38,112 --> 00:28:42,564
memory, 10% less bandwidth, almost no disk IO,

440
00:28:42,924 --> 00:28:46,700
and the data rights, the samples right at the position they

441
00:28:46,732 --> 00:28:50,508
should be. And it's throttled over time, so it's very gentle,

442
00:28:50,596 --> 00:28:53,624
doesn't introduce any big regional rights,

443
00:28:54,004 --> 00:28:57,424
and we have amazing disk footprint.

444
00:28:58,964 --> 00:29:02,380
The University of Amsterdam did a research

445
00:29:02,492 --> 00:29:05,948
in December 23 to find the

446
00:29:05,996 --> 00:29:09,104
impact of monitoring tools on energy efficient,

447
00:29:10,894 --> 00:29:14,470
the impact of monitoring tools and the energy efficiency of monitoring

448
00:29:14,502 --> 00:29:17,926
tools for docker based systems. They found that the

449
00:29:17,950 --> 00:29:21,166
data is the most efficient agent and

450
00:29:21,350 --> 00:29:24,814
it excelled in terms of performance

451
00:29:24,934 --> 00:29:28,806
impact, allowing containers to run without

452
00:29:28,950 --> 00:29:33,074
any measurable impact due to observability

453
00:29:34,014 --> 00:29:37,354
and data is in the CNCF landscape.

454
00:29:37,894 --> 00:29:42,064
Data does not incubate in CNCF, but we sponsors CNCF

455
00:29:42,134 --> 00:29:45,532
and we support CNCF and we are in

456
00:29:45,548 --> 00:29:49,252
the CNCF landscape and it is the top

457
00:29:49,388 --> 00:29:53,228
project in the CNCF landscape in the observability category

458
00:29:53,396 --> 00:29:56,824
in terms of users love, of course, GitHub stars.

459
00:29:58,484 --> 00:30:03,404
Now let's see how

460
00:30:03,444 --> 00:30:06,980
we do the first thing. That is how we automate everything,

461
00:30:07,052 --> 00:30:10,254
how we manage to install the agent

462
00:30:10,634 --> 00:30:14,266
to detect the data sources, but then it comes

463
00:30:14,330 --> 00:30:18,706
up by itself, dashboards and alerts everything without

464
00:30:18,770 --> 00:30:22,546
you doing anything. So the first thing

465
00:30:22,570 --> 00:30:26,466
that we understood is that we have a

466
00:30:26,490 --> 00:30:30,290
lot in common. So each of us has

467
00:30:30,322 --> 00:30:34,654
an infrastructure that's completely different things

468
00:30:35,354 --> 00:30:39,042
from the outside it seems a completely different thing. But if

469
00:30:39,058 --> 00:30:42,594
you check the components that we will use, so we use the same

470
00:30:42,634 --> 00:30:46,322
database service, the same web servers, the same Linux systems, the similar

471
00:30:46,418 --> 00:30:49,490
disks, similar network interface, et cetera,

472
00:30:49,522 --> 00:30:53,214
et cetera, et cetera. So the components of all

473
00:30:53,554 --> 00:30:56,974
our infrastructure are pretty similar.

474
00:30:57,274 --> 00:31:00,954
So what Netida did is that we

475
00:31:00,994 --> 00:31:05,026
went through and developed a model to

476
00:31:05,090 --> 00:31:09,242
actually describe

477
00:31:09,378 --> 00:31:13,250
the metrics in a way that will allow the automatic

478
00:31:13,362 --> 00:31:16,842
and automated dashboarder to work to visualize all the

479
00:31:16,858 --> 00:31:19,842
metrics. So we developed this needle framework.

480
00:31:19,978 --> 00:31:23,730
Needle stands for nodes, instances, dimensions and labels.

481
00:31:23,922 --> 00:31:28,828
And what we do is that actually we

482
00:31:28,876 --> 00:31:33,100
have a method, let's say, of describing the metrics,

483
00:31:33,292 --> 00:31:37,956
allowing us to have both fully

484
00:31:37,980 --> 00:31:42,224
automated dashboards without any manual intervention

485
00:31:42,564 --> 00:31:46,356
and at the same time fully automated alerts.

486
00:31:46,540 --> 00:31:49,524
All the data alerts are attached to components.

487
00:31:49,644 --> 00:31:53,116
So you say, I want this alert to be attached to all my

488
00:31:53,140 --> 00:31:56,404
disks, I want this alert to be attached to all my web servers,

489
00:31:56,444 --> 00:32:00,626
to all database tables, whatever the instance

490
00:32:00,770 --> 00:32:06,370
of the component is. Now the

491
00:32:06,402 --> 00:32:09,946
result of the middle framework is that it

492
00:32:09,970 --> 00:32:13,194
allows the data to come up. So you install it, you don't do

493
00:32:13,234 --> 00:32:17,210
anything else. It auto detects all the data sources and you

494
00:32:17,242 --> 00:32:21,170
have fully functional dashboards

495
00:32:21,282 --> 00:32:24,558
for every single metric, no exceptions.

496
00:32:24,746 --> 00:32:27,990
And at the same time it allows you to slice and

497
00:32:28,022 --> 00:32:30,278
dice the data,

498
00:32:30,406 --> 00:32:34,198
correlate and slice and dice the data from the UI

499
00:32:34,246 --> 00:32:36,274
without learning a query language.

500
00:32:38,934 --> 00:32:41,154
This is our mission. Unconquelist.

501
00:32:42,454 --> 00:32:47,094
Then the next thing was how to get

502
00:32:47,134 --> 00:32:50,374
rid of the query language. The query language is the

503
00:32:50,414 --> 00:32:53,684
biggest problem of monitoring tools.

504
00:32:53,984 --> 00:32:57,804
Why? Because first it has a

505
00:32:58,184 --> 00:33:02,504
learning curve. You need to learn the query language. And for

506
00:33:02,544 --> 00:33:06,376
most of the cases learning the query language

507
00:33:06,440 --> 00:33:10,096
is hard. Some people can do it, but most of the

508
00:33:10,120 --> 00:33:14,244
monitoring users. Come on, what are you talking about? No is the answer.

509
00:33:15,544 --> 00:33:19,022
The second thing is that when you have a query language,

510
00:33:19,168 --> 00:33:22,346
the whole process of extending,

511
00:33:22,450 --> 00:33:25,858
enhancing the monitoring, the observability

512
00:33:25,946 --> 00:33:29,334
platform goes through a development process.

513
00:33:29,674 --> 00:33:33,658
So you have some people that know the query language and you ask

514
00:33:33,706 --> 00:33:37,386
them to create the dashboard you want. So they have to bake the

515
00:33:37,410 --> 00:33:40,850
dashboards, they have to test the dashboards and then you can use

516
00:33:40,882 --> 00:33:44,570
them. That's a big problem also, because at crisis,

517
00:33:44,682 --> 00:33:49,110
most likely you want to explore

518
00:33:49,262 --> 00:33:52,754
and correlate stuff on the fly without.

519
00:33:53,774 --> 00:33:57,142
Let's do this, let's do that. It should be very

520
00:33:57,198 --> 00:34:00,702
simple, so all people should be fluent.

521
00:34:00,878 --> 00:34:04,246
What we as the data, we had another problem

522
00:34:04,310 --> 00:34:07,966
to solve, mainly because all our visualization is fully

523
00:34:07,990 --> 00:34:12,062
automated. The biggest problem is how

524
00:34:12,238 --> 00:34:15,812
do we make a user, allow a user

525
00:34:15,868 --> 00:34:19,292
that sees a dashboard for the first time to

526
00:34:19,348 --> 00:34:23,012
grasp what the dashboard is about, what every

527
00:34:23,068 --> 00:34:26,756
chart, every metric is about. This was a big challenge

528
00:34:26,820 --> 00:34:30,884
because for most monitoring tools, the chart

529
00:34:30,924 --> 00:34:34,116
is just a time series visualization. It has

530
00:34:34,140 --> 00:34:37,900
some settings like this, like that line chart, area chart, etcetera.

531
00:34:38,052 --> 00:34:41,544
But what is

532
00:34:41,584 --> 00:34:45,528
incorporated in there is never shown.

533
00:34:45,656 --> 00:34:49,520
It's never. So you need to do queries by hand

534
00:34:49,552 --> 00:34:52,960
to actually pivot the data to understand, oh,

535
00:34:53,032 --> 00:34:56,288
this is coming from five servers, or you have to do

536
00:34:56,296 --> 00:35:00,096
the visualization like this. So what we did

537
00:35:00,120 --> 00:35:04,128
in the data, in a data, this chart, it looks like a chart,

538
00:35:04,176 --> 00:35:08,004
like all the charts. So this is, in this case scenario,

539
00:35:08,604 --> 00:35:12,396
an area chart. This chart has of course, an info that we have

540
00:35:12,460 --> 00:35:15,548
added some information about. What is this chart about?

541
00:35:15,596 --> 00:35:19,804
So that people can read some

542
00:35:19,844 --> 00:35:22,704
text to understand, to get a context of the,

543
00:35:23,284 --> 00:35:26,708
of the chart. But then we added these controls.

544
00:35:26,796 --> 00:35:31,100
Now look what happens here. The first is this purple

545
00:35:31,172 --> 00:35:34,654
ribbon. We call it the anomaly Ray ribbon.

546
00:35:34,804 --> 00:35:38,458
So when there are anomalies, they are visualized

547
00:35:38,506 --> 00:35:42,174
in this ribbon, anomalies across

548
00:35:42,554 --> 00:35:46,290
all the matrix. In this case, for example, this comes from

549
00:35:46,362 --> 00:35:50,254
seven nodes, 115 applications,

550
00:35:50,954 --> 00:35:54,706
and there are 32 labels in

551
00:35:54,730 --> 00:35:58,214
there, 32 different labels. Now,

552
00:36:00,544 --> 00:36:04,760
the whole point of this middle ribbon is

553
00:36:04,792 --> 00:36:08,424
to allow people grasp what

554
00:36:08,464 --> 00:36:11,984
is the source. Now let's see it. In this case,

555
00:36:12,024 --> 00:36:15,176
we click the nodes. In the nodes.

556
00:36:15,360 --> 00:36:19,296
When you click the nodes, this dropdown

557
00:36:19,360 --> 00:36:22,704
appears, and this explains the nodes.

558
00:36:22,744 --> 00:36:26,816
The data are coming from the number of instances

559
00:36:26,920 --> 00:36:30,504
at metrics each of the nodes provides to

560
00:36:30,544 --> 00:36:34,728
this chart, and then the

561
00:36:34,816 --> 00:36:38,432
volume contribution of each node. So this node

562
00:36:38,528 --> 00:36:42,976
contributes about 16% of the total volume

563
00:36:43,120 --> 00:36:47,604
of the chart. Whatever we see there, 16% is

564
00:36:47,904 --> 00:36:51,816
from this node. This is the anomaly rate

565
00:36:51,960 --> 00:36:55,512
that each node contributes. And of course, you can

566
00:36:55,568 --> 00:36:58,960
see the raw database values.

567
00:36:58,992 --> 00:37:02,768
What's the mean average and maximum of the raw database

568
00:37:02,816 --> 00:37:04,604
values. Now,

569
00:37:05,664 --> 00:37:09,304
the interesting part is that this is also a filter,

570
00:37:09,464 --> 00:37:14,024
so you can filter some

571
00:37:14,104 --> 00:37:17,152
of the nodes to immediately change the chart.

572
00:37:17,248 --> 00:37:20,344
And of course, the same happens for,

573
00:37:20,764 --> 00:37:24,876
oh, I don't have it, but the same exactly happens for

574
00:37:24,980 --> 00:37:28,068
applications and for labels.

575
00:37:28,236 --> 00:37:32,664
So you can see per label, per application,

576
00:37:33,244 --> 00:37:36,196
what is the volume, what is contribution,

577
00:37:36,300 --> 00:37:39,824
its anomaly rate, what is the minimum average maximum value.

578
00:37:40,724 --> 00:37:44,124
Now, we went a step ahead and we

579
00:37:44,164 --> 00:37:48,120
also added grouping functionality. So this

580
00:37:48,152 --> 00:37:52,280
grouping functionality allows you to select one or

581
00:37:52,352 --> 00:37:57,360
more groups. So in this case, I selected label

582
00:37:57,472 --> 00:38:02,004
device type and dimension. Dimension is written rights.

583
00:38:03,184 --> 00:38:07,536
And you see that I got reads

584
00:38:07,600 --> 00:38:11,464
physical, reads virtual rights, physical rights

585
00:38:11,584 --> 00:38:15,360
virtually. So the idea is that if you can

586
00:38:15,432 --> 00:38:19,360
group by the chart on the fly without knowing

587
00:38:19,432 --> 00:38:23,136
any query language or whatever, you can group by

588
00:38:23,200 --> 00:38:27,444
and get the chart you want with just point and click.

589
00:38:28,424 --> 00:38:31,744
The next important thing with metadata is

590
00:38:31,784 --> 00:38:35,192
that there is this info ribbon

591
00:38:35,328 --> 00:38:38,812
at the bottom that it may

592
00:38:38,868 --> 00:38:42,132
present empty data. Empty data means

593
00:38:42,268 --> 00:38:44,972
that data are missing there.

594
00:38:45,108 --> 00:38:47,784
So unlike most monitoring solutions,

595
00:38:49,004 --> 00:38:52,824
if you have a chart and you have every 10 seconds and

596
00:38:54,124 --> 00:38:57,732
one sample is missing, for most monitoring solutions,

597
00:38:57,788 --> 00:39:01,276
this means nothing. So it will just smooth

598
00:39:01,300 --> 00:39:04,844
it out. It will just collect this point. With that

599
00:39:04,884 --> 00:39:09,002
point. The data, however, works in a bit.

600
00:39:09,178 --> 00:39:13,266
So it needs to collect data every second.

601
00:39:13,410 --> 00:39:17,210
The data runs with the smallest priority

602
00:39:17,282 --> 00:39:20,298
in a system, and this is on purpose.

603
00:39:20,426 --> 00:39:24,002
We want the data to run with the smallest priority,

604
00:39:24,138 --> 00:39:26,774
because if you miss a sample,

605
00:39:27,394 --> 00:39:31,130
it means that your system is severely

606
00:39:31,242 --> 00:39:34,644
overloaded. Since the data could not

607
00:39:34,684 --> 00:39:38,704
connect there, they could not collect the sample.

608
00:39:39,164 --> 00:39:43,372
So gaps is an important aspect of

609
00:39:43,428 --> 00:39:46,812
monitoring in the data world. And we visualize

610
00:39:46,868 --> 00:39:49,864
them and we explain where they come from, etcetera.

611
00:39:51,164 --> 00:39:55,204
Now that's another mission accomplished, to get rid of

612
00:39:55,244 --> 00:39:58,988
the query language and allow people to work

613
00:39:59,036 --> 00:40:02,714
and navigate the dashboard without, without any help and any

614
00:40:02,754 --> 00:40:06,374
preparation and any skills.

615
00:40:06,794 --> 00:40:09,894
Then the next is about machine learning.

616
00:40:11,194 --> 00:40:16,474
Most likely a lot of you have seen this. This is a presentation

617
00:40:16,514 --> 00:40:19,762
that made in 2019 by Google.

618
00:40:19,898 --> 00:40:22,614
And the guy said that, you know what,

619
00:40:24,114 --> 00:40:27,690
ML, it's the bolt here. ML cannot solve

620
00:40:27,802 --> 00:40:31,232
most of the problems. Most people wanted to. So it's not

621
00:40:31,248 --> 00:40:35,136
that the ML cannot be helpful. Is that what people expect

622
00:40:35,200 --> 00:40:38,536
from ML is not the right thing? And we are talking

623
00:40:38,560 --> 00:40:42,480
about not general people, Google DevOps. Google developers

624
00:40:42,512 --> 00:40:45,952
and Google DevOps. So they expected from machine learning to

625
00:40:45,968 --> 00:40:49,208
solve a certain number of problems that, of course cannot

626
00:40:49,256 --> 00:40:52,368
do. So what we

627
00:40:52,416 --> 00:40:56,040
do in a data with machine learning, the first thing

628
00:40:56,072 --> 00:41:00,222
is that we train model per, we train

629
00:41:00,358 --> 00:41:04,734
a model per metric every 3 hours for

630
00:41:04,774 --> 00:41:08,142
6 hours of data. Too complex. We train

631
00:41:08,238 --> 00:41:11,414
18 ML models per time series.

632
00:41:11,574 --> 00:41:14,990
So every time series has 18 models

633
00:41:15,182 --> 00:41:18,630
that are trained based using its past,

634
00:41:18,702 --> 00:41:22,470
its past data. Now the data detects

635
00:41:22,502 --> 00:41:26,406
anomalies in real time. It uses these 18 models, and if all 18

636
00:41:26,470 --> 00:41:30,786
models agree that a collected sample is anomalous,

637
00:41:30,930 --> 00:41:33,414
it just marked as anomalous.

638
00:41:34,554 --> 00:41:38,242
The anomaly bit is stored in the database.

639
00:41:38,378 --> 00:41:41,514
So when we store the anomaly bit in the database,

640
00:41:41,554 --> 00:41:45,494
then we can query for it. So we can do

641
00:41:46,074 --> 00:41:48,954
past queries for anomalies only.

642
00:41:49,034 --> 00:41:52,538
No data, not the value of the samples

643
00:41:52,706 --> 00:41:56,514
of the metric, but the anomaly rate of the metric.

644
00:41:57,534 --> 00:42:01,510
And of course, we use, we calculate host level anomaly

645
00:42:01,542 --> 00:42:05,766
scores, et cetera, which we will see how it works. So this

646
00:42:05,790 --> 00:42:09,534
is the scoring engine that I told you earlier. Netata has a scoring

647
00:42:09,614 --> 00:42:13,526
engine. The scoring engine goes through all

648
00:42:13,630 --> 00:42:18,430
metrics and scores them according to an algorithm.

649
00:42:18,582 --> 00:42:22,298
So let's assume that you see a spike or a dive on the dashboard,

650
00:42:22,426 --> 00:42:26,098
instead of speculating what could be wrong,

651
00:42:26,226 --> 00:42:29,494
and I see this dive in, I don't know my sales,

652
00:42:30,114 --> 00:42:33,914
is it the web server? Is the database server, is the storage? Is the network.

653
00:42:33,994 --> 00:42:38,934
Do I have retransmits? What's wrong? Instead of going through these assumptions,

654
00:42:40,674 --> 00:42:44,490
the scoring engine takes this window that you see the

655
00:42:44,522 --> 00:42:47,840
spike or the dive, so you highlight it, you give it to it,

656
00:42:48,002 --> 00:42:51,908
and the scoring engine goes through all metrics,

657
00:42:52,076 --> 00:42:55,516
across all your servers for that window

658
00:42:55,660 --> 00:42:59,612
to score them for the rate of change or the anomaly rate

659
00:42:59,668 --> 00:43:02,916
or whatever you ask. And then the

660
00:43:02,940 --> 00:43:06,624
data gives you an ordered list of everything,

661
00:43:07,164 --> 00:43:10,884
of the top things that were

662
00:43:10,924 --> 00:43:14,476
scored higher than the others. So your aha moment

663
00:43:14,540 --> 00:43:18,390
or the display that the, I don't know, the network did

664
00:43:18,422 --> 00:43:21,154
that is in the results.

665
00:43:21,614 --> 00:43:25,294
That was a point to flip, actually, the troubleshooting

666
00:43:25,334 --> 00:43:30,062
process. But let's see, overall, what other

667
00:43:30,118 --> 00:43:34,334
uses of that thing. One is this is the data dashboard.

668
00:43:34,414 --> 00:43:37,790
It has a menu where all the metrics are organized. As I said,

669
00:43:37,862 --> 00:43:41,898
everything. All metrics and charts

670
00:43:41,946 --> 00:43:45,174
appear here by default. There's no option to hide something.

671
00:43:46,034 --> 00:43:49,178
So when you click, there is an AR button here.

672
00:43:49,226 --> 00:43:53,346
When you click this button, actually in the data, the scoring engine

673
00:43:53,490 --> 00:43:57,734
gives you an anomaly rate per

674
00:43:58,954 --> 00:44:02,554
section of the dashboard. This allows you, for example, to identify

675
00:44:02,634 --> 00:44:06,214
immediately that you know what. In the system we have 14%

676
00:44:06,554 --> 00:44:09,186
and in application I have 2%.

677
00:44:09,330 --> 00:44:13,234
And you can see immediately the anomaly rate per section

678
00:44:13,314 --> 00:44:16,014
for the visible time frame, always.

679
00:44:16,794 --> 00:44:20,130
So if you pan this to the past, if you go to the past

680
00:44:20,242 --> 00:44:23,134
and click the button, it will do it for that time frame,

681
00:44:25,634 --> 00:44:29,174
the host anomaly rate is

682
00:44:29,514 --> 00:44:32,778
the number of metrics in a host

683
00:44:32,946 --> 00:44:35,850
that are anomalous concurrently.

684
00:44:35,922 --> 00:44:39,978
So a 10% host anomaly rate means

685
00:44:40,146 --> 00:44:45,282
that 10% of the total number of metrics are anomalous concurrently.

686
00:44:45,458 --> 00:44:48,850
And what we do then is that this host

687
00:44:48,922 --> 00:44:52,482
anomaly rate, we visualize

688
00:44:52,538 --> 00:44:56,282
it in a chart like this. If you see this chart,

689
00:44:56,458 --> 00:45:00,098
every dimension of this chart, every line on this chart

690
00:45:00,226 --> 00:45:04,658
is a node. And you see that anomalies,

691
00:45:04,746 --> 00:45:08,224
even across nodes, happen in clusters.

692
00:45:08,384 --> 00:45:12,224
So you see here four nodes concurrently.

693
00:45:12,344 --> 00:45:15,992
You see here one very big spike for one node,

694
00:45:16,048 --> 00:45:20,016
but other nodes concurrently had

695
00:45:20,200 --> 00:45:23,440
anomaly spikes. Now, when you

696
00:45:23,472 --> 00:45:26,736
highlight a spike, then metadata gives

697
00:45:26,760 --> 00:45:30,404
you an ordered list of the things that are related

698
00:45:32,334 --> 00:45:35,566
to that spike. Which other metrics?

699
00:45:35,710 --> 00:45:39,518
Which metrics had most the anomaly rate

700
00:45:39,566 --> 00:45:44,074
within this window. So that's another mission accomplished

701
00:45:44,894 --> 00:45:48,502
on trying to use

702
00:45:48,598 --> 00:45:53,134
AI and machine learning help in the troubleshooting

703
00:45:53,174 --> 00:45:56,446
process and reveal insights that

704
00:45:56,510 --> 00:46:00,262
otherwise go unnoticed. Then it was about

705
00:46:00,438 --> 00:46:04,110
logs. So for logs, everything we saw so far, it was

706
00:46:04,142 --> 00:46:08,254
about metrics. For logs, data has a very similar distributed

707
00:46:08,334 --> 00:46:11,714
approach. So we rely on system djournal.

708
00:46:12,334 --> 00:46:16,814
So instead of centralizing logs to some

709
00:46:16,894 --> 00:46:20,598
database, other database server, Loki or elasticsearch

710
00:46:20,686 --> 00:46:25,192
or splunk or whatever, we keep the data in

711
00:46:25,328 --> 00:46:29,776
system djournal at the place they are, probably already are.

712
00:46:29,920 --> 00:46:33,152
So once the data are there, the data can

713
00:46:33,208 --> 00:46:36,084
query the data directly from that place.

714
00:46:36,584 --> 00:46:40,008
And we found out,

715
00:46:40,056 --> 00:46:44,184
actually I found about the system data journal a

716
00:46:44,224 --> 00:46:48,440
year ago. I realized how good this

717
00:46:48,472 --> 00:46:51,832
thing is. So the first thing with the system, the journal, is that

718
00:46:51,888 --> 00:46:54,874
it is available everywhere. It is secured by design.

719
00:46:55,064 --> 00:46:58,742
It is unique. It indexes all fields and all values.

720
00:46:58,878 --> 00:47:00,874
This is amazing flexibility.

721
00:47:01,694 --> 00:47:05,230
So it works like for logs, it works like

722
00:47:05,262 --> 00:47:08,846
this. Either you have a plaintext file, all the logs

723
00:47:08,870 --> 00:47:11,990
together, doesn't matter, not much you can

724
00:47:12,022 --> 00:47:15,830
do. Then you can put them in low key. In low

725
00:47:15,862 --> 00:47:20,038
key, nothing is indexed, it's just a few labels.

726
00:47:20,126 --> 00:47:23,966
So you create streams that you say, okay, if this is

727
00:47:24,110 --> 00:47:26,714
a and this b and this c and this is d,

728
00:47:27,174 --> 00:47:30,518
four labels. For example, this is the stream

729
00:47:30,646 --> 00:47:34,358
of logs of that thing. And the

730
00:47:34,406 --> 00:47:38,486
number of streams that you have influenced significantly,

731
00:47:38,510 --> 00:47:41,974
of course the performance, the memory footprint, etcetera, etcetera.

732
00:47:42,134 --> 00:47:45,714
So logie is like log files,

733
00:47:46,134 --> 00:47:49,886
almost the same with log files, of course has amazing

734
00:47:49,950 --> 00:47:52,950
disk footprint. On the other side is elastic,

735
00:47:53,022 --> 00:47:56,078
elastic indexes every word that is found

736
00:47:56,166 --> 00:48:00,634
inside all text messages.

737
00:48:01,734 --> 00:48:04,694
Of course it's good and powerful,

738
00:48:04,774 --> 00:48:08,554
but at the same time requires a lot of resources.

739
00:48:10,014 --> 00:48:11,714
This indexing is heavy,

740
00:48:12,454 --> 00:48:16,230
eventually requires more significantly

741
00:48:16,262 --> 00:48:21,550
more resources than the roll logs. System digital is

742
00:48:21,582 --> 00:48:25,774
a balance between the two. So it indexes all fields role values,

743
00:48:25,814 --> 00:48:28,990
but it doesn't split the words. So they hold value as it is.

744
00:48:29,062 --> 00:48:32,654
If it is a string or whatever, it is there. The good

745
00:48:32,694 --> 00:48:35,934
thing is that it has amazing injection performance,

746
00:48:36,054 --> 00:48:39,122
so almost no resources.

747
00:48:39,318 --> 00:48:43,450
And it can also be used to build log centralization

748
00:48:43,562 --> 00:48:47,054
points. Now the Nadir UI looks like this.

749
00:48:47,594 --> 00:48:51,010
It's the typical thing. Kibana is like this, Grafana is like this.

750
00:48:51,122 --> 00:48:55,674
So you have the messages, et cetera, you have the different fields

751
00:48:55,714 --> 00:48:59,570
that have been detected. The good thing about metadata is that

752
00:48:59,762 --> 00:49:03,770
you see even in this presentation it's about 15.6

753
00:49:03,842 --> 00:49:07,704
million log entries. We start sampling

754
00:49:07,864 --> 00:49:11,776
at 1 million. So most other solutions,

755
00:49:11,880 --> 00:49:15,448
Kibana and Grafana Loki,

756
00:49:15,576 --> 00:49:18,744
they sample just the last 5000 entries or something like

757
00:49:18,784 --> 00:49:22,400
this to give you the percentages of how

758
00:49:22,432 --> 00:49:26,256
much a field value is there to what percentage.

759
00:49:26,400 --> 00:49:30,400
So in a data we sample at

760
00:49:30,432 --> 00:49:33,824
least a million or more. And it's fast.

761
00:49:34,924 --> 00:49:38,668
Actually people have complained

762
00:49:38,716 --> 00:49:41,836
in the past that system, the journal CTl

763
00:49:41,900 --> 00:49:45,580
is slow. We submitted patches. We found the problem.

764
00:49:45,732 --> 00:49:49,084
We submitted patches to system D to make,

765
00:49:49,204 --> 00:49:52,344
to make system DJ 14 times faster.

766
00:49:53,364 --> 00:49:55,784
I think they should be merged by now.

767
00:49:56,124 --> 00:49:59,364
And we have the system explorer.

768
00:49:59,404 --> 00:50:02,596
That's a plugin of net data that can query the

769
00:50:02,620 --> 00:50:05,900
logs at the place they are now.

770
00:50:06,052 --> 00:50:09,756
Systemdit journal lacks some tooling to push logs

771
00:50:09,780 --> 00:50:12,588
into it. So I wonder. So guys,

772
00:50:12,676 --> 00:50:16,052
unfortunately my audio died. The microphone died

773
00:50:16,228 --> 00:50:19,516
five minutes before finishing the presentation. So I was

774
00:50:19,540 --> 00:50:24,732
shooting the presentation. I had to leave immediately for a wedding in

775
00:50:24,748 --> 00:50:29,334
a greek island. So here I am in a beautiful greek island.

776
00:50:30,554 --> 00:50:33,374
You see the sea. It's very nice, very nice weather.

777
00:50:33,834 --> 00:50:36,986
So sorry for that. I will shoot the

778
00:50:37,010 --> 00:50:40,762
last five video for you to have audio. I was telling that

779
00:50:40,938 --> 00:50:44,794
system did journal lacks some integration so it's not easy

780
00:50:44,954 --> 00:50:48,034
to extract feeds in a structured way.

781
00:50:48,154 --> 00:50:51,694
Convert plain text files to structured

782
00:50:52,354 --> 00:50:55,654
journal fields structured journal log files

783
00:50:56,324 --> 00:50:59,944
and send them to system the journal. So we created log two journal.

784
00:51:00,724 --> 00:51:04,436
This command, this program tails

785
00:51:04,620 --> 00:51:08,196
clean log files and it can extract

786
00:51:08,380 --> 00:51:12,100
any fields from them using regular expressions.

787
00:51:12,252 --> 00:51:16,260
It can also automatically parse JSON files and log

788
00:51:16,292 --> 00:51:19,804
FMT files and it outputs

789
00:51:19,884 --> 00:51:23,528
systemd native format. This systemd native format

790
00:51:23,576 --> 00:51:27,004
is then sent to systemdcat native,

791
00:51:27,744 --> 00:51:31,604
another tool that we created which

792
00:51:32,184 --> 00:51:35,744
sends it in real time to a local or

793
00:51:35,864 --> 00:51:39,840
a remote system ld. Both of

794
00:51:39,872 --> 00:51:43,688
these tools work on any operating system so they dont require

795
00:51:43,736 --> 00:51:47,256
any special libraries or anything. They are

796
00:51:47,280 --> 00:51:50,624
available on FreeBSD, macOS of course

797
00:51:50,664 --> 00:51:53,074
Linux and even on Windows.

798
00:51:53,814 --> 00:51:58,034
This concludes our work for

799
00:51:58,334 --> 00:52:01,966
making logs a lot easier

800
00:52:02,110 --> 00:52:05,910
and affordable to run. So systemd journal

801
00:52:06,022 --> 00:52:10,750
is very performant today, especially after the patches

802
00:52:10,782 --> 00:52:15,542
that we supplied to system D. And it's

803
00:52:15,598 --> 00:52:19,314
extremely lightweight. Of course, system digital files are

804
00:52:19,474 --> 00:52:23,002
open, so you have all the tools to dump data

805
00:52:23,058 --> 00:52:26,354
from them, etcetera. They are also very secure.

806
00:52:26,394 --> 00:52:29,818
It has been designed to be secure system journal.

807
00:52:29,986 --> 00:52:33,698
So I think that having all

808
00:52:33,746 --> 00:52:37,122
the fun happening at the edge, all the process happening at

809
00:52:37,138 --> 00:52:40,586
the edge in a distributed way actually eliminates

810
00:52:40,610 --> 00:52:45,168
the needs for the need for heavy

811
00:52:45,336 --> 00:52:48,524
logs, centralization and database servers,

812
00:52:49,224 --> 00:52:52,720
and makes logs management a lot more affordable

813
00:52:52,832 --> 00:52:56,564
system. As I said before, support centralization points

814
00:52:57,224 --> 00:53:01,208
and our plugin is able, the data plugin is able

815
00:53:01,296 --> 00:53:05,536
to use the journals

816
00:53:05,560 --> 00:53:09,764
of centralization points to be multinode. So all the logs now are

817
00:53:10,124 --> 00:53:14,024
multiplex. Logs across multiplex are multiplexed.

818
00:53:14,444 --> 00:53:18,164
The next challenge is about going beyond

819
00:53:18,284 --> 00:53:22,124
metrics, logs and traces. So we want metadata to

820
00:53:22,164 --> 00:53:25,764
be a lot more than just metrics, logs and traces. We want

821
00:53:25,804 --> 00:53:29,324
metadata to be a console for any kind

822
00:53:29,404 --> 00:53:33,924
of information. For this we created what

823
00:53:33,964 --> 00:53:37,134
we call functions. So the

824
00:53:37,174 --> 00:53:40,474
functions are used,

825
00:53:41,894 --> 00:53:45,274
functions are exposed by the data plugins.

826
00:53:45,574 --> 00:53:48,742
So the postgres plugin, for example, may expose a

827
00:53:48,758 --> 00:53:54,246
function that says, hey, I can provide the

828
00:53:54,270 --> 00:53:56,474
currently running slow queries.

829
00:53:57,054 --> 00:54:00,154
Similarly, our network viewer

830
00:54:00,534 --> 00:54:04,704
exposes a function that visualizes

831
00:54:04,864 --> 00:54:08,032
all the system active connections,

832
00:54:08,088 --> 00:54:11,960
the connections from containers and all applications

833
00:54:12,032 --> 00:54:15,448
running. This allows the data to be

834
00:54:15,496 --> 00:54:19,200
used as a console tool to

835
00:54:19,232 --> 00:54:22,552
explore any kind of information. It doesn't matter

836
00:54:22,648 --> 00:54:26,032
what the information is, we just can have a custom

837
00:54:26,088 --> 00:54:30,204
visualization or whatever required for this to

838
00:54:30,244 --> 00:54:33,676
work. The tricky part here and the challenge

839
00:54:33,740 --> 00:54:36,964
was the routing. So in order for this to work,

840
00:54:37,124 --> 00:54:40,876
we had to solve the routing problem. Since all functions

841
00:54:41,020 --> 00:54:44,944
provide live information, we had to root

842
00:54:45,524 --> 00:54:49,500
requests through the data servers to the right

843
00:54:49,572 --> 00:54:52,868
server and the right plugin, run the function,

844
00:54:53,036 --> 00:54:56,914
get the result back, and send it to your

845
00:54:56,954 --> 00:55:00,042
web browser, no matter where your web browser is connected,

846
00:55:00,218 --> 00:55:04,186
even the data cloud. This way we

847
00:55:04,210 --> 00:55:08,234
can this is, for example, our network connections

848
00:55:08,274 --> 00:55:11,906
Explorer. You see that there is a visualization

849
00:55:12,010 --> 00:55:15,162
that actually shows all the applications,

850
00:55:15,298 --> 00:55:19,394
the number of connections that they have and the kind

851
00:55:19,474 --> 00:55:21,174
of connections that they have,

852
00:55:22,054 --> 00:55:25,870
listening client outbound,

853
00:55:26,022 --> 00:55:29,486
inbound to private IP address spaces or

854
00:55:29,510 --> 00:55:33,794
the Internet, etcetera. That's another mission accomplished on

855
00:55:34,334 --> 00:55:38,118
creating the mechanics to actually have

856
00:55:38,206 --> 00:55:41,606
any kind of plugins to expose any kind

857
00:55:41,670 --> 00:55:44,926
of information. And the last

858
00:55:44,990 --> 00:55:48,314
part is about our monetization strategy

859
00:55:51,214 --> 00:55:55,134
is open source, but we monetize it through a SaaS through

860
00:55:55,174 --> 00:55:59,110
Netata cloud. So Netdata cloud offers horizontal

861
00:55:59,142 --> 00:56:02,934
scalability. So you can have totally independent data agents,

862
00:56:03,094 --> 00:56:06,790
but all of them appear as one uniform

863
00:56:06,862 --> 00:56:09,754
infrastructure at visualization time.

864
00:56:10,214 --> 00:56:13,194
We added role based access control to it,

865
00:56:13,734 --> 00:56:17,966
allow the ability to access your observability

866
00:56:18,070 --> 00:56:22,154
from anywhere. And of course we have push notifications

867
00:56:22,234 --> 00:56:25,866
for alerts. We have a mobile app for that for iOS and

868
00:56:25,890 --> 00:56:29,214
Android, and a lot more customizability

869
00:56:29,634 --> 00:56:32,494
and configurability via data cloud.

870
00:56:32,794 --> 00:56:36,266
Thank you very much. That was the presentation. I hope you

871
00:56:36,290 --> 00:56:40,090
enjoyed it. I am very sorry for my microphone problem.

872
00:56:40,282 --> 00:56:43,842
I hope I will see you again and I hope you

873
00:56:43,858 --> 00:56:44,970
enjoyed it. Of course.

