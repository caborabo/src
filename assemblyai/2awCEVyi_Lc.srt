1
00:00:24,650 --> 00:00:28,658
Hello everyone. Welcome to my session. My name is Samuel Baruffi

2
00:00:28,754 --> 00:00:32,646
and today I'm here to present a session called

3
00:00:32,748 --> 00:00:36,962
Mastering Generative AI, harnessing AWS Genai

4
00:00:37,026 --> 00:00:40,610
for your solutions. Let's look at a quick agenda.

5
00:00:40,690 --> 00:00:44,374
What I'm going to be covering on my session, I'm going to start

6
00:00:44,412 --> 00:00:48,546
a presentation talking at a very high level about generative AI

7
00:00:48,658 --> 00:00:51,854
and the big impact in the world and also into

8
00:00:51,972 --> 00:00:55,486
applications being built today. Then I'm going to

9
00:00:55,508 --> 00:00:59,534
talk about at the very infrastructure level, what AWS is

10
00:00:59,572 --> 00:01:03,242
doing with our own chipsets called inferential

11
00:01:03,306 --> 00:01:07,102
and trainium, and also talk about a wide variety

12
00:01:07,166 --> 00:01:10,926
of EC two instances with Nvidia graphic

13
00:01:10,958 --> 00:01:14,226
cards. After that I'm going to start talking about more

14
00:01:14,248 --> 00:01:18,434
on the application services and the platforms that you can build models

15
00:01:18,482 --> 00:01:22,210
and use models. So I'm going to talk about Amazon Sagemaker,

16
00:01:22,290 --> 00:01:25,526
I'm going to talk about Amazon Sagemaker Jumpstart and

17
00:01:25,548 --> 00:01:29,002
then I'm going to spend the majority of the time talking about

18
00:01:29,056 --> 00:01:33,142
Bedrock. Bedrock is our foundational models

19
00:01:33,206 --> 00:01:36,698
as a service is the ability for users and companies

20
00:01:36,784 --> 00:01:39,794
and organizations to call a single API,

21
00:01:39,942 --> 00:01:43,354
choose different models from large language models

22
00:01:43,402 --> 00:01:47,946
to text generations from embedding models, and easily receive

23
00:01:47,978 --> 00:01:51,374
the response and actually build solutions on top of that.

24
00:01:51,492 --> 00:01:55,838
Bedrock is a very exciting service that has a lot of features

25
00:01:56,014 --> 00:01:58,818
baked in and being shipped as we speak.

26
00:01:58,904 --> 00:02:01,906
And I'm going to cover some of those features as well.

27
00:02:02,088 --> 00:02:05,886
Then I'm going to jump into a single slide that talks

28
00:02:05,918 --> 00:02:09,714
about vector databases. I'm going to talk what is a vector database,

29
00:02:09,762 --> 00:02:13,446
why it's important for generative AI solutions. And of course I'm going to be

30
00:02:13,468 --> 00:02:17,346
talking about the solutions within the AWS platform that allows

31
00:02:17,378 --> 00:02:21,770
you to run, create and operate vector databases.

32
00:02:22,270 --> 00:02:25,834
Then to finalize the presentation piece of

33
00:02:25,872 --> 00:02:29,642
my session, I'm going to quickly talk about code

34
00:02:29,696 --> 00:02:33,422
Whisper, which is a very exciting tool for developers that

35
00:02:33,476 --> 00:02:37,440
can actually have a companion helping with

36
00:02:38,050 --> 00:02:41,546
generation of code on many different types of programming

37
00:02:41,578 --> 00:02:44,830
languages and also infrastructure as a code.

38
00:02:44,980 --> 00:02:49,002
And then I've done quite a few talks

39
00:02:49,066 --> 00:02:52,946
with comfort e two in the past. I always like to end the session with

40
00:02:52,968 --> 00:02:56,622
a demo. So I'm going to do a demo potentially using bedrock

41
00:02:56,686 --> 00:03:00,674
and showing you how easy it is to use bedrock, some of the functionality

42
00:03:00,722 --> 00:03:04,434
of bedrock, and hopefully showing you with a simple piece of python

43
00:03:04,482 --> 00:03:07,722
code how easily you can call bedrock using

44
00:03:07,776 --> 00:03:11,674
AWS SDK and receive a response from

45
00:03:11,712 --> 00:03:14,540
a specific large language model that you're going to choose.

46
00:03:14,990 --> 00:03:17,020
So let's just get started.

47
00:03:18,110 --> 00:03:21,614
So what is generative AI? Right and how

48
00:03:21,652 --> 00:03:25,646
are those generative AI powered? The most important thing to

49
00:03:25,668 --> 00:03:29,614
be familiar with is, in order for this

50
00:03:29,652 --> 00:03:33,166
generative AI explosion

51
00:03:33,198 --> 00:03:36,814
and evolution, what is actually powering them behind the scenes

52
00:03:36,862 --> 00:03:40,466
is what we call foundational model. Foundational model.

53
00:03:40,648 --> 00:03:44,146
You can think about an AI model that

54
00:03:44,168 --> 00:03:48,046
has been actually pretrained on vast amounts of instruction

55
00:03:48,078 --> 00:03:51,150
data. Those state of art models that are

56
00:03:51,160 --> 00:03:54,754
going to be talking later on in my presentation are known to be trained

57
00:03:54,802 --> 00:03:58,106
across pretty much the whole Internet, right? Like, if you think about

58
00:03:58,128 --> 00:04:01,402
it, like, all the amount of data

59
00:04:01,456 --> 00:04:04,954
that the Internet has, that is actually all the data that those

60
00:04:04,992 --> 00:04:08,106
models are being trained, then it

61
00:04:08,128 --> 00:04:11,902
contains a large amount of parameters that

62
00:04:11,956 --> 00:04:15,566
makes them capable. By learning all this data, which are the

63
00:04:15,588 --> 00:04:20,090
parameters, they make their own word interpretations,

64
00:04:20,170 --> 00:04:23,634
and they actually can learn very complex concepts by

65
00:04:23,672 --> 00:04:27,460
that. And there is a technology that was

66
00:04:28,710 --> 00:04:32,414
invented, I think, 2017, by a group of researchers

67
00:04:32,462 --> 00:04:36,006
from the University of Toronto that

68
00:04:36,028 --> 00:04:40,514
is called transformers, and that is the architecture using neural networks

69
00:04:40,562 --> 00:04:44,582
that allows generative AI and large language models to

70
00:04:44,636 --> 00:04:48,758
actually generate text. The good thing about those models,

71
00:04:48,854 --> 00:04:52,614
those large language models, is they are generic

72
00:04:52,662 --> 00:04:56,086
and general on their own. So it's

73
00:04:56,118 --> 00:04:59,020
not only trained for a specific use case,

74
00:04:59,550 --> 00:05:03,354
it can actually solve a lot of different use

75
00:05:03,392 --> 00:05:06,014
cases. And I'm going to talk in a moment some of the use cases that

76
00:05:06,052 --> 00:05:10,074
companies are starting to build. And these number of use cases

77
00:05:10,122 --> 00:05:13,314
are pretty much infinite because you can think about those

78
00:05:13,432 --> 00:05:17,314
large language models as a model that can answer any type

79
00:05:17,352 --> 00:05:20,782
of question. And of course, we need to evaluate

80
00:05:20,846 --> 00:05:24,354
performance and how big those

81
00:05:24,392 --> 00:05:27,206
models are and how smart they are.

82
00:05:27,308 --> 00:05:30,358
Not every model is exactly the same as others.

83
00:05:30,524 --> 00:05:33,958
So there is that. And we are going to talk about

84
00:05:34,044 --> 00:05:37,830
how AWS will help you choose

85
00:05:37,900 --> 00:05:39,900
different models for your use case.

86
00:05:41,390 --> 00:05:44,934
Another thing that a lot of companies are doing now is customizing

87
00:05:44,982 --> 00:05:48,586
foundational models for their own data. So let's say you

88
00:05:48,608 --> 00:05:52,766
are a financial company and you really want to have a specific model

89
00:05:52,948 --> 00:05:56,174
super well trainium, to know every single piece

90
00:05:56,212 --> 00:06:00,110
of information that your company has. There are different strategies.

91
00:06:00,690 --> 00:06:03,470
You can use retrieval, argument generation,

92
00:06:03,810 --> 00:06:07,106
or you can use agents to actually retrieve the data,

93
00:06:07,208 --> 00:06:11,198
or you can actually customize foundational models. Customizing foundational models

94
00:06:11,214 --> 00:06:14,626
are also known as fine tuning. That is also a capability that

95
00:06:14,648 --> 00:06:18,242
you retrain the model to recalculate

96
00:06:18,306 --> 00:06:21,974
the weights of those models, to add

97
00:06:22,092 --> 00:06:25,606
your data, that potentially your data was private, it was not

98
00:06:25,628 --> 00:06:29,474
available on the Internet. And now we want to actually have those models

99
00:06:29,602 --> 00:06:32,906
also expert on your own data. We are not

100
00:06:32,928 --> 00:06:36,134
going to talk in depth about those strategies,

101
00:06:36,182 --> 00:06:39,482
but that's something that you just need to be aware that on top of having

102
00:06:39,536 --> 00:06:42,654
foundational models, you can actually customize those to

103
00:06:42,692 --> 00:06:46,654
even be more expert on the data that you as

104
00:06:46,692 --> 00:06:49,886
a person or potentially organization have

105
00:06:49,988 --> 00:06:53,194
control of. So some of the use cases

106
00:06:53,242 --> 00:06:56,354
that we've seen the industry actually building.

107
00:06:56,472 --> 00:07:00,066
So there are kind of three main categories, and this is just a very small

108
00:07:00,168 --> 00:07:04,466
amount of use cases that companies are building. But the first category is

109
00:07:04,568 --> 00:07:08,182
you want to enhance customer experience. So using

110
00:07:08,236 --> 00:07:10,310
chat bots, virtual assistants,

111
00:07:11,210 --> 00:07:14,774
conversational analytics, you can personalize a specific

112
00:07:14,972 --> 00:07:18,514
user interaction, because those models are really good at

113
00:07:18,652 --> 00:07:21,978
behaving and answering like humans, even though they are not

114
00:07:22,064 --> 00:07:25,434
humans, but they answer like a human. It makes really

115
00:07:25,472 --> 00:07:28,746
good to actually enhance customer experience, right.

116
00:07:28,848 --> 00:07:32,414
Especially with chatbots. The other section is you can

117
00:07:32,452 --> 00:07:35,726
boost employee productivity and creativity. So you

118
00:07:35,748 --> 00:07:39,022
can think about conversational search. If an employee or

119
00:07:39,076 --> 00:07:42,622
a customer wants to really retrieve an information from

120
00:07:42,676 --> 00:07:46,546
a wide variety of data set, instead of

121
00:07:46,568 --> 00:07:50,562
just doing a quick, simple search, you can actually have

122
00:07:50,616 --> 00:07:54,286
conversational with those documents. You can do summarization,

123
00:07:54,398 --> 00:07:58,178
you can do content creation, you can do code generation like we are

124
00:07:58,184 --> 00:08:01,574
going to be talking about code Whisper. And the other category is

125
00:08:01,612 --> 00:08:04,806
you can also optimize business processes. So let's say you

126
00:08:04,828 --> 00:08:08,274
want to do a lot of document processing, you want to do data documentation.

127
00:08:08,322 --> 00:08:11,810
Let's say you have a specific form that needs to be filled.

128
00:08:11,890 --> 00:08:15,100
Given a specific data, you can join those two

129
00:08:16,270 --> 00:08:19,866
sets of data and ask the model to potentially feel in

130
00:08:19,888 --> 00:08:23,642
a specific way, right? So there is a lot of use cases

131
00:08:23,706 --> 00:08:28,320
that companies are building or have built already to

132
00:08:28,930 --> 00:08:32,742
enhance customer experience, boost employee productivity,

133
00:08:32,826 --> 00:08:36,818
or optimize business processes. So that is just something you need to keep in

134
00:08:36,824 --> 00:08:40,194
mind. Now, in order to run those

135
00:08:40,232 --> 00:08:44,270
models, those models require a lot of computational,

136
00:08:44,430 --> 00:08:48,534
specifically parallel computational. One good thing about

137
00:08:48,652 --> 00:08:53,062
GPUs, graphic process units are that they are

138
00:08:53,196 --> 00:08:56,566
inherent good at calculations that can be run

139
00:08:56,588 --> 00:08:59,882
in parallel. So the reason why I'm saying that

140
00:08:59,936 --> 00:09:03,398
it's really important for us to understand how AWs

141
00:09:03,494 --> 00:09:07,702
have a wide variety of selections of different instances.

142
00:09:07,766 --> 00:09:11,750
You can choose to run those models. So AWS have,

143
00:09:11,920 --> 00:09:15,402
since 2010, a very good partnership with Nvidia.

144
00:09:15,466 --> 00:09:19,258
Nvidia have been the leader of having very powerful

145
00:09:19,434 --> 00:09:23,050
GPUs. And those GPUs are really good for large language models.

146
00:09:23,130 --> 00:09:26,738
You can see here, there are widely variety of instances that offer

147
00:09:26,824 --> 00:09:30,926
specific instances, specific GPUs from Nvidia.

148
00:09:31,038 --> 00:09:34,674
The most powerful one is the P five EC two instance that

149
00:09:34,712 --> 00:09:38,102
comes with the Nvidia age 100 pencil car.

150
00:09:38,236 --> 00:09:41,526
Those are really big GPUs that can

151
00:09:41,548 --> 00:09:45,160
run very big, large language models, and they have been used

152
00:09:45,530 --> 00:09:49,526
for many companies, specifically on the generative AI

153
00:09:49,558 --> 00:09:53,066
solution. But apart from the good

154
00:09:53,088 --> 00:09:56,166
partnership that AWS have with Nvidia,

155
00:09:56,278 --> 00:10:00,782
AWS is also in the front of innovating our

156
00:10:00,836 --> 00:10:04,474
own chipsets. So we have graviton,

157
00:10:04,522 --> 00:10:07,920
which is a general CPU, but we also have

158
00:10:09,890 --> 00:10:12,938
accelerators called trainium and infra.

159
00:10:13,114 --> 00:10:17,534
So trainium is the GPU accelerator

160
00:10:17,662 --> 00:10:21,662
that is focused on giving you better price performance,

161
00:10:21,726 --> 00:10:25,134
up to 40% better price performance than other comparable

162
00:10:25,182 --> 00:10:29,042
GPUs for training your models. So it's a very

163
00:10:29,096 --> 00:10:32,370
specific car that has been built by AWS

164
00:10:32,450 --> 00:10:35,926
to allow you to have a better performance and price when

165
00:10:35,948 --> 00:10:39,866
you're training machine learning models. And then once you have

166
00:10:39,888 --> 00:10:43,094
those models, you need to run inference on top of that. And AWS

167
00:10:43,142 --> 00:10:47,306
have also created inferential chipsets. We have two different

168
00:10:47,408 --> 00:10:51,242
chipsets, inference one and inferient two, that also give you a better

169
00:10:51,296 --> 00:10:54,874
price performance to actually run inference on top of those

170
00:10:54,912 --> 00:10:58,510
models. I'll highly encourage you to just quickly search

171
00:10:58,580 --> 00:11:02,046
about those. There is a lot of good documentation. Feel free to reach out

172
00:11:02,068 --> 00:11:04,420
to me on LinkedIn as well if you have any questions.

173
00:11:04,870 --> 00:11:08,802
Now, let's move from the infrastructure level to actually

174
00:11:08,936 --> 00:11:13,106
platform and services that AWS offers customers

175
00:11:13,288 --> 00:11:16,078
to actually run those models. Right? But in a day,

176
00:11:16,184 --> 00:11:19,382
organizations are looking, how can they run those models and

177
00:11:19,436 --> 00:11:23,670
use those models for all the use cases we've just discussed?

178
00:11:24,010 --> 00:11:28,390
So, the first platform I want to talk is Sagemaker.

179
00:11:28,470 --> 00:11:32,566
So, Sagemaker is the AWS

180
00:11:32,678 --> 00:11:35,830
machine learning AI platform that encompass

181
00:11:35,910 --> 00:11:40,254
a wide variety of features, from building new

182
00:11:40,292 --> 00:11:43,662
data sets, cleaning new data sets, enriching new data

183
00:11:43,716 --> 00:11:47,470
sets, training different models, different machine learning

184
00:11:47,540 --> 00:11:51,518
models, neural networks, you name it. You can actually

185
00:11:51,604 --> 00:11:55,186
run on Sagemaker once you run those models, you can

186
00:11:55,208 --> 00:11:59,554
actually deploy those machine learning models at scale. You actually manage

187
00:11:59,752 --> 00:12:04,370
the computational for you. You have monitoring, you can actually have evaluation.

188
00:12:04,950 --> 00:12:08,706
You can do a roll of the automatically fine tuning and distributed

189
00:12:08,738 --> 00:12:11,654
training for big models on top of that.

190
00:12:11,852 --> 00:12:14,450
And you can actually, after you've trained,

191
00:12:14,530 --> 00:12:17,590
operating all those models on Sagemaker.

192
00:12:18,430 --> 00:12:22,566
This is a very, in the last six years since Sagemaker

193
00:12:22,598 --> 00:12:26,154
was launched, we have introduced many, many new

194
00:12:26,272 --> 00:12:30,182
innovations like automatically model tuning. So you can deploy

195
00:12:30,246 --> 00:12:33,478
and train the model. And as you train the model, we will find

196
00:12:33,504 --> 00:12:36,686
the right parameters to actually fine tune and

197
00:12:36,708 --> 00:12:40,362
tune the model for the better performance of what you're trying to achieve.

198
00:12:40,426 --> 00:12:43,866
And when I mean model, I'm not only talking about large language models,

199
00:12:43,898 --> 00:12:47,234
I'm talking about any type of machine learning model that you want to

200
00:12:47,272 --> 00:12:49,810
actually build, train, and deploy.

201
00:12:50,310 --> 00:12:53,406
Now, when it comes to generative AI,

202
00:12:53,598 --> 00:12:57,542
Amazon Sagemaker has a very specific feature that really

203
00:12:57,596 --> 00:13:01,154
helps developers to get quickly testing

204
00:13:01,202 --> 00:13:05,442
different larger language models. The name of the feature is called Amazon

205
00:13:05,506 --> 00:13:09,782
Sagemaker Jumpstart. Sagemaker Jumpstart

206
00:13:09,846 --> 00:13:13,510
is a machine learning hub with foundational

207
00:13:13,590 --> 00:13:17,754
models made available that you can literally just click and

208
00:13:17,792 --> 00:13:21,282
deploy. It'll give you the recommended

209
00:13:21,366 --> 00:13:25,262
instance types that those models should run, depending on the size

210
00:13:25,316 --> 00:13:29,134
of those models. So right now there are more than hundreds of

211
00:13:29,172 --> 00:13:33,054
different models that have built in algorithms that have

212
00:13:33,092 --> 00:13:36,626
been pre trained with foundational models. So a lot

213
00:13:36,648 --> 00:13:40,334
of those models are available on hugging face. Some of the Amazon Alexa

214
00:13:40,382 --> 00:13:43,410
models are also available for you to deploy.

215
00:13:44,470 --> 00:13:48,134
The good thing is this is all UI and API based with

216
00:13:48,172 --> 00:13:51,702
a single click of buttons or simple API calls. You can actually

217
00:13:51,756 --> 00:13:55,874
have a machine, an EC two machine

218
00:13:56,002 --> 00:13:59,398
running on Sagemaker actually hosting that model,

219
00:13:59,484 --> 00:14:03,546
and all the things that you need to do manually by running

220
00:14:03,648 --> 00:14:07,226
to actually run that model and do inference on that model will be taken care

221
00:14:07,248 --> 00:14:10,390
of for you. We have a lot of notebooks,

222
00:14:10,550 --> 00:14:14,174
Jupyter notebooks that have examples on how you can actually do

223
00:14:14,212 --> 00:14:18,026
that. And the good thing about Sagemaker Jumpstart, some models

224
00:14:18,058 --> 00:14:21,694
that are available on Sagemaker Jumpstart also have

225
00:14:21,732 --> 00:14:24,990
the capability for fine tuning. So if you want to customize

226
00:14:25,150 --> 00:14:28,114
a model, let's say the Falcon model,

227
00:14:28,232 --> 00:14:32,146
180,000,000,000 parameters you want to customize with your

228
00:14:32,168 --> 00:14:35,694
own data, you can go on sage maker Jumpstart and

229
00:14:35,752 --> 00:14:40,018
you have actually an ease walkthrough

230
00:14:40,194 --> 00:14:44,294
way of fine tuning those models. One thing to note here,

231
00:14:44,412 --> 00:14:48,022
that is going to be very important to differentiate it from other services like

232
00:14:48,076 --> 00:14:51,990
bedrock that I'm going to talk in a moment. Sagemaker Jumpstart

233
00:14:52,150 --> 00:14:55,526
helps you run those models, but where you're

234
00:14:55,558 --> 00:14:59,414
running those models is actually on a EC two instance

235
00:14:59,462 --> 00:15:02,634
that you're paying every single minute or

236
00:15:02,672 --> 00:15:05,854
second that you're running those, even though if you might not be using that,

237
00:15:05,892 --> 00:15:10,160
you are actually paying for that instance. Those models are actually running

238
00:15:10,530 --> 00:15:13,946
on your account, on your ECQ, on your sage maker

239
00:15:13,978 --> 00:15:17,330
that behind the scenes runs ECQs, but you're paying for those.

240
00:15:17,400 --> 00:15:20,946
So it's kind of pay as you go, but for

241
00:15:20,968 --> 00:15:24,910
the whole instance that it requires, right? So you're not paying per tokens,

242
00:15:24,990 --> 00:15:28,262
you're paying for the whole instance. That is something that you just want to watch

243
00:15:28,316 --> 00:15:32,150
out because depending on the models, it can be very expensive.

244
00:15:32,730 --> 00:15:36,082
But let's continue our journey regarding generative

245
00:15:36,146 --> 00:15:39,450
AI. So when we look about

246
00:15:39,520 --> 00:15:42,922
what customers are asking for generative AI is

247
00:15:43,056 --> 00:15:46,426
which model should I use for a specific use case? How can

248
00:15:46,448 --> 00:15:50,086
I move quickly? Most customers are

249
00:15:50,128 --> 00:15:53,822
nodding to the exercise of training large

250
00:15:53,876 --> 00:15:57,726
language models or fine tuning. They really have use cases that

251
00:15:57,748 --> 00:16:01,866
they want to boost customer experience or increase employee

252
00:16:01,898 --> 00:16:05,466
productivity. For example. They just want to reinterate run POCs

253
00:16:05,498 --> 00:16:09,394
very quickly. And most important, how can they keep the

254
00:16:09,432 --> 00:16:12,862
data that they're going to be running through those models? Secure and private,

255
00:16:12,926 --> 00:16:16,406
right. That is a very important thing. You have your data. Your data shouldn't be

256
00:16:16,428 --> 00:16:19,926
used to train new models if you don't want to.

257
00:16:20,028 --> 00:16:24,386
And they should be kept secure encrypt by default.

258
00:16:24,578 --> 00:16:28,570
So with all those three questions being asked by customer,

259
00:16:28,720 --> 00:16:32,218
AWS have introduced last year a service called

260
00:16:32,304 --> 00:16:35,820
Amazon Bedrock, which is the easiest way

261
00:16:37,150 --> 00:16:41,094
to build and scale generative AI applications

262
00:16:41,142 --> 00:16:44,906
with foundational models. And we talked about foundational models. Those are the models

263
00:16:44,938 --> 00:16:48,302
that, large language models that are very big and

264
00:16:48,356 --> 00:16:52,346
they can do a lot of general tasks. What does

265
00:16:52,468 --> 00:16:56,366
Amazon bedrock offers you? First, it offers

266
00:16:56,398 --> 00:17:00,050
you a choice, a democratization of

267
00:17:00,120 --> 00:17:03,506
leading foundational models with a single API. And this is one

268
00:17:03,528 --> 00:17:06,694
of the most amazing things about bedrock. You can

269
00:17:06,732 --> 00:17:10,722
use the same service and the same API,

270
00:17:10,866 --> 00:17:14,230
just choosing a parameter of your API

271
00:17:15,130 --> 00:17:18,774
by choosing what model you want. And the model list has

272
00:17:18,812 --> 00:17:22,362
been growing every single month. And you see in a moment

273
00:17:22,416 --> 00:17:27,382
what are the models and model providers that bedrock currently offers.

274
00:17:27,446 --> 00:17:31,146
But you can expect the model list and those capabilities to grow as

275
00:17:31,168 --> 00:17:34,510
we speak. You can also run retrieval

276
00:17:34,850 --> 00:17:37,982
augmented generation on top of that.

277
00:17:38,036 --> 00:17:40,286
And I'm going to keep that on hold for now because I'm going to be

278
00:17:40,308 --> 00:17:43,602
talking about a feature on bedrock that helps you do that.

279
00:17:43,736 --> 00:17:47,134
You can also have agents that execute multiple steps

280
00:17:47,182 --> 00:17:50,558
tasks by running lambda and calling your own APIs

281
00:17:50,654 --> 00:17:54,114
or outside APIs automatically. And most important,

282
00:17:54,232 --> 00:17:57,810
bedrock is security, private and safe.

283
00:18:00,470 --> 00:18:04,278
Every data that you put to bedrock is not going

284
00:18:04,284 --> 00:18:07,890
to be used to train your models. It's encrypted by default

285
00:18:07,970 --> 00:18:11,722
and nobody else has access. This is really important to

286
00:18:11,776 --> 00:18:15,862
keep in mind. You can also have vpc endpoints from bedrock

287
00:18:15,926 --> 00:18:19,498
so the data never leaves your VPC. It goes through your VPC to a

288
00:18:19,504 --> 00:18:22,734
vpc endpoint to bedrock where it hosts the service.

289
00:18:22,932 --> 00:18:27,146
One important thing to note about bedrock, different than Sagemaker

290
00:18:27,178 --> 00:18:31,290
Jumpstart, you pay AWS, you go. There are different pricing

291
00:18:31,370 --> 00:18:34,574
modes on bedrock, but you start with which is the most.

292
00:18:34,692 --> 00:18:38,350
I guess the way we start with bedrock, it's called on demand.

293
00:18:38,430 --> 00:18:41,922
So depending on the large language model, the foundational model

294
00:18:41,976 --> 00:18:45,374
that you pick, you're going to have a price per input

295
00:18:45,422 --> 00:18:48,878
token and output token. When you're talking about text

296
00:18:49,064 --> 00:18:52,466
you have a different pricing mechanisms for image generation.

297
00:18:52,498 --> 00:18:55,462
But for now let's just keep it simple. You're going to pay for that,

298
00:18:55,516 --> 00:18:59,302
right? So it's just the traditional AWS

299
00:18:59,366 --> 00:19:02,860
cloud approach of pay as you go. And that becomes very

300
00:19:03,710 --> 00:19:07,174
promising because instead of paying for big instances

301
00:19:07,222 --> 00:19:10,726
to run those large language models for you, you can experiment,

302
00:19:10,838 --> 00:19:14,334
iterate and create new products very easily by

303
00:19:14,372 --> 00:19:18,942
still keeping your application in your solutions very

304
00:19:18,996 --> 00:19:22,542
cost conscious. So now let's just

305
00:19:22,596 --> 00:19:26,306
quickly talk about some of the what are

306
00:19:26,408 --> 00:19:29,854
the model providers that are available on bedrock?

307
00:19:29,982 --> 00:19:33,410
So the way bedrock is architected is

308
00:19:33,560 --> 00:19:37,078
you have model providers. So those are companies that

309
00:19:37,164 --> 00:19:40,390
have trained foundational models. And each model

310
00:19:40,460 --> 00:19:44,322
provider, we have a different foundational models

311
00:19:44,386 --> 00:19:47,960
available on that. Right. So here you can see

312
00:19:48,750 --> 00:19:52,394
a list of seven different model providers that you can

313
00:19:52,432 --> 00:19:55,434
pick from on Amazon Bedrock AWS,

314
00:19:55,552 --> 00:19:59,254
the date of this presentation. So today is March

315
00:19:59,302 --> 00:20:02,826
eigth 2024. As I'm recording this session,

316
00:20:02,938 --> 00:20:06,526
we currently have seven model providers available for you

317
00:20:06,548 --> 00:20:11,038
on Bedrock. So you have AI 21 that has the Jurassic two

318
00:20:11,204 --> 00:20:14,558
models available for you. Then you

319
00:20:14,564 --> 00:20:18,146
have entropic. And I'm going to talk about entropic in a moment. But they are

320
00:20:18,328 --> 00:20:21,406
state of the art models with a very big performance.

321
00:20:21,518 --> 00:20:25,300
Then you have cohere. With cohere you have both text

322
00:20:25,850 --> 00:20:30,486
large language models and embedding models as well. So if

323
00:20:30,508 --> 00:20:34,374
you want to create embeddings for your vector database, Cohere also offers you

324
00:20:34,412 --> 00:20:38,134
with very performance embedding models. And it

325
00:20:38,172 --> 00:20:41,174
was just introduced I think a couple of weeks ago, Mistral AI.

326
00:20:41,222 --> 00:20:44,614
So you have two different models with Mistral AI. The Mistro

327
00:20:44,742 --> 00:20:48,314
seven D and the mixture mix of

328
00:20:48,352 --> 00:20:52,110
exports which is eight models that are put together

329
00:20:52,180 --> 00:20:55,806
into a single API. Very good performance. Then you also of

330
00:20:55,828 --> 00:20:59,150
course have meta with Leomachu which is an open

331
00:20:59,220 --> 00:21:02,822
science model. Then you have stability

332
00:21:02,906 --> 00:21:06,526
AI. So stability AI is one of the leaders

333
00:21:06,718 --> 00:21:10,734
research labs for image generation. So the stable

334
00:21:10,782 --> 00:21:14,274
diffusion XL 1.0 is a model that allows you to

335
00:21:14,312 --> 00:21:17,682
generate images. So instead of just generating text,

336
00:21:17,736 --> 00:21:20,662
it actually generates image. You input a text,

337
00:21:20,796 --> 00:21:24,226
a cat walking in the park. It will actually generate an image

338
00:21:24,258 --> 00:21:27,722
for you with a cat walking in the park. Then you also

339
00:21:27,776 --> 00:21:31,386
have our own models from Amazon. Those are

340
00:21:31,408 --> 00:21:35,386
called Titan. So Titan models offers you a text to

341
00:21:35,408 --> 00:21:39,414
text model, traditional large language models. It also offers embedding

342
00:21:39,462 --> 00:21:43,034
models and it also offers image generation models.

343
00:21:43,082 --> 00:21:46,734
Right? So it's a set of models available for

344
00:21:46,772 --> 00:21:50,446
you. The very important thing to keep in mind with

345
00:21:50,468 --> 00:21:54,354
bedrock is we are democratizing the ability for people

346
00:21:54,472 --> 00:21:58,386
to consume different models for a specific use case.

347
00:21:58,488 --> 00:22:03,262
And you can go right now on Bedrock webpage

348
00:22:03,326 --> 00:22:07,346
and click on the pricing and you see the different pricings for each model.

349
00:22:07,448 --> 00:22:11,126
Depending on the performance, the size of those models, you might be paying a

350
00:22:11,148 --> 00:22:15,414
specific price. And that is really important because you can now decide how

351
00:22:15,452 --> 00:22:17,720
you want to build your applications. Remember,

352
00:22:18,490 --> 00:22:21,770
it's the single API call and you do not need to manage

353
00:22:21,840 --> 00:22:25,910
any infrastructure. That is something I want to highlight as well. All the infrastructure

354
00:22:25,990 --> 00:22:29,670
and GPUs to actually run those models, which is very complex

355
00:22:29,750 --> 00:22:33,486
and it takes a lot of capacity, is taken care by AWS for

356
00:22:33,508 --> 00:22:36,880
you, and that is something very beautiful that

357
00:22:38,530 --> 00:22:41,520
you should be using and taking benefit of.

358
00:22:42,050 --> 00:22:46,350
Now, I really want to talk about the partnership that we have with entropic.

359
00:22:46,430 --> 00:22:50,670
So entropic is a longtime AWS customer, and entropic

360
00:22:50,750 --> 00:22:54,900
is one of the top leading research

361
00:22:55,290 --> 00:22:59,058
AI companies in the world. And I'm

362
00:22:59,074 --> 00:23:03,314
going to talk about some of the models that they have published

363
00:23:03,362 --> 00:23:06,898
in the last years. But AWS,

364
00:23:07,074 --> 00:23:10,854
Amazon have invested heavily. I think we've announced

365
00:23:10,902 --> 00:23:14,646
a $4 billion investment last year. So entropic

366
00:23:14,678 --> 00:23:18,346
has a very good partnership with Amazon. And the way we showed the

367
00:23:18,368 --> 00:23:22,400
results of that partnership is, well, first of all, let's talk about

368
00:23:23,410 --> 00:23:26,638
the story about entropic very quickly, right? So if

369
00:23:26,644 --> 00:23:30,330
you look here at the timeline, we are in a very fast paced

370
00:23:30,410 --> 00:23:32,830
environment. In 2019,

371
00:23:33,670 --> 00:23:36,850
GPT-2 was launched from OpenAI.

372
00:23:37,510 --> 00:23:41,202
Then some researchers have

373
00:23:41,256 --> 00:23:44,702
published some papers about the performance of transformers.

374
00:23:44,846 --> 00:23:48,518
And GPT-3 was launched sometime in 2020

375
00:23:48,684 --> 00:23:50,760
with Codex as well. Right?

376
00:23:52,010 --> 00:23:55,798
Most of the people that have founded entropic were

377
00:23:55,964 --> 00:23:59,426
employees from OpenAI. So they left on OpenAI

378
00:23:59,458 --> 00:24:02,870
in 2021 and they found Entropic.

379
00:24:03,310 --> 00:24:07,926
You can see how quickly they went from founding

380
00:24:07,958 --> 00:24:12,174
a new research lab and actually publishing very good and

381
00:24:12,212 --> 00:24:16,266
making available very good models. So they published some papers

382
00:24:16,378 --> 00:24:19,760
in 2021. Then in 2022

383
00:24:21,970 --> 00:24:25,650
they finished training clot, right? And they have

384
00:24:25,720 --> 00:24:29,506
something called Constitution AI. I would highly recommend you to search is

385
00:24:29,528 --> 00:24:34,100
their whole way how they take very important

386
00:24:34,550 --> 00:24:38,414
care on safety and alignment for those models.

387
00:24:38,542 --> 00:24:42,440
And then in 2023, they have released the first cloud one model.

388
00:24:43,050 --> 00:24:47,078
Then they have released one of the first companies to release 100,000.

389
00:24:47,164 --> 00:24:50,474
Context window. Context window just means how much text

390
00:24:50,592 --> 00:24:54,042
you can put per request. Then after that,

391
00:24:54,096 --> 00:24:58,230
in 2023, they have released cloud two, which was a big improvement

392
00:24:58,310 --> 00:25:02,346
from cloud one. Then a couple of months after they've

393
00:25:02,378 --> 00:25:05,722
released cloud 2.1 with more improvements and performance.

394
00:25:05,866 --> 00:25:10,000
Then this year, actually last week or this week?

395
00:25:10,450 --> 00:25:13,650
To be honest, Monday this week. They have,

396
00:25:13,800 --> 00:25:17,438
I would say, shocked the industry with very performant

397
00:25:17,614 --> 00:25:21,374
and set of models of three different models

398
00:25:21,422 --> 00:25:25,970
called cloud tree. And I'm going to talk about those. So cloud tree

399
00:25:26,330 --> 00:25:30,386
comes with three different models. The first one is cloudtree haiku.

400
00:25:30,498 --> 00:25:33,990
And you can see these on this graph is cloudtree haiku.

401
00:25:34,330 --> 00:25:37,842
You can see the intelligence is a very performance

402
00:25:37,906 --> 00:25:41,994
model, but most important is a very low cost and

403
00:25:42,032 --> 00:25:44,060
very fast inference model.

404
00:25:44,510 --> 00:25:47,734
Then they have also released cloud tree Sonnet,

405
00:25:47,782 --> 00:25:51,394
which is their mid tier model, which is very, very intelligent.

406
00:25:51,462 --> 00:25:54,894
It beats all the previous quad models in terms

407
00:25:54,932 --> 00:25:59,198
of benchmarks, and it's in the middle when it comes to

408
00:25:59,284 --> 00:26:02,782
cost. As a matter of fact, claw three sonnet is much more

409
00:26:02,836 --> 00:26:06,402
performance than any previous quad models, but is actually

410
00:26:06,456 --> 00:26:09,710
cheaper than cloud two and cloud 2.1 models.

411
00:26:09,790 --> 00:26:13,794
And then of course, client tree, opposite is the most intelligent model

412
00:26:13,992 --> 00:26:18,002
and has actually beat state of the art models

413
00:26:18,066 --> 00:26:21,894
on most benchmarks. And you can see this data just

414
00:26:21,932 --> 00:26:25,910
search cloud tree report paper, you see all those benchmarks

415
00:26:26,330 --> 00:26:29,930
that are available. So now how

416
00:26:30,080 --> 00:26:34,838
this entropic incredible performance

417
00:26:34,934 --> 00:26:38,294
and innovation we call three model impacts

418
00:26:38,342 --> 00:26:43,146
bedrock? Well, because the relationship that Amazon has with entropic

419
00:26:43,338 --> 00:26:47,162
claw three models are already available on bedrock

420
00:26:47,226 --> 00:26:50,814
as we speak right now. Claw three sonnet, which is their

421
00:26:50,852 --> 00:26:54,162
mid tier model, very big performance with

422
00:26:54,216 --> 00:26:58,334
very good price. All these models are multimodal.

423
00:26:58,382 --> 00:27:01,810
Let me just say that multimodal means you can input text,

424
00:27:01,880 --> 00:27:06,162
but also you can input images. Previous quad models could

425
00:27:06,216 --> 00:27:10,354
only receive text as inputs. Those models

426
00:27:10,402 --> 00:27:13,874
can actually receive image as input.

427
00:27:13,922 --> 00:27:16,818
So you can put an image and you can ask questions about that image.

428
00:27:16,834 --> 00:27:20,474
You can actually put multiple images per input. And all those three

429
00:27:20,512 --> 00:27:24,314
models actually have that capability. And all those

430
00:27:24,352 --> 00:27:27,974
models have now an even bigger context window.

431
00:27:28,102 --> 00:27:31,946
Not only they have bigger context window, but the claim

432
00:27:32,058 --> 00:27:35,774
is that with this bigger context window, doesn't matter

433
00:27:35,812 --> 00:27:38,618
where you put the text on those context window,

434
00:27:38,714 --> 00:27:42,446
the performance remains very similar and very good,

435
00:27:42,548 --> 00:27:46,066
which was not actually true in previous models and actually not in

436
00:27:46,088 --> 00:27:49,634
the industry as well. Claw three oppos and cloud

437
00:27:49,672 --> 00:27:53,394
three haiku are going to be made available on bedrock very

438
00:27:53,432 --> 00:27:56,742
soon. They are currently not available, but they're going to be made very

439
00:27:56,796 --> 00:28:00,134
soon. Now that I talked about it,

440
00:28:00,252 --> 00:28:03,506
let's just talk about some of the functionality that bedrock allows

441
00:28:03,538 --> 00:28:05,800
you to use. So first,

442
00:28:06,970 --> 00:28:10,826
you can actually use those foundational models as it is.

443
00:28:10,928 --> 00:28:14,214
But if you really want to fine tune and customize

444
00:28:14,262 --> 00:28:17,414
those models with your data, because you really believe you've

445
00:28:17,462 --> 00:28:21,318
tried rag, you've tried prompt engineering and you're

446
00:28:21,334 --> 00:28:24,186
not achieving the performance your use case require.

447
00:28:24,298 --> 00:28:27,850
In my opinion this should be the last resort. But if you need to privately

448
00:28:27,930 --> 00:28:31,550
customize models, you can actually use right now,

449
00:28:31,700 --> 00:28:35,658
bedrock supports to automatically customize those

450
00:28:35,684 --> 00:28:39,230
models. You put your data on s three in a private s three bucket.

451
00:28:39,310 --> 00:28:43,342
You connect that s three bucket to bedrock and bedrock will automatically

452
00:28:43,406 --> 00:28:47,582
fine tune and customize those models for you. Currently you can customize

453
00:28:47,646 --> 00:28:50,730
models with Titan, Cohere and Lemachu.

454
00:28:50,830 --> 00:28:54,642
Very soon we are going to open the ability to also customize

455
00:28:54,706 --> 00:28:59,510
cloud models and potentially other models from other model providers.

456
00:28:59,850 --> 00:29:03,674
So the good thing about this functionality from bedrock, if you have

457
00:29:03,712 --> 00:29:07,242
done some fine tuning customization from auto in the past, it actually

458
00:29:07,296 --> 00:29:11,290
requires a lot of science and it can be very complex. Bedrock completely

459
00:29:11,360 --> 00:29:14,080
removes that. You just put your data on s three,

460
00:29:15,250 --> 00:29:19,294
you go on bedrock and you point the data from bedrock on

461
00:29:19,332 --> 00:29:23,034
s three and you choose the model. And behind the scenes bedrock, you just customize

462
00:29:23,082 --> 00:29:26,866
new models and you notify when your specific model has

463
00:29:26,888 --> 00:29:30,146
been trained. No one else will be able to use this model.

464
00:29:30,248 --> 00:29:33,826
None of the data that you have actually provided from bedrock on

465
00:29:33,848 --> 00:29:37,826
s three will be used by anyone else or to train other models.

466
00:29:37,858 --> 00:29:41,494
It's just your model. And you can then consume that

467
00:29:41,532 --> 00:29:46,040
model and run inference on that model by

468
00:29:46,410 --> 00:29:49,834
making an API call. The same way you call API from

469
00:29:49,872 --> 00:29:53,066
Bedrock, you can call Bedrock API to

470
00:29:53,088 --> 00:29:55,820
use your own customizable model.

471
00:29:56,270 --> 00:29:59,674
Now another very good thing, if you don't need to

472
00:29:59,712 --> 00:30:03,994
customize your model is actually running retrieval augmented generation.

473
00:30:04,042 --> 00:30:07,486
Retrieval augmented generation, for folks that are not familiar is just

474
00:30:07,508 --> 00:30:10,942
the idea that, let's say you have a big data

475
00:30:10,996 --> 00:30:15,746
set of documents and those documents talk about the

476
00:30:15,768 --> 00:30:19,342
way your company operated and you want to have a chat bot

477
00:30:19,406 --> 00:30:22,670
that actually answer questions about those documentations,

478
00:30:22,750 --> 00:30:25,982
right? Well, those documents are likely private.

479
00:30:26,046 --> 00:30:29,654
So the foundational model, that model providers made it

480
00:30:29,692 --> 00:30:33,810
available on Bedrock, they don't know about your company operational

481
00:30:33,890 --> 00:30:37,746
procedures. But when you're creating a chatbot, you actually want to make that

482
00:30:37,788 --> 00:30:41,386
available for the

483
00:30:41,408 --> 00:30:44,970
model itself to actually consume. So what you can do,

484
00:30:45,040 --> 00:30:48,422
you can use what we call vector databases.

485
00:30:48,486 --> 00:30:52,174
And I'm going to talk in a moment on what vector databases are

486
00:30:52,212 --> 00:30:56,202
made available on AWS. But Bedrock has a feature

487
00:30:56,266 --> 00:31:00,186
called knowledge base that makes all this process of running retrieval

488
00:31:00,218 --> 00:31:03,854
augmented generation very simple. The way it works is

489
00:31:03,972 --> 00:31:07,666
you go on bedrock, you first create an

490
00:31:07,688 --> 00:31:11,086
S three bucket and you put all your documents on this s three bucket.

491
00:31:11,118 --> 00:31:13,778
It's your s three bucket. Nobody has access.

492
00:31:13,944 --> 00:31:18,150
Then you go on bedrock you choose

493
00:31:18,300 --> 00:31:21,410
which model you actually want to run embeddings.

494
00:31:21,490 --> 00:31:24,694
So you can choose between Titan for now, Titan and

495
00:31:24,732 --> 00:31:28,390
cohere embeddings are just going through those documents,

496
00:31:28,550 --> 00:31:32,102
converting those texts into vector numerical,

497
00:31:32,246 --> 00:31:35,974
vector vector representations. And then finally you choose a vector

498
00:31:36,022 --> 00:31:39,942
database. And right now you have a variety of databases

499
00:31:40,006 --> 00:31:43,406
that you can select from. I think there are four options right now

500
00:31:43,428 --> 00:31:46,478
that you can select and those numbers are going to be increasing in the future.

501
00:31:46,564 --> 00:31:50,682
But you can select, for example, the open search serverless vector database.

502
00:31:50,826 --> 00:31:54,806
Then automatically bedrock will run the embeddings

503
00:31:54,938 --> 00:31:58,882
on the data that is on s three will store the vectors on your open

504
00:31:58,936 --> 00:32:02,130
search vector database. And finally, which is,

505
00:32:02,280 --> 00:32:05,654
let's say you want to run this chatbot. When you ask

506
00:32:05,692 --> 00:32:08,710
a question, let's say you ask a question, what is the

507
00:32:08,780 --> 00:32:12,498
HR policy for vacation

508
00:32:12,594 --> 00:32:16,386
in New York as an example, right? What bedrock

509
00:32:16,418 --> 00:32:19,786
can do, it can then retrieve your vector database by running what

510
00:32:19,808 --> 00:32:23,018
we call semantic search. It can find the specific

511
00:32:23,104 --> 00:32:26,554
chunks of text that are very likely to respond my question.

512
00:32:26,672 --> 00:32:30,714
You copy those chunks of text into bedrock and

513
00:32:30,752 --> 00:32:34,586
then you run your question, plus the combination

514
00:32:34,618 --> 00:32:38,238
of chunks of text that has been retrieved from the

515
00:32:38,324 --> 00:32:41,454
database, from the vector database, and you send that to your

516
00:32:41,492 --> 00:32:44,878
foundational model. Then the foundational model, let's say cloud three,

517
00:32:44,964 --> 00:32:48,098
will see all the chunks of text that talks about vacation policy,

518
00:32:48,184 --> 00:32:52,146
New York. And you see your question. And then based on the information that you

519
00:32:52,168 --> 00:32:55,506
have provided, because now the model has access to

520
00:32:55,528 --> 00:32:59,222
the chunks of data that has the answer, will be able to provide an answer

521
00:32:59,276 --> 00:33:02,658
for you. That is what is called retrieval augmented generation.

522
00:33:02,754 --> 00:33:08,106
And you can actually run very simple with

523
00:33:08,128 --> 00:33:12,314
knowledge bases. So that is one capability that

524
00:33:12,352 --> 00:33:15,962
you can run. It's all managed for you and you can choose different

525
00:33:16,016 --> 00:33:19,706
models to actually run. Another functionality is the

526
00:33:19,728 --> 00:33:23,386
ability to enable generative AI applications to execute

527
00:33:23,498 --> 00:33:27,694
steps outside your model. So let's say you have an

528
00:33:27,732 --> 00:33:31,854
API where if

529
00:33:31,892 --> 00:33:36,050
someone on your chat bot asks the question about what is the current

530
00:33:36,120 --> 00:33:40,162
price of this stock, right? The model is not going to be able to answer

531
00:33:40,216 --> 00:33:43,886
that question. Or probably if he answered that question, it's going to hallucinate,

532
00:33:43,918 --> 00:33:47,394
meaning it's not going to be accurate, right? Because the training data

533
00:33:47,432 --> 00:33:50,998
from that model was probably months ago or years ago. What you

534
00:33:51,004 --> 00:33:54,934
can do on bedrock, you can use agents for bedrock. What allows you to

535
00:33:54,972 --> 00:33:58,502
do is to provide. So you select a model, let's say cloud model,

536
00:33:58,636 --> 00:34:02,086
you provide the basic of set instructions,

537
00:34:02,278 --> 00:34:06,330
then you choose different data sources, maybe different APIs,

538
00:34:06,670 --> 00:34:09,482
and then you specify the actions that it can take.

539
00:34:09,536 --> 00:34:13,854
So the example I provided, right, you can say if someone asks you

540
00:34:13,972 --> 00:34:16,750
about the pricing of a stock,

541
00:34:17,090 --> 00:34:21,038
you need to call this API. Here is the open

542
00:34:21,204 --> 00:34:25,358
API spec of my API and this is how you can call the API.

543
00:34:25,454 --> 00:34:29,060
So what agents for bedrock do you ask? A question

544
00:34:29,510 --> 00:34:32,770
for your model. Your model realizes it needs to actually

545
00:34:32,840 --> 00:34:36,242
make a action, take an action on that request.

546
00:34:36,386 --> 00:34:39,858
Behind the scenes, what bedrock will do, we will actually call Lambda,

547
00:34:39,954 --> 00:34:43,078
which is a serverless compute platform

548
00:34:43,244 --> 00:34:47,650
with lambda. The model will actually trigger a lambda.

549
00:34:47,810 --> 00:34:51,914
The lambda code will already be prebuilt. Behind the scenes you call

550
00:34:51,952 --> 00:34:55,850
the API that you have told bedrock to do

551
00:34:55,920 --> 00:34:59,466
and then that API will come with a response, let's say the

552
00:34:59,488 --> 00:35:03,294
value of your stock. And then you return to the larger language model to provide

553
00:35:03,332 --> 00:35:06,686
you with the response. This is just one example, but what you can do with

554
00:35:06,708 --> 00:35:10,110
bedrock, you can break down and orchestrate tasks.

555
00:35:10,850 --> 00:35:14,618
You can invoke whatever API on your behalf so

556
00:35:14,644 --> 00:35:18,014
you can do a lot of automation. And the capabilities

557
00:35:18,062 --> 00:35:22,002
here are really infinite. It's just you configuring those

558
00:35:22,056 --> 00:35:25,734
agents properly so you can do a lot of chain of thought as well

559
00:35:25,772 --> 00:35:29,880
on top of that. So moving on, on the ability that,

560
00:35:30,890 --> 00:35:34,934
what are the ability that we have for making

561
00:35:35,052 --> 00:35:38,694
the responses very secure and safe? On bedrock

562
00:35:38,742 --> 00:35:42,902
we have a functionality that is currently in preview, but it's called guardrails.

563
00:35:42,966 --> 00:35:47,206
What guardrails allows you to do is to create consistently

564
00:35:47,398 --> 00:35:51,306
safeguards, including on your models. Doesn't matter

565
00:35:51,328 --> 00:35:54,430
if they are fine tuned or agents what it does,

566
00:35:54,500 --> 00:35:57,806
you can create filters for harmful content both on

567
00:35:57,828 --> 00:36:01,866
the input that you're sending to bedrock and also the output that bedrock

568
00:36:01,898 --> 00:36:05,138
will tell you, right? So I'll give you an example,

569
00:36:05,224 --> 00:36:08,386
right, let's say the example you

570
00:36:08,408 --> 00:36:12,414
see here on the screen. Let's say someone asks you about investment

571
00:36:12,462 --> 00:36:16,690
and device on your chat bot and you don't want to have that

572
00:36:16,840 --> 00:36:20,118
input and actually output to be sent to the customer.

573
00:36:20,204 --> 00:36:23,526
Right? So what you can do, you can create those filters and you can

574
00:36:23,548 --> 00:36:26,966
say these topics deny and then this

575
00:36:26,988 --> 00:36:30,890
is the response you should be giving back if someone is trying to

576
00:36:30,960 --> 00:36:34,534
ask questions about investment advice. So you don't get into legal

577
00:36:34,582 --> 00:36:37,898
complaints or problems that you might get into the future,

578
00:36:37,984 --> 00:36:41,594
right? So this is one of the capabilities that is available

579
00:36:41,792 --> 00:36:46,122
on bedrock and it's called guardrails. Another functionality

580
00:36:46,186 --> 00:36:49,002
that I'm going to talk about it is batch.

581
00:36:49,146 --> 00:36:52,426
So everything I've talked about so far is you just run an

582
00:36:52,468 --> 00:36:56,130
API call and you receive a response. Pretty much

583
00:36:56,200 --> 00:37:00,162
synchronous, right? API call goes in, API call

584
00:37:00,216 --> 00:37:04,350
comes back. There are some use cases that don't require

585
00:37:04,510 --> 00:37:07,718
live interaction, but you want to run a lot of

586
00:37:07,804 --> 00:37:11,362
inference for a lot of documents in a batch mode.

587
00:37:11,426 --> 00:37:15,202
So what Bedrock can do, its batch

588
00:37:15,266 --> 00:37:19,186
mode allows you to efficiently run inference on the large volumes

589
00:37:19,218 --> 00:37:22,442
of data. So you can put the data on s three, you can put different

590
00:37:22,496 --> 00:37:26,214
json files with the prompt and the data you want to run behind the scenes.

591
00:37:26,262 --> 00:37:30,506
Bedrock, you grab those files, you run the inference, you save the

592
00:37:30,528 --> 00:37:34,126
results of those inference in another s three as the result,

593
00:37:34,228 --> 00:37:37,390
and it's completely managed for you. And once

594
00:37:37,540 --> 00:37:41,118
the batch is completed, you can get notified and you can do a lot

595
00:37:41,124 --> 00:37:44,654
of different automation. So you don't need to write any code for

596
00:37:44,692 --> 00:37:48,218
handling failures or restarts. Bedrock would

597
00:37:48,244 --> 00:37:51,902
take care of that for you. And you can run that with base foundational

598
00:37:51,966 --> 00:37:55,178
models or your custom trainium models

599
00:37:55,214 --> 00:37:58,770
as well. One last nice feature

600
00:37:58,930 --> 00:38:02,658
about Bedrock is model evaluation. It's still in preview,

601
00:38:02,754 --> 00:38:06,370
but model evaluation is a really good feature of Bedrock.

602
00:38:06,450 --> 00:38:09,946
As you saw, Bedrock offers you a wide variety of

603
00:38:09,968 --> 00:38:14,166
model providers and models available. From those models providers,

604
00:38:14,358 --> 00:38:17,626
it can be really complex to evaluate those

605
00:38:17,648 --> 00:38:21,646
foundational models and select the best one. So what model evaluation on

606
00:38:21,668 --> 00:38:25,390
bedrock allows you to do is to choose different

607
00:38:25,460 --> 00:38:29,514
tests. And those evaluation tests can be either automatic

608
00:38:29,642 --> 00:38:34,230
benchmarks that the industry use and bedrock

609
00:38:34,330 --> 00:38:37,950
makes available, but you can also create your own human evaluation.

610
00:38:38,030 --> 00:38:41,950
You can have actually humans evaluating the response from different models

611
00:38:42,030 --> 00:38:45,646
and rating those models without actually knowing which model it is.

612
00:38:45,688 --> 00:38:49,094
So there is no bias into the place. And you can have

613
00:38:49,132 --> 00:38:52,374
your own data sets of questions and you can create your

614
00:38:52,412 --> 00:38:55,960
own custom metrics or use

615
00:38:56,410 --> 00:39:00,298
the metrics that comes with it. So some of the metrics that are there

616
00:39:00,384 --> 00:39:04,262
are the accuracy, are the toxicity and the robustness

617
00:39:04,326 --> 00:39:07,626
of the response. And you can see here a screenshot of a

618
00:39:07,648 --> 00:39:11,826
human evaluation report across different models being tested

619
00:39:11,878 --> 00:39:15,034
and automatic evaluation report. So I've

620
00:39:15,082 --> 00:39:18,106
talked a lot about the different features

621
00:39:18,138 --> 00:39:21,358
that bedrock makes available for you. But one of

622
00:39:21,364 --> 00:39:24,580
the things that is important to highlight is right now,

623
00:39:25,030 --> 00:39:28,942
thousands and thousands of customers are using bedrock

624
00:39:29,086 --> 00:39:31,998
because the capability, the democratization,

625
00:39:32,174 --> 00:39:35,966
the flexibility and the feature set that bedrock allows

626
00:39:35,998 --> 00:39:40,150
them to build generative AI on top of pretty much every single

627
00:39:40,220 --> 00:39:44,018
industry, right? So you can see big names like Adidas,

628
00:39:44,114 --> 00:39:47,046
you can see names like the BMW group,

629
00:39:47,148 --> 00:39:50,666
Salesforce and many, many others so highly encourage you to

630
00:39:50,688 --> 00:39:54,314
test bedrock because it's a really cool feature. Two more

631
00:39:54,352 --> 00:39:58,794
things before we go to the demo is we talked about the

632
00:39:58,832 --> 00:40:03,018
retrieval, augmented generation and the need for vector databases.

633
00:40:03,114 --> 00:40:06,654
And I just want to quickly tell you the story about vector databases on

634
00:40:06,692 --> 00:40:10,062
AWS. AWS has a wide variety of

635
00:40:10,116 --> 00:40:14,114
different databases that support vectors. As you can see here,

636
00:40:14,232 --> 00:40:17,794
we have six databases that are now

637
00:40:17,912 --> 00:40:21,362
supporting vector databases and depending on the use case,

638
00:40:21,416 --> 00:40:24,798
you might choose one versus the other. The important thing here

639
00:40:24,904 --> 00:40:28,390
is to understand that AWS is giving

640
00:40:28,460 --> 00:40:31,686
the flexibility to pick and choose from the

641
00:40:31,708 --> 00:40:35,254
database that makes the most sense for you. So a

642
00:40:35,292 --> 00:40:39,518
very popular database on AWS for vector is opensearch.

643
00:40:39,634 --> 00:40:42,902
So OpenSearch has a functionality for vectors

644
00:40:42,966 --> 00:40:46,742
and you can actually even run OpenSearch serverless for Vector

645
00:40:46,806 --> 00:40:51,274
database that have a very good performance and price. But you can see here documentDB

646
00:40:51,322 --> 00:40:54,766
now has support for vector. MemoryDB for

647
00:40:54,788 --> 00:40:58,298
redis has also support for vector RDS for postgres.

648
00:40:58,314 --> 00:41:02,074
So if you're running a SQL database and you have a relational

649
00:41:02,202 --> 00:41:06,366
use case and you also want to run specific vectors,

650
00:41:06,398 --> 00:41:09,890
you can run pgvector which is a plugin library for

651
00:41:09,960 --> 00:41:13,582
postgres that can run also on top of both RDs

652
00:41:13,646 --> 00:41:16,878
and Aurora postgres. If you're doing graph databases,

653
00:41:16,974 --> 00:41:20,678
you can actually run on top of Netun. And as I talked about it

654
00:41:20,764 --> 00:41:25,074
right now, the direct integration for knowledge databases on bedrock supports

655
00:41:25,122 --> 00:41:28,402
open search, redis, enterprise, cloud and Pinecone.

656
00:41:28,466 --> 00:41:32,346
But very soon Aurora, Amazon, Aurora and MongoDB are

657
00:41:32,368 --> 00:41:35,866
going to be made available as well. So that is

658
00:41:35,888 --> 00:41:39,820
about vector database. The last thing I want to talk about it is the

659
00:41:40,450 --> 00:41:43,934
capability for code generation and code assistant for

660
00:41:43,972 --> 00:41:47,754
developers. So AWS has a service called code Whisper

661
00:41:47,882 --> 00:41:51,840
which is AI powered code suggestion, as you see here

662
00:41:52,470 --> 00:41:55,746
on the small video that has actually let me play

663
00:41:55,768 --> 00:41:59,234
it again, the small video that is

664
00:41:59,272 --> 00:42:04,386
demonstrating here, in this case it's a JavaScript code.

665
00:42:04,488 --> 00:42:07,954
You can provide a single comment, in this case, parse the CSV

666
00:42:08,002 --> 00:42:11,974
string and return a list of songs with positional or position

667
00:42:12,092 --> 00:42:15,746
original chart date, artist title and ignore lines.

668
00:42:15,778 --> 00:42:18,774
We're starting with hashtag, right?

669
00:42:18,972 --> 00:42:23,318
Then you just click tab and it automatically returns the code generation.

670
00:42:23,494 --> 00:42:27,482
This is pretty cool. And the way it works is you just have your

671
00:42:27,536 --> 00:42:31,354
ID and there is support for a variety of vs

672
00:42:31,402 --> 00:42:34,938
code, jetbrains, cloud, nine, lambda,

673
00:42:35,034 --> 00:42:39,722
Jupyter notebooks. There are supports for pretty much all the popular

674
00:42:39,866 --> 00:42:43,882
IDs out there. Install the plugin from AWS

675
00:42:43,946 --> 00:42:47,950
that has code whisper support, then you can receive code suggestions,

676
00:42:48,030 --> 00:42:52,082
and code whisper can actually do more than that. You receive real

677
00:42:52,136 --> 00:42:56,182
time code suggestions for a variety of programming languages like

678
00:42:56,316 --> 00:42:58,680
Java, JavaScript, go.

679
00:42:59,210 --> 00:43:02,706
Net, and many, many others, but not also programming

680
00:43:02,738 --> 00:43:06,578
languages. If you're building infrastructure, AWS, a code terraform or cloud formation,

681
00:43:06,674 --> 00:43:10,522
you can also have suggestions for those. On top of being

682
00:43:10,576 --> 00:43:15,414
an assistant for developers and improving productivity quite significantly,

683
00:43:15,542 --> 00:43:19,594
you can also have a security scam. So the code that is being suggested for

684
00:43:19,632 --> 00:43:23,754
you can actually give you security suggestions

685
00:43:23,882 --> 00:43:28,154
to make sure you're writing actually secure code. And you can also have reference

686
00:43:28,282 --> 00:43:32,286
tracker for different licenses on open source on

687
00:43:32,308 --> 00:43:35,954
the data that has been trained. So if whatever suggestion is being given

688
00:43:35,992 --> 00:43:40,050
to you has been trained from an open

689
00:43:40,120 --> 00:43:44,014
source repository that has a potentially prohibitive

690
00:43:44,142 --> 00:43:48,674
license, you can actually have that warning telling

691
00:43:48,722 --> 00:43:52,086
you. And if you are an enterprise version of code whisper, you could say

692
00:43:52,188 --> 00:43:55,654
developers should never receive recommendation for code that has been

693
00:43:55,692 --> 00:43:58,982
generated on this specific license that is prohibitive

694
00:43:59,046 --> 00:44:02,874
for my business use case one of the great things about

695
00:44:02,912 --> 00:44:07,082
code Whisper is code whisper for individuals are

696
00:44:07,136 --> 00:44:11,440
free. We are only one of the only companies that have an

697
00:44:12,050 --> 00:44:15,726
enterprise grade product that if you're using

698
00:44:15,828 --> 00:44:18,974
for an individual user like not

699
00:44:19,012 --> 00:44:23,194
a company, you can install codebisper created

700
00:44:23,242 --> 00:44:27,246
an AWS builder account. You don't even need to have an AWS

701
00:44:27,278 --> 00:44:31,374
account, you just need to have a login with build

702
00:44:31,422 --> 00:44:34,802
Id we call builder ID. You can use it for free.

703
00:44:34,936 --> 00:44:38,294
Some features are only for enterprise, but most features and most

704
00:44:38,332 --> 00:44:41,670
important feature which is code suggestions are actually available

705
00:44:41,740 --> 00:44:46,486
for free. So I highly encourage everyone to take a look on this and

706
00:44:46,508 --> 00:44:51,114
then hopefully this was a good overview of the

707
00:44:51,152 --> 00:44:55,114
offerings on AWS for generative AI, most important for

708
00:44:55,152 --> 00:44:58,698
bedrock. So I'll pause here and I'll come back

709
00:44:58,784 --> 00:45:03,120
sharing my screen to actually do a presentation and a demo on

710
00:45:03,970 --> 00:45:08,750
how can you utilize some of those functionalities in

711
00:45:08,820 --> 00:45:12,518
the real world? Actually clicking buttons and making API calls

712
00:45:12,554 --> 00:45:16,290
and writing some code. Awesome. So let's quickly

713
00:45:16,360 --> 00:45:19,806
jump into the demo. Very simple. I have logged

714
00:45:19,838 --> 00:45:23,694
in into my AWS account. I can search here the bedrock

715
00:45:23,742 --> 00:45:27,154
service. I'll go and I'll jump inside the bedrock

716
00:45:27,202 --> 00:45:31,186
service. Right now bedrock

717
00:45:31,218 --> 00:45:34,726
is available in a few AWS regions. In this example we are

718
00:45:34,748 --> 00:45:38,422
using North Virginia US east one region. If I click here

719
00:45:38,476 --> 00:45:41,994
on my left side you can see the menu, you can

720
00:45:42,032 --> 00:45:45,754
see some examples how to get start. You can just open those on

721
00:45:45,792 --> 00:45:49,306
playground here. If you click on the provider you can see the

722
00:45:49,328 --> 00:45:52,606
providers that I just actually showed to you on the

723
00:45:52,628 --> 00:45:56,106
presentation. You can see some of the base models.

724
00:45:56,138 --> 00:45:59,374
So each provider, for example entropic here have

725
00:45:59,412 --> 00:46:03,534
the cloud models. You can see all the different cloud models that are currently available.

726
00:46:03,652 --> 00:46:07,634
So I have for example cloud three sonnet, which is the median model that have

727
00:46:07,672 --> 00:46:11,730
just got released this week. Have cloud 2.1, cloud two

728
00:46:11,800 --> 00:46:15,410
and cloud 1.2 instant.

729
00:46:15,990 --> 00:46:18,734
In this case, I don't have any custom model,

730
00:46:18,792 --> 00:46:22,502
but if I had trainium before a custom model,

731
00:46:22,556 --> 00:46:26,438
I would see the list of training models here. If I wanted to customize a

732
00:46:26,444 --> 00:46:29,638
new model, could just go here, create a new fine tune job,

733
00:46:29,724 --> 00:46:33,258
or continue fine tune job. But the thing I want

734
00:46:33,264 --> 00:46:36,986
to show you, you can actually get started and play around and test some of

735
00:46:37,008 --> 00:46:40,550
the models by just going to the playground. So if you look here, the playground,

736
00:46:40,630 --> 00:46:44,026
you have the chat option. And what I really like about the

737
00:46:44,048 --> 00:46:47,726
chat option, I'll just give you first example how you can actually talk to a

738
00:46:47,748 --> 00:46:50,654
model. Let's say you want to talk to Claude and you want to talk to

739
00:46:50,692 --> 00:46:54,398
the new Claude tree model, which is one of the most performance

740
00:46:54,574 --> 00:46:58,734
in the industry. So let's just say, write me a poem

741
00:46:58,862 --> 00:47:02,590
about AWS and its ecosystem.

742
00:47:02,750 --> 00:47:05,938
Just a simple poem here.

743
00:47:06,024 --> 00:47:09,546
So I can put the entry here. Because this is a multi modality

744
00:47:09,598 --> 00:47:13,138
model, I can also put an image, I'll do a demo of an image

745
00:47:13,154 --> 00:47:16,562
in a moment. You can see all the configurations of the hyperparameters,

746
00:47:16,626 --> 00:47:20,150
like temperature, top P, top K. The length

747
00:47:20,230 --> 00:47:23,786
of the output can be controlled here. In this case I'm just

748
00:47:23,888 --> 00:47:27,286
keeping for 2000 tokens maximum.

749
00:47:27,398 --> 00:47:31,100
You can see this on demand. If I click run,

750
00:47:31,550 --> 00:47:35,414
it's actually calling the model and then it's actually generating,

751
00:47:35,462 --> 00:47:39,082
in this case generating a poem for AWS and its ecosystem.

752
00:47:39,146 --> 00:47:42,782
Right. You can see that it's pretty cool. One thing that I really like

753
00:47:42,836 --> 00:47:46,334
about the playground are the following, as it's finishing

754
00:47:46,382 --> 00:47:50,450
generating here, if we click on the three dots on the top menu,

755
00:47:51,110 --> 00:47:54,706
you can export it as JSON and you can see streaming preference because

756
00:47:54,728 --> 00:47:58,406
you are streaming. But the other thing that I like, you can

757
00:47:58,428 --> 00:48:01,990
go down and you can see some model metrics. So you can see this actually

758
00:48:02,060 --> 00:48:05,734
took 15,000

759
00:48:05,852 --> 00:48:09,702
milliseconds. You tell me how many input tokens, how many

760
00:48:09,756 --> 00:48:12,826
output tokens, and this is the cost, it's 0.0.

761
00:48:12,848 --> 00:48:16,682
Because it's less than 0.0, there is like be a zero point

762
00:48:16,816 --> 00:48:20,554
something that this will cost. Right. What I really like about

763
00:48:20,592 --> 00:48:23,806
it on the chat AWS, well, we can compare models. So let's say I want

764
00:48:23,828 --> 00:48:27,390
to compare claw three versus claw 2.1,

765
00:48:27,460 --> 00:48:30,160
right? And I'm going to talk about it here. Let's see.

766
00:48:31,010 --> 00:48:34,690
Talk about the

767
00:48:34,760 --> 00:48:39,218
word economy in the 99

768
00:48:39,304 --> 00:48:42,638
year, right? And I can go and I can run. So it's

769
00:48:42,654 --> 00:48:46,226
going to run both models at the same time and I will

770
00:48:46,248 --> 00:48:49,894
be able to compare the performance of both models, this is just like

771
00:48:49,932 --> 00:48:53,414
by reading them. So let's just wait a little bit.

772
00:48:53,452 --> 00:48:57,014
So you can see here he has outputted. So cloud 2.1

773
00:48:57,052 --> 00:49:00,726
has outputted. I can see here, compare the response,

774
00:49:00,758 --> 00:49:03,946
but I can see down below here how many tokens each one of

775
00:49:03,968 --> 00:49:07,226
them had and so forth. Now you also have

776
00:49:07,248 --> 00:49:10,714
a text playground instead of a chat. It's just like you

777
00:49:10,752 --> 00:49:14,702
send one request and you get the response. What I like about this,

778
00:49:14,756 --> 00:49:17,614
you go here and you select the model. Let me show you what I like

779
00:49:17,652 --> 00:49:21,550
about it. And let's say write a small poem about

780
00:49:21,620 --> 00:49:24,494
New York City, right? Let's just run this.

781
00:49:24,612 --> 00:49:28,130
What I really like about this. So it's streaming back. But the best

782
00:49:28,200 --> 00:49:31,794
thing about it, if I click on the three dots, I can actually see the

783
00:49:31,832 --> 00:49:35,506
API request. And this is actually how I

784
00:49:35,528 --> 00:49:39,574
would actually call this model through API. Right? In this

785
00:49:39,612 --> 00:49:43,206
case it's using AWS CLI, but you can see the message here and

786
00:49:43,228 --> 00:49:46,214
all the formats get properly configured for me.

787
00:49:46,252 --> 00:49:49,930
And in a moment I'll show you some python code on how you can actually

788
00:49:50,000 --> 00:49:52,858
do that. Few more things I want to quickly show.

789
00:49:52,944 --> 00:49:56,860
If you want to generate images, you can actually generate images with

790
00:49:57,710 --> 00:50:01,566
stable diffusion stability, AI and Amazon Titan. So I can say

791
00:50:01,748 --> 00:50:05,150
create an image of a cat in the moon.

792
00:50:05,890 --> 00:50:09,086
Let's just ask this for the

793
00:50:09,108 --> 00:50:12,240
model. Let's see what actually outputs for me.

794
00:50:12,630 --> 00:50:16,034
And then you could do whatever you want with that

795
00:50:16,072 --> 00:50:20,130
image, right? So you can see it's cat with the moon. There's very simple

796
00:50:20,280 --> 00:50:25,790
image. We can say create picture

797
00:50:25,950 --> 00:50:29,906
of a cat that is super realistic.

798
00:50:30,018 --> 00:50:33,334
Let's see if it does more like instead of you saw, there was

799
00:50:33,452 --> 00:50:36,578
more like a paint with the moon in the background.

800
00:50:36,674 --> 00:50:39,526
Let's see if this does. And this is what I'm doing here is just prompt

801
00:50:39,558 --> 00:50:43,366
engineering. I'm not using anything specific. There you go. You can see an image

802
00:50:43,398 --> 00:50:46,742
here that is a more realistic cat image,

803
00:50:46,806 --> 00:50:50,806
right? Remember I talked about some of the other features

804
00:50:50,838 --> 00:50:54,138
like guardrails. You can see the guardrails here, you can create the

805
00:50:54,144 --> 00:50:57,566
guardrails, you can create the knowledge base, you can

806
00:50:57,588 --> 00:51:00,974
create all the agents. Those are the things you can do. One of the things

807
00:51:01,012 --> 00:51:04,482
I want to highlight, if you were to start using Bedrock for the first time,

808
00:51:04,536 --> 00:51:08,386
the first thing I would recommend you doing is actually going model access

809
00:51:08,488 --> 00:51:12,260
and enabling those models to have access on your account.

810
00:51:13,590 --> 00:51:16,786
You don't pay anything for just enabling them, but if you

811
00:51:16,808 --> 00:51:20,390
don't enable them, you can't use, and it's as simple as just going on manage

812
00:51:20,460 --> 00:51:23,686
model access, selecting the models you want. In my account you can see

813
00:51:23,708 --> 00:51:27,110
I have access to all those models, right? So it's pretty straightforward.

814
00:51:27,530 --> 00:51:30,538
But now let's jump in into some code, right?

815
00:51:30,704 --> 00:51:34,218
Likely most people that are here watching

816
00:51:34,304 --> 00:51:38,298
my session are probably developers or people that do code.

817
00:51:38,384 --> 00:51:42,220
So how can I actually call those models on bedrock using

818
00:51:42,750 --> 00:51:46,846
a programmatic way. So the first example here I'll show you is just calling

819
00:51:46,948 --> 00:51:50,766
cloud, right? So you can see I import bodo tree which is the

820
00:51:50,788 --> 00:51:54,594
AWS ssdk for python. And then I

821
00:51:54,632 --> 00:51:58,018
instantiated the bedrock runtime from the SDK. You can

822
00:51:58,024 --> 00:52:01,342
see the bedrock runtime in the region. Here is the payload.

823
00:52:01,406 --> 00:52:05,534
So I'm providing the model version.

824
00:52:05,582 --> 00:52:08,898
So this is quadrisoned then the body.

825
00:52:08,984 --> 00:52:12,950
Each model have a specific body and format that the model

826
00:52:13,020 --> 00:52:16,530
providers have configured. And you can see that in the documentation.

827
00:52:16,610 --> 00:52:20,780
I can show you the link in a moment. But once you have actually this,

828
00:52:22,590 --> 00:52:26,218
you create a model. In this case you create a prompt. In this case just

829
00:52:26,224 --> 00:52:29,526
saying write a text about going to the moon

830
00:52:29,558 --> 00:52:33,520
and its technical challenges. Right? Then I create

831
00:52:33,970 --> 00:52:37,998
that payload into JSON and then finally call the single API that

832
00:52:38,004 --> 00:52:41,802
I've talked to you about, bedrock. So we always call bedrock

833
00:52:41,946 --> 00:52:45,714
invoke model. And this is not using streaming. I'll show any

834
00:52:45,752 --> 00:52:49,710
streaming version in a moment. And then I wait for the response.

835
00:52:49,870 --> 00:52:53,374
I parse the response into JSON and then I print a response

836
00:52:53,422 --> 00:52:56,946
where exactly the text from the

837
00:52:56,968 --> 00:53:00,578
response is. So if I go on and run pythoncloud

838
00:53:00,674 --> 00:53:04,342
py, I'm going to call behind the scenes that's actually

839
00:53:04,396 --> 00:53:08,258
calling bedrock, sending the payload that I request to claw

840
00:53:08,274 --> 00:53:12,218
tree. Running the prompt. Remember my prompt is talked about the

841
00:53:12,224 --> 00:53:15,594
technical challenges about going to the moon. I write a text about

842
00:53:15,632 --> 00:53:19,180
that. So once he actually returns the text from

843
00:53:19,550 --> 00:53:23,238
bedrock then I will be able to just see the text. And there

844
00:53:23,264 --> 00:53:26,382
you go. You see it wasn't streaming. So it's a pretty long

845
00:53:26,436 --> 00:53:29,934
text saying embarking on a journey to the moon. Present multitude of

846
00:53:29,972 --> 00:53:33,566
technical challenges. Not going to evaluate this but you get the

847
00:53:33,588 --> 00:53:37,410
gist, right. So this is example one. The second example is calling

848
00:53:37,480 --> 00:53:40,770
the same bedrock API but for a different model.

849
00:53:40,840 --> 00:53:44,370
So you can see here I'm also invocating a bedrock.

850
00:53:45,750 --> 00:53:49,170
And then I'm just saying can you write me a poem about apples?

851
00:53:49,250 --> 00:53:52,306
Right? So let's just call this python

852
00:53:52,338 --> 00:53:55,794
three titan Ui. So now this is calling the Amazon

853
00:53:55,842 --> 00:54:00,014
Titan text model. And you can see it was very simple poem

854
00:54:00,082 --> 00:54:03,738
about apple. Now there might be applications that

855
00:54:03,744 --> 00:54:07,242
you're trying to build that require streaming. Like a chatbot, you don't want to make

856
00:54:07,296 --> 00:54:11,114
the user waiting for, I don't know, like a minute to get

857
00:54:11,152 --> 00:54:15,002
a response back. Sometimes those models take a while to finalize

858
00:54:15,066 --> 00:54:18,638
the whole text completion so you can do streaming. So in this case,

859
00:54:18,724 --> 00:54:21,934
very similar to what I've done before. This is a demonstration of

860
00:54:21,972 --> 00:54:24,994
using clotry sonnet, but with a streaming right.

861
00:54:25,192 --> 00:54:29,058
So I'm not using multimodality yet. This is just text. So I have

862
00:54:29,064 --> 00:54:32,450
an input text and you see this just creating

863
00:54:33,590 --> 00:54:37,326
the input payload. Then you can see here the API

864
00:54:37,358 --> 00:54:40,726
that I call is just a little bit different. It is the invoke model with

865
00:54:40,748 --> 00:54:43,814
response streaming. So what bedrock does,

866
00:54:43,932 --> 00:54:47,238
as soon as you start receiving some chunks of text from the model,

867
00:54:47,324 --> 00:54:50,502
we actually output back for you. And then here it's just like,

868
00:54:50,556 --> 00:54:54,214
as you get the response, just display that for me, just do a print

869
00:54:54,262 --> 00:54:57,866
on the console for me. And then on my main here, I'm finally

870
00:54:58,048 --> 00:55:01,726
providing some model IDs and providing the prompt. In this case,

871
00:55:01,828 --> 00:55:05,680
what can you tell me about the, what can you tell me about

872
00:55:06,130 --> 00:55:10,026
brazilian economy? And then I'm

873
00:55:10,058 --> 00:55:13,906
just starting the border tree with bedrock and then calling the

874
00:55:13,928 --> 00:55:17,010
function that I created above. So if you go here,

875
00:55:17,080 --> 00:55:21,730
clean the screen and you go cloud streaming,

876
00:55:23,350 --> 00:55:26,050
and we try to run, oh,

877
00:55:26,120 --> 00:55:29,806
sorry, python three, apologies for that. Python three cloud

878
00:55:29,848 --> 00:55:33,766
streaming. So it's invoking my row and you can see now it's streaming the

879
00:55:33,788 --> 00:55:37,106
response back and it's actually getting the response about the brazilian

880
00:55:37,138 --> 00:55:39,938
economy. And you can see here it finalized.

881
00:55:40,034 --> 00:55:43,286
So I even predicting, like, okay, why it stopped

882
00:55:43,318 --> 00:55:46,666
because it was end of the turn. It finalized the response and also how

883
00:55:46,688 --> 00:55:50,826
many output tokens you got. So that is just one example. The other example

884
00:55:50,928 --> 00:55:54,830
is I want to use clotrisonet because it's a multimodal

885
00:55:55,250 --> 00:55:58,330
large language model that also accepts

886
00:55:58,410 --> 00:56:02,074
images as inputs. So what I want to do, I have this image

887
00:56:02,122 --> 00:56:05,634
of a very cute cat. I want to provide this image to the model.

888
00:56:05,752 --> 00:56:09,300
And you see here what I'm doing. Very similar again,

889
00:56:09,830 --> 00:56:13,886
but now what I'm actually doing, I'm receiving the cat image.

890
00:56:13,998 --> 00:56:18,094
I'm encoding that on base 64. Then I'm providing

891
00:56:18,142 --> 00:56:21,858
on my messages for clot tree as a content. You can see I'm

892
00:56:21,874 --> 00:56:25,874
providing now an image and a text. So the first I'm providing the image

893
00:56:25,922 --> 00:56:29,558
AWS base 64 mastering. And then I'm saying as the

894
00:56:29,564 --> 00:56:32,746
prompt, write me a detailed description of this photo and

895
00:56:32,768 --> 00:56:36,314
then upon talking about it. So that's the request. And if you see down

896
00:56:36,352 --> 00:56:39,930
below here, I'm invoking the model. So this is not using

897
00:56:40,000 --> 00:56:44,006
streaming. Remember the example before was using streaming. In this case it's

898
00:56:44,038 --> 00:56:47,406
not using streaming. So it's going to send everything, it's going to process the

899
00:56:47,428 --> 00:56:51,214
response. Once the response is finalized it's actually going to show me and

900
00:56:51,252 --> 00:56:54,706
it's going to just print the result. Let's just quickly do

901
00:56:54,728 --> 00:56:58,866
this prod multimodality again.

902
00:56:58,968 --> 00:57:02,162
I keep using Python two instead of Python three.

903
00:57:02,216 --> 00:57:05,780
Apologies for that. Now I'm going to run Python three.

904
00:57:06,170 --> 00:57:10,978
Hopefully this is going to start printing

905
00:57:11,074 --> 00:57:14,694
the whole description. And you can see here the image show

906
00:57:14,732 --> 00:57:18,834
a close up portrait of a cat with striking green eyes and a sweet

907
00:57:18,882 --> 00:57:22,234
brownish gray fur coat. The cat says face a

908
00:57:22,272 --> 00:57:25,994
slightly stern, yet alert and yet alert and

909
00:57:26,032 --> 00:57:29,942
attentive expression. So it talks about the cat in the image. It's very accurate.

910
00:57:30,006 --> 00:57:33,446
And then like I said, he writes a poem, emerald depth gaze.

911
00:57:33,478 --> 00:57:36,910
So king blah blah blah blah blah, he talks about it. So you can see

912
00:57:36,980 --> 00:57:40,234
bedrock is amazing because with very simple API

913
00:57:40,282 --> 00:57:43,854
calls, I can call different models with different configurations with different

914
00:57:43,892 --> 00:57:47,314
I parameters. And this is pay as you go. All what I've done

915
00:57:47,352 --> 00:57:50,466
here is probably less than a penny because it's all

916
00:57:50,488 --> 00:57:54,242
on demand. I'm not paying for any provisioned capacity because I don't need

917
00:57:54,296 --> 00:57:57,686
in this example. Last thing is I'll recommend if you want

918
00:57:57,708 --> 00:58:01,670
to look for some of the code that I've used. I based myself

919
00:58:01,740 --> 00:58:05,778
on this GitHub public repository called Amazon

920
00:58:05,874 --> 00:58:09,686
bedrock samples. You can go here introduction to bedrock and

921
00:58:09,708 --> 00:58:12,986
you can see some examples. For example cloud tree. You can see the

922
00:58:13,008 --> 00:58:16,906
example for cloud know with the image. This is the one that I've used

923
00:58:17,008 --> 00:58:20,538
so highly recommend you get in there. And last, if you really want to

924
00:58:20,624 --> 00:58:23,886
look in more in detail into each model and the hyperparameters on

925
00:58:23,908 --> 00:58:27,626
how you call, you can go on the AWS bedrock documentation.

926
00:58:27,738 --> 00:58:31,674
Within the foundational model submenu we have the model inference

927
00:58:31,722 --> 00:58:34,906
parameters and when you click on specific models,

928
00:58:34,938 --> 00:58:38,526
for example cloud, you can see the different cloud completion and

929
00:58:38,548 --> 00:58:42,334
cloud messages API. On the messages you can see here, you can see

930
00:58:42,372 --> 00:58:46,102
some code examples. So you have a very descriptive documentation for

931
00:58:46,156 --> 00:58:49,842
you as a developer to actually take a look and deep dive.

932
00:58:49,986 --> 00:58:53,302
So that is all I had to show for today. Hopefully it was very

933
00:58:53,356 --> 00:58:56,966
useful. Feel free to connect with me via LinkedIn on Twitter if you have any

934
00:58:56,988 --> 00:59:00,722
questions. And happy coding, happy genai applications

935
00:59:00,786 --> 00:59:03,590
and I hope you find this useful. Thank you so much.

