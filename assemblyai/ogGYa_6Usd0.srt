1
00:00:25,010 --> 00:00:28,946
Hi and welcome to our presentation and demo about building internal development

2
00:00:28,978 --> 00:00:32,406
platforms, leveraging the power of Kubernetes operators my

3
00:00:32,428 --> 00:00:36,066
name is Dan McKean and I'm a product manager at MongoDB and I'm joined

4
00:00:36,098 --> 00:00:39,954
by George Hanzaris, who's an engineering director also at MongoDB.

5
00:00:40,082 --> 00:00:44,194
We're responsible for enabling our customers and users to run MongoDB

6
00:00:44,322 --> 00:00:48,002
on or through Kubernetes. Throughout this presentation,

7
00:00:48,066 --> 00:00:52,074
we're going to talk about the advanced in internal developer

8
00:00:52,122 --> 00:00:55,578
platforms, how Kubernetes

9
00:00:55,674 --> 00:00:58,766
has become the standard tool for the

10
00:00:58,788 --> 00:01:02,630
underlying infrastructure for building internal developer platforms,

11
00:01:02,730 --> 00:01:05,010
how Kubernetes can be extended.

12
00:01:05,430 --> 00:01:09,102
And finally, we're going to see how by extending Kubernetes,

13
00:01:09,166 --> 00:01:12,466
we can build platform capabilities and a

14
00:01:12,488 --> 00:01:16,120
short demo, how we build a database as a service.

15
00:01:17,050 --> 00:01:20,422
We're going to start by looking at internal development platforms and

16
00:01:20,476 --> 00:01:24,006
platform engineering. The term platform has been around for a

17
00:01:24,028 --> 00:01:27,906
while. Its most basic definition is a foundation

18
00:01:28,018 --> 00:01:31,210
that developers can use to build software applications.

19
00:01:31,710 --> 00:01:34,874
It provides a set of tools and services that make it easier to

20
00:01:34,912 --> 00:01:37,450
develop, deploy and manage applications.

21
00:01:37,870 --> 00:01:41,726
But the term internal developer platform has sprung up as a

22
00:01:41,748 --> 00:01:45,310
new term in recent years to take things further.

23
00:01:45,810 --> 00:01:49,562
According to the CNCF, this should include reducing

24
00:01:49,626 --> 00:01:53,230
the cognitive load on developers and product teams by

25
00:01:53,300 --> 00:01:56,754
providing them with a set of tools and services that they can use

26
00:01:56,792 --> 00:02:00,686
to build, deploy and manage applications. These are often described

27
00:02:00,718 --> 00:02:04,334
as golden paths. It aims to provide a consistent

28
00:02:04,382 --> 00:02:08,870
experience for developers, which can help to improve productivity and reduce errors.

29
00:02:09,210 --> 00:02:12,614
And lastly, it can be designed with the user in mind and should

30
00:02:12,652 --> 00:02:16,630
be, which can make them more user friendly and efficient.

31
00:02:17,050 --> 00:02:20,362
Platform engineering, unsurprisingly, is the effort that goes into

32
00:02:20,416 --> 00:02:23,994
designing, building and iterating and maintaining such

33
00:02:24,032 --> 00:02:28,054
a platform. So a platform offers and composes

34
00:02:28,102 --> 00:02:31,642
capabilities and services from many supporting infrastructure

35
00:02:31,706 --> 00:02:33,790
or capability providers.

36
00:02:34,130 --> 00:02:37,978
Platforms bridge the gap between underlying capability

37
00:02:38,074 --> 00:02:41,486
providers and platform users such as application

38
00:02:41,588 --> 00:02:45,102
developers, and in the process they implement

39
00:02:45,166 --> 00:02:48,546
and enforce desired practices that

40
00:02:48,568 --> 00:02:52,066
we call golden paths platforms capabilities may

41
00:02:52,088 --> 00:02:55,954
comprise several features of an IDP, meaning aspects

42
00:02:56,002 --> 00:03:00,022
or attributes of a platform that users can use, such as

43
00:03:00,076 --> 00:03:02,870
observability, tooling, managed databases,

44
00:03:04,330 --> 00:03:07,850
secrets management and more. As platform

45
00:03:07,920 --> 00:03:12,054
interfaces, we describe the ways that platform capabilities are exposed

46
00:03:12,102 --> 00:03:16,330
to users of an IDP. These can include a CLI,

47
00:03:16,990 --> 00:03:20,286
a Githubs workflow using a tool like argo, CD or

48
00:03:20,308 --> 00:03:24,190
flux, or developer portals like backstage.

49
00:03:24,690 --> 00:03:28,190
So now we're going to look at the rise of Kubernetes in

50
00:03:28,260 --> 00:03:31,614
platform engineering. While there's not that much

51
00:03:31,652 --> 00:03:35,342
data to categorically prove that the majority of IDPs

52
00:03:35,406 --> 00:03:39,266
are built on Kubernetes. It does seem to be the consensus in the industry that

53
00:03:39,288 --> 00:03:43,010
Kubernetes is winning as the platform of platforms.

54
00:03:43,670 --> 00:03:47,042
Reports from reputable industry analysts like Gartner,

55
00:03:47,106 --> 00:03:51,474
Forrester or IDC often discuss the role of container orchestration

56
00:03:51,602 --> 00:03:54,920
and specifically Kubernetes in the IDP space.

57
00:03:55,530 --> 00:03:58,538
Many IDP vendors highlight their use or support of

58
00:03:58,544 --> 00:04:02,086
Kubernetes on their websites, and many publish case studies

59
00:04:02,118 --> 00:04:05,334
that mention Kubernetes usage within the IDP

60
00:04:05,382 --> 00:04:09,194
implementation. But why is Kubernetes making such

61
00:04:09,232 --> 00:04:11,870
a powerful impact for IDPs?

62
00:04:12,450 --> 00:04:15,850
Firstly, containerization. Kubernetes unsurprisingly

63
00:04:15,930 --> 00:04:19,226
excels at managing containerized applications.

64
00:04:19,418 --> 00:04:23,374
IDPs, by their nature, often involve integrating various

65
00:04:23,422 --> 00:04:27,326
services and tools, and Kubernetes makes this easier by packaging

66
00:04:27,358 --> 00:04:31,054
these components as containers, making them portable and easier

67
00:04:31,102 --> 00:04:33,650
to deploy and manage across different environments.

68
00:04:34,390 --> 00:04:37,506
IDPs can experience fluctuating workloads depending

69
00:04:37,538 --> 00:04:41,842
on their integration needs. Kubernetes facilitates automatic

70
00:04:41,906 --> 00:04:45,362
scaling of resources up and down based on demand,

71
00:04:45,506 --> 00:04:49,290
which ensures efficient resource usage and optimal performance.

72
00:04:49,950 --> 00:04:53,702
IDPs often orchestrate complex workflows involving

73
00:04:53,766 --> 00:04:57,382
multiple services. Kubernetes excels there automating,

74
00:04:57,446 --> 00:05:00,814
deployment, scaling and networking of these services within

75
00:05:00,852 --> 00:05:04,906
the IDP. Kubernetes offers

76
00:05:04,938 --> 00:05:08,814
a platform agnostic approach, allowing IDPs to run on

77
00:05:08,852 --> 00:05:12,314
various infrastructure platforms, cloud on premise

78
00:05:12,362 --> 00:05:16,078
or hybrid, without needing to rewrite the code for each app environment.

79
00:05:16,174 --> 00:05:19,838
This gives flexibility and declarative management.

80
00:05:19,934 --> 00:05:23,294
Kubernetes inherently uses a declarative approach where you specify

81
00:05:23,342 --> 00:05:27,154
the desired state of your application and Kubernetes manages achieving

82
00:05:27,202 --> 00:05:30,866
and maintaining that state. This simplifies IDP management

83
00:05:30,898 --> 00:05:33,682
and reduces configuration errors. In essence,

84
00:05:33,746 --> 00:05:37,994
Kubernetes provides a robust container orchestration platform that

85
00:05:38,032 --> 00:05:41,450
simplifies IDP development, deployment management,

86
00:05:41,790 --> 00:05:45,542
offering scalability, flexibility and efficient resource

87
00:05:45,606 --> 00:05:49,162
usage. But Kubernetes hasn't always

88
00:05:49,216 --> 00:05:52,574
been ready to be the platform of platforms. There's been a few

89
00:05:52,612 --> 00:05:56,366
key developments. Originally, Kubernetes focused just on

90
00:05:56,388 --> 00:06:00,058
container orchestration, but huge improvements in extensibility

91
00:06:00,154 --> 00:06:03,738
have meant it could provide a platform for far more like databases,

92
00:06:03,834 --> 00:06:07,826
security systems and other cloud native components as

93
00:06:07,848 --> 00:06:11,358
well as extensibility. The CNCF has fostered

94
00:06:11,454 --> 00:06:15,166
an amazing, rich ecosystem of tools and services that

95
00:06:15,208 --> 00:06:17,510
can be deployed and managed on kubernetes,

96
00:06:18,170 --> 00:06:21,960
and standardization and abstraction have played a big part.

97
00:06:22,410 --> 00:06:25,766
They've meant that Kubernetes has become far more interoperable and

98
00:06:25,788 --> 00:06:29,814
the abstractions have provided simplified management. The aim

99
00:06:29,862 --> 00:06:33,638
is to make the underlying complexity invisible, further solidifying

100
00:06:33,654 --> 00:06:37,014
its role as a foundational platform. And lastly,

101
00:06:37,062 --> 00:06:40,566
there's various extensible interfaces, which make integration with external

102
00:06:40,598 --> 00:06:44,102
tools much easier. Container storage interface

103
00:06:44,166 --> 00:06:48,362
provides a welldefined API for attaching, mounting, and managing

104
00:06:48,426 --> 00:06:51,150
persistent or ephemeral storage for containers.

105
00:06:51,650 --> 00:06:55,390
Container runtime interface acts as a bridge between Kubernetes

106
00:06:55,470 --> 00:06:59,394
and the underlying container runtime engine like docker or container d

107
00:06:59,512 --> 00:07:03,298
in each node of your cluster. In simpler terms, it allows Kubernetes to

108
00:07:03,304 --> 00:07:07,240
talk to different runtime environments without needing to be rebuilt for each one

109
00:07:07,690 --> 00:07:11,154
and container network interface defines how network

110
00:07:11,202 --> 00:07:15,426
plugins manage the creation and configuration of network interfaces.

111
00:07:15,618 --> 00:07:18,986
This allows you to use various networking solutions with your

112
00:07:19,008 --> 00:07:23,050
clusters, providing flexibility in how containers communicate.

113
00:07:23,390 --> 00:07:27,830
However, nothing is perfect, and there are often discussions about how Kubernetes

114
00:07:27,990 --> 00:07:30,410
is potentially becoming too complex.

115
00:07:30,830 --> 00:07:34,046
While it can handle various deployments, it might not always be the

116
00:07:34,068 --> 00:07:37,370
most efficient choice, and the future may see a balance

117
00:07:37,450 --> 00:07:40,938
between leveraged kubernetes strengths as a platform and using simpler

118
00:07:40,954 --> 00:07:44,514
tools for specific needs. But abstraction might yet

119
00:07:44,552 --> 00:07:47,010
provide the solution to simplify Kubernetes.

120
00:07:47,750 --> 00:07:51,394
Before we go further, to understand how some of those key

121
00:07:51,432 --> 00:07:55,634
developments have happened in a more practical sense, it's critical

122
00:07:55,682 --> 00:07:59,622
to recap some of the fundamentals so

123
00:07:59,756 --> 00:08:03,410
we can dive into the control plane, which acts

124
00:08:03,490 --> 00:08:07,378
as the brain of the operation. It manages the worker nodes

125
00:08:07,394 --> 00:08:09,420
and applications running on them,

126
00:08:09,870 --> 00:08:13,690
so there are multiple components on the control

127
00:08:13,760 --> 00:08:17,242
plane. The API server acts as a central point

128
00:08:17,296 --> 00:08:20,862
of contact. It receives requests from the users and applications to manage

129
00:08:20,916 --> 00:08:24,282
resources in the cluster. The scheduler

130
00:08:24,346 --> 00:08:28,474
decides where to run pods on the worker nodes. It considers factors

131
00:08:28,522 --> 00:08:32,474
like resource availability, pod requirements to ensure efficient

132
00:08:32,522 --> 00:08:36,014
allocation. Controller manager is a collection

133
00:08:36,062 --> 00:08:39,406
of controllers that together ensure that the state of the cluster matches

134
00:08:39,438 --> 00:08:42,658
the desired state, and we'll come back to that in a

135
00:08:42,664 --> 00:08:46,710
minute. And finally, etCD, which is a highly available

136
00:08:46,780 --> 00:08:50,674
key value store that acts as the cluster's single source of truth.

137
00:08:50,802 --> 00:08:54,870
It stores all the configuration data about the cluster and its resources.

138
00:08:55,290 --> 00:08:58,986
These components work together to automate and manage the

139
00:08:59,008 --> 00:09:02,970
lifecycle of containerized applications within the Kubernetes cluster,

140
00:09:03,310 --> 00:09:07,050
it's worth digging a little deeper into the controller

141
00:09:07,470 --> 00:09:11,222
manager and the control loop. They're intertwined

142
00:09:11,286 --> 00:09:14,606
concepts in Kubernetes, and they're also critical as we start to talk

143
00:09:14,628 --> 00:09:18,474
about operators. As mentioned, the controller manager

144
00:09:18,602 --> 00:09:21,918
is a collection of controllers that together ensure that the state of

145
00:09:21,924 --> 00:09:25,362
the cluster matches the desired state. It's made

146
00:09:25,416 --> 00:09:27,742
up of several controllers.

147
00:09:27,886 --> 00:09:31,902
Examples include the replication controller, which ensures a desired

148
00:09:31,966 --> 00:09:36,982
number of replicas are running for deployment the

149
00:09:37,036 --> 00:09:41,720
endpoint controller, which maintains a list of pods that provide a particular service,

150
00:09:42,490 --> 00:09:46,406
or the namespace controller that manages the creation and deletions of

151
00:09:46,508 --> 00:09:49,862
namespaces in a cluster. Now each controller

152
00:09:49,926 --> 00:09:53,706
follows a control loop pattern. Initially we

153
00:09:53,728 --> 00:09:57,526
have the getting the desired state. The controller retrieves

154
00:09:57,558 --> 00:10:00,990
the desired state for a specific resource like

155
00:10:01,140 --> 00:10:04,954
number of replicas in a deployment from the Kubernetes

156
00:10:05,002 --> 00:10:08,766
API server. Then you

157
00:10:08,788 --> 00:10:12,446
get the current state. So again the controller queries the

158
00:10:12,468 --> 00:10:15,780
API server to determine the actual number of pods running,

159
00:10:16,630 --> 00:10:20,434
and then the controller compares the desired and current state.

160
00:10:20,632 --> 00:10:24,094
If there's discrepancy, it issues commands to the API

161
00:10:24,142 --> 00:10:27,720
server to take corrective actions like scaling pods up or down.

162
00:10:28,170 --> 00:10:31,382
Now the control loop is the core principle behind each

163
00:10:31,436 --> 00:10:35,282
controller. Within the controller manager. It's a continuous iterative

164
00:10:35,346 --> 00:10:39,302
process that ensures the cluster state remains aligned with the desired

165
00:10:39,366 --> 00:10:43,958
state defined in Kubernetes configuration. Control loops enable

166
00:10:44,134 --> 00:10:47,894
self healing if a pod crashes, the control loop

167
00:10:47,942 --> 00:10:50,622
automatically detects it and launches a new one.

168
00:10:50,756 --> 00:10:54,282
Scalability. So when pod replicas

169
00:10:54,346 --> 00:10:58,282
need to be adjusted based on demand, the control loop ensures

170
00:10:58,346 --> 00:11:01,902
the automatic scaling up or scaling down and

171
00:11:01,956 --> 00:11:05,486
consistency. So the control loop continuously works on rectifying

172
00:11:05,518 --> 00:11:09,474
any deviation from the desired state, keeping the cluster in

173
00:11:09,512 --> 00:11:13,010
a stable and predictable condition.

174
00:11:13,590 --> 00:11:17,458
So in essence, the controller manager

175
00:11:17,634 --> 00:11:21,014
acts as the conductor, orchestrating the control loops of

176
00:11:21,052 --> 00:11:24,482
individual controllers. These loops constantly

177
00:11:24,546 --> 00:11:27,938
monitor and react to changes, ensuring the cluster

178
00:11:28,114 --> 00:11:31,638
maintains the desired state. So now we're

179
00:11:31,654 --> 00:11:34,490
going to take a look at how Kubernetes can be extended.

180
00:11:34,830 --> 00:11:38,154
The inbuilt controllers of kubernetes are critical, but one of the most

181
00:11:38,192 --> 00:11:41,406
significant developments made to Kubernetes in recent years is

182
00:11:41,428 --> 00:11:45,390
the ability to extend Kubernetes with custom controllers.

183
00:11:45,730 --> 00:11:48,842
What if you need to manage something beyond the basic functions?

184
00:11:48,986 --> 00:11:53,134
That's where these custom controllers come in. Custom controllers are

185
00:11:53,172 --> 00:11:56,734
user defined programs that extend the capabilities of kubernetes.

186
00:11:56,862 --> 00:12:00,658
They work in conjunction with custom resource definitions to manage

187
00:12:00,744 --> 00:12:03,620
custom resources specific to your needs.

188
00:12:04,230 --> 00:12:08,242
Here's a breakdown of how they work in a custom resource definition.

189
00:12:08,306 --> 00:12:12,546
You define using YaML, the schema for your custom resource.

190
00:12:12,658 --> 00:12:16,642
This essentially creates a new type of object that Kubernetes can recognize

191
00:12:16,706 --> 00:12:20,758
and manage. Custom controllers typically written in go,

192
00:12:20,924 --> 00:12:24,854
then interact with the Kubernetes API and follow the same control loop

193
00:12:24,902 --> 00:12:28,054
pattern as built in controllers. They get the desired

194
00:12:28,102 --> 00:12:31,754
state, they get the current state, and then they reconcile comparing the desired

195
00:12:31,802 --> 00:12:34,778
and current state, and if there's any differences,

196
00:12:34,874 --> 00:12:38,526
they take action to achieve the desired state. There are

197
00:12:38,548 --> 00:12:41,886
several key benefits of custom controllers. They extend the

198
00:12:41,908 --> 00:12:46,002
native Kubernetes functionality by enabling you to manage resources specific

199
00:12:46,056 --> 00:12:49,426
to your application or infrastructure needs. They allow you

200
00:12:49,448 --> 00:12:52,626
to manage these resources, these custom resources in

201
00:12:52,648 --> 00:12:56,230
a declarative way. You just define the desired state of your custom

202
00:12:56,300 --> 00:13:00,470
resource and the controller handles achieving and maintaining it and

203
00:13:00,540 --> 00:13:03,922
automation. Custom controllers automate tasks related

204
00:13:03,986 --> 00:13:07,266
to managing your custom resources, for example,

205
00:13:07,388 --> 00:13:10,890
rolling updates to your pods. So what exactly

206
00:13:10,960 --> 00:13:14,714
are Kubernetes operators? How do they fit in

207
00:13:14,752 --> 00:13:18,380
with custom controllers and how are they different?

208
00:13:18,910 --> 00:13:22,830
So operators package up custom controllers and

209
00:13:22,900 --> 00:13:26,254
a few more components to make it easier to deploy and manage

210
00:13:26,372 --> 00:13:30,282
applications. So where a custom controller

211
00:13:30,346 --> 00:13:33,774
executes a single control loop to manage a specific kubernetes,

212
00:13:33,822 --> 00:13:37,774
custom resource operators not only package multiple

213
00:13:37,822 --> 00:13:42,020
controllers, but also additional assets needed to deploy and manage an application.

214
00:13:42,550 --> 00:13:46,310
These can include CRD definitions specifying the custom

215
00:13:46,380 --> 00:13:49,778
resource type. The operator manages

216
00:13:49,954 --> 00:13:53,922
controller code so the custom controller program that enforces

217
00:13:53,986 --> 00:13:58,406
the desired state for the custom resource. Deployment manifests,

218
00:13:58,518 --> 00:14:02,054
which are YAML files defining how the application components

219
00:14:02,102 --> 00:14:06,010
like pod services are deployed in Kubernetes

220
00:14:06,750 --> 00:14:11,722
service manifests defining Kubernetes

221
00:14:11,786 --> 00:14:15,754
services needed by the application helm charts

222
00:14:15,802 --> 00:14:19,802
packaging configurations for the application in a standardized

223
00:14:19,866 --> 00:14:23,770
format documentation monitoring tools

224
00:14:23,930 --> 00:14:28,370
like information utilities for understanding and managing applications in Kubernetes.

225
00:14:28,870 --> 00:14:31,540
So by building these elements together,

226
00:14:32,390 --> 00:14:36,546
operators offer a self contained unit for simplifying management

227
00:14:36,658 --> 00:14:40,546
within Kubernetes of an entire application throughout

228
00:14:40,578 --> 00:14:44,338
its lifecycle. So why are operators

229
00:14:44,434 --> 00:14:47,154
so useful? Firstly,

230
00:14:47,202 --> 00:14:50,458
simplified application management by providing a single

231
00:14:50,544 --> 00:14:54,374
tool for deploying and managing applications throughout

232
00:14:54,422 --> 00:14:58,150
their whole lifecycle. They offer a declarative

233
00:14:58,230 --> 00:15:02,074
approach where instead of writing complex deployment scripts,

234
00:15:02,202 --> 00:15:06,302
you simply find the desired state of the application using

235
00:15:06,356 --> 00:15:08,750
the operator's manifest files.

236
00:15:09,730 --> 00:15:13,582
Reduced errors by automating manual operational tasks,

237
00:15:13,646 --> 00:15:17,570
minimizing the risk of human error during deployment or configuration.

238
00:15:18,310 --> 00:15:21,854
Standardized packaging operators promote

239
00:15:21,902 --> 00:15:25,930
a consistent way to package applications. This standardization

240
00:15:26,110 --> 00:15:30,150
makes them more portable and reusable across different environments.

241
00:15:30,970 --> 00:15:34,742
Operators can be designed with domain specific knowledge for

242
00:15:34,796 --> 00:15:38,826
particular types of applications like databases, messaging systems and

243
00:15:38,848 --> 00:15:43,366
so on. And this expertise ensures the operator

244
00:15:43,558 --> 00:15:47,354
understands the application's intricacies and configures it

245
00:15:47,392 --> 00:15:50,090
optimally within the Kubernetes environments.

246
00:15:50,770 --> 00:15:54,506
Finally, a rich ecosystem of operators exists

247
00:15:54,538 --> 00:15:57,742
for various applications and functionality, so you can find pre

248
00:15:57,796 --> 00:16:02,358
built operators for popular databases like MongoDB,

249
00:16:02,554 --> 00:16:05,970
monitoring tools and other popular components,

250
00:16:06,310 --> 00:16:10,050
which saves you time and effort in managing them individually.

251
00:16:10,550 --> 00:16:16,802
But not all operators are alike, so Kubernetes

252
00:16:16,866 --> 00:16:20,854
operators can be broadly categorized into two types based on

253
00:16:20,892 --> 00:16:22,600
the resources they manage.

254
00:16:23,370 --> 00:16:27,174
Internal operators focus on managing resources that

255
00:16:27,212 --> 00:16:30,380
are entirely within the Kubernetes cluster itself.

256
00:16:30,990 --> 00:16:34,870
These resources are the core building blocks of applications deployed

257
00:16:34,950 --> 00:16:38,714
within Kubernetes. They use both inbuilt and

258
00:16:38,752 --> 00:16:41,982
custom resource types, including things

259
00:16:42,036 --> 00:16:45,818
like deployment, stateful sets, persistent volumes

260
00:16:45,834 --> 00:16:49,882
and so on. On the other hand, external operators

261
00:16:49,946 --> 00:16:53,762
extend Kubernetes's reach by managing resources that

262
00:16:53,816 --> 00:16:56,542
reside outside of the Kubernetes cluster.

263
00:16:56,686 --> 00:17:00,098
This might be

264
00:17:00,184 --> 00:17:03,646
a self hosted service running outside of Kubernetes,

265
00:17:03,838 --> 00:17:07,426
something running in one of the hyperscalers, or any

266
00:17:07,448 --> 00:17:11,880
other external service like MongoDB's Atlas developer data platform.

267
00:17:12,570 --> 00:17:16,290
Despite their differences, both internal and external operators

268
00:17:16,370 --> 00:17:19,850
offer the advantages associated with operators in general,

269
00:17:20,000 --> 00:17:24,198
so simplified application management, declarative configuration,

270
00:17:24,374 --> 00:17:27,702
reduced errors and standardized packaging.

271
00:17:27,846 --> 00:17:31,374
And by leveraging the appropriate operator type, you can

272
00:17:31,412 --> 00:17:35,950
effectively manage both internal and external resource

273
00:17:36,290 --> 00:17:39,614
dependencies, leading to a more

274
00:17:39,652 --> 00:17:43,034
robust and streamlined application deployment and management

275
00:17:43,082 --> 00:17:46,994
experience within Kubernetes. So now we're going to move

276
00:17:47,032 --> 00:17:51,154
on to look at a demo in the demo, we're going to focus on

277
00:17:51,192 --> 00:17:54,930
just one element of an IDP, using Kubernetes and operators

278
00:17:55,270 --> 00:17:59,314
to show that. We're going to demonstrate how you can build a database

279
00:17:59,362 --> 00:18:02,758
as a service into your IDP. We're going

280
00:18:02,764 --> 00:18:06,546
to use an external database as a service. I'm going to be Atlas.

281
00:18:06,738 --> 00:18:10,186
Atlas is a software as a service offering value as

282
00:18:10,208 --> 00:18:13,894
a developer data platform of different complementary

283
00:18:13,942 --> 00:18:18,006
tools like search, analytics and support for mobile apps,

284
00:18:18,118 --> 00:18:21,114
as well as support for serverless time series data,

285
00:18:21,232 --> 00:18:25,642
geospatial data, and multicloud global distribution

286
00:18:25,706 --> 00:18:29,802
and resilience. We're also going to use the Atlas Kubernetes

287
00:18:29,866 --> 00:18:33,566
operator. It enables you to use the same tooling and processes to

288
00:18:33,588 --> 00:18:36,850
manage Atlas as you use for your services in Kubernetes.

289
00:18:37,190 --> 00:18:41,246
Atlas doesn't run in the Kubernetes cluster, but the operator allows

290
00:18:41,278 --> 00:18:45,710
you to use declarative configuration files that can be applied into Kubernetes

291
00:18:45,870 --> 00:18:49,190
where the operator picks them up and using control loops makes

292
00:18:49,260 --> 00:18:52,390
changes to Atlas via the Atlas admin API.

293
00:18:52,810 --> 00:18:56,710
We're also going to build a Gitops interface for our users

294
00:18:57,050 --> 00:19:01,030
using Argo CD as the mechanism to bridge the gap between the IOC

295
00:19:01,110 --> 00:19:04,250
files in the repositories and Kubernetes.

296
00:19:04,910 --> 00:19:08,998
This gives us a highly automated and standardized Gitops workflow,

297
00:19:09,094 --> 00:19:13,194
and in doing so cuts down on the expertise and permissions that individual teams

298
00:19:13,242 --> 00:19:17,198
need, as no more direct interactions with Kubernetes are needed.

299
00:19:17,284 --> 00:19:20,846
This provides a self service mechanism. Let's see now

300
00:19:20,868 --> 00:19:24,334
an operator in action and how managing external resources

301
00:19:24,382 --> 00:19:27,966
really works. An operator generally can be installed

302
00:19:27,998 --> 00:19:31,774
in a few different ways through Kubectl helm

303
00:19:31,822 --> 00:19:35,334
charts automation scripts. In this case,

304
00:19:35,372 --> 00:19:39,062
we'll use an automated way. So we consider having

305
00:19:39,116 --> 00:19:43,234
installed the Atlas CLI and this makes installation

306
00:19:43,282 --> 00:19:45,720
and configuration super simple.

307
00:19:46,410 --> 00:19:50,102
So by running the Kubernetes

308
00:19:50,166 --> 00:19:53,962
operator install command, we firstly install

309
00:19:54,016 --> 00:19:57,738
the Kubernetes operator. But all the necessary setup happens in

310
00:19:57,744 --> 00:20:01,166
the background, such as creating new API keys for the operator to

311
00:20:01,188 --> 00:20:03,790
manage external resources.

312
00:20:04,210 --> 00:20:07,770
We can see that the operator is now running in our cluster,

313
00:20:07,930 --> 00:20:11,614
and if we go into our Atlas UI we see

314
00:20:11,652 --> 00:20:14,660
that the relevant API keys have been created to use.

315
00:20:15,190 --> 00:20:19,806
And finally in the background the operator has safely stored

316
00:20:19,998 --> 00:20:23,694
the API keys as Kubernetes secrets,

317
00:20:23,742 --> 00:20:27,670
so we can use as we make the API calls to reconcile.

318
00:20:28,090 --> 00:20:31,474
So going to our text editor we are going to create an Atlas

319
00:20:31,522 --> 00:20:34,898
project. This is defined

320
00:20:34,914 --> 00:20:38,922
as a Yaml file we apply in Kubernetes and we see

321
00:20:38,976 --> 00:20:43,450
that the new custom resource has been created in Kubernetes

322
00:20:44,110 --> 00:20:47,798
and under the hood the operator has read the resource

323
00:20:47,974 --> 00:20:51,534
and through the Atlas Admin API has created this new

324
00:20:51,572 --> 00:20:54,874
project in our Atlas

325
00:20:54,922 --> 00:20:58,714
account. So we are now ready to deploy our first cluster

326
00:20:58,842 --> 00:21:02,474
back again to our text editor simple Yaml file. To create

327
00:21:02,532 --> 00:21:06,514
an Atlas deployment we describe the

328
00:21:06,552 --> 00:21:09,954
cloud provider, the instance type, the region we want

329
00:21:10,152 --> 00:21:12,580
this cluster to be deployed at.

330
00:21:13,270 --> 00:21:16,360
Again, a simple Kubectl apply,

331
00:21:17,050 --> 00:21:20,710
and we see that the custom resource again is created

332
00:21:21,210 --> 00:21:25,254
in our Kubernetes cluster and the

333
00:21:25,292 --> 00:21:28,550
operator has created it through the Atlas admin API

334
00:21:28,630 --> 00:21:32,282
in atlas. So let's move on

335
00:21:32,336 --> 00:21:35,990
to build the platform interface as well. So integrating

336
00:21:36,070 --> 00:21:39,922
and exposing this database as a service functionality

337
00:21:40,086 --> 00:21:44,094
through a GitHubs workflow using ArgoCd now

338
00:21:44,132 --> 00:21:46,480
again, installing argocd is pretty simple.

339
00:21:47,090 --> 00:21:50,414
Firstly, we'll be creating a namespace where

340
00:21:50,452 --> 00:21:53,540
all of the operator components are going to be running in,

341
00:21:54,070 --> 00:21:57,486
and then the installation of the operator is automated

342
00:21:57,598 --> 00:22:01,106
through an installation script. Just a single command and

343
00:22:01,128 --> 00:22:04,386
a couple of seconds later we can see all

344
00:22:04,408 --> 00:22:08,402
of the argo CD operator pods are running in our

345
00:22:08,536 --> 00:22:12,866
ArgoCd namespace. Now. Next we'll be creating an ArgoCD

346
00:22:12,898 --> 00:22:16,130
application. We are going to define the repositories

347
00:22:16,210 --> 00:22:20,054
that argo CD will be watching to find new Kubernetes

348
00:22:20,102 --> 00:22:23,942
configuration files and the destination Kubernetes cluster,

349
00:22:24,086 --> 00:22:27,930
the namespace that we're going to be using to deploy resources.

350
00:22:28,430 --> 00:22:33,386
Going into the Argo UI we can see this TBAs application created

351
00:22:33,578 --> 00:22:36,800
and we can monitor the sync status and so on.

352
00:22:37,170 --> 00:22:41,150
So to deploy an atlas project through this interface,

353
00:22:41,570 --> 00:22:45,090
we use the exact same file as before, same configuration.

354
00:22:45,910 --> 00:22:49,762
The main difference is instead of Kubectl apply, we just

355
00:22:49,816 --> 00:22:53,650
do a git push in our infrastructure as code repository

356
00:22:54,390 --> 00:22:57,906
and then looking into the Arco UI. We see that our Argo

357
00:22:57,938 --> 00:23:02,146
application has picked up the change and has created the Atlas

358
00:23:02,178 --> 00:23:06,230
project custom resource in our Kubernetes cluster.

359
00:23:06,730 --> 00:23:09,878
And here it's same as before. The Atlas

360
00:23:09,894 --> 00:23:13,882
Kubernetes operator has picked up this custom resource and

361
00:23:13,936 --> 00:23:17,734
has deployed the new project in our Atlas

362
00:23:17,782 --> 00:23:21,454
account. Now deploying the cluster again, we're going to

363
00:23:21,492 --> 00:23:25,630
use the same Atlas deployment file.

364
00:23:26,290 --> 00:23:30,090
We're going to be using it in the Githubs project that we just created.

365
00:23:30,250 --> 00:23:33,918
Git push for the cluster creation and

366
00:23:34,004 --> 00:23:37,766
all the same magic happens under the hood. Argo CD

367
00:23:37,818 --> 00:23:42,130
has picked up this change in. We can see this in the UI.

368
00:23:42,550 --> 00:23:46,086
It deploys this custom resource in Kubernetes and then the

369
00:23:46,108 --> 00:23:50,258
operator creates the new database in our GitHubs

370
00:23:50,274 --> 00:23:53,000
project. Now for the last part,

371
00:23:54,810 --> 00:23:58,770
as we've streamlined the deployment and management of new databases,

372
00:23:58,930 --> 00:24:02,330
we want to see how we can leverage the operators and make it easy

373
00:24:02,400 --> 00:24:05,820
to connect to those applications in a Kubernetes native way.

374
00:24:06,350 --> 00:24:09,954
So we'll be using a new custom resource called Atlas database

375
00:24:10,022 --> 00:24:14,094
user in order to manage database access. And since

376
00:24:14,292 --> 00:24:17,690
each Atlas database user is just another YAmL file

377
00:24:17,770 --> 00:24:21,262
in our repository, the operator again makes

378
00:24:21,316 --> 00:24:24,846
that super easy to manage through our Githubs workflow.

379
00:24:25,038 --> 00:24:28,850
So we've already created a simple Kubernetes secret which

380
00:24:28,920 --> 00:24:32,942
contains the user password. We have a custom resource

381
00:24:33,006 --> 00:24:36,774
definition for our database users where we refer to the password as

382
00:24:36,812 --> 00:24:40,680
we create the user. Similarly, as we did before

383
00:24:41,930 --> 00:24:45,346
and through our GitHubs workflow, argocd picks

384
00:24:45,378 --> 00:24:48,870
up the changes in our git repository,

385
00:24:48,950 --> 00:24:52,298
sees the new resource here and deploys the

386
00:24:52,304 --> 00:24:55,210
Atlas database user in Kubernetes.

387
00:24:56,030 --> 00:25:00,134
Now an additional step here is that the operator creates a Kubernetes

388
00:25:00,182 --> 00:25:04,046
secret which contains all

389
00:25:04,068 --> 00:25:07,498
the connection information we're going to be needing to connect to our database.

390
00:25:07,594 --> 00:25:10,922
If we explore the secret here we can see the connection string

391
00:25:10,986 --> 00:25:15,006
that our applications need. So for the last part we'll be creating

392
00:25:15,038 --> 00:25:18,654
a simple Kubernetes deployment. We'll be using the Mongo

393
00:25:18,702 --> 00:25:22,526
image to run a Mongo shell to connect to our atlas

394
00:25:22,558 --> 00:25:26,310
database and we read and

395
00:25:26,380 --> 00:25:29,686
use the connection string as an environment variable for our

396
00:25:29,708 --> 00:25:33,320
Mongo shell. We deploy this application,

397
00:25:34,410 --> 00:25:37,826
we use Kubectl and we can see that our

398
00:25:37,868 --> 00:25:40,460
Mongo shell pod is running.

399
00:25:41,150 --> 00:25:44,826
And if we inspect the logs of our pod, we can see

400
00:25:44,928 --> 00:25:48,506
that this pod has connected to our atlas database using

401
00:25:48,608 --> 00:25:52,286
our connection string. Now as this was the

402
00:25:52,308 --> 00:25:55,982
end of the demo, I'm pretty

403
00:25:56,036 --> 00:25:59,582
sure you were expecting fireworks towards the end.

404
00:25:59,716 --> 00:26:02,762
The good news is that when it comes to managing infrastructure

405
00:26:02,826 --> 00:26:06,766
in critical production environments, not having fireworks is

406
00:26:06,788 --> 00:26:10,602
a good thing. And if you also want to experience this transformation,

407
00:26:10,666 --> 00:26:14,414
this is a good time to prep your phone.

408
00:26:14,532 --> 00:26:18,326
In the next slide, Dan will show you a QR code where we

409
00:26:18,348 --> 00:26:23,110
can help enable you build an IDP through operators.

410
00:26:23,770 --> 00:26:27,206
Through the link or the QR code you can find a landing page that gives

411
00:26:27,228 --> 00:26:30,630
you all the information you need to replicate our demo or

412
00:26:30,700 --> 00:26:34,834
to even take the next step and build your own IDP database

413
00:26:34,882 --> 00:26:38,566
as a service using MongoDB Atlas thank you very much

414
00:26:38,588 --> 00:26:39,140
for joining us.

