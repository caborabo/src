1
00:00:20,760 --> 00:00:25,030
Hello, welcome to my talk. The title is self hosted arms

2
00:00:25,102 --> 00:00:29,022
across all your devices and GPU's. So in this talk we're

3
00:00:29,038 --> 00:00:32,942
going to talk about self hosted oems and we're going

4
00:00:32,958 --> 00:00:36,366
to primarily use three open source projects.

5
00:00:36,510 --> 00:00:40,238
One is called was match and this is the CNC project that

6
00:00:40,286 --> 00:00:44,054
I founded and it is a webassembly

7
00:00:44,094 --> 00:00:47,486
runtime that we see great adoption in

8
00:00:47,670 --> 00:00:50,822
AI and the large language model space. And then the

9
00:00:50,838 --> 00:00:54,358
second project is called Lama Edge. Lama Edge is open source project that build

10
00:00:54,406 --> 00:00:58,500
upon was matched. So it provides tools

11
00:00:58,532 --> 00:01:02,652
and SDKs and programming primaries for building

12
00:01:02,748 --> 00:01:06,572
large language model applications, especially around the llama two model on

13
00:01:06,588 --> 00:01:10,108
the WASM edge app runtime itself. So when it's a runtime you

14
00:01:10,116 --> 00:01:14,724
can see this as operating system. And then Llamaedge is infrastructure

15
00:01:14,764 --> 00:01:18,172
that specifically for building web, for building arm applications on

16
00:01:18,188 --> 00:01:22,332
top of WASM edge. And then the third project is Gaia. Net. Gaynet further

17
00:01:22,388 --> 00:01:25,666
builds upon Lama Edge to provide what we

18
00:01:25,690 --> 00:01:28,978
call a rag application server, meaning that

19
00:01:29,026 --> 00:01:33,090
we supplement the large language model with real world knowledge

20
00:01:33,122 --> 00:01:37,402
base, either from public data, from public knowledge, or from our proprietary knowledge.

21
00:01:37,538 --> 00:01:41,618
And that just makes AI models and applications

22
00:01:41,666 --> 00:01:45,490
that associated with that much more useful. So it's designed

23
00:01:45,522 --> 00:01:49,374
for use with AI agent applications and such.

24
00:01:51,574 --> 00:01:55,354
Well, start off the talk with a demo of how easy it is to run

25
00:01:55,814 --> 00:01:58,774
a chatbot using the open source arm on your own device.

26
00:01:58,854 --> 00:02:02,302
Using Lama H in this demo

27
00:02:02,358 --> 00:02:05,566
I'll show you the easiest way to run a large language model on your own

28
00:02:05,590 --> 00:02:09,790
computer. First go to the Lamaedge Llamaedge

29
00:02:09,822 --> 00:02:13,274
repository on GitHub. It is an entirely open source project.

30
00:02:13,654 --> 00:02:17,592
So in the readme file while you are here just

31
00:02:17,648 --> 00:02:21,104
give it a start and in the readme file

32
00:02:21,224 --> 00:02:24,724
you will see a quick start guide and there's just one line of command.

33
00:02:25,064 --> 00:02:28,232
Copy this and then go to

34
00:02:28,248 --> 00:02:31,776
your Mac or Linux computer or even windows with WSl

35
00:02:31,920 --> 00:02:35,392
to run this command. What it does is that

36
00:02:35,448 --> 00:02:39,244
can install a large language model runtime called the Wasmatch

37
00:02:40,344 --> 00:02:43,550
plugins that required to run was to

38
00:02:43,582 --> 00:02:46,590
run the large language model and it goes

39
00:02:46,622 --> 00:02:50,454
fast here. But as you can see it downloads a large

40
00:02:50,494 --> 00:02:54,502
language model called Gamma two B which is one of Google's

41
00:02:54,558 --> 00:02:59,022
large language models. And you can actually run any

42
00:02:59,078 --> 00:03:02,286
large language model that you find on hugging

43
00:03:02,310 --> 00:03:06,318
face. There's over 5000 of those. And just

44
00:03:06,406 --> 00:03:09,906
go back to the readme and, and look for the parameters that

45
00:03:09,930 --> 00:03:13,242
you can pass to the script where you can specify the model which even as

46
00:03:13,258 --> 00:03:17,334
a hugging based URL. And once the model is started it says

47
00:03:18,434 --> 00:03:22,530
the application starts a local web server on your own device,

48
00:03:22,722 --> 00:03:26,458
and it's started on port 8080. So what you can do

49
00:03:26,586 --> 00:03:30,066
is that you can go to that server and

50
00:03:30,090 --> 00:03:34,166
it would load a URL. And it's

51
00:03:34,230 --> 00:03:37,254
the URL said lama h chat. Now you can start to chat with it.

52
00:03:37,294 --> 00:03:41,870
Say, where is the capital

53
00:03:41,942 --> 00:03:43,794
of Japan?

54
00:03:46,934 --> 00:03:50,222
The first time it is a little slow because it needs to load the

55
00:03:50,238 --> 00:03:54,094
model into memory, but it's fast enough. The capital of Japan is Tokyo,

56
00:03:54,134 --> 00:03:59,880
and it's now you can ask what

57
00:03:59,912 --> 00:04:03,960
about the USA? And it clearly shows

58
00:04:04,032 --> 00:04:07,392
that the model understands

59
00:04:07,408 --> 00:04:10,568
the conversation. Right? Because the second question I did not tell

60
00:04:10,696 --> 00:04:14,360
to find the capital of the USA. I said, what about the USA? It knows

61
00:04:14,392 --> 00:04:17,712
about Washington DC because

62
00:04:17,768 --> 00:04:21,200
it knows about the previous conversation.

63
00:04:21,352 --> 00:04:25,688
If we go back to the log, we can see that it's generating

64
00:04:25,816 --> 00:04:29,048
with the gamma two b. It was generating over 60 tokens

65
00:04:29,096 --> 00:04:32,270
per second. That's a lot faster than a human can speak.

66
00:04:32,382 --> 00:04:36,390
A human typically speaks when I'm speaking. Now it's about three

67
00:04:36,422 --> 00:04:39,614
to five tokens per second. So it's ten times faster than human

68
00:04:39,654 --> 00:04:43,394
can speak. Now I can ask a longer question.

69
00:04:48,294 --> 00:04:53,134
Plan me a one day trip there, meaning one

70
00:04:53,174 --> 00:04:55,354
day trip to Japan. Okay, so it's,

71
00:04:56,784 --> 00:05:00,712
I should have said Washington DC, but you know, that's close enough.

72
00:05:00,768 --> 00:05:04,440
You know, that's, oh, it's giving me two days as well. But this

73
00:05:04,472 --> 00:05:07,720
is one of, because this is a very small, it's a two b

74
00:05:07,752 --> 00:05:11,144
model. If you change it to a seven B model or even

75
00:05:11,184 --> 00:05:14,672
larger model, you're going to see. You're going to see it respond a

76
00:05:14,688 --> 00:05:18,248
lot better. But even so, it is still pretty good. It gave me

77
00:05:18,416 --> 00:05:21,760
itinerary of on day four. Now it's maybe

78
00:05:21,792 --> 00:05:24,748
a week in Japan. Right? So that's it.

79
00:05:24,796 --> 00:05:27,804
That's how you run a large language model on your own computer. This is,

80
00:05:27,844 --> 00:05:31,420
I'm running it on the Mac and I'm recording this video while running it.

81
00:05:31,532 --> 00:05:34,868
And so it's even then I

82
00:05:34,876 --> 00:05:38,076
can get 60 tokens per second.

83
00:05:38,140 --> 00:05:41,812
So it is one of the easiest way and the fastest way for

84
00:05:41,828 --> 00:05:45,324
you to run again. Yeah, it gave me the whole

85
00:05:45,364 --> 00:05:49,252
whole week. So again,

86
00:05:49,428 --> 00:05:52,542
the URL is GitHub Lama edge.

87
00:05:52,638 --> 00:05:56,286
Lama Edge. And they're just one line

88
00:05:56,310 --> 00:05:59,590
of command. And once you get started running, you can run other models by reading

89
00:05:59,662 --> 00:06:03,302
the rest of the readme. And you can use that as an API server and

90
00:06:03,318 --> 00:06:06,582
you can do a lot of things with it. Okay, now you have seen the

91
00:06:06,598 --> 00:06:10,110
demo. So what's it all for? We have seen

92
00:06:10,142 --> 00:06:13,710
that you can run a large language model application, you can chat with it

93
00:06:13,782 --> 00:06:17,414
on your own device, but what's it for? Most people just use

94
00:06:17,494 --> 00:06:21,292
OpenAI for this purpose and OpenAI also provided SDK API,

95
00:06:21,348 --> 00:06:25,060
right? So you can build applications around it as well. So why is there

96
00:06:25,092 --> 00:06:28,732
need to run those large language, open source, large language models on your

97
00:06:28,748 --> 00:06:32,732
own device? Well, so there are a couple of reasons. The first,

98
00:06:32,908 --> 00:06:37,052
biggest reason really is that OpenAI or any other larger

99
00:06:37,068 --> 00:06:40,652
language providers, basically it takes a one size fits all approach.

100
00:06:40,788 --> 00:06:44,284
So basically they're using the largest model for the smallest tasks. So even

101
00:06:44,324 --> 00:06:47,324
if you ask for something that can be easily handled by a smaller model,

102
00:06:47,364 --> 00:06:51,224
it will still use the largest model, which generates a lot of waste.

103
00:06:51,564 --> 00:06:54,956
It was very expensive and also makes the models very difficult

104
00:06:54,980 --> 00:06:58,492
to fine tune because it's much harder to fine tune a large model than

105
00:06:58,508 --> 00:07:01,964
a smaller model and because they're running on other people's server,

106
00:07:02,004 --> 00:07:04,580
because you can't, because the model is too big to run on server. So it's

107
00:07:04,612 --> 00:07:08,108
lack of privacy and control. And perhaps more interesting,

108
00:07:08,156 --> 00:07:12,716
and I think people are noticing this more and more, is censorship and bias because

109
00:07:12,900 --> 00:07:16,476
those companies like Openi or Microsoft all

110
00:07:16,500 --> 00:07:20,212
have their own political stand and agendas. So you would

111
00:07:20,228 --> 00:07:23,264
find a lot of questions that those models refuse to answer.

112
00:07:24,644 --> 00:07:28,156
There's very compelling needs for

113
00:07:28,220 --> 00:07:31,876
enterprises or users to run their own models, preferably on

114
00:07:31,900 --> 00:07:33,664
their own devices or in the cloud.

115
00:07:35,524 --> 00:07:38,744
So then that raise the second question.

116
00:07:39,164 --> 00:07:42,220
The llama edge server probably is not the first or the

117
00:07:42,252 --> 00:07:45,764
only open source software that can run large

118
00:07:45,804 --> 00:07:49,436
language models. So for instance llama CPP can run and Allama can run it,

119
00:07:49,580 --> 00:07:53,942
arm studio can run it. So why is it that we choose the

120
00:07:53,958 --> 00:07:57,974
llama edge was stacked to run our API

121
00:07:58,014 --> 00:08:02,434
server around those models as we have shown in the previous example?

122
00:08:03,094 --> 00:08:06,558
First, I think there are several features that distinguish llama

123
00:08:06,606 --> 00:08:09,966
edge from other products or

124
00:08:09,990 --> 00:08:13,278
other open source projects in the space. The first

125
00:08:13,326 --> 00:08:17,150
is to support a wide variety of models. The lamo edge

126
00:08:17,182 --> 00:08:21,014
application server supports over 6000 large language models,

127
00:08:21,094 --> 00:08:24,532
including all the ones

128
00:08:24,548 --> 00:08:28,452
that you have heard of. And there's not just the base models and

129
00:08:28,468 --> 00:08:32,348
chat models, but also embedding models which are essential and very important

130
00:08:32,516 --> 00:08:36,772
for running sophisticated applications as we are seeing like RAC applications.

131
00:08:36,948 --> 00:08:40,996
It even support larger models like X AI squawk model which is

132
00:08:41,060 --> 00:08:44,588
I think 314 billion parameters. And it

133
00:08:44,636 --> 00:08:48,064
requires top of the line, say a Mac studio or,

134
00:08:49,494 --> 00:08:53,910
or two H 100 chips graphic

135
00:08:53,942 --> 00:08:57,958
cards to run those models. But they can run using

136
00:08:58,006 --> 00:09:00,874
Lama H on the other hand.

137
00:09:01,654 --> 00:09:05,078
On one hand that supports a wide variety of models. But on the

138
00:09:05,086 --> 00:09:08,470
other hand, it also supports a wide range of devices, because once

139
00:09:08,502 --> 00:09:11,566
we start to run on our own devices, in our own

140
00:09:11,590 --> 00:09:15,094
cloud, we have to deal with many different types of different devices,

141
00:09:15,174 --> 00:09:19,230
different gpu's, different cpu's and different graphic

142
00:09:19,262 --> 00:09:23,334
cards, different accelerators and all that. So the

143
00:09:23,374 --> 00:09:27,326
architecture of Lama H, because it's based on the wash runtime which we'll get into

144
00:09:27,390 --> 00:09:30,854
in a minute, it provides abstraction layer that

145
00:09:30,894 --> 00:09:34,894
abstracts the application from the underlying runtime

146
00:09:34,934 --> 00:09:38,398
and hardware. So that allows it to the same application

147
00:09:38,446 --> 00:09:42,022
to run on a wide range of devices and drivers,

148
00:09:42,158 --> 00:09:46,318
ranging from Nvidia to AMD to ARM devices,

149
00:09:46,446 --> 00:09:50,658
to Intel AMD CPU

150
00:09:50,706 --> 00:09:54,170
and GPU devices, Apple devices and all of those.

151
00:09:54,322 --> 00:09:57,850
So in a minute. So I just showed you it runs on

152
00:09:57,922 --> 00:10:01,658
a Mac device and I'm going to show you how it runs on Nvidia device.

153
00:10:01,786 --> 00:10:05,090
And we also have tutorials on how to run, say on raspberry PI

154
00:10:05,202 --> 00:10:08,322
or the Jetson devices. All those,

155
00:10:08,498 --> 00:10:12,226
I would say, devices more

156
00:10:12,250 --> 00:10:13,814
commonly find in the edge cloud.

157
00:10:15,734 --> 00:10:19,134
The Lama Edge API server also provides enough flexibility to

158
00:10:19,174 --> 00:10:22,582
run multiple models in the same server. So, meaning that I can run a

159
00:10:22,598 --> 00:10:25,942
chat model to chat with people and run the embedding model in the

160
00:10:25,958 --> 00:10:29,854
same server, so that the user can upload

161
00:10:29,894 --> 00:10:34,230
or process their external knowledge base with the same application server.

162
00:10:34,422 --> 00:10:37,662
It can also do things like function calling by forcing the output to

163
00:10:37,678 --> 00:10:41,534
be JSON. That's something that needs coordination between both

164
00:10:41,574 --> 00:10:45,302
the model itself, the prompting, and the model runtime, meaning that the

165
00:10:45,318 --> 00:10:49,070
model runtime would have to force the output to meet certain grammar checks,

166
00:10:49,102 --> 00:10:52,862
to pass certain grammar check, for instance, that's provided JSON. Right. And as

167
00:10:52,878 --> 00:10:56,446
you can see, it's very easy to install in a run. It's just one line

168
00:10:56,470 --> 00:10:59,974
of command, and I highly suggest recommend you to try it out.

169
00:11:00,134 --> 00:11:03,838
And it's also very lightweight because it's the entire

170
00:11:03,886 --> 00:11:07,278
runtime wash. Plus the application itself

171
00:11:07,366 --> 00:11:11,254
is less than say 30 megabytes. That compared to say,

172
00:11:11,414 --> 00:11:14,620
the Pytorch Docker image we talked, you know, the Pytorch

173
00:11:14,652 --> 00:11:19,636
Docker image is 4gb by itself. So that's

174
00:11:19,660 --> 00:11:23,308
why you, you know, even when running

175
00:11:23,356 --> 00:11:26,188
a large language model and put the API server in front of it, put a

176
00:11:26,196 --> 00:11:29,620
chatbot in front of it, we use Lama edge, right? But all those

177
00:11:29,652 --> 00:11:33,292
benefits actually come from one most important

178
00:11:33,388 --> 00:11:37,004
attributes of the llama edge runtime, which is it is

179
00:11:37,044 --> 00:11:40,776
not really designed to be, to be a standalone

180
00:11:40,960 --> 00:11:44,520
server to run large language model. It is

181
00:11:44,552 --> 00:11:48,384
actually a developer platform. You can write your own code to

182
00:11:48,424 --> 00:11:52,232
extend it and to customize and build your own applications on

183
00:11:52,248 --> 00:11:56,016
top of it. If you think about how large language

184
00:11:56,040 --> 00:11:59,112
model applications are built today, typically you have

185
00:11:59,128 --> 00:12:01,992
an application server that provides API.

186
00:12:02,088 --> 00:12:05,234
It's either OpenAI or some other SaaS provider,

187
00:12:05,344 --> 00:12:09,230
or you can use your own, you know, like using Lama H, you provide open

188
00:12:09,302 --> 00:12:12,886
compatible API server and then you build an application that consumes

189
00:12:12,910 --> 00:12:16,326
this API in say in a relatively

190
00:12:16,350 --> 00:12:19,942
heavyweight stack like in Python or LAN chain or lamba index.

191
00:12:20,038 --> 00:12:22,742
There are lots of tools out there to do that. And you would also have

192
00:12:22,758 --> 00:12:26,114
a UI, have a web server, have all those

193
00:12:27,014 --> 00:12:30,622
components either contained in a docker file,

194
00:12:30,718 --> 00:12:34,206
something like that, and have them all tied together and build

195
00:12:34,230 --> 00:12:37,856
an entire application like that. But while this provides

196
00:12:37,880 --> 00:12:42,128
flexibility and easy allows people, allow developers to experiment

197
00:12:42,176 --> 00:12:45,960
with different parts of the stack, it is also making it really difficult

198
00:12:45,992 --> 00:12:49,664
to deploy those applications because they have too much dependencies.

199
00:12:49,824 --> 00:12:52,992
They are huge because of the python dependency and they

200
00:12:53,008 --> 00:12:56,604
are often very slow. Llama Edge

201
00:12:57,144 --> 00:13:00,816
is a rust based SDK. Essentially it allows you

202
00:13:00,840 --> 00:13:04,536
to build a single portable and deployable app. So basically you can write all

203
00:13:04,560 --> 00:13:08,076
your logic into one single application. There's no need to write part

204
00:13:08,100 --> 00:13:11,084
of the logic in C, part of that in Python,

205
00:13:11,204 --> 00:13:14,492
and then use the HTTP API to connect them together. You don't need

206
00:13:14,508 --> 00:13:18,172
to do any of those. So he improves efficiency, make it a

207
00:13:18,188 --> 00:13:21,524
lot easier to deploy and it simplifies

208
00:13:21,564 --> 00:13:25,068
the workflow. And again, there's no python dependency. A lot of

209
00:13:25,076 --> 00:13:28,620
people, if you haven't really worked in the large language model

210
00:13:28,652 --> 00:13:32,276
space in the past, that Python dependency is actually a

211
00:13:32,300 --> 00:13:35,424
nightmare even for very experienced people in this field.

212
00:13:37,084 --> 00:13:41,004
You can use rust or you can use JavaScript

213
00:13:41,124 --> 00:13:44,988
that allows us to build applications that are similar to

214
00:13:45,116 --> 00:13:48,388
say the latest advance in OpenAI. So if you look at OpenAI,

215
00:13:48,436 --> 00:13:51,940
they have a systems API, they have a stateful API that

216
00:13:51,972 --> 00:13:54,612
build a lot of functionality into the API server itself.

217
00:13:54,748 --> 00:13:58,436
So the Lama H platform allows you to develop your

218
00:13:58,460 --> 00:14:02,010
own API server to serve whatever

219
00:14:02,042 --> 00:14:05,578
the front end that you want to do. So that

220
00:14:05,706 --> 00:14:09,330
gives you a lot of flexibility in terms of building

221
00:14:09,402 --> 00:14:12,682
advanced applications if you pay

222
00:14:12,698 --> 00:14:16,242
close attention. There's a word that I put in bold here that

223
00:14:16,258 --> 00:14:19,490
is single portable and the deployable application,

224
00:14:19,602 --> 00:14:23,330
meaning you are talking about a developer platform that is based

225
00:14:23,362 --> 00:14:27,002
on rust that has no python dependency. One of the biggest issues

226
00:14:27,058 --> 00:14:31,112
people are going to ask is that how about cross platform compatibility?

227
00:14:31,248 --> 00:14:34,512
Because for highly efficient

228
00:14:34,568 --> 00:14:38,444
native applications like that, if you write it

229
00:14:38,744 --> 00:14:41,880
on one platform, it's likely you are not going to be able to deploy it

230
00:14:41,912 --> 00:14:45,880
on a different platform. That sort of makes

231
00:14:45,912 --> 00:14:49,248
it really difficult for developers because I have a Mac or Windows,

232
00:14:49,336 --> 00:14:52,324
but my application that I compiled and tested there,

233
00:14:53,744 --> 00:14:56,632
is that really true that I can't deploy it on a media device in the

234
00:14:56,648 --> 00:15:00,414
cloud? So in the next demo I'm going to show you how

235
00:15:00,754 --> 00:15:04,138
the was manage and the laminage stack address this problem

236
00:15:04,226 --> 00:15:08,154
that we allow you to build truly portable large

237
00:15:08,194 --> 00:15:12,050
language model applications. So here's a demo. In this demo I'll

238
00:15:12,082 --> 00:15:15,138
show you a key benefit of using webassembly

239
00:15:15,186 --> 00:15:18,214
or wasm to run ARM applications. Portability.

240
00:15:18,634 --> 00:15:22,170
The webassembly application is a binary application that is directly

241
00:15:22,202 --> 00:15:24,962
portable across different operating systems,

242
00:15:25,058 --> 00:15:29,018
hardwares and drivers. So on this screen I have

243
00:15:29,186 --> 00:15:33,322
opened a terminal to

244
00:15:33,338 --> 00:15:36,818
my remote machine on Microsoft Azure. That machine runs Linux,

245
00:15:36,906 --> 00:15:40,498
has Nvidia tiffo card and have the CudA twelve

246
00:15:40,546 --> 00:15:44,014
driver installed on it. If I see what's in here,

247
00:15:45,514 --> 00:15:48,602
you can see I have downloaded the large language model,

248
00:15:48,778 --> 00:15:51,534
the Gamma two B model in GguI format,

249
00:15:52,234 --> 00:15:55,934
and then a bunch of HTML and JavaScript files in the chatbot UI.

250
00:15:56,524 --> 00:16:00,108
I have also installed the was image runtime here. So if I say wash,

251
00:16:00,236 --> 00:16:04,028
it's was 13.5. So now we have the large language

252
00:16:04,076 --> 00:16:07,132
model and we have the runtime for the large language model. We are

253
00:16:07,148 --> 00:16:11,028
still missing the application. The application is something that

254
00:16:11,156 --> 00:16:14,788
takes a user input, runs a large language model, prompting the larger language,

255
00:16:14,836 --> 00:16:17,612
runs a large language model, generate a response and then send it back to the

256
00:16:17,628 --> 00:16:21,300
user. So you can call it a chatbot application, it could be a web

257
00:16:21,332 --> 00:16:25,008
application, it could be a discord, or it could be asian

258
00:16:25,056 --> 00:16:28,944
application that connects to other applications that takes the large language

259
00:16:28,984 --> 00:16:32,884
model output to do things. So the application

260
00:16:34,504 --> 00:16:38,528
is a key part that as developers that you would write in

261
00:16:38,536 --> 00:16:42,804
the past, you wouldn't expect this application to be portable because

262
00:16:43,144 --> 00:16:46,536
when you develop this application you are probably on the Mac or Windows machine

263
00:16:46,600 --> 00:16:50,160
and you compared to say Apple silicon and use the metal

264
00:16:50,192 --> 00:16:53,970
framework, you have all this built in. And you wouldn't

265
00:16:54,002 --> 00:16:57,938
think that by just copying this binary application to

266
00:16:58,026 --> 00:17:01,498
Nvidia machine it would just run there, right? And of course

267
00:17:01,546 --> 00:17:04,770
there are ways to make it easier. For instance, if you use Python, Python has

268
00:17:04,802 --> 00:17:08,454
a lot of abstractions that allows you to write

269
00:17:09,834 --> 00:17:13,946
at a fairly high level so that you can write a Python script on

270
00:17:13,970 --> 00:17:18,707
Mac and then try to run this same script on

271
00:17:18,715 --> 00:17:21,843
the Linux machine. However, as you would imagine Python,

272
00:17:21,963 --> 00:17:25,547
in order to achieve that, Python have a huge amount of dependencies.

273
00:17:25,715 --> 00:17:29,307
Like if you go to the Pytorch official docker image, it is

274
00:17:29,395 --> 00:17:32,947
4gb right there. So the Pytorch docker image

275
00:17:32,995 --> 00:17:36,275
itself is 4gb and it's platform dependent,

276
00:17:36,379 --> 00:17:39,683
so you have to install the right Python version. And within Python

277
00:17:39,723 --> 00:17:43,499
you also have oftentimes you need to specify what is the underlying architecture.

278
00:17:43,611 --> 00:17:46,863
So it is huge dependency and it's not really that portable

279
00:17:47,003 --> 00:17:50,672
for any other languages like rust or go, or if you

280
00:17:50,688 --> 00:17:53,004
write your application in any of those other languages,

281
00:17:53,984 --> 00:17:58,176
you would not imagine that it would be portable. That's because the

282
00:17:58,200 --> 00:18:01,864
underlying GPU and CPU architectures are entirely different. What WASM

283
00:18:01,904 --> 00:18:06,216
does is that it provides abstraction for those applications

284
00:18:06,280 --> 00:18:09,632
so that it can run smoothly across all different platforms. In order to

285
00:18:09,648 --> 00:18:13,084
demonstrate that, I switch to a window. This is my,

286
00:18:13,514 --> 00:18:16,714
this is on my local machine, which is a MacBook,

287
00:18:16,874 --> 00:18:20,322
and what I have already downloaded one

288
00:18:20,338 --> 00:18:23,986
of the API server applications from the Lama Edge project which is a rust

289
00:18:24,050 --> 00:18:27,442
application. And I compiled it on my Mac and I tested it on

290
00:18:27,458 --> 00:18:31,614
my Mac, right? So now what I'm going to do is I'm going to just

291
00:18:31,954 --> 00:18:35,930
scp this entire file to the remote azure machine.

292
00:18:36,082 --> 00:18:40,662
So as you can see, the entire file is only nine megabytes and

293
00:18:40,838 --> 00:18:44,406
we didn't package it in any way, we didn't have a docker

294
00:18:44,430 --> 00:18:47,774
image around, you know, wrapped around it.

295
00:18:47,894 --> 00:18:51,926
We just scp the whole thing to another machine and

296
00:18:52,070 --> 00:18:55,486
with entirely different architecture in both hardware and software and

297
00:18:55,510 --> 00:18:59,062
expect to run there. Can we run there? So let's see. So we use

298
00:18:59,078 --> 00:19:01,234
the WASM edge runtime to run it.

299
00:19:02,534 --> 00:19:06,214
So the WASM edge runtime starts instantly. It's because

300
00:19:06,254 --> 00:19:09,790
it's an application, so it loads the large language model and then

301
00:19:09,902 --> 00:19:14,134
it starts an API server. The web server is actually accessible

302
00:19:14,174 --> 00:19:16,554
through port 8080. So if you have this machine,

303
00:19:16,974 --> 00:19:20,342
the port open public, you would be able to load up browser and go

304
00:19:20,358 --> 00:19:23,710
to port 880 and see it. But for

305
00:19:23,742 --> 00:19:26,998
now we want to just stay on this machine because we have it under a

306
00:19:27,006 --> 00:19:30,238
firewall. What are we going to try is that we're going to do API

307
00:19:30,286 --> 00:19:34,366
request, because this API server also takes open AI style

308
00:19:34,430 --> 00:19:38,288
API requests that allows us to integrate with all the

309
00:19:38,416 --> 00:19:42,144
openi ecosystem tools. So here's

310
00:19:42,184 --> 00:19:46,240
how the request looks like. So as you can see,

311
00:19:46,312 --> 00:19:49,616
we request at the localhost and then we send a

312
00:19:49,640 --> 00:19:53,480
message with a row of user and say where's Paris? We ask the model,

313
00:19:53,512 --> 00:19:56,856
where's Paris? Right. If I do this, it does the

314
00:19:56,880 --> 00:20:00,672
inference, its result come back before I can finish speaking.

315
00:20:00,848 --> 00:20:04,260
So the result is, the role of the result is

316
00:20:04,292 --> 00:20:07,996
system and the content is Paris located in hardware friends, blah, blah, blah.

317
00:20:08,140 --> 00:20:11,148
So now we have achieved something, I think very interesting,

318
00:20:11,196 --> 00:20:14,476
that we compiled a rust application on the

319
00:20:14,500 --> 00:20:18,036
Mac and fully taking advantage of the Mac GPU

320
00:20:18,140 --> 00:20:21,676
and the metal framework. And then we just copied this

321
00:20:21,700 --> 00:20:25,348
WASM application into a remote Linux machine

322
00:20:25,396 --> 00:20:28,356
running on Nvidia. And we can see this application,

323
00:20:28,460 --> 00:20:32,058
that's which the role of the application start server and interact

324
00:20:32,106 --> 00:20:35,850
with the underlying large

325
00:20:35,882 --> 00:20:39,082
language model runs just perfectly on

326
00:20:39,098 --> 00:20:42,930
the new hardware, fully utilizing the

327
00:20:42,962 --> 00:20:46,774
media GPU capabilities to accelerate. Without the GPU

328
00:20:47,074 --> 00:20:51,322
capabilities, you would not be able to have nearly 100

329
00:20:51,378 --> 00:20:55,098
tokens per second speed on this

330
00:20:55,186 --> 00:20:58,570
Linux machine. If you're just doing the cpu, you'll be more like one tokens

331
00:20:58,602 --> 00:21:01,394
per second, you know, it would be two orders,

332
00:21:02,334 --> 00:21:06,180
magnitude's difference. So that's it, that's what

333
00:21:06,182 --> 00:21:09,394
we have shown, that the WASM application is truly portable.

334
00:21:09,694 --> 00:21:13,110
All right, to recap the demo,

335
00:21:13,262 --> 00:21:17,030
there's so for the longest time we

336
00:21:17,062 --> 00:21:20,390
have platform engineering, or DevOps,

337
00:21:20,502 --> 00:21:24,534
that combines the role of developer and Ops. But with the new

338
00:21:24,574 --> 00:21:27,990
hardware, with more and more different devices and different drivers,

339
00:21:28,102 --> 00:21:31,402
all that stuff, that's that coming along for AI applications.

340
00:21:31,578 --> 00:21:35,426
And I think it's time to separate the dev and Ops role all

341
00:21:35,450 --> 00:21:39,338
over again. So the way it works is that

342
00:21:39,466 --> 00:21:43,250
Webassembly is a virtual machine format. You can think of it

343
00:21:43,442 --> 00:21:47,066
like a Java Java bytecode, so it provides abstraction over

344
00:21:47,090 --> 00:21:50,850
the real hardware. And for developers, you just need to

345
00:21:50,882 --> 00:21:54,250
write to the Webassembly interface. In our case, you write

346
00:21:54,282 --> 00:21:57,690
to the Lama edge SDK interface and it

347
00:21:57,722 --> 00:22:01,410
tells the SDK to say load a model and

348
00:22:01,442 --> 00:22:05,426
do the inference. And the

349
00:22:05,450 --> 00:22:08,778
developer only need to write application this way. So if the application,

350
00:22:08,906 --> 00:22:12,386
once the application is compiled to WASM, the developer's

351
00:22:12,410 --> 00:22:15,986
job is done. He or she can ship the application anywhere that

352
00:22:16,010 --> 00:22:19,866
they want and lets the runtime takes

353
00:22:19,890 --> 00:22:23,450
over the rest. So it's the Ops people that

354
00:22:23,482 --> 00:22:26,626
needs to install the correct runtime and driver for each device.

355
00:22:26,690 --> 00:22:29,908
So for instance, on a Mac we want to install the Mac version of was

356
00:22:29,956 --> 00:22:33,580
made. It's sort of like Java. On the Mac you need to install the Mac

357
00:22:33,612 --> 00:22:36,908
version of Java, right, the JVM, right. You know, so it's the

358
00:22:36,916 --> 00:22:40,804
same thing here. So you want to install the Mac version of the was runtime.

359
00:22:40,924 --> 00:22:44,956
If you have Nvidia device, you need to Ubuntu with CUDA twelve,

360
00:22:45,020 --> 00:22:48,636
you need to install the appropriate was runtime

361
00:22:48,660 --> 00:22:52,224
in there as well. So for the Ops guys, they can,

362
00:22:52,844 --> 00:22:55,948
once they take care of that, they would be able to just run

363
00:22:55,996 --> 00:22:59,106
that wasm application without any modification.

364
00:22:59,250 --> 00:23:02,626
Because the WASM edge runtime has a contract, has a

365
00:23:02,650 --> 00:23:06,042
standard API that's, that is defined in the

366
00:23:06,058 --> 00:23:10,050
llama HSDK, right? You know, so once it sees those instructions to

367
00:23:10,082 --> 00:23:13,546
say, load a model and you know, send some text to the model

368
00:23:13,570 --> 00:23:17,034
for inference, and it sees those instructions,

369
00:23:17,074 --> 00:23:21,186
the byte code, you automatically run those code and it would translate those code

370
00:23:21,250 --> 00:23:24,590
into that's instructions that are appropriate for the

371
00:23:24,622 --> 00:23:28,910
underlying accelerator and the drivers. So it allows developers

372
00:23:28,942 --> 00:23:32,662
to write truly portable applications that can be deployed anywhere,

373
00:23:32,718 --> 00:23:36,774
and it can be managed by tools like kubernetes and

374
00:23:36,814 --> 00:23:40,534
let ops people worry about installing the right driver and

375
00:23:40,574 --> 00:23:44,670
installing the right was match runtime on every single node or

376
00:23:44,702 --> 00:23:47,514
every single edge devices that you have in your cluster.

377
00:23:48,634 --> 00:23:52,362
So that leads us to our last demo, because we

378
00:23:52,378 --> 00:23:56,026
have talked about Lama Edge being a developer platform,

379
00:23:56,170 --> 00:23:59,938
and one of the most popular applications people do with Python, at least

380
00:23:59,986 --> 00:24:03,354
today, is what they call a rack application, meaning that

381
00:24:03,394 --> 00:24:06,978
you use a standard large language model, a fine tuned large

382
00:24:07,026 --> 00:24:10,962
language model, but you feed it with your proprietary

383
00:24:11,058 --> 00:24:15,030
knowledge base. The knowledge base was divided,

384
00:24:15,142 --> 00:24:18,558
it's typically a text, a PDF or image, or you

385
00:24:18,566 --> 00:24:22,574
know, or a text file, and it was divided into segments and each

386
00:24:22,614 --> 00:24:26,190
segment was turned into a vector and stored in a vector database.

387
00:24:26,382 --> 00:24:30,190
And when the user

388
00:24:30,222 --> 00:24:33,734
asks a new question from the API, the application

389
00:24:33,854 --> 00:24:37,390
would take, would take that question, turn that into

390
00:24:37,422 --> 00:24:41,158
vector as well, and perform a vector search in the database to find out

391
00:24:41,206 --> 00:24:45,234
which other, which texts in the knowledge base are most related

392
00:24:47,054 --> 00:24:51,350
to the question. And then it would add the context that retracted

393
00:24:51,382 --> 00:24:54,310
from the knowledge base and the new question into the prompt,

394
00:24:54,382 --> 00:24:57,878
and asks the large language model to give an answer. As you

395
00:24:57,886 --> 00:25:01,782
can see, this is a fairly complex process, and it involves

396
00:25:01,958 --> 00:25:05,254
not just the large language model, not just the runtime, but also

397
00:25:05,334 --> 00:25:08,630
things like the vector database, the embedding model and all

398
00:25:08,662 --> 00:25:12,074
that. Things that you have to tie together,

399
00:25:12,974 --> 00:25:17,038
as we talked about previously in this talk, is that

400
00:25:17,126 --> 00:25:21,606
things like that was traditionally done by say fairly complicated

401
00:25:21,670 --> 00:25:25,006
Python programs that does the orchestration, the queries,

402
00:25:25,150 --> 00:25:28,114
the turn into act with embeddings and all that stuff,

403
00:25:28,774 --> 00:25:32,790
and then you attach another UI in front of it. So it's a fairly involved

404
00:25:32,822 --> 00:25:36,374
and a complicated process. But with Lama Edge, we will

405
00:25:36,414 --> 00:25:40,914
be able to build a single application that can

406
00:25:41,094 --> 00:25:45,034
talk to the vector database, call the embeddings when it's needed,

407
00:25:45,194 --> 00:25:49,306
and perform the vector search, and then at

408
00:25:49,330 --> 00:25:52,658
the end prompts a large language model to get an answer.

409
00:25:52,786 --> 00:25:55,866
So this project, we call it an

410
00:25:55,890 --> 00:25:59,746
integrated assistant API server, which because it looks

411
00:25:59,770 --> 00:26:04,106
a lot like open eyes assistant API and

412
00:26:04,130 --> 00:26:07,894
we call this project the Garnet. And here's a demo for that.

413
00:26:08,964 --> 00:26:12,660
Hi. In this demo I'll show you the easiest way to

414
00:26:12,692 --> 00:26:16,636
run reg or RaC API server

415
00:26:16,740 --> 00:26:20,932
for a large language model. So if you are familiar with RAC,

416
00:26:21,108 --> 00:26:25,692
it is, it is a way to add knowledge.

417
00:26:25,748 --> 00:26:29,556
It could be additional public knowledge or proprietary

418
00:26:29,700 --> 00:26:32,884
knowledge that you don't want other people to see to an existing

419
00:26:32,924 --> 00:26:36,620
large language model, so that the larger language model can answer questions and chat

420
00:26:36,652 --> 00:26:40,064
with people based on additional context that you provide to it.

421
00:26:40,444 --> 00:26:43,916
So in order to do that, a typical rack

422
00:26:43,940 --> 00:26:47,664
setup requires a fairly complex setup that requires say

423
00:26:48,924 --> 00:26:52,420
install digital vector database UI, how to upload

424
00:26:52,452 --> 00:26:56,308
the knowledge and the tools like Lanchain

425
00:26:56,356 --> 00:27:00,036
to orchestrate how to manage retrieve data from the vector

426
00:27:00,060 --> 00:27:04,024
database and how to prompt application.

427
00:27:04,364 --> 00:27:08,296
So in our approach that we want to introduce a project called

428
00:27:08,320 --> 00:27:11,832
Gayanet and the Garnet is an application that build on was

429
00:27:11,888 --> 00:27:15,560
matched and also using the Lama edge

430
00:27:15,592 --> 00:27:19,200
framework. So what it does is that it allows

431
00:27:19,312 --> 00:27:23,072
applications, you can write simple

432
00:27:23,168 --> 00:27:27,364
rock applications using rust and then compile it into a single

433
00:27:27,704 --> 00:27:31,184
wasm binary with zero python dependency and

434
00:27:31,224 --> 00:27:34,648
run it very efficiently at the server. So let's see how

435
00:27:34,656 --> 00:27:38,126
it works. If you go to the Garnet GitHub repository, by the way,

436
00:27:38,150 --> 00:27:40,074
if you are here, just give us a star.

437
00:27:41,534 --> 00:27:46,390
And here is a quick start guide and

438
00:27:46,462 --> 00:27:49,926
this is called Gaia node. And there is a one line

439
00:27:49,950 --> 00:27:55,006
of command which you can use to install Gaia which let's

440
00:27:55,030 --> 00:27:59,126
do that and explain what it does. So this is on my local Mac machine.

441
00:27:59,230 --> 00:28:02,542
And if I say install, what it does is that it installs the

442
00:28:02,558 --> 00:28:05,898
was match runtime which is required to run a

443
00:28:05,906 --> 00:28:09,690
large language model. It installs a quadrant vector

444
00:28:09,722 --> 00:28:13,234
database which is required, and it's download a chat

445
00:28:13,314 --> 00:28:17,066
model which it has already downloaded. We call it a

446
00:28:17,090 --> 00:28:20,746
standard Lama two seven b chat model and download the embedding model

447
00:28:20,770 --> 00:28:23,854
which used to process the vector.

448
00:28:24,794 --> 00:28:27,970
But here what's really interesting, you can ignore the error here,

449
00:28:28,082 --> 00:28:31,562
what's really interesting is that it also downloads a knowledge collection

450
00:28:31,618 --> 00:28:34,688
which is a knowledge base we vectorized into

451
00:28:34,736 --> 00:28:38,000
the quadrant format. You can read in the documentation how

452
00:28:38,032 --> 00:28:42,200
we do that, and we have a separate rust application that

453
00:28:42,232 --> 00:28:46,488
helps you to do that. And then it

454
00:28:46,536 --> 00:28:50,004
installs that snapshot into the quantum database.

455
00:28:51,384 --> 00:28:55,336
So what it does is that it creates a Gaia net

456
00:28:55,400 --> 00:28:59,008
directory in your home directory

457
00:28:59,056 --> 00:29:02,508
and put everything in there and including this wasm file

458
00:29:02,556 --> 00:29:07,412
that starts up the rack application server. And here

459
00:29:07,468 --> 00:29:09,784
if you look at the config JSon,

460
00:29:11,244 --> 00:29:14,204
you will be able to see the chat model that's being used.

461
00:29:14,284 --> 00:29:17,984
The parameters for the chat model and the snapshot

462
00:29:18,524 --> 00:29:22,324
is knowledge

463
00:29:22,364 --> 00:29:25,964
base that vertebrates knowledge base and the prompt and you can change,

464
00:29:26,084 --> 00:29:28,952
you can modify any of the things that you want, use a different model,

465
00:29:29,068 --> 00:29:32,464
use a different knowledge base and rerun install. Right.

466
00:29:32,584 --> 00:29:36,016
So once you have the, once you have everything installed that

467
00:29:36,120 --> 00:29:41,136
you would be able to say run

468
00:29:41,160 --> 00:29:44,576
the start script. What it does is that it's going to start the vector

469
00:29:44,600 --> 00:29:47,928
database like we said, and they're going to start the was image application

470
00:29:47,976 --> 00:29:51,136
server and it's going to start a domain server which give it

471
00:29:51,160 --> 00:29:54,144
the public access for domain for the server.

472
00:29:54,184 --> 00:29:58,424
Right. So what are we going to do is that we're going to open while

473
00:29:58,464 --> 00:30:02,232
this is runs on our local machine. I can use localhost, but I can

474
00:30:02,328 --> 00:30:05,592
because it gives me a publicly accessible domain. So what I'm going to do is

475
00:30:05,608 --> 00:30:09,124
that I'm going to go to load this public accessible domain.

476
00:30:10,024 --> 00:30:13,064
So it gives me how to run access API.

477
00:30:13,104 --> 00:30:16,216
But what I would do is that I would just chat with the node while

478
00:30:16,240 --> 00:30:20,144
it's loading. I would come back here and tell you and open

479
00:30:20,184 --> 00:30:21,324
up the log here.

480
00:30:31,284 --> 00:30:34,692
So this is a lama edge startup log which

481
00:30:34,748 --> 00:30:37,828
logs all the interactions with large language model,

482
00:30:37,876 --> 00:30:41,036
right? So now it's,

483
00:30:41,180 --> 00:30:44,492
the UI has come up. I want to demonstrate to

484
00:30:44,508 --> 00:30:48,172
you that it does use a knowledge base to chat with us. So if I

485
00:30:48,188 --> 00:30:52,726
say where's Paris? Because the knowledge base I'm using right

486
00:30:52,750 --> 00:30:56,686
now by default is a

487
00:30:56,710 --> 00:31:00,726
guide, the knowledge comes from Paris guidebook.

488
00:31:00,870 --> 00:31:04,350
So I vectorized that Paris guidebook and put that into the vector

489
00:31:04,382 --> 00:31:08,006
database so that the large language model

490
00:31:08,030 --> 00:31:11,526
can generate answers for me. So it says thank you for the

491
00:31:11,550 --> 00:31:14,646
question based on the context. Paris located at north central part

492
00:31:14,670 --> 00:31:17,858
of France, blah blah, blah blah. How do

493
00:31:17,866 --> 00:31:21,290
we know that this is actually using

494
00:31:21,362 --> 00:31:24,890
our context? How does we know that our RAC

495
00:31:24,922 --> 00:31:28,418
application server works? We go back to the log and we can

496
00:31:28,466 --> 00:31:32,762
see the actual prompt. So you can see this is the

497
00:31:32,818 --> 00:31:35,882
actual prompt the application server sends to the database. So you

498
00:31:35,898 --> 00:31:39,674
are helpful assistant. And then here's the context. If you don't

499
00:31:39,714 --> 00:31:42,738
know if the answer is not in the context, you don't answer it. So those

500
00:31:42,786 --> 00:31:46,874
three paragraph context come from our vector database.

501
00:31:46,954 --> 00:31:50,954
That's come from the Paris guidebook. It's about general information about

502
00:31:50,994 --> 00:31:54,410
Paris. And then we ask question, where's Paris? Right. You know,

503
00:31:54,442 --> 00:31:58,494
so it's answers based on that, based on that context.

504
00:31:58,914 --> 00:32:02,034
So you can ask follow up questions. Of course you can plan when they trip

505
00:32:02,074 --> 00:32:05,970
or whatever, but. And you can also use it as open AI

506
00:32:06,082 --> 00:32:09,866
API server. As you can imagine, you can have your knowledge base

507
00:32:09,930 --> 00:32:13,246
about a code repository and then use

508
00:32:13,270 --> 00:32:17,126
a fine tuned model that generates code or jSon, and have this

509
00:32:17,190 --> 00:32:21,374
packaged together as an API server that connects to a chatbot or

510
00:32:21,454 --> 00:32:24,554
something like that. That allows you to build

511
00:32:25,014 --> 00:32:28,974
an entirely portable application that can

512
00:32:29,014 --> 00:32:32,798
run across many different GPU architectures and drivers without

513
00:32:32,886 --> 00:32:36,914
any need for large dependencies like Python.

514
00:32:37,094 --> 00:32:41,330
All right, so that brings the end to our

515
00:32:41,362 --> 00:32:46,242
talk. I think we're going to have 30 minutes. And so we

516
00:32:46,258 --> 00:32:49,650
have done three different demos, from the

517
00:32:49,682 --> 00:32:52,922
easy to the most sophisticated. And I hope you

518
00:32:52,938 --> 00:32:56,714
would have time to at least install Lama

519
00:32:56,754 --> 00:32:59,626
edge on your own computer and install large language models so you can play with

520
00:32:59,650 --> 00:33:04,004
it. And if you find it interesting, you could install garnet

521
00:33:04,044 --> 00:33:07,224
node and build your own knowledge base and have

522
00:33:07,604 --> 00:33:10,844
a large language model on your own device, answer questions in the way that you

523
00:33:10,884 --> 00:33:13,744
want. So yeah,

524
00:33:14,364 --> 00:33:17,548
there are a lot more that we can get into. So for instance,

525
00:33:17,596 --> 00:33:21,004
what's the rust application look like, how does SDK look like? And things

526
00:33:21,044 --> 00:33:24,036
like that. But I don't think we have time for that at this moment.

527
00:33:24,100 --> 00:33:27,876
And if you're interested, we have those three open

528
00:33:27,900 --> 00:33:30,144
source GitHub repositories that you can,

529
00:33:31,754 --> 00:33:33,054
that you can go to.

530
00:33:34,514 --> 00:33:37,850
And please feel free to raise the issue and find

531
00:33:37,882 --> 00:33:40,682
us there and chat with us. All right, thank you so much.

