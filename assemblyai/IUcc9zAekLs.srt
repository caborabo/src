1
00:00:27,594 --> 00:00:31,386
I am Antonio and today with Francesco we are going to present

2
00:00:31,490 --> 00:00:35,202
GDPR and beyond demystifying data governance

3
00:00:35,258 --> 00:00:39,138
challenges. Francesco and I are data architects

4
00:00:39,186 --> 00:00:43,210
at Agilelab, an italian consulting firm specializing in

5
00:00:43,242 --> 00:00:46,602
large scale data management. Agileab is

6
00:00:46,618 --> 00:00:50,594
an effective and dynamic company structured around an olacosy

7
00:00:50,634 --> 00:00:53,374
inspired model with multiple business units.

8
00:00:53,814 --> 00:00:57,766
Through these business units, we are lucky enough to have several Fortune

9
00:00:57,790 --> 00:01:01,742
500 companies as our customers. Ok,

10
00:01:01,878 --> 00:01:05,974
lets take a look at what we have in the agenda. Today we will

11
00:01:06,014 --> 00:01:08,834
talk about data privacy and GDPR,

12
00:01:09,174 --> 00:01:12,726
why this european regulation is so important and we

13
00:01:12,750 --> 00:01:15,474
should design systems to be compliant with it.

14
00:01:15,854 --> 00:01:19,518
We will have an overview of different techniques that we can leverage

15
00:01:19,566 --> 00:01:23,704
to be compliant and secure like anonymization and encryption.

16
00:01:24,004 --> 00:01:27,604
Then we will compare these different techniques focusing

17
00:01:27,644 --> 00:01:30,908
our attention on pros and cons of each one.

18
00:01:31,076 --> 00:01:34,724
Finally, we represent a viable data sharing strategy

19
00:01:34,764 --> 00:01:38,332
for real use. Case data is an UI,

20
00:01:38,468 --> 00:01:41,852
am I right? Data is the fuel

21
00:01:41,908 --> 00:01:45,412
for innovation. Machine learning, artificial intelligence and

22
00:01:45,468 --> 00:01:48,984
analytics simply can't be possible without data.

23
00:01:49,404 --> 00:01:53,868
Just as oil power engines data fuels algorithms,

24
00:01:53,996 --> 00:01:57,044
enabling machines to learn and improve over time,

25
00:01:57,164 --> 00:02:00,956
this is the case for machine learning. Also. Data is

26
00:02:00,980 --> 00:02:04,612
the lifeblood of AI driving smart ecosystem

27
00:02:04,668 --> 00:02:06,864
that can mimic human intelligence.

28
00:02:07,484 --> 00:02:10,844
Finally, data analytics extracts valuable

29
00:02:10,924 --> 00:02:14,984
insights just as refining oil produces useful products.

30
00:02:15,494 --> 00:02:19,022
Yeah, data is the neural. Oil is the

31
00:02:19,038 --> 00:02:22,774
neural also in the bad parts of it.

32
00:02:22,934 --> 00:02:27,086
For example, data breaches are very similar to oil spills.

33
00:02:27,270 --> 00:02:30,994
They cause extensive damage and leak sensitive information

34
00:02:31,374 --> 00:02:34,114
that erode trust of our customers.

35
00:02:34,654 --> 00:02:38,254
We can also have privacy violations which are very similar

36
00:02:38,334 --> 00:02:42,234
to the pollution that can harm ecosystems.

37
00:02:42,754 --> 00:02:46,874
Privacy violation disrupts the digital environment and arms

38
00:02:46,954 --> 00:02:51,234
individuals. Then we have regulatory fines which

39
00:02:51,394 --> 00:02:55,374
are very comparable to environmental fines,

40
00:02:56,234 --> 00:02:59,962
which means that when your data is non compliance

41
00:03:00,018 --> 00:03:03,226
with data protection regulation, you will get

42
00:03:03,290 --> 00:03:07,130
very huge fines. Finally, you will suffer

43
00:03:07,202 --> 00:03:10,722
for reputational damage because like environmental

44
00:03:10,778 --> 00:03:14,242
damage, data breaches can severely

45
00:03:14,298 --> 00:03:18,098
impact brand trust and loyalty. So let's

46
00:03:18,146 --> 00:03:22,066
reflect on the reality of data breaches over the past 20

47
00:03:22,170 --> 00:03:25,914
years. This visualization showcases the top 50

48
00:03:25,994 --> 00:03:29,442
biggest data breaches. From 2004 to 2020,

49
00:03:29,498 --> 00:03:33,290
117 billion records

50
00:03:33,362 --> 00:03:36,722
were compromised. As we can see,

51
00:03:36,818 --> 00:03:40,394
the severity of breaches is escalating during the years,

52
00:03:40,474 --> 00:03:43,872
particularly from 2020 2016 onwards, with the web

53
00:03:43,928 --> 00:03:47,888
sector being the hardest hit, accounting for nearly 10

54
00:03:47,936 --> 00:03:51,680
billion records lost. Significant breaches span

55
00:03:51,712 --> 00:03:55,240
across various sectors including finance, government and

56
00:03:55,272 --> 00:03:58,124
tech, highlighting the widespread vulnerability.

57
00:03:58,864 --> 00:04:02,624
Notable breaches include Yahoo 2013 losing

58
00:04:02,664 --> 00:04:06,208
3 billion records and Facebook in 2019 with

59
00:04:06,336 --> 00:04:09,484
530 million records exposed.

60
00:04:10,004 --> 00:04:13,676
This growing trend underscores the critical need for robust data

61
00:04:13,740 --> 00:04:17,276
security measures. These breaches not only

62
00:04:17,340 --> 00:04:20,460
compromise personal information, but also erode

63
00:04:20,492 --> 00:04:23,980
public trust and pose severe financial risks.

64
00:04:24,172 --> 00:04:28,384
As we move forward, it is imperative to prioritize

65
00:04:28,844 --> 00:04:32,164
data protection and adopt stringent security protocols

66
00:04:32,204 --> 00:04:34,744
to safeguard these digital assets.

67
00:04:35,304 --> 00:04:39,204
Thats why European Union came up with GDPR.

68
00:04:39,584 --> 00:04:43,404
GDPR stands for General Data protection Regulation

69
00:04:43,704 --> 00:04:47,728
and is a regulation that requires businesses around the world

70
00:04:47,816 --> 00:04:51,256
to protect the personal data and privacies of European

71
00:04:51,320 --> 00:04:55,408
Union citizens. Starting from on the

72
00:04:55,496 --> 00:04:59,400
25 May 2018, GDPR puts

73
00:04:59,432 --> 00:05:03,456
in place certain restrictions on the collection, use and retention

74
00:05:03,520 --> 00:05:06,942
of personal data. Personal data is defined

75
00:05:06,998 --> 00:05:11,638
as information relating to unidentified or identifiable

76
00:05:11,726 --> 00:05:15,398
natural person. This includes data such as name,

77
00:05:15,486 --> 00:05:19,110
email, phone number, in addition to data that may be

78
00:05:19,142 --> 00:05:23,014
less obvious like API addresses, gps location, phone id

79
00:05:23,094 --> 00:05:26,958
and more. GDPR is based on some

80
00:05:27,006 --> 00:05:30,414
key principles and we will briefly

81
00:05:30,454 --> 00:05:34,450
run first of all is lawfulness. Personal data

82
00:05:34,522 --> 00:05:38,210
must be processed legally, adhering to established laws such

83
00:05:38,242 --> 00:05:41,586
as GDPR. Then we have fairness.

84
00:05:41,730 --> 00:05:45,426
Data processing should be fair, protecting vital

85
00:05:45,490 --> 00:05:48,882
interest, performing tasks carried out in the public interest,

86
00:05:49,018 --> 00:05:52,658
or pursuing legitimate interest of the data controller

87
00:05:52,706 --> 00:05:56,394
or third party. Then we have transparency because

88
00:05:56,474 --> 00:06:00,386
organizations must be open about their data processing activities.

89
00:06:00,570 --> 00:06:04,450
They should provide clear, accessible and understandable information

90
00:06:04,562 --> 00:06:08,026
to individuals about how their data is being used,

91
00:06:08,170 --> 00:06:11,850
who is collecting it and why. And this is

92
00:06:11,882 --> 00:06:16,106
why you have the cookie in every european

93
00:06:16,210 --> 00:06:20,146
website now. Then we have purpose limitations.

94
00:06:20,250 --> 00:06:23,666
So personal data must be collected for specified,

95
00:06:23,850 --> 00:06:27,416
explicit and legitimate purposes and not further

96
00:06:27,480 --> 00:06:31,684
process it in a manner that is not compatible with those purposes.

97
00:06:32,304 --> 00:06:36,272
Then you have the data minimization principle. That means

98
00:06:36,408 --> 00:06:40,168
organizations should collect only the personal data that is

99
00:06:40,216 --> 00:06:43,648
necessary to achieve the specified purpose, and we will have

100
00:06:43,696 --> 00:06:47,144
more on this later. Then we have accuracy. So personal

101
00:06:47,224 --> 00:06:49,844
data must be accurate and kept up to date,

102
00:06:50,344 --> 00:06:54,444
and inaccurate data should be corrected or deleted.

103
00:06:54,814 --> 00:06:58,854
Storage limitation means that personal data should not be kept longer than

104
00:06:58,894 --> 00:07:02,486
necessary for the purpose for which it was collected. For example,

105
00:07:02,630 --> 00:07:05,798
an example of storage limitation is the right to be forgotten.

106
00:07:05,846 --> 00:07:09,326
If I ask to be forgotten by some company, they should

107
00:07:09,470 --> 00:07:13,514
delete all data about me. This is storage limitation.

108
00:07:14,094 --> 00:07:17,166
Then we have integrity and confidentiality.

109
00:07:17,270 --> 00:07:20,466
So organization must ensure that the security

110
00:07:20,530 --> 00:07:24,026
of personal data, protecting it against unauthorized or

111
00:07:24,130 --> 00:07:27,602
unlawful processing, accidental loss, destruction or damage

112
00:07:27,658 --> 00:07:31,402
like data breaches. And then you have accountability. So data

113
00:07:31,458 --> 00:07:35,202
controllers are responsible for complying with GDPR principles

114
00:07:35,338 --> 00:07:38,454
and must be able to demonstrate their compliance.

115
00:07:38,834 --> 00:07:43,094
In order to do that, GDPR creates some requirements around

116
00:07:43,794 --> 00:07:47,526
the regulation itself. It requires that companies

117
00:07:47,630 --> 00:07:51,630
have data protection impact assessments, which are tools used

118
00:07:51,702 --> 00:07:55,614
to identify and mitigate risk associated with data processing

119
00:07:55,654 --> 00:07:58,798
activity. They must follow data

120
00:07:58,846 --> 00:08:02,206
breach notification regulation so they

121
00:08:02,270 --> 00:08:06,006
should timely report data breaches to both authorities

122
00:08:06,110 --> 00:08:09,390
and individuals. Then they need to

123
00:08:09,422 --> 00:08:13,240
appoint a data protection officer, someone inside

124
00:08:13,272 --> 00:08:16,960
the company that ensure that there is a person

125
00:08:17,152 --> 00:08:20,944
responsible for overseeing data protection strategy

126
00:08:20,984 --> 00:08:24,240
and compliance with GDPR. Obviously,

127
00:08:24,312 --> 00:08:27,872
they need to implement data protection by design and by default.

128
00:08:27,968 --> 00:08:31,752
So every data initiative and the company should

129
00:08:31,888 --> 00:08:35,456
comply with this regulation without any

130
00:08:35,600 --> 00:08:39,630
need to integrate it after. And then they

131
00:08:39,662 --> 00:08:43,438
have some record keeping obligations. So they need to be sure

132
00:08:43,566 --> 00:08:47,406
the companies maintain a detailed record of their data

133
00:08:47,550 --> 00:08:51,270
processing activities. This for accountability and

134
00:08:51,302 --> 00:08:55,190
compliance purposes. Obviously, GDPR had

135
00:08:55,262 --> 00:08:59,462
huge implications for data governance. But what is data governance?

136
00:08:59,558 --> 00:09:03,294
Data governance is the process of managing the availability,

137
00:09:03,414 --> 00:09:07,166
usability, integrity and security of the data in enterprise

138
00:09:07,230 --> 00:09:11,370
systems based on internal standards and policy that also control

139
00:09:11,442 --> 00:09:15,098
data usage. An effective data governance

140
00:09:15,186 --> 00:09:19,514
ensures that data is consistent and trustworthy

141
00:09:19,554 --> 00:09:23,778
and doesn't get misused. GDPR had some implications

142
00:09:23,866 --> 00:09:28,414
on internal data governance strategies for companies or enterprises,

143
00:09:29,034 --> 00:09:32,874
such as they had to enhance data security and privacy control.

144
00:09:32,954 --> 00:09:36,616
To be compliant. They needed to improve data quality

145
00:09:36,680 --> 00:09:40,244
and accuracy. Because of the principle we've seen before,

146
00:09:41,024 --> 00:09:44,296
they need to put an increase to accountability and

147
00:09:44,320 --> 00:09:46,844
transparency in how data was used.

148
00:09:47,304 --> 00:09:51,352
And obviously, GDPR put pressure and

149
00:09:51,448 --> 00:09:55,416
created the necessity for regular audits and assessments around

150
00:09:55,480 --> 00:09:58,912
data. Today, we will focus mostly

151
00:09:58,968 --> 00:10:02,818
on the data minimization principle, which in our opinion

152
00:10:02,906 --> 00:10:06,894
is one of the most important ones in GDPR.

153
00:10:07,274 --> 00:10:11,282
Data minimization principle is foundational to responsible data

154
00:10:11,338 --> 00:10:14,642
handling and privacy protection. Under GDPR,

155
00:10:14,738 --> 00:10:18,866
data minimization principle mandates that organizations should

156
00:10:18,930 --> 00:10:22,202
only collect and process the personal data that is

157
00:10:22,258 --> 00:10:25,654
absolutely necessary for their specified purposes.

158
00:10:26,074 --> 00:10:29,784
Imagine you're building a house. You wouldn't order extra

159
00:10:29,824 --> 00:10:33,824
bricks that you'll never use as it would be wasteful and

160
00:10:33,864 --> 00:10:37,496
clutter your space. Similarly, in data processing,

161
00:10:37,600 --> 00:10:40,960
we should avoid collecting excess data. By adhering

162
00:10:40,992 --> 00:10:44,504
to this principle, we not only streamline

163
00:10:44,584 --> 00:10:48,432
our data management practices, but also enhance security

164
00:10:48,568 --> 00:10:51,992
and compliance. Collecting minimal data

165
00:10:52,128 --> 00:10:55,198
reduces the risk of breaches and misuse,

166
00:10:55,376 --> 00:10:58,714
ensuring we respect our customers privacy and build their

167
00:10:58,754 --> 00:11:02,306
trust. This also simplifies data management and

168
00:11:02,330 --> 00:11:06,122
can lead to more efficient processes. So let's commit

169
00:11:06,178 --> 00:11:09,186
to collecting only what we need, protecting privacy

170
00:11:09,290 --> 00:11:12,054
and fostering a culture of data responsibility.

171
00:11:12,594 --> 00:11:15,294
So, does this meme look familiar?

172
00:11:15,834 --> 00:11:19,538
We know that people working on AI, machine learning and

173
00:11:19,586 --> 00:11:22,894
analytics need real data to do their job,

174
00:11:23,074 --> 00:11:27,126
but this clashes with GDPR regulation 99% of the

175
00:11:27,150 --> 00:11:30,254
time. I will leave now the stage to Francesco,

176
00:11:30,334 --> 00:11:34,526
who will show you how we can build a compliant data sharing strategy

177
00:11:34,630 --> 00:11:38,434
and still allow data practice share to be effective.

178
00:11:38,774 --> 00:11:41,974
Here we go, Francesco. This meme will look familiar.

179
00:11:42,094 --> 00:11:45,678
This is a quite common scenario since most of ML

180
00:11:45,766 --> 00:11:49,702
engineers and data scientists need to prototype everyday

181
00:11:49,758 --> 00:11:52,954
their models. In the majority of the cases they

182
00:11:52,994 --> 00:11:56,410
start using development data, but the risk is that when

183
00:11:56,442 --> 00:11:59,634
moving to production, the performance of the model is low,

184
00:11:59,754 --> 00:12:03,202
so they fall back in using sampler data from production.

185
00:12:03,338 --> 00:12:06,770
This also increased the risk of sensitivity, data leakage

186
00:12:06,842 --> 00:12:10,690
and exposure in the minor environment. In the next slide,

187
00:12:10,802 --> 00:12:14,914
let's see how we can unfold this issue and which techniques

188
00:12:14,954 --> 00:12:18,802
could help our case. So the first thing we are going to

189
00:12:18,818 --> 00:12:21,254
talk about is about anonymization.

190
00:12:21,674 --> 00:12:26,266
So data anonymization is one of the techniques that organizations

191
00:12:26,290 --> 00:12:29,858
can use in order to adapt restrict data privacy regulation,

192
00:12:29,946 --> 00:12:35,154
but require the security of personal identifiable information such

193
00:12:35,194 --> 00:12:38,770
as health reports, contact information and

194
00:12:38,802 --> 00:12:42,748
financial details. It affects pronunciation

195
00:12:42,866 --> 00:12:45,804
since it is not a reversible operation.

196
00:12:46,184 --> 00:12:49,944
Cellular initiation simply reduce the correlation of

197
00:12:49,984 --> 00:12:53,912
data set with the original identity of a data subject

198
00:12:54,008 --> 00:12:58,248
and is therefore a useful but not an absolute security

199
00:12:58,336 --> 00:13:02,192
measure. And let's take a look now at the most

200
00:13:02,288 --> 00:13:06,808
common one technique about below the act of anonymization.

201
00:13:06,976 --> 00:13:10,840
The first one that we are going to talk about is generalization.

202
00:13:11,032 --> 00:13:15,064
Generalization usually changes the scale of the data set,

203
00:13:15,184 --> 00:13:18,744
attributes or the order of magnitude. As you may see,

204
00:13:18,824 --> 00:13:22,528
we have a simple table with several columns like name,

205
00:13:22,696 --> 00:13:25,968
age, birth date, state, and disease.

206
00:13:26,096 --> 00:13:29,824
In the example one, a field that includes number like

207
00:13:29,904 --> 00:13:33,688
age can be generalized by expressing an interval.

208
00:13:33,816 --> 00:13:37,202
As you may see, mark age has been

209
00:13:37,368 --> 00:13:40,638
put within an interval between 20 and 30.

210
00:13:40,806 --> 00:13:44,334
In the example two, a filtering class dates like

211
00:13:44,414 --> 00:13:47,558
1993 1019 can be

212
00:13:47,606 --> 00:13:51,630
generalized by using only the year 1993,

213
00:13:51,822 --> 00:13:55,166
and this is the very first method. The other one is

214
00:13:55,230 --> 00:13:59,462
randomization. Randomization involves changing attributes

215
00:13:59,518 --> 00:14:02,638
in a dataset so that they are less precise while

216
00:14:02,686 --> 00:14:06,382
maintaining their overall distribution. Below the app

217
00:14:06,438 --> 00:14:10,558
of randomization, we have textbooks such as noise

218
00:14:10,606 --> 00:14:14,230
addition and shuffling. Noise addition methods

219
00:14:14,262 --> 00:14:17,622
provides to inject some modification within the data

220
00:14:17,678 --> 00:14:21,622
set in order to make it less accurate, for example,

221
00:14:21,758 --> 00:14:24,838
increasing or decreasing the age of a person,

222
00:14:25,006 --> 00:14:28,582
as we can see in our example inside the slide

223
00:14:28,758 --> 00:14:32,494
while shuffling, simply swap the the age of mark and

224
00:14:32,534 --> 00:14:36,046
john and this is the second method.

225
00:14:36,190 --> 00:14:39,374
The third method is the most common one is the

226
00:14:39,414 --> 00:14:42,654
suppression, another useful technique, the most

227
00:14:42,694 --> 00:14:45,534
used in the space of an analyzation. In my opinion,

228
00:14:45,614 --> 00:14:49,638
suppression is the process of removing an attribute's value

229
00:14:49,766 --> 00:14:53,926
entirely from a data set, while reduction removes

230
00:14:53,990 --> 00:14:57,450
part of the attribute value from a data set.

231
00:14:57,622 --> 00:15:00,786
With such techniques you can have multiple issues.

232
00:15:00,930 --> 00:15:04,226
For example, the warning number one is if

233
00:15:04,250 --> 00:15:07,722
the data is collecting for the purpose of determining

234
00:15:07,858 --> 00:15:11,858
at which age individuals are most likely to develop a specific

235
00:15:11,986 --> 00:15:15,946
illness condition suppressing the age data

236
00:15:16,090 --> 00:15:20,042
would make the data itself useless. The warning number

237
00:15:20,098 --> 00:15:23,530
two is that the data type will change from integer

238
00:15:23,602 --> 00:15:27,086
to string and this will break the contract for all the

239
00:15:27,110 --> 00:15:31,154
data consumer of that kind of data asset.

240
00:15:31,694 --> 00:15:35,638
In this slide, as you can see, we have a brief comparison

241
00:15:35,686 --> 00:15:39,742
of the methods explained previously. For each strategy

242
00:15:39,798 --> 00:15:43,150
we evaluated three main secrecy,

243
00:15:43,222 --> 00:15:46,526
privacy and utility. And for each capital

244
00:15:46,670 --> 00:15:50,334
strategy factor we assigned a rating ranging

245
00:15:50,374 --> 00:15:54,654
from poor to best. As you may see, every method has

246
00:15:54,694 --> 00:15:58,550
its weakness, so we do not have an evidence of

247
00:15:58,582 --> 00:16:02,558
a superior technique that could address all the items and

248
00:16:02,606 --> 00:16:06,078
factors around data. In this case c four c,

249
00:16:06,126 --> 00:16:09,798
privacy and utility. What we can say is it

250
00:16:09,846 --> 00:16:12,674
depends a lot from the use case,

251
00:16:13,454 --> 00:16:16,702
but now let's take a look at the encryption methods in

252
00:16:16,718 --> 00:16:20,122
order to understand if they could help in the context of

253
00:16:20,178 --> 00:16:23,466
compliance. The first

254
00:16:23,530 --> 00:16:27,698
method we are going to talk about is format preserving encryption.

255
00:16:27,866 --> 00:16:31,634
Format preserving encryption, or SPE is

256
00:16:31,674 --> 00:16:35,530
a symmetric encryption algorithm which preserves

257
00:16:35,642 --> 00:16:39,290
the format and of the information while it is

258
00:16:39,362 --> 00:16:42,810
being encrypted. FPE is weaker than

259
00:16:42,842 --> 00:16:46,354
advanced decryption starter. AE's performance

260
00:16:46,474 --> 00:16:49,930
presenting encryption can present the length of data as

261
00:16:49,962 --> 00:16:53,866
well as its format. FP is by Nissan standard and

262
00:16:53,890 --> 00:16:57,434
there are three different model of operation, ff one, ff two

263
00:16:57,474 --> 00:17:01,682
and ff three. FPE works very

264
00:17:01,738 --> 00:17:05,826
well with existing applications as well as new applications.

265
00:17:05,970 --> 00:17:09,362
If an application needs data of a certain language

266
00:17:09,418 --> 00:17:12,704
format, then FBE is the way to grow.

267
00:17:13,284 --> 00:17:16,828
In order to operate with this algorithm,

268
00:17:16,916 --> 00:17:21,156
you should use a separate key and a tweak. Another implementation

269
00:17:21,220 --> 00:17:24,460
is provided by Bouncy Castle and the other one is

270
00:17:24,492 --> 00:17:27,744
available on Google Cloud as well as provincial toolkit.

271
00:17:28,284 --> 00:17:32,012
Now that we have seen the first encryption maple,

272
00:17:32,068 --> 00:17:34,388
let's take a look at another one.

273
00:17:34,516 --> 00:17:38,232
Homomorphic encryption omofic encryption provides the

274
00:17:38,248 --> 00:17:42,352
ability to compute on data while the data is encrypted.

275
00:17:42,488 --> 00:17:46,360
It sounds like magic, don't you? There are three different

276
00:17:46,472 --> 00:17:49,920
modes. In this case partially Omar pick encryption that

277
00:17:49,952 --> 00:17:53,528
allows a ten mathematical function to be used,

278
00:17:53,616 --> 00:17:56,784
for example addition or multiplication.

279
00:17:56,944 --> 00:18:00,376
Some automorphic encryption. Some function can be

280
00:18:00,400 --> 00:18:03,750
performed only a fixed number of times or

281
00:18:03,782 --> 00:18:06,994
up to a certain level of complexity.

282
00:18:07,414 --> 00:18:11,950
Or in the end we have also fully mm that

283
00:18:11,982 --> 00:18:15,806
allows all the function mathematical function to be performed

284
00:18:15,870 --> 00:18:19,870
on unlimited times up to any level of complexity

285
00:18:20,022 --> 00:18:22,914
without requiring the decryption of the data.

286
00:18:23,254 --> 00:18:27,254
So suppose you want to overwrite some sensitive data

287
00:18:27,334 --> 00:18:30,832
in a cloud. In the picture you can see you

288
00:18:30,848 --> 00:18:34,384
can have that you have on the left the traditional approach.

289
00:18:34,544 --> 00:18:38,232
In the example you can encrypt the files before moving

290
00:18:38,288 --> 00:18:42,520
to the cloud. For example, with a standard Andrew algorithm

291
00:18:42,552 --> 00:18:46,552
like AE's then if you want to perform some transformation

292
00:18:46,608 --> 00:18:49,976
on these files, you have to decrypt, apply the transformation and

293
00:18:50,000 --> 00:18:54,016
then encrypt again. This will expose data

294
00:18:54,120 --> 00:18:57,922
at risk and also introduce a complex operator on

295
00:18:57,938 --> 00:19:02,494
the right side. Instead you are going to use the amomorphic encryption.

296
00:19:03,034 --> 00:19:06,770
Once you are on the cloud you can do computation on separate

297
00:19:06,802 --> 00:19:10,074
text. Also decrypt you will obtain the same result of

298
00:19:10,114 --> 00:19:13,210
applying the function to the plaintext data.

299
00:19:13,402 --> 00:19:17,250
Unfortunately, it requires a significant computational

300
00:19:17,322 --> 00:19:19,894
operator to perform the intensity calculation,

301
00:19:20,314 --> 00:19:24,434
making this kind of strategy very slow and very

302
00:19:24,514 --> 00:19:28,074
resource intensive. In addition to passwords

303
00:19:28,114 --> 00:19:32,226
concern, implementation of this specific caliber

304
00:19:32,330 --> 00:19:36,454
can be very challenging with highly complex techniques.

305
00:19:36,994 --> 00:19:40,930
Is it all? No, we have also other strategies and

306
00:19:40,962 --> 00:19:44,714
methods to present on the table. One of these

307
00:19:44,794 --> 00:19:49,444
is tokenization. Tokenization involves substituting

308
00:19:49,634 --> 00:19:53,632
sensitive data like credit card number with non

309
00:19:53,688 --> 00:19:57,048
sensitive token which are stored securely

310
00:19:57,176 --> 00:20:00,176
in a separate database called Totembo.

311
00:20:00,360 --> 00:20:03,872
Synthetic data is artificial data generated

312
00:20:03,968 --> 00:20:08,344
with the purpose of preserving privacy testing system

313
00:20:08,504 --> 00:20:12,544
or creating training data for machine learning algorithms.

314
00:20:12,664 --> 00:20:16,232
Synthetic data generation is a critical and very

315
00:20:16,288 --> 00:20:20,166
complicated for two main reasons, quality and

316
00:20:20,230 --> 00:20:23,878
secrecy. Synthetic data that can be reverse engineered

317
00:20:23,966 --> 00:20:27,214
to identify real data would not be useful in

318
00:20:27,254 --> 00:20:31,278
privacy context. Faker is a Python

319
00:20:31,326 --> 00:20:35,366
package that generates fake data for you. There is also mockru

320
00:20:35,430 --> 00:20:39,238
that is another representative into the ecosystem of

321
00:20:39,366 --> 00:20:43,006
mock data that allows you to quickly and easily

322
00:20:43,070 --> 00:20:46,960
the low large amounts of randomly generated test data

323
00:20:47,112 --> 00:20:50,664
based on the specification that you define. This is

324
00:20:50,704 --> 00:20:54,056
all in this case. Now let's take a

325
00:20:54,080 --> 00:20:57,404
look and summarize what we have learned in the previous slide.

326
00:20:57,704 --> 00:21:00,904
The first thing that you could do in order to have

327
00:21:01,024 --> 00:21:04,832
to share data for machine learning purpose and

328
00:21:04,928 --> 00:21:08,864
analysis in minor environment is to use sample data

329
00:21:08,984 --> 00:21:12,060
coming from probably from the production environment.

330
00:21:12,252 --> 00:21:15,924
The first thing as a pro ultra realist sampled production

331
00:21:15,964 --> 00:21:19,332
data provide a realistic representation of actual

332
00:21:19,388 --> 00:21:22,780
data, helping developers and testers to identify

333
00:21:22,892 --> 00:21:26,820
issues that may not be evident with synthetic

334
00:21:26,892 --> 00:21:30,836
or mock data. You have an improved testing, so using real

335
00:21:30,900 --> 00:21:34,620
data allow for more comprehensive and accurate

336
00:21:34,692 --> 00:21:37,906
testing of functionality that integrity,

337
00:21:38,050 --> 00:21:41,778
performance and scalability. Then you have stakeholder

338
00:21:41,826 --> 00:21:45,618
confidence. Using real data increase stakeholder confidence

339
00:21:45,706 --> 00:21:49,770
in the testing process and the reliability of the development

340
00:21:49,922 --> 00:21:53,130
lifecycle. Within the cons you have

341
00:21:53,242 --> 00:21:56,594
that you have a lot of issues with privacy and

342
00:21:56,634 --> 00:21:59,914
compliance. Even sample data

343
00:22:00,034 --> 00:22:03,488
can contain sense of information, raising privacy

344
00:22:03,536 --> 00:22:06,712
concern and potential non compliance with data

345
00:22:06,768 --> 00:22:10,584
protection regulations such as GDPR and

346
00:22:10,744 --> 00:22:13,924
other ones like Hapa.

347
00:22:14,584 --> 00:22:17,928
Security risk using production data in minor

348
00:22:17,976 --> 00:22:21,696
environment like development or QA increase

349
00:22:21,760 --> 00:22:25,000
the risk of data breach and unauthorized access.

350
00:22:25,192 --> 00:22:29,272
Data freshness. This is another issue. Sample data might become

351
00:22:29,368 --> 00:22:33,262
outdated quickly, leading to scenarios where dead

352
00:22:33,358 --> 00:22:37,318
environments are not completely aligned with

353
00:22:37,366 --> 00:22:41,062
current production environment. Let's take a look at

354
00:22:41,118 --> 00:22:45,454
standard synthetic data privacy and securities for sure

355
00:22:45,614 --> 00:22:49,678
are pros. So synthetic data can be generated

356
00:22:49,766 --> 00:22:54,654
without any real world personal data, significantly reducing

357
00:22:54,814 --> 00:22:58,626
privacy concern and the risk of data leakage

358
00:22:58,810 --> 00:23:02,294
availability. Synth data can be created on demand

359
00:23:02,754 --> 00:23:06,090
efficiency. Generating syntactic data can be more

360
00:23:06,122 --> 00:23:09,802
cost effective than collecting and labeling a large

361
00:23:09,858 --> 00:23:13,786
volume of real world data. On the contrary,

362
00:23:13,850 --> 00:23:17,866
what we have lack of real is synthetic data may not capture all the

363
00:23:17,890 --> 00:23:21,534
complexities and the nuance of the real world data.

364
00:23:21,894 --> 00:23:25,494
We can have problem with overfitting. Also,

365
00:23:25,614 --> 00:23:29,158
there will be validation challenges. Validating the accuracy

366
00:23:29,206 --> 00:23:33,006
and the reliability of synthetic diagonal could be very challenging

367
00:23:33,150 --> 00:23:37,358
as it requires ensuring that the synthetic data closely

368
00:23:37,406 --> 00:23:41,366
mimics real warm data distribution. There is also

369
00:23:41,430 --> 00:23:45,118
concern about the complexity. It will require

370
00:23:45,286 --> 00:23:49,462
isotope complexity creating high quality synthetic

371
00:23:49,518 --> 00:23:53,986
data the wheel requires sophisticated techniques and domain knowledge,

372
00:23:54,130 --> 00:23:58,534
making the initial setup complex and resource intensive.

373
00:23:59,234 --> 00:24:03,098
Encrypted data source let's talk about it, focusing our

374
00:24:03,146 --> 00:24:06,970
attention on standard algorithm privacy protection.

375
00:24:07,082 --> 00:24:10,922
Anonymized data with encryption reduce the risk of

376
00:24:10,978 --> 00:24:14,490
exposing personal information. We are

377
00:24:14,642 --> 00:24:17,912
compliant in regulatory sets.

378
00:24:18,018 --> 00:24:21,844
Using anonymized data helps organization to comply with

379
00:24:21,964 --> 00:24:25,436
GDPR, CPI and so on. We can enable

380
00:24:25,540 --> 00:24:29,504
easily a data sharing mechanism and

381
00:24:30,524 --> 00:24:33,564
we can share this data with a little

382
00:24:33,604 --> 00:24:35,812
bit of freedom within department,

383
00:24:35,948 --> 00:24:39,304
organization or also with external partners.

384
00:24:39,684 --> 00:24:42,940
We also have a risk mitigation because we are going to

385
00:24:42,972 --> 00:24:46,698
reduce the potential for data breach and as

386
00:24:46,786 --> 00:24:51,066
the data no longer contains personal identifiable

387
00:24:51,170 --> 00:24:54,922
information. But on the contrary, we have some

388
00:24:54,978 --> 00:24:58,498
complexity. Working with encrypted data,

389
00:24:58,666 --> 00:25:02,218
especially in the case of AE's algorithm,

390
00:25:02,266 --> 00:25:05,506
can complicate development phase and also

391
00:25:05,570 --> 00:25:08,658
testing activities because data will lose any kind

392
00:25:08,706 --> 00:25:11,574
of meaning, will only keep this distribution.

393
00:25:11,974 --> 00:25:15,470
Key management challenges effective key management

394
00:25:15,582 --> 00:25:18,254
is crucial and can be complex,

395
00:25:18,414 --> 00:25:21,574
especially in non production environment where multiple

396
00:25:21,614 --> 00:25:25,394
teams and individuals may need access to encryption.

397
00:25:26,134 --> 00:25:29,462
Limited testing the curvature testing with encrypted data

398
00:25:29,518 --> 00:25:33,494
may not reflect through application behavior. If decryption process introduces

399
00:25:33,534 --> 00:25:36,926
delays or errors that wouldn't be or

400
00:25:36,990 --> 00:25:41,254
wouldn't occur in production. Anonymized data as

401
00:25:41,294 --> 00:25:44,534
we have seen before, the complex it would be

402
00:25:44,574 --> 00:25:48,302
complex. The anonymous process probably anonymizing

403
00:25:48,358 --> 00:25:52,230
data can be very challenging, requiring some sophisticated techniques

404
00:25:52,302 --> 00:25:56,206
and ongoing management to ensure data remains anonymous.

405
00:25:56,310 --> 00:26:00,230
There is also the problem of radio identification

406
00:26:00,422 --> 00:26:04,462
on the data subject. There is a risk that anonymized data

407
00:26:04,518 --> 00:26:08,152
can be reverted, especially if combined with

408
00:26:08,208 --> 00:26:11,896
other data sets, for example, knit attack

409
00:26:12,000 --> 00:26:15,288
and so forth. In some case we lose the utility,

410
00:26:15,416 --> 00:26:19,568
as we have seen for suppression, but now that

411
00:26:19,616 --> 00:26:23,104
we have summarized all the possible methods and

412
00:26:23,144 --> 00:26:26,360
techniques, at least the most important within the

413
00:26:26,432 --> 00:26:29,824
compliance context, let's take a look at the

414
00:26:29,864 --> 00:26:33,708
next slide. We are going to present a possible strategy

415
00:26:33,756 --> 00:26:37,956
for sharing data in a quite secure way in a minor environment.

416
00:26:38,140 --> 00:26:41,988
The practice I'm going to show you will combine some of the methods

417
00:26:42,036 --> 00:26:45,744
that we have seen before in the context of a data lake.

418
00:26:46,244 --> 00:26:49,556
So before moving forward, let's have a little bit

419
00:26:49,580 --> 00:26:53,236
of context. We are in the cloud and we have a data layer

420
00:26:53,300 --> 00:26:57,148
in the specified case. In the specific case, we leverage the medallion

421
00:26:57,196 --> 00:27:01,114
architecture for our storage layer. The most of you already

422
00:27:01,194 --> 00:27:04,490
know what it is a medallion architecture. It is also known

423
00:27:04,522 --> 00:27:07,962
as a multi hole architecture. Data at each

424
00:27:08,018 --> 00:27:11,954
stage get richer by increasing the intrinsic value.

425
00:27:12,114 --> 00:27:16,330
At each stage, PIi can be present and usually machine

426
00:27:16,362 --> 00:27:20,250
learning engineer and other scientists operate at silver

427
00:27:20,362 --> 00:27:24,274
gold layer. In this slide, let's see what

428
00:27:24,314 --> 00:27:28,014
we can do to shift data and minor environments and enable

429
00:27:28,094 --> 00:27:31,582
a safe data consumption. This is a receipt for a

430
00:27:31,598 --> 00:27:35,598
cloud based scenario, for example AWS, but can be

431
00:27:35,646 --> 00:27:38,914
easily replicated in other cloud vendors.

432
00:27:39,334 --> 00:27:43,150
So you have a production account on the top and

433
00:27:43,182 --> 00:27:47,078
for simplification purpose, an on production account in

434
00:27:47,086 --> 00:27:50,662
the bottom. In each layer we have the usual

435
00:27:50,718 --> 00:27:53,434
medallion architecture that we have seen before.

436
00:27:53,824 --> 00:27:57,344
The step one requires that data teams

437
00:27:57,424 --> 00:28:00,960
are in charge to prioritize their job and anonymize

438
00:28:01,032 --> 00:28:04,936
data. The encryption process becomes a mandatory step

439
00:28:05,040 --> 00:28:08,584
in the data lifecycle made of data ingestion,

440
00:28:08,664 --> 00:28:11,644
data normalization and delivery.

441
00:28:12,184 --> 00:28:16,072
In step two, we will open a read only cross policy

442
00:28:16,208 --> 00:28:20,076
account from production to run product. The minor environment

443
00:28:20,180 --> 00:28:24,012
never writes to prod. It is all enabled for reading operations.

444
00:28:24,188 --> 00:28:27,364
The encryption key is never shared with manner

445
00:28:27,404 --> 00:28:31,308
environments. By this way, user Personas that

446
00:28:31,356 --> 00:28:35,064
walk in the lower end barnet are enabled in doing their job.

447
00:28:35,364 --> 00:28:39,364
Data in video can prototype new silver dataset reading

448
00:28:39,404 --> 00:28:43,308
from the bronze encrypted layer. Analysts can model

449
00:28:43,356 --> 00:28:46,744
new schema and generate new anonymized report

450
00:28:47,204 --> 00:28:51,036
data. Scientists can propagate their model on quite

451
00:28:51,140 --> 00:28:54,548
realistic data set since only sensitive

452
00:28:54,636 --> 00:28:58,892
column will be encrypted. Let's take a look at

453
00:28:59,068 --> 00:29:02,984
which are the benefits of this kind of practice and strategies.

454
00:29:03,324 --> 00:29:06,780
Analyst says format preserving encryption

455
00:29:06,892 --> 00:29:10,388
guarantees reference integrity, no schema

456
00:29:10,436 --> 00:29:13,964
change across difference data sets and allows

457
00:29:14,004 --> 00:29:17,580
to reuse business logic. Derek Jones still

458
00:29:17,652 --> 00:29:20,436
works. After the encryption data,

459
00:29:20,500 --> 00:29:25,140
engineers are allowed to read only the encrypted layers liberated

460
00:29:25,252 --> 00:29:28,996
on a doc IAM policy. This is going to simplify

461
00:29:29,140 --> 00:29:33,744
the data movement and the orchestration process between environments.

462
00:29:34,084 --> 00:29:38,064
Machine learning engineers can prototype and train their job

463
00:29:38,444 --> 00:29:41,696
models on acquired real data on

464
00:29:41,720 --> 00:29:45,184
a safe layer. DevOps practice is still in

465
00:29:45,224 --> 00:29:49,592
place since deployments of new artifacts and models

466
00:29:49,728 --> 00:29:52,504
can follow the standard CICB flow.

467
00:29:52,664 --> 00:29:56,944
Secops says minimization principle is respected

468
00:29:56,984 --> 00:30:01,232
on minor end since most of the time you have encrypted

469
00:30:01,328 --> 00:30:05,276
information. Now let's take a look at and

470
00:30:05,300 --> 00:30:09,020
a don't feature and let's talk about all

471
00:30:09,132 --> 00:30:12,740
the right to be forgotten below the ecosystem of

472
00:30:12,772 --> 00:30:16,700
GDPR. In this slide we are going to talk and present

473
00:30:16,772 --> 00:30:20,012
the crypto shredding technique crypto shredding

474
00:30:20,108 --> 00:30:23,148
is the practice of deleting data by deleting

475
00:30:23,236 --> 00:30:26,604
or overdriving bankruptcry. This is going

476
00:30:26,644 --> 00:30:30,744
to require that the data have been encrypted from

477
00:30:31,304 --> 00:30:35,656
deleting. The key will automatically logically delete the record

478
00:30:35,840 --> 00:30:39,072
and all the existing copies since all the

479
00:30:39,088 --> 00:30:42,328
encrypted info are not reversible anymore.

480
00:30:42,496 --> 00:30:46,264
This approach is very useful when you have multiple copy

481
00:30:46,304 --> 00:30:50,048
of data, for example the card or multiple layers

482
00:30:50,096 --> 00:30:52,888
of data like in the Medellin architecture.

483
00:30:53,056 --> 00:30:56,656
If you are in the early stage of creating your data layer

484
00:30:56,720 --> 00:31:00,200
and building the foundation, you can combine crypto

485
00:31:00,232 --> 00:31:03,856
shredding and format preserving encryption in order to

486
00:31:03,880 --> 00:31:07,512
enable a very interesting scenario that will catch

487
00:31:07,608 --> 00:31:11,928
the sacred data sharing practice that Yves explained before

488
00:31:12,096 --> 00:31:15,760
and the deletion problem of multiple layers on

489
00:31:15,952 --> 00:31:19,304
environments. It is worthless to say that

490
00:31:19,344 --> 00:31:23,304
all these techniques and strategies will function with a strong

491
00:31:23,384 --> 00:31:26,792
than a governance practice place, knowing in other

492
00:31:26,848 --> 00:31:30,544
bounds that where Pii are stored and

493
00:31:30,584 --> 00:31:34,512
their lineage is fundamental. But this is another story.

494
00:31:34,648 --> 00:31:38,400
Thank you everybody and let's get in touch from any question

495
00:31:38,472 --> 00:31:38,904
and answer.

