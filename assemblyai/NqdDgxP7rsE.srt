1
00:00:20,560 --> 00:00:24,294
Hi everyone, I'm Raghav Roy, and today we'll be talking about coroutines

2
00:00:24,334 --> 00:00:27,124
and go. A little bit about myself.

3
00:00:27,864 --> 00:00:31,944
I'm a software engineer at VMware. I work with the EsXi

4
00:00:31,984 --> 00:00:35,484
hypervisor. When I'm using Go is generally

5
00:00:35,824 --> 00:00:39,120
on my personal projects or when I'm contributing to open source,

6
00:00:39,152 --> 00:00:42,440
something like kubernetes, maybe. So, just to set

7
00:00:42,472 --> 00:00:46,920
the expectations of the talk a little bit, we're going to be covering coroutines

8
00:00:46,992 --> 00:00:50,408
as generalized subroutines. So a little bit of

9
00:00:50,536 --> 00:00:53,774
we'll cover a little bit of basics over there, how it all

10
00:00:53,814 --> 00:00:58,366
started, where and how coroutines came to be classifying

11
00:00:58,390 --> 00:01:01,994
coroutines, and why the concept of full coroutines is important,

12
00:01:02,534 --> 00:01:05,434
and how some languages actually don't provide full coroutines.

13
00:01:06,214 --> 00:01:10,182
Implementing coroutines in Go using just pure go definitions

14
00:01:10,278 --> 00:01:14,094
and go semantics, and finally, some core runtime

15
00:01:14,134 --> 00:01:17,294
changes that could be made to support coroutines

16
00:01:17,374 --> 00:01:20,834
natively and far more efficiently. So, to begin

17
00:01:20,874 --> 00:01:24,814
with, brushing up on some basics. What are subroutines?

18
00:01:25,154 --> 00:01:28,738
Subroutines can do subroutines are essentially your regular

19
00:01:28,786 --> 00:01:32,394
function. Calls can do certain things. They can suspend themselves

20
00:01:32,514 --> 00:01:36,154
and run another function, or they could terminate themselves and

21
00:01:36,194 --> 00:01:39,946
yield back control to its caller. These are the two things that your functions can

22
00:01:39,970 --> 00:01:43,706
do. So in this case, say you have a main subroutine,

23
00:01:43,810 --> 00:01:47,672
suspends itself, starts another function, this function can execute,

24
00:01:47,808 --> 00:01:51,120
and as soon as it has to call another function, it can suspend itself.

25
00:01:51,272 --> 00:01:55,744
Call this third function, which can start functioning again after

26
00:01:55,784 --> 00:01:58,976
this outermost function has finished

27
00:01:59,000 --> 00:02:02,528
processing, it can terminate yield back control to its parent

28
00:02:02,576 --> 00:02:06,444
caller, which can terminate again and yield back control to its parent.

29
00:02:07,064 --> 00:02:10,284
So this is what a regular subroutine looks like.

30
00:02:10,624 --> 00:02:13,914
Now, subroutines. Now, these definitions differ

31
00:02:14,034 --> 00:02:16,882
depending on where you are living on the Internet, but for this talk,

32
00:02:17,018 --> 00:02:21,066
eager and closed means that, well, eager means expressions that a

33
00:02:21,090 --> 00:02:24,658
function encounters are evaluated as soon as they are encountered,

34
00:02:24,786 --> 00:02:28,970
and closed means that your function only returns

35
00:02:29,042 --> 00:02:33,042
after it has evaluated the expression. So it can't sort of suspend itself

36
00:02:33,178 --> 00:02:36,762
in the middle of processing and yield control back to its invoker.

37
00:02:36,858 --> 00:02:40,504
That's the important difference that we'll see soon. With coroutines

38
00:02:40,884 --> 00:02:44,612
now coroutines as generalized subroutines, can we define them

39
00:02:44,628 --> 00:02:48,308
as generalized subroutines? And what I mean by that is, a coroutine

40
00:02:48,396 --> 00:02:51,876
can do everything a subroutine can, which means can suspend itself,

41
00:02:52,020 --> 00:02:56,108
run another function, terminate itself, yield back control to its invoker,

42
00:02:56,236 --> 00:03:00,004
or this magic special thing at the end, which is suspend

43
00:03:00,044 --> 00:03:04,086
itself and yield back control to its invoker, instead of,

44
00:03:04,220 --> 00:03:08,346
so it just passes the control back to its invoker, pausing itself,

45
00:03:08,490 --> 00:03:12,522
essentially. So how could that look like, say you have a subroutine running over here,

46
00:03:12,618 --> 00:03:16,834
and you want to run this coroutine on the left. So your subroutine suspends itself

47
00:03:16,954 --> 00:03:21,146
and say it wants to read from some output,

48
00:03:21,170 --> 00:03:25,026
that is, output from this coroutine. So your

49
00:03:25,050 --> 00:03:28,962
coroutine starts functioning. After it has the output, it can suspend itself

50
00:03:29,098 --> 00:03:33,126
and yield control back to your subroutine with the output. And now

51
00:03:33,150 --> 00:03:36,766
you can see that the coroutine has actually stopped here. It has saved the state,

52
00:03:36,830 --> 00:03:40,366
and it's actually paused in the middle of its execution. The next

53
00:03:40,390 --> 00:03:44,790
time the subroutine wants to read from it, it actually can just resume this

54
00:03:44,822 --> 00:03:49,074
coroutine, which starts running from its previously stopped position

55
00:03:49,574 --> 00:03:53,542
and output the new value that the subroutine requires, which can be

56
00:03:53,558 --> 00:03:56,630
read by the subroutine, and so on and so forth. So it ends up looking

57
00:03:56,662 --> 00:03:59,680
like this. Every time this subroutine was read from a coroutine,

58
00:03:59,782 --> 00:04:03,420
it plays this coroutine back, which starts executing again from last

59
00:04:03,452 --> 00:04:07,700
time it stopped. Now you will start seeing why this

60
00:04:07,732 --> 00:04:10,956
is important, and you can probably already start seeing why something, why a paradigm

61
00:04:10,980 --> 00:04:15,068
like this could be pretty useful. So coroutines

62
00:04:15,116 --> 00:04:18,556
are like functions that can return multiple times and keep their state,

63
00:04:18,660 --> 00:04:21,996
which means all the local variables plus the command pointer,

64
00:04:22,100 --> 00:04:25,600
everything, and they can resume from where they are yielded.

65
00:04:25,772 --> 00:04:28,904
So let's look at a quick sample. Right? So comparing binary

66
00:04:28,944 --> 00:04:32,352
trees, those in the audience who are, who like

67
00:04:32,448 --> 00:04:36,536
doing Leet code, if you've seen something like, seen a question like this, how would

68
00:04:36,560 --> 00:04:39,552
you do this? You would have some sort of a recursive logic.

69
00:04:39,688 --> 00:04:43,224
You know, you have two binary trees, and you want to compare

70
00:04:43,264 --> 00:04:46,672
them and maybe ascertain if they are equal. So you

71
00:04:46,688 --> 00:04:50,684
would go through all the nodes, save in maybe like an in order fashion,

72
00:04:51,004 --> 00:04:54,228
and save all the values of the nodes. And after you

73
00:04:54,236 --> 00:04:56,904
have traversed both the trees, you compare those values.

74
00:04:57,884 --> 00:05:01,300
But what if you could actually do this in

75
00:05:01,332 --> 00:05:02,144
one pass?

76
00:05:04,724 --> 00:05:08,516
Every time you are at a node, you can pass through both these trees,

77
00:05:08,620 --> 00:05:12,188
compare the values of the current node, go to the next node in

78
00:05:12,196 --> 00:05:16,020
the next node, say, in order fashion, and just

79
00:05:16,052 --> 00:05:19,694
keep comparing the values as you go through them. Something like we just

80
00:05:19,734 --> 00:05:23,274
saw in our subroutine and coroutine example.

81
00:05:23,774 --> 00:05:27,318
Let's see that with code. Right? So in this case,

82
00:05:27,486 --> 00:05:31,102
your subroutine is the CMP. This comparison function and

83
00:05:31,118 --> 00:05:34,470
your coroutine is this visit function. And how this is working over here

84
00:05:34,502 --> 00:05:38,006
is your comparison function is just instantiating two coroutines.

85
00:05:38,070 --> 00:05:41,478
We don't need to worry about how they're implemented, just assume they're just black box

86
00:05:41,526 --> 00:05:44,454
implementations. It creates a new coroutine,

87
00:05:44,494 --> 00:05:48,338
in this case the visit function. So you're passing this visit as a function

88
00:05:48,386 --> 00:05:52,146
argument, and in this while loop, all you're doing

89
00:05:52,210 --> 00:05:55,906
is you first start your first coroutine, your c o one, and you're

90
00:05:55,930 --> 00:05:59,506
passing t one, which could be this head of the first tree,

91
00:05:59,530 --> 00:06:01,894
and your t two is probably the head of the second tree.

92
00:06:02,474 --> 00:06:05,530
So you resume this visit function.

93
00:06:05,642 --> 00:06:08,442
So your visit first goes to t left. So it goes all the way to

94
00:06:08,458 --> 00:06:12,188
the leftmost node in your tree,

95
00:06:12,386 --> 00:06:16,696
your left most node in the left subtree, and it

96
00:06:16,720 --> 00:06:19,784
then yields the value back to your

97
00:06:19,904 --> 00:06:23,168
call back to your regime function. So next line yield

98
00:06:23,216 --> 00:06:27,136
actually yields back control to comparison function, and this value is stored in

99
00:06:27,200 --> 00:06:30,496
v one. Now this comparison function then calls

100
00:06:30,520 --> 00:06:34,240
CO2, which is another instance of your visit function, and you pass t two

101
00:06:34,272 --> 00:06:37,560
to it, and it goes through that tree and gets

102
00:06:37,592 --> 00:06:40,820
the leftmost node in this case, which is two. And now you

103
00:06:40,852 --> 00:06:44,820
have both the values actually yielded back to v one and v two

104
00:06:44,892 --> 00:06:48,620
that you can then compare. So essentially you

105
00:06:48,652 --> 00:06:51,956
have, you started traversing the tree in sort of

106
00:06:51,980 --> 00:06:55,684
a one pass sort of a way, you just passing

107
00:06:55,724 --> 00:06:58,748
back control to visit, and as soon as it has an output, you're reading the

108
00:06:58,796 --> 00:07:02,700
visit function yields back control to your comparison function, and you can do your comparisons.

109
00:07:02,892 --> 00:07:06,444
So again, you know, you call you in the while loop, you call your

110
00:07:06,484 --> 00:07:09,634
resume function. Again, it goes to the right subtree from,

111
00:07:09,714 --> 00:07:12,922
from two, which is four, in this case for both the trees,

112
00:07:12,978 --> 00:07:15,774
because you know, it resumes both the coroutines. One by one,

113
00:07:16,274 --> 00:07:19,602
it yields back the values, and now you can compare four so on and

114
00:07:19,618 --> 00:07:21,906
so forth till you, up till, you know, both the trees are same. In this

115
00:07:21,930 --> 00:07:25,442
case, this is actually, this sort of

116
00:07:25,458 --> 00:07:28,914
an example was actually why coroutines came in the first place,

117
00:07:28,954 --> 00:07:31,814
or why Conway thought of them.

118
00:07:32,394 --> 00:07:36,170
It's 1958, and you want to compile your CObOl program in the modern

119
00:07:36,202 --> 00:07:39,638
nine part compiler. That's Grace Hopper, by the way,

120
00:07:39,686 --> 00:07:43,150
who is the co developer of Cobol. Now the terms over

121
00:07:43,182 --> 00:07:46,874
here, even if you don't understand it, I'll simplify it. So,

122
00:07:47,894 --> 00:07:51,314
just your basic symbol reducer, you can think of it as your lexer.

123
00:07:51,614 --> 00:07:55,486
And if you don't know what Lexa means, that's fine too, because all

124
00:07:55,510 --> 00:07:58,566
you need to care about over here is that you have two processes, like you

125
00:07:58,590 --> 00:08:02,414
had in your binary comparison example, that sort of depend

126
00:08:02,454 --> 00:08:06,030
on each other for their outputs. So in this case, your basic symbol

127
00:08:06,062 --> 00:08:09,446
reducer acts like a Lexus. So what your main program

128
00:08:09,510 --> 00:08:12,614
does is that it has actual physical punched cards.

129
00:08:12,654 --> 00:08:16,754
That's how compilation worked back in 1958.

130
00:08:17,054 --> 00:08:20,294
And you give this punched card to your basic symbol

131
00:08:20,334 --> 00:08:23,902
reducer of your lexer, which eats this punched card and spews a

132
00:08:23,918 --> 00:08:27,246
bunch of tokens. Tokens is what the output

133
00:08:27,270 --> 00:08:30,918
of your symbol reducer is, or your lexus is right.

134
00:08:31,006 --> 00:08:34,662
Now, these tokens can be read by a parser, which these tokens

135
00:08:34,678 --> 00:08:37,862
become the input to your parser, in this case, your basic

136
00:08:37,918 --> 00:08:40,754
name reducer, or name lookup as it's called today,

137
00:08:42,654 --> 00:08:45,982
which, you know, puts the output into the next state.

138
00:08:46,118 --> 00:08:49,726
So what's happening over here is sort of like our first subroutine example.

139
00:08:49,870 --> 00:08:53,510
Your main program, your control, goes to your basic

140
00:08:53,542 --> 00:08:57,198
symbol reducer, which comes up with a bunch of

141
00:08:57,326 --> 00:09:00,942
tokens as outputs. And it returns

142
00:09:01,038 --> 00:09:04,742
essentially these outputs back to your main program, which can then, your main program

143
00:09:04,798 --> 00:09:07,990
then calls your namereducer with your output of the basic

144
00:09:08,022 --> 00:09:11,406
symbol reducer as an input to your namereducer.

145
00:09:11,550 --> 00:09:15,134
And, you know, your name reducer can then start parsing your tokens.

146
00:09:15,214 --> 00:09:17,846
So this is kind of what it looks like initially, what we saw, your main

147
00:09:17,870 --> 00:09:21,726
program called symbol reducer. Symbol reducer writes a bunch of

148
00:09:21,750 --> 00:09:25,486
tapes to your. And these tapes

149
00:09:25,510 --> 00:09:28,034
are basically used as input to your name reducer.

150
00:09:28,574 --> 00:09:31,462
You have a bunch of extra tapes that you don't need anymore.

151
00:09:31,518 --> 00:09:35,554
And this entire course is actually involved a bunch of extra machinery.

152
00:09:36,294 --> 00:09:40,314
So convoy thought there had to be a better way to pass the input,

153
00:09:40,814 --> 00:09:44,430
pass the output that you get from your symbol reducer or your lexer to your

154
00:09:44,462 --> 00:09:47,742
parcel, which is your name lookup. Without all this expensive

155
00:09:47,798 --> 00:09:51,522
machinery, we'll start to see how he actually

156
00:09:51,578 --> 00:09:55,338
thought of the coroutine. And he realized that subroutines,

157
00:09:55,466 --> 00:09:59,658
which was the previous implementation, were just a special case for more generalized coroutine.

158
00:09:59,706 --> 00:10:03,234
The first thing that we saw in this talk, and we don't need to write

159
00:10:03,274 --> 00:10:06,746
on tape, we can just pass control back and forth between

160
00:10:06,810 --> 00:10:10,626
these two processes, because the input of one process is the

161
00:10:10,650 --> 00:10:14,002
output of the previous process. So you don't have to return.

162
00:10:14,098 --> 00:10:17,836
You can just yield back control, and you bypass all

163
00:10:17,860 --> 00:10:22,076
this machinery. And this is how it ends up looking, which is very similar to

164
00:10:22,140 --> 00:10:25,972
our previous example. So every time your name reducer, your parser

165
00:10:26,068 --> 00:10:29,292
wants to read a token from your lexer or your symbol reducer,

166
00:10:29,428 --> 00:10:32,916
it invokes this coroutine suspends itself,

167
00:10:33,020 --> 00:10:36,844
and as soon as your symbol reducer has the tokens available, suspends itself

168
00:10:37,004 --> 00:10:40,756
yields control along with the output to your namereducer, which is your parser,

169
00:10:40,780 --> 00:10:43,620
which can do whatever it wants with the output. And when it wants the next

170
00:10:43,652 --> 00:10:47,776
token, it again resumes your coroutine and

171
00:10:47,880 --> 00:10:51,176
which yields back another token, so on and so forth till you reach the

172
00:10:51,240 --> 00:10:54,816
end of your main function. Right? So this way,

173
00:10:54,920 --> 00:10:58,392
raising the level of abstraction actually led to less costly control

174
00:10:58,448 --> 00:11:02,016
structures, leading to your one pass global compiler. So this

175
00:11:02,040 --> 00:11:05,560
was an example of a negative cost abstraction, because generally

176
00:11:05,672 --> 00:11:09,464
when you're trying to abstract away logic, you're increasing

177
00:11:09,544 --> 00:11:13,608
the cost because something else is now taking care of this easier

178
00:11:13,656 --> 00:11:16,324
abstraction that you've created, something under the hood.

179
00:11:17,224 --> 00:11:21,120
So this side

180
00:11:21,152 --> 00:11:24,432
note, this was the paper that actually coined the term coroutine by Melvin Conway.

181
00:11:24,528 --> 00:11:26,856
I've linked to this paper at the end of the talk and you should definitely

182
00:11:26,880 --> 00:11:29,976
check it out. Where are coroutines now?

183
00:11:30,160 --> 00:11:33,336
Considering all that we've talked about so far, coroutines should have been a

184
00:11:33,360 --> 00:11:37,684
common pattern that is provided by most languages, right? Seems pretty useful,

185
00:11:38,284 --> 00:11:42,064
but with a few rare exceptions, that's similar. Few languages do.

186
00:11:42,404 --> 00:11:45,924
And those that do, and I'm guessing some of you already

187
00:11:46,044 --> 00:11:50,148
have some ideas of, you know, some other languages calling their processes

188
00:11:50,196 --> 00:11:53,356
coroutines. They are actually a limited variant

189
00:11:53,380 --> 00:11:57,624
of a coroutine, and we'll see how, what makes them limited and why we want,

190
00:11:58,124 --> 00:12:02,836
why this could actually affect the

191
00:12:02,860 --> 00:12:06,704
expressive power of a coroutine. So the problems with coroutines

192
00:12:06,744 --> 00:12:10,352
were also, apart from, you know,

193
00:12:10,368 --> 00:12:14,120
the reason why we don't see it everywhere, is one, there's a lack of uniform

194
00:12:14,152 --> 00:12:17,888
view of this concept. There aren't a lot of formal definitions that

195
00:12:17,936 --> 00:12:21,104
people agree on on what core routines means, there's no

196
00:12:21,184 --> 00:12:25,032
precise definitions for it. And secondly, more importantly,

197
00:12:25,208 --> 00:12:28,872
my core routines aren't provided as a facility in most mainstream languages

198
00:12:29,048 --> 00:12:32,490
is the advent of ALGOL 60. So this language brought

199
00:12:32,522 --> 00:12:35,494
with it block scope variables.

200
00:12:35,874 --> 00:12:39,306
So you no longer had parameters and return values stored as global

201
00:12:39,370 --> 00:12:43,130
memory. Instead everything was stored as relative to a stack pointer.

202
00:12:43,202 --> 00:12:46,786
So your functions are stored on a stack, stack frames,

203
00:12:46,930 --> 00:12:49,946
right. So with the return addresses, all your local variables,

204
00:12:49,970 --> 00:12:53,938
everything is on a stack. Now you have your stack, you have f's

205
00:12:53,986 --> 00:12:57,710
activation record. So f is a function and its activation record just

206
00:12:57,742 --> 00:13:01,294
contains all its local variables, return addresses. And the stack

207
00:13:01,334 --> 00:13:05,238
pointer is just pointing at the top of the stack. Your function can call

208
00:13:05,366 --> 00:13:08,554
a mole function. In this case it calls function g, which has its own

209
00:13:09,574 --> 00:13:12,686
activation record, which can call function h, and so on.

210
00:13:12,870 --> 00:13:16,046
So how would you try to implement a coroutine in this

211
00:13:16,070 --> 00:13:19,678
sort of a parity? Well, you could have your f,

212
00:13:19,726 --> 00:13:23,284
in this case wants to call a coroutine g. You could have

213
00:13:23,324 --> 00:13:26,940
g running as a separate side, a separate stack on the side of this

214
00:13:27,052 --> 00:13:29,492
thread, one stack right, as a side stack.

215
00:13:29,628 --> 00:13:32,988
And your stack pointer now has to move from one stack to

216
00:13:32,996 --> 00:13:36,700
another stack with all the thread context, your return addresses,

217
00:13:36,732 --> 00:13:39,184
saved registers, everything has to move to this stack now,

218
00:13:40,484 --> 00:13:44,540
which calls the function g, which is coroutine coroutine g

219
00:13:44,572 --> 00:13:48,020
can call more coroutines h. Now what

220
00:13:48,052 --> 00:13:52,086
if there is another function, not f, but some other function that's interested in the

221
00:13:52,190 --> 00:13:55,454
outputs that this coroutines are producing? Well,

222
00:13:55,494 --> 00:13:58,654
you can think of it as second stack. You have a thread two stack.

223
00:13:58,734 --> 00:14:01,918
This is different from f's stack, and you have some function

224
00:14:01,966 --> 00:14:04,806
z that's running over here with the stack pointer.

225
00:14:04,830 --> 00:14:08,614
Now when the control comes to z, the stack mode has to move over here

226
00:14:08,734 --> 00:14:11,994
into the second stack with other thread context and everything else.

227
00:14:12,454 --> 00:14:16,388
So this almost starts mimicking heavy multithreading,

228
00:14:16,486 --> 00:14:20,024
which increases the memory footprint, rather than the cheap

229
00:14:20,064 --> 00:14:23,448
abstraction that coroutines were meant to be in our previous examples.

230
00:14:23,536 --> 00:14:26,840
Right. What about the precise definition

231
00:14:26,912 --> 00:14:30,320
problem? Well, Marlin's doctoral thesis

232
00:14:30,472 --> 00:14:33,408
is actually widely acknowledged as a reference for this,

233
00:14:33,456 --> 00:14:36,736
and it just summarizes whatever we have discussed so

234
00:14:36,760 --> 00:14:39,896
far, which is the values local to a coroutine

235
00:14:39,960 --> 00:14:43,156
persist between successive calls. You know, it can pause itself and

236
00:14:43,180 --> 00:14:46,460
save the state there. And the execution of coroutines

237
00:14:46,532 --> 00:14:49,900
is suspended as control leaves it, only to carry on from where

238
00:14:49,932 --> 00:14:53,604
it left off when control re enters it. So when a coroutine

239
00:14:53,644 --> 00:14:56,740
yields, the control leaves it where it

240
00:14:56,812 --> 00:15:00,468
left, where it yielded. And when you resume a coroutine,

241
00:15:00,516 --> 00:15:04,584
it starts processing from when it had stopped, basically what we've already discussed.

242
00:15:04,884 --> 00:15:08,272
So now that we have the basics and, and the history out of the

243
00:15:08,288 --> 00:15:11,184
way, and hopefully have made a case for its usefulness,

244
00:15:11,304 --> 00:15:15,200
let's build up to what a coroutine looks like in go. But before that,

245
00:15:15,232 --> 00:15:18,364
very quickly, I promise this is relevant.

246
00:15:19,184 --> 00:15:22,352
Let's look at what we mean by a full core routine.

247
00:15:22,528 --> 00:15:25,928
And some languages like Python and Kotlin, which actually provide coroutines,

248
00:15:25,976 --> 00:15:29,520
don't actually provide full coroutines. So let's

249
00:15:29,552 --> 00:15:33,590
start, just for the sake of completeness,

250
00:15:33,712 --> 00:15:36,802
let's start classifying the coroutines. Well, one kind of

251
00:15:36,818 --> 00:15:40,426
classification is symmetric coroutines, which is different from what we've been seeing so far.

252
00:15:40,530 --> 00:15:44,226
A symmetric coroutine only has one control transfer operation,

253
00:15:44,370 --> 00:15:48,254
and so it just allows coroutines to explicitly pass control amongst themselves.

254
00:15:48,594 --> 00:15:52,146
But what we're interested in is asymmetric coroutine mechanisms that

255
00:15:52,170 --> 00:15:55,722
provides two control transfer operations. In our case it was the resume

256
00:15:55,738 --> 00:15:59,130
and yield operation. So one for

257
00:15:59,162 --> 00:16:02,130
invoking the coroutine you resume, and one for suspending it yield,

258
00:16:02,242 --> 00:16:06,530
and the latter returns the control to the invoker. So coroutine

259
00:16:06,562 --> 00:16:10,570
mechanisms that support concurrent programming usually provide symmetric coroutines.

260
00:16:10,682 --> 00:16:14,362
On the other hand, coroutine mechanisms intended for producing

261
00:16:14,418 --> 00:16:18,282
sequences of values are provided by asymmetric coroutines. And you

262
00:16:18,298 --> 00:16:21,842
might think, wait, then, you know, why not just use symmetric coroutines? It seems like

263
00:16:21,858 --> 00:16:25,290
it's doing way more important things. You know, concurrent programming

264
00:16:25,322 --> 00:16:29,102
is what a go is all about. But actually

265
00:16:29,278 --> 00:16:32,990
the good news is that you can use asymmetric coroutines to mimic symmetric

266
00:16:33,022 --> 00:16:36,566
coroutines, and asymmetric coroutines are way easier to write

267
00:16:36,590 --> 00:16:40,206
and maintain. And actually, I have linked an article to the end of this

268
00:16:40,230 --> 00:16:43,422
slide to actually you can see

269
00:16:43,518 --> 00:16:47,102
how symmetric coroutines and asymmetric coroutines are implemented and how you

270
00:16:47,118 --> 00:16:51,030
can actually implement symmetric coroutines using asymmetric coroutines using your

271
00:16:51,062 --> 00:16:54,556
resume and yield functions. But you know, with the advantage

272
00:16:54,580 --> 00:16:58,532
of that, it's easier to write asymmetric coroutines. And we'll see that when we implement

273
00:16:58,588 --> 00:17:01,868
our go API, we implement

274
00:17:01,996 --> 00:17:05,644
asymmetric coroutines. Secondly, you would

275
00:17:05,684 --> 00:17:09,084
want your coroutine to be provided as first class objects

276
00:17:09,124 --> 00:17:12,364
by your program or your language. You know, it has a huge

277
00:17:12,404 --> 00:17:16,268
influence on its expressive power, and coroutines that are constrained

278
00:17:16,316 --> 00:17:20,104
within the language bounds cannot, which and cannot be directly manipulated

279
00:17:20,144 --> 00:17:23,744
by the programmer actually has a negative effect

280
00:17:23,784 --> 00:17:27,528
on its expressive power. So what do we mean by a first class object?

281
00:17:27,616 --> 00:17:31,184
Something we've already seen with a binary comparison example. The coroutine

282
00:17:31,224 --> 00:17:34,928
should be able to appear in an expression. You should be able to assign it

283
00:17:34,936 --> 00:17:37,564
to a variable, you should be able to use it as an argument,

284
00:17:38,024 --> 00:17:41,164
return it by a function call, you know, all the good stuff.

285
00:17:41,744 --> 00:17:45,308
Finally, stackfulness. This is the important one. This is what

286
00:17:45,356 --> 00:17:49,156
actually differentiates Python and Kotlin's coroutines, or most

287
00:17:49,180 --> 00:17:52,380
of the languages from a full coroutine is

288
00:17:52,412 --> 00:17:55,900
because when you have stack full coroutines, you can actually

289
00:17:56,052 --> 00:17:59,620
transfer control between nested functions. So even if your core routine

290
00:17:59,652 --> 00:18:02,812
isn't at the top most stack, even if the coroutine is somewhere in the

291
00:18:02,828 --> 00:18:06,076
middle of the stack, you can still yield control from the coroutine back to its

292
00:18:06,100 --> 00:18:09,852
invoker. When you have stack less coroutines like Python and Kotlin, they are

293
00:18:09,868 --> 00:18:13,342
in full routines. And the only way you can,

294
00:18:13,438 --> 00:18:16,902
you know, pass control between two coroutines or a

295
00:18:16,918 --> 00:18:21,022
coroutine or a sub routine. And a subroutine is by sort

296
00:18:21,038 --> 00:18:24,994
of only the top most coroutine. And the top most subroutine can

297
00:18:25,774 --> 00:18:29,550
pass control between themselves, which actually limits what your

298
00:18:29,582 --> 00:18:33,086
coroutine can do. Basically, your full coroutine should be

299
00:18:33,110 --> 00:18:36,200
stack full, should be provided as first class objects, and we,

300
00:18:36,222 --> 00:18:38,944
we can implement them using asymmetric coroutines.

301
00:18:39,484 --> 00:18:43,108
So full coroutines can be used to implement generators,

302
00:18:43,196 --> 00:18:46,844
iterators, all the way up to cooperative multitasking.

303
00:18:47,004 --> 00:18:50,748
And just by providing asymmetric coroutine mechanisms, it's sufficient

304
00:18:50,876 --> 00:18:54,476
because we can use it to implement symmetric coroutines and

305
00:18:54,500 --> 00:18:57,224
asymmetric coroutines are just much easier to maintain and implement.

306
00:18:58,364 --> 00:19:02,012
This is a nice way to show the limitations of a coroutine.

307
00:19:02,108 --> 00:19:04,904
So, cooperative multitasking, right?

308
00:19:05,484 --> 00:19:09,572
In cooperative in a cooperative multitasking environment, your concurrent tasks

309
00:19:09,628 --> 00:19:13,468
are interleaved. One task runs and

310
00:19:13,556 --> 00:19:17,504
stops itself, the second task runs. So this is interleaving and

311
00:19:18,124 --> 00:19:22,264
it needs to be deterministic. But when you're using coroutines,

312
00:19:23,804 --> 00:19:27,620
coroutines by definition are not preemptive. So there's a

313
00:19:27,652 --> 00:19:30,732
fairness problem that can arise if you have a bunch of coroutines running

314
00:19:30,748 --> 00:19:34,390
in your kernels. Because eco space doesn't have just

315
00:19:34,422 --> 00:19:37,314
one program that's trying to operate,

316
00:19:40,014 --> 00:19:43,502
that's collaborating amongst itself. Actually a bunch of

317
00:19:43,518 --> 00:19:47,430
programs running that, waiting for the cpu resources. Now,

318
00:19:47,462 --> 00:19:50,554
if a routine is using cpu resource,

319
00:19:51,134 --> 00:19:54,558
that the code routine panics, hangs, or just taking a

320
00:19:54,566 --> 00:19:58,054
long time to execute higher priority tasks could

321
00:19:58,094 --> 00:20:01,544
be waiting for this, uh, team to finish

322
00:20:01,584 --> 00:20:05,304
executing. Right? This sort of fairness problem can arise

323
00:20:05,384 --> 00:20:08,752
because of coroutines being preemptive.

324
00:20:08,888 --> 00:20:12,264
So, but on the other hand, if you have user level multitasking,

325
00:20:12,384 --> 00:20:15,744
uh, implementation, your core routines are generally going

326
00:20:16,244 --> 00:20:19,352
to be part of the same program that of course collaborating with your common goal.

327
00:20:19,448 --> 00:20:23,444
So since you can still have minus problems, but since it's restricted

328
00:20:23,984 --> 00:20:27,506
one collaborative environment, it's much more easy to

329
00:20:27,680 --> 00:20:31,830
identify them, reproduce them, and makes it less difficult to implement.

330
00:20:31,982 --> 00:20:35,846
So this, you start to see why full blown core routines in your

331
00:20:36,030 --> 00:20:39,870
kernel space, for example, might not be a good idea why

332
00:20:39,902 --> 00:20:43,582
coroutines in go co routines aren't directly served or

333
00:20:43,638 --> 00:20:46,670
natively served in go concurrency

334
00:20:46,702 --> 00:20:50,494
libraries. And this was an interesting talk by rockbike, actually lexical

335
00:20:50,534 --> 00:20:53,594
scanning and go. I won't go into the definitions, but I've linked to the talk.

336
00:20:53,954 --> 00:20:57,554
What they did was for their implementation, they used go

337
00:20:57,594 --> 00:21:01,226
routines connected by a channel. Now, full go routines

338
00:21:01,370 --> 00:21:05,050
provided to be a bit too much, because go routines provide a lot of

339
00:21:05,082 --> 00:21:08,714
parallelism, right? So the parallelism that

340
00:21:08,754 --> 00:21:12,274
comes with go routines caused a lot of races. Proper coroutines,

341
00:21:12,314 --> 00:21:15,586
as we'll see, would have avoided the races and would have been way more

342
00:21:15,610 --> 00:21:18,674
efficient than go routines. Because of their concurrency constructs, only one

343
00:21:18,714 --> 00:21:22,392
coroutine could be, can be running at a time. Now,

344
00:21:22,528 --> 00:21:26,216
coroutines, threads, generators, they sort of start sounding the

345
00:21:26,240 --> 00:21:29,712
same. So let's get the definitions out of the way a little bit.

346
00:21:29,848 --> 00:21:33,744
Coroutines provide concurrency without parallelism. That's the big idea

347
00:21:33,784 --> 00:21:37,164
here. When one coroutine is running, the others aren't.

348
00:21:37,584 --> 00:21:41,048
Threads, on the other hand, are definitely way more powerful than co routines,

349
00:21:41,136 --> 00:21:44,596
but with more cost. They require more memory, more cpu allocation,

350
00:21:44,720 --> 00:21:47,996
and also it comes with parallelism. So you have

351
00:21:48,060 --> 00:21:50,944
the cost of scheduling these tasks,

352
00:21:51,444 --> 00:21:54,932
and you have way more expensive context switches and the fact that

353
00:21:54,948 --> 00:21:58,324
you need to add preemption for threads.

354
00:21:58,484 --> 00:22:02,308
Go routines are like threads, they're just cheaper, so they use lesser memory.

355
00:22:02,356 --> 00:22:06,092
And all the scheduling is now taken care of. Go's own user space

356
00:22:06,148 --> 00:22:09,860
go scheduler. So your go routine switch is closer to a few hundred nanoseconds,

357
00:22:09,892 --> 00:22:13,704
which is way faster than a thread. Generators are like

358
00:22:13,744 --> 00:22:17,000
coroutines, but they're stacked less, they're not stacked full.

359
00:22:17,152 --> 00:22:21,944
So you know you have that. The problem that we discussed, they can only transfer

360
00:22:21,984 --> 00:22:25,284
control between the top most stack frames between two threads, for example.

361
00:22:26,144 --> 00:22:29,928
So with all this in mind, let's start building an API

362
00:22:29,976 --> 00:22:33,304
of coroutines in go by using existing Go

363
00:22:33,344 --> 00:22:36,768
definitions that's available today. And this,

364
00:22:36,856 --> 00:22:40,600
this next part of the talk is heavily borrowed from Russell's research proposal,

365
00:22:40,632 --> 00:22:43,672
which I've linked to at the end of the slides for implementing core routines.

366
00:22:43,728 --> 00:22:47,096
And check that out, because have skipped a lot of the information here.

367
00:22:47,280 --> 00:22:51,208
So it's very neat that we can do this using existing Go definitions.

368
00:22:51,336 --> 00:22:55,204
Your core routines channels, the fact that go supports function values,

369
00:22:55,664 --> 00:23:00,672
the fact that unbuffered channels can act like blocking mechanisms

370
00:23:00,688 --> 00:23:04,592
between coroutines. So it makes coroutines safe. A coroutine can

371
00:23:04,608 --> 00:23:08,288
do a bunch of things. It can suspend itself, run another

372
00:23:08,336 --> 00:23:11,256
function, coroutine, terminate itself,

373
00:23:11,360 --> 00:23:14,728
yield back control, or suspend itself. It doesn't need to terminate itself.

374
00:23:14,816 --> 00:23:18,376
It can just suspend itself to yield back control. So let's

375
00:23:18,400 --> 00:23:21,364
start with a simple implementation of the package coral.

376
00:23:21,824 --> 00:23:24,816
So in this case, you have a caller and the callee. And right now we

377
00:23:24,840 --> 00:23:28,392
are looking at the suspend and run scenario. So you have

378
00:23:28,408 --> 00:23:32,266
a caller which you know. So now this can be two coroutines that

379
00:23:32,290 --> 00:23:35,938
are, say, connected by a channel. Your coroutine, which is

380
00:23:35,946 --> 00:23:39,250
your callee, could be waiting to receive from Cin. Let's call the first

381
00:23:39,282 --> 00:23:43,282
channel, C in. So as if you, if your caller or your subroutine

382
00:23:43,378 --> 00:23:46,970
wants to resume your coroutine or your colleague, all it needs to do is write

383
00:23:47,002 --> 00:23:50,490
into cin. So as soon as it writes into cin, your colleague starts running.

384
00:23:50,642 --> 00:23:53,802
And now you need a way to, for your caller to sort of

385
00:23:53,898 --> 00:23:57,220
stop executing as soon as it resumes the function. So what it can

386
00:23:57,252 --> 00:24:00,796
do is it can wait on another channel, it can block itself on,

387
00:24:00,820 --> 00:24:04,588
say, c out. So as soon as it starts, as soon as

388
00:24:04,596 --> 00:24:08,836
it resumes, a coroutine, you know, writes into c in which your coroutine

389
00:24:08,860 --> 00:24:12,484
was blocked on. Your caller can be blocked on cout.

390
00:24:12,564 --> 00:24:16,076
So as soon as your callee or your coroutine stops

391
00:24:16,100 --> 00:24:19,784
executing, it's finished executing, it can write into cout

392
00:24:20,124 --> 00:24:24,008
so and block itself and c in again. So your coroutine is blocked,

393
00:24:24,096 --> 00:24:27,544
and you have now successfully resumed your

394
00:24:27,584 --> 00:24:31,280
caller or your subroutine. So this sort of, we're already starting to see

395
00:24:31,312 --> 00:24:34,592
like a control transfer mechanism coming to

396
00:24:34,608 --> 00:24:37,968
play. Your new function over here. This function new

397
00:24:38,016 --> 00:24:41,576
in your package coral. All it's doing is it's instantiating

398
00:24:41,640 --> 00:24:46,164
two unbuffered channels, your c in, and you see out your resume function.

399
00:24:47,184 --> 00:24:50,858
Okay, let's look at the go function first. Your go function, this coroutine is

400
00:24:50,906 --> 00:24:54,106
essentially what your cor is, your coroutine with a c,

401
00:24:54,250 --> 00:24:58,098
right? So you can see that your function, that this go function is running,

402
00:24:58,146 --> 00:25:01,466
is blocked on cin. When you want to resume this function,

403
00:25:01,530 --> 00:25:05,170
you call this resume function. All it does is writes into cin and waits

404
00:25:05,202 --> 00:25:09,522
on cout. And as soon as your go routine, finish executing,

405
00:25:09,578 --> 00:25:12,770
finishes executing your function, f it writes the output into

406
00:25:12,802 --> 00:25:16,638
cout, you know, which unblocks the. This resume function.

407
00:25:16,766 --> 00:25:19,554
And your go function can be blocked on cn again.

408
00:25:20,534 --> 00:25:23,234
So you can pause and see how this works.

409
00:25:24,174 --> 00:25:27,790
So the new go routine blocks on cn. So far,

410
00:25:27,822 --> 00:25:31,406
you have no parallelism, because only one function,

411
00:25:31,510 --> 00:25:34,834
one routine, has the control.

412
00:25:35,494 --> 00:25:39,126
Now let's add the definition of yield that

413
00:25:39,190 --> 00:25:42,618
we've been talking about so far and basically return its value

414
00:25:42,666 --> 00:25:46,402
to the core routine that resumed it. Right? So right

415
00:25:46,418 --> 00:25:50,066
now we're looking at suspend and yield. So your yield is just the

416
00:25:50,090 --> 00:25:53,514
inversion of your resume. So in your yield function,

417
00:25:53,634 --> 00:25:57,826
you're writing into cout, and when you write into cout, you wait to receive

418
00:25:57,850 --> 00:26:01,818
from c in. So you call, your coroutine is writing into cout,

419
00:26:01,866 --> 00:26:04,414
which your subroutine is blocked on.

420
00:26:05,194 --> 00:26:08,552
And as soon as you do that blocked on, your coroteons blocked

421
00:26:08,568 --> 00:26:12,432
on Cn, your caller starts functioning and you

422
00:26:12,448 --> 00:26:16,040
know which was which one. Once it wants to resume the function

423
00:26:16,072 --> 00:26:19,336
again, it can write to Cn again. So that's

424
00:26:19,360 --> 00:26:23,192
great. Now you have, now if you notice, it's just the inverse of

425
00:26:23,208 --> 00:26:26,704
your resume function. It writes into cout and blocks itself

426
00:26:26,744 --> 00:26:31,084
from c in and you can actually pass this function

427
00:26:31,864 --> 00:26:35,224
in your go routine. So that is

428
00:26:35,264 --> 00:26:39,124
great. That go allows you to pass function values

429
00:26:40,624 --> 00:26:43,884
in other functions, so as callback functions.

430
00:26:44,304 --> 00:26:48,000
So on a note, you have only added

431
00:26:48,072 --> 00:26:51,304
another send receipt pair and there's still no parallelism that's happening.

432
00:26:51,464 --> 00:26:54,444
So that's the whole point of coroutines.

433
00:26:54,824 --> 00:26:58,444
So let's pause for a bit. Are these actually coroutines?

434
00:26:59,064 --> 00:27:02,890
They're still full coroutines, and they can do everything that an ordinary go routine

435
00:27:02,922 --> 00:27:06,490
can. Your core routine, your core new, just creates

436
00:27:06,522 --> 00:27:09,534
coroutines with access to resume and yield operations.

437
00:27:10,074 --> 00:27:13,450
Unlike the Go statement, we are adding new concurrence to the program

438
00:27:13,522 --> 00:27:14,734
without parallelism.

439
00:27:16,114 --> 00:27:19,734
So if you have go statements in your main function,

440
00:27:20,474 --> 00:27:23,174
say you have ten go statements in your main function.

441
00:27:23,714 --> 00:27:27,206
Now you have eleven go routines running and they can be running

442
00:27:27,230 --> 00:27:30,194
at once. So your parallelism is actually gone up to eleven.

443
00:27:31,174 --> 00:27:34,862
But if you have one main coroutine that runs ten new

444
00:27:34,878 --> 00:27:38,638
core new calls, then there are eleven control flows.

445
00:27:38,726 --> 00:27:42,354
But the parallelism of the program is still what it was before, just one.

446
00:27:42,814 --> 00:27:46,190
Right? So go creates new concurrent parallel control flows,

447
00:27:46,262 --> 00:27:49,798
whereas your coro new, which is the implementation that we are developing,

448
00:27:49,926 --> 00:27:53,004
creates new concurrent non parallel control flows.

449
00:27:53,304 --> 00:27:56,520
So let's get back to implementing our core API. It was

450
00:27:56,552 --> 00:28:00,284
pretty crude and we can start improving it a little bit. So first,

451
00:28:00,584 --> 00:28:04,160
what if your callee or your coroutine wants to terminate instead of yielding

452
00:28:04,192 --> 00:28:07,200
back control, it wants to just stop executing.

453
00:28:07,352 --> 00:28:10,712
So say your code has stopped executing. How does your caller know that it

454
00:28:10,728 --> 00:28:14,096
stopped executing? Right, it can try to resume this

455
00:28:14,160 --> 00:28:17,944
coroutine that does not exist anymore and, you know, block itself and

456
00:28:17,984 --> 00:28:21,360
cout. But there is nothing to write into cout because your coroutine does not

457
00:28:21,392 --> 00:28:24,680
exist anymore. So all your, so a simple,

458
00:28:24,752 --> 00:28:28,224
you know, solution to this could be just have a global variable

459
00:28:28,264 --> 00:28:31,296
called running. And as soon as a coroutine stops functioning,

460
00:28:31,320 --> 00:28:35,152
just set running to false. All your caller needs to do is, before it

461
00:28:35,208 --> 00:28:38,640
resumes its coroutine, just, just ask, is this coroutine still

462
00:28:38,672 --> 00:28:42,336
running? If not, don't try

463
00:28:42,360 --> 00:28:47,070
to resume this function. You might think that having

464
00:28:47,102 --> 00:28:50,510
a global variable when you could have potentially a bunch of

465
00:28:50,702 --> 00:28:54,430
coroutines that are running a place for data races.

466
00:28:54,582 --> 00:28:58,150
But if you recall, you can have only one coroutine running at a time.

467
00:28:58,262 --> 00:29:02,542
So this sequential, consistent execution

468
00:29:02,598 --> 00:29:06,486
ensures that there are no data races happening at this running

469
00:29:06,510 --> 00:29:10,554
variable. So, a very simple implementation over here, you have the sunning variable.

470
00:29:11,594 --> 00:29:15,010
As soon as your go routine, which is your coroutine,

471
00:29:15,042 --> 00:29:18,614
in this case, executes this function. X, f,

472
00:29:18,954 --> 00:29:22,562
your running can be set to false. And the next time you want

473
00:29:22,578 --> 00:29:26,306
to start this function again, you check

474
00:29:26,370 --> 00:29:29,970
is running set to false. If it is set to false, then just return.

475
00:29:30,082 --> 00:29:32,914
If it is not, write into c and resume that function,

476
00:29:33,034 --> 00:29:36,250
and so on and so forth. Another thing is

477
00:29:36,282 --> 00:29:40,246
that, sure, if it terminates properly, then you

478
00:29:40,270 --> 00:29:43,702
have your running properly set to false.

479
00:29:43,798 --> 00:29:47,942
But what if the caller, what if your coroutine actually panics, never gets

480
00:29:47,958 --> 00:29:51,158
a chance to set running to false. How does your caller then know

481
00:29:51,206 --> 00:29:54,790
that the coroutine does not exist anymore and not

482
00:29:54,822 --> 00:29:58,766
potentially bread lock itself? So all we

483
00:29:58,790 --> 00:30:02,574
have, your caller could be blocked waiting for news

484
00:30:02,614 --> 00:30:06,736
and couture, but it gets nothing. So yeah, it panics.

485
00:30:06,800 --> 00:30:10,480
There are no rights to call and your caller is blocked. What you can do

486
00:30:10,552 --> 00:30:13,920
is simply just, whenever your coroutine

487
00:30:13,952 --> 00:30:17,960
panics, just propagate this panic up to your caller, and you can just write

488
00:30:17,992 --> 00:30:21,112
into cout, which is what the caller is waiting on.

489
00:30:21,208 --> 00:30:25,480
Write in the code that, you know, I have panicked and don't

490
00:30:25,512 --> 00:30:28,404
exist anymore. So don't try to resume this particular co routine.

491
00:30:28,964 --> 00:30:32,724
And this can be implemented using this defer funk.

492
00:30:32,804 --> 00:30:36,700
So how defer funk, how defer works, if you don't know, is that

493
00:30:36,812 --> 00:30:41,164
it just runs at the end of your function. So after

494
00:30:41,204 --> 00:30:45,452
everything is stopped running, your defer runs, and all it's doing is

495
00:30:45,508 --> 00:30:48,980
that checking if running is set to true. Now,

496
00:30:49,092 --> 00:30:52,460
if your control has reached your defer function and running

497
00:30:52,492 --> 00:30:56,356
was not set to false, which basically means that something went wrong

498
00:30:56,500 --> 00:31:00,064
over here in this gray dot portion, which means that

499
00:31:00,484 --> 00:31:03,932
your colleague possibly panicked. So if

500
00:31:03,988 --> 00:31:07,724
it reaches a def function and running is set to true, you know, that something

501
00:31:07,844 --> 00:31:11,164
bad happened. Set running to false and into cout,

502
00:31:11,244 --> 00:31:14,716
which your resume is waiting to read from. Just write

503
00:31:14,740 --> 00:31:18,036
the message panic. And now your resume

504
00:31:18,060 --> 00:31:21,212
function can handle this panic however it wants to. It can return

505
00:31:21,268 --> 00:31:24,448
in this case, and you can do whatever you want to with this panic.

506
00:31:24,536 --> 00:31:26,964
So, you know, you solve that deadlock situation.

507
00:31:27,624 --> 00:31:31,320
Finally, we need some way to tell the coroutine

508
00:31:31,352 --> 00:31:34,992
or your callee that it's no longer needed. And that could happen because

509
00:31:35,088 --> 00:31:38,424
maybe your caller is panicking or because the caller is simply returning.

510
00:31:38,544 --> 00:31:42,576
Right, and it's the same thing, but the same thing, but in reverse.

511
00:31:42,720 --> 00:31:47,016
Your subroutine just needs to tell your callee, you know,

512
00:31:47,200 --> 00:31:52,206
on the channel that it's waiting on, in that the

513
00:31:52,230 --> 00:31:55,782
parent of this particular function does not exist anymore and you can

514
00:31:55,798 --> 00:31:59,154
stop working as well. So it's just the reverse of what we implemented.

515
00:32:00,014 --> 00:32:04,102
But what we can do is so, yeah, right into cin that

516
00:32:04,238 --> 00:32:07,406
the caller is cancelling, just propagate

517
00:32:07,430 --> 00:32:10,486
this cancel call t or callee, which once it

518
00:32:10,510 --> 00:32:13,430
receives the cancel, it can write into co that, you know,

519
00:32:13,502 --> 00:32:17,294
I acknowledge that the caller is cancelled and

520
00:32:17,714 --> 00:32:21,098
the caller knows that successfully canceled its quote routine

521
00:32:21,266 --> 00:32:24,410
before, you know, stopping itself. So you have your cancel

522
00:32:24,442 --> 00:32:26,694
function over here, and all it's doing,

523
00:32:27,314 --> 00:32:30,746
it's writing into cin a special kind of a panic, which is in

524
00:32:30,770 --> 00:32:34,722
this case this error cancelled into c, in which your

525
00:32:34,778 --> 00:32:38,738
yield function can then handle. So if in

526
00:32:38,746 --> 00:32:42,204
your yield function, if it gets a panic on cin,

527
00:32:42,324 --> 00:32:45,956
it knows that the caller or the invoker

528
00:32:45,980 --> 00:32:49,452
of this particular code routine does not exist anymore. And I can go ahead

529
00:32:49,508 --> 00:32:52,772
and cancel myself, you can pause and look at a bunch of other things that's

530
00:32:52,788 --> 00:32:55,604
happening over here. So finally,

531
00:32:55,764 --> 00:32:58,660
what are the runtime changes that can be made? Right,

532
00:32:58,852 --> 00:33:02,484
that's the, that's the interesting bit. So far we have

533
00:33:02,524 --> 00:33:06,642
defined coroutines using pure go rust builds on this

534
00:33:06,788 --> 00:33:10,006
use of optimized use, use of an

535
00:33:10,030 --> 00:33:13,502
optimized runtime implementation. And what he

536
00:33:13,518 --> 00:33:17,086
did was initially collections of performance data. And he saw that this

537
00:33:17,190 --> 00:33:21,078
Coro, new coro implementation that we've been describing took

538
00:33:21,126 --> 00:33:25,022
approximately 190 nanoseconds per switch, which lies

539
00:33:25,118 --> 00:33:28,814
in the realm of what a regular go routine switch takes at a

540
00:33:29,314 --> 00:33:33,154
few hundred nanoseconds. He changes one thing. First he

541
00:33:33,194 --> 00:33:36,626
marks in the compiler, he marks as a resume

542
00:33:36,650 --> 00:33:40,066
and yield the send receive pairs as

543
00:33:40,090 --> 00:33:43,682
a single operation. So now these are two separate operations

544
00:33:43,738 --> 00:33:47,906
that need to be scheduled by the co, scheduler. It's one single atomic operation.

545
00:33:48,050 --> 00:33:52,186
So you are completely bypassing the scheduler. And you can just directly jump

546
00:33:52,250 --> 00:33:55,794
between coroutines instead of waiting for it to be scheduled,

547
00:33:55,874 --> 00:33:58,494
your resume to be scheduled, and your yield to be scheduled,

548
00:33:59,094 --> 00:34:02,918
bypassing the scheduler entirely. So this implementation actually required

549
00:34:02,966 --> 00:34:06,694
just 118 nanoseconds, which was 38% faster from our

550
00:34:06,734 --> 00:34:10,278
exist current implementation. Then he talks about

551
00:34:10,326 --> 00:34:13,950
adding a direct coroutine switch to the runtime, so you're

552
00:34:13,982 --> 00:34:17,870
avoiding channels entirely. Channels are pretty heavyweight

553
00:34:18,062 --> 00:34:22,022
in the sense that they are pretty general purpose. They are intended

554
00:34:22,078 --> 00:34:25,550
to do a bunch of things, not just a coroutine

555
00:34:25,582 --> 00:34:29,076
switch mechanism. So instead you can have a direct coroutine

556
00:34:29,100 --> 00:34:32,892
switch in runtime. And that implementation actually

557
00:34:32,948 --> 00:34:36,148
just took 20 nanoseconds per switch, which is ten times

558
00:34:36,196 --> 00:34:39,772
faster than the original channel implementation. So, you know, if this ends

559
00:34:39,788 --> 00:34:43,052
up becoming native to go, it would

560
00:34:43,068 --> 00:34:46,864
be really interesting to see how developers use this really,

561
00:34:47,284 --> 00:34:51,212
you know, super fast implementation in go in

562
00:34:51,228 --> 00:34:55,535
various creative ways, and, you know, to see how it changes the

563
00:34:55,559 --> 00:34:59,883
concurrency landscape that go has sort of spearheaded.

564
00:35:00,903 --> 00:35:04,567
We covered quite a bit, and thanks for making it here. We are

565
00:35:04,575 --> 00:35:08,183
able to show that having a full coroutine facility in go makes it even

566
00:35:08,223 --> 00:35:12,103
more powerful for implementing very robust generalized concurrency

567
00:35:12,143 --> 00:35:15,175
patterns. We covered the basics of coroutines,

568
00:35:15,239 --> 00:35:18,683
its history, why it's not as prolific, the problems with it,

569
00:35:19,394 --> 00:35:22,426
why it's not present in a lot of ancient languages. We showed

570
00:35:22,450 --> 00:35:25,770
why we care about full coroutines, what full

571
00:35:25,802 --> 00:35:29,402
coroutines are as different classifications, why we

572
00:35:29,418 --> 00:35:32,858
need to have coroutines in go, and how they differ from go routines,

573
00:35:32,986 --> 00:35:37,050
and how they would differ from existing implementations in other languages like

574
00:35:37,082 --> 00:35:41,298
Python and Kotlin. We then implemented a coroutine API

575
00:35:41,346 --> 00:35:44,746
using existing go definitions, and we built on it to make it a little more

576
00:35:44,770 --> 00:35:48,330
robust. We showed what runtime changes can be made

577
00:35:48,522 --> 00:35:52,138
to make this implementation even more efficient. Finally, these are

578
00:35:52,146 --> 00:35:56,010
the references I highly recommend going through all of these

579
00:35:56,042 --> 00:36:01,994
articles and videos. They're super interesting and quick

580
00:36:02,034 --> 00:36:05,654
reads, and definitely go through co routine for go

581
00:36:06,114 --> 00:36:09,362
by Russ Cox. That's where a lot of this talk was taken from.

582
00:36:09,498 --> 00:36:12,594
Finally, the artwork is by Rene French,

583
00:36:12,674 --> 00:36:16,394
tenten quaslite. Check out their artwork, some really interesting

584
00:36:16,434 --> 00:36:19,354
things and yeah, thank you, thanks a lot.

