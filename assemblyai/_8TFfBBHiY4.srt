1
00:00:27,400 --> 00:00:30,934
Good day everyone. Today we'll be talking about the

2
00:00:30,974 --> 00:00:34,710
different best practices and strategies when dealing with

3
00:00:34,742 --> 00:00:38,594
self hosted, open source, large language models.

4
00:00:39,454 --> 00:00:43,350
Let's quickly introduce ourselves. So, I am Joshua Arvin

5
00:00:43,382 --> 00:00:47,014
Latt and I am the chief technology officer of Newworks

6
00:00:47,054 --> 00:00:50,510
Interactive Labs. I am also an AWS machine

7
00:00:50,542 --> 00:00:54,114
Learning hero, and I'm the author of three books,

8
00:00:54,474 --> 00:00:58,194
the first one being machine learning with Amazon Sage

9
00:00:58,234 --> 00:01:01,354
maker cookbook machine learning engineering

10
00:01:01,434 --> 00:01:05,202
on AWS, and my third book with

11
00:01:05,218 --> 00:01:08,714
the title building and automating penetration

12
00:01:08,794 --> 00:01:12,642
testing labs in the cloud. Hi everyone,

13
00:01:12,818 --> 00:01:16,666
I am Sophie Sullivan. I am the operations director

14
00:01:16,770 --> 00:01:20,642
at Edamama. Previously I was the general manager

15
00:01:20,698 --> 00:01:24,400
of e commerce services and dropship at Beauty MNL,

16
00:01:24,472 --> 00:01:27,736
Dealgrocer and Shoplight. I also have a

17
00:01:27,760 --> 00:01:31,200
couple of certifications in cloud computing and data

18
00:01:31,272 --> 00:01:34,384
analytics. Lastly, I was also

19
00:01:34,424 --> 00:01:38,120
a technical reviewer of the book machine learning engineering on

20
00:01:38,152 --> 00:01:41,648
AWS. So before we do like a deep

21
00:01:41,696 --> 00:01:45,344
dive on this topic, I know everyone here has

22
00:01:45,384 --> 00:01:48,624
already used or tried out AI tools such as

23
00:01:48,664 --> 00:01:52,704
chat, GPT, and and Gemini. Have you ever wondered

24
00:01:52,824 --> 00:01:56,144
how long it will take to build that generative,

25
00:01:56,224 --> 00:01:59,936
AI powered application? Actually, let me

26
00:01:59,960 --> 00:02:04,048
rephrase myself. Have you ever wondered how

27
00:02:04,096 --> 00:02:06,712
long it will take to build that scalable,

28
00:02:06,888 --> 00:02:10,044
secure, reliable, high performance,

29
00:02:10,504 --> 00:02:14,560
low cost, generative AI powered application?

30
00:02:14,752 --> 00:02:18,514
So these are the attributes that are necessary to build

31
00:02:18,634 --> 00:02:20,814
a proper AI application.

32
00:02:21,634 --> 00:02:25,534
So before we do like a deep dive and answer this question,

33
00:02:25,834 --> 00:02:29,258
we have to first discuss the fundamentals of this topic.

34
00:02:29,386 --> 00:02:31,934
So we'll start with the concepts first.

35
00:02:32,794 --> 00:02:36,530
Usually people confuse machine learning

36
00:02:36,642 --> 00:02:39,874
with AI. It's typical for people to

37
00:02:39,914 --> 00:02:43,334
think they are the same, but actually they're not.

38
00:02:43,654 --> 00:02:47,262
Machine learning is a subset of AI, as you

39
00:02:47,278 --> 00:02:51,358
can see here. But as you know, the AI universe

40
00:02:51,486 --> 00:02:54,894
has a number of concepts and terms under

41
00:02:54,934 --> 00:02:58,470
this umbrella. So here we illustrated the different

42
00:02:58,542 --> 00:03:02,094
concepts to illustrate how these interrelate with

43
00:03:02,134 --> 00:03:05,302
one another. As you can see here,

44
00:03:05,438 --> 00:03:08,794
the overarching theme or concept,

45
00:03:09,344 --> 00:03:13,296
and then under AI, there are for now, three different

46
00:03:13,360 --> 00:03:15,844
stages. So we have Ani,

47
00:03:16,224 --> 00:03:19,644
AGI and Asi. Right now,

48
00:03:19,944 --> 00:03:23,904
at this stage, it would seem that technology has really evolved

49
00:03:23,944 --> 00:03:27,424
in advance, but we're at its initial infancy

50
00:03:27,504 --> 00:03:31,124
or under artificial narrow intelligence.

51
00:03:31,544 --> 00:03:35,328
The next stage is AGI, wherein the AI

52
00:03:35,376 --> 00:03:38,240
tools are now more advanced and complex,

53
00:03:38,312 --> 00:03:41,946
wherein it could help solve more complex

54
00:03:42,010 --> 00:03:45,690
problems than the ANI stage. And lastly,

55
00:03:45,722 --> 00:03:49,906
we have ASI, wherein at this stage,

56
00:03:50,090 --> 00:03:53,370
the AI tools can now possibly help or

57
00:03:53,402 --> 00:03:57,074
solve the world's problem. So under AI we have

58
00:03:57,114 --> 00:04:00,298
machine learning. Under machine learning we have deep

59
00:04:00,346 --> 00:04:04,042
learning. And under deep learning we have generative AI,

60
00:04:04,098 --> 00:04:07,950
which is our topic for today. And as

61
00:04:07,982 --> 00:04:12,070
you know, there are different formats under Genai.

62
00:04:12,222 --> 00:04:15,582
So we have LLM, which is a text based AI tool,

63
00:04:15,758 --> 00:04:19,230
images and audio. So I'll turn you

64
00:04:19,262 --> 00:04:22,434
over to orbs to discuss more in detail

65
00:04:23,014 --> 00:04:26,634
how to answer the question that I posed a while ago.

66
00:04:27,214 --> 00:04:30,554
So now that we have a better understanding of the concepts,

67
00:04:30,734 --> 00:04:34,370
let's now dive deeper into self hosted open

68
00:04:34,442 --> 00:04:38,066
source large language models. So years

69
00:04:38,090 --> 00:04:41,974
ago, AI developers and engineers

70
00:04:42,874 --> 00:04:46,530
asked themselves the question, would they be building

71
00:04:46,682 --> 00:04:51,602
everything from scratch, or would they be using existing

72
00:04:51,658 --> 00:04:55,214
machine learning or deep learning frameworks

73
00:04:55,554 --> 00:04:58,014
and managed services or solutions?

74
00:04:59,174 --> 00:05:02,414
And of course, in the previous year also, most of

75
00:05:02,454 --> 00:05:06,414
the AI developers and engineers would now face the

76
00:05:06,454 --> 00:05:10,214
following would they have to start

77
00:05:10,294 --> 00:05:14,478
with a self hosted open source large language

78
00:05:14,526 --> 00:05:17,894
model setup? Or would they just go

79
00:05:17,934 --> 00:05:22,094
straight and use a paid generative AI API,

80
00:05:22,174 --> 00:05:25,582
something like chat GPT API or bedrock API,

81
00:05:25,758 --> 00:05:27,794
where they can get started right away?

82
00:05:28,614 --> 00:05:32,582
So while that is not the topic of this session,

83
00:05:32,718 --> 00:05:36,574
it's important for us to know, because that

84
00:05:36,734 --> 00:05:40,534
plays a significant role whenever we're going to build generative AI

85
00:05:40,574 --> 00:05:43,902
applications. And our goal is to also make the most

86
00:05:43,958 --> 00:05:47,394
out of our open source large language model setup.

87
00:05:47,774 --> 00:05:51,550
And when we talk about dire to entry from

88
00:05:51,582 --> 00:05:55,524
my own experience, starting with an existing API

89
00:05:56,464 --> 00:06:00,320
involves a much more straightforward approach, meaning there's

90
00:06:00,352 --> 00:06:03,204
going to be less code, less installation.

91
00:06:03,544 --> 00:06:07,952
And if the documentation is updated, then in most cases

92
00:06:08,128 --> 00:06:11,784
you just need to add your credit card there

93
00:06:11,904 --> 00:06:15,344
and you may be able to get your first hello world

94
00:06:15,464 --> 00:06:20,046
AI project. So in terms of buyer to entry with

95
00:06:20,110 --> 00:06:23,614
no infrastructure needed, we should be able to

96
00:06:23,654 --> 00:06:27,554
get started already with an existing paid generative

97
00:06:28,134 --> 00:06:31,510
how about infrastructure cost? So this is

98
00:06:31,542 --> 00:06:34,974
where it gets tricky. When we're playing around with

99
00:06:35,014 --> 00:06:39,342
simple prompts, then we may simply underestimate

100
00:06:39,518 --> 00:06:42,894
the overall cost involved when using these

101
00:06:42,934 --> 00:06:46,016
generative AI APIs and when

102
00:06:46,040 --> 00:06:49,672
trying to compute infrastructure cost. Once we get

103
00:06:49,768 --> 00:06:54,216
to deal with more complex queries, and when there's longer

104
00:06:54,360 --> 00:06:58,608
token lengths involved, and then where there are multiple attempts

105
00:06:58,736 --> 00:07:02,416
to query those APIs, the cost

106
00:07:02,560 --> 00:07:05,880
would add up without us noticing it. And once we're

107
00:07:05,912 --> 00:07:09,088
locked in to these types of APIs

108
00:07:09,136 --> 00:07:13,140
and services, it becomes harder to migrate to

109
00:07:13,212 --> 00:07:17,044
other options. However, on the other end, open source

110
00:07:17,084 --> 00:07:21,428
LLMs generally involve a fixed infrastructure

111
00:07:21,476 --> 00:07:26,564
cost. Of course, at the start, a higher fixed

112
00:07:26,604 --> 00:07:30,344
cost amount because you have to pay for the server

113
00:07:30,804 --> 00:07:34,820
or servers involved when running the

114
00:07:34,852 --> 00:07:38,490
deployed large language models. So in the past,

115
00:07:38,562 --> 00:07:42,954
when dealing with, let's say, machine learning models, which are not necessarily

116
00:07:42,994 --> 00:07:46,906
large language models, when they are small enough, they can

117
00:07:46,930 --> 00:07:51,026
be deployed inside serverless endpoints where the

118
00:07:51,050 --> 00:07:54,210
cost scales depending on the or the

119
00:07:54,242 --> 00:07:57,314
usage of those endpoints.

120
00:07:57,394 --> 00:08:00,962
However, when dealing with large language models, generally the

121
00:08:00,978 --> 00:08:05,160
cloud platforms may not necessarily be able to provide that

122
00:08:05,192 --> 00:08:08,584
serverless option for large language models.

123
00:08:08,664 --> 00:08:12,688
So you have to pay for that per hour rate of

124
00:08:12,736 --> 00:08:16,244
that very large server where the model is deployed.

125
00:08:16,824 --> 00:08:19,684
But when it comes to flexibility and level of control,

126
00:08:20,304 --> 00:08:24,208
having an open source large language model self

127
00:08:24,256 --> 00:08:28,344
hosted setup would give us this highest

128
00:08:28,384 --> 00:08:31,544
level of control because for one thing, you have full control of

129
00:08:31,584 --> 00:08:35,115
everything. If you want to fine tune,

130
00:08:35,179 --> 00:08:40,739
if you want to modify the

131
00:08:40,771 --> 00:08:44,019
flow, you want to change the model right

132
00:08:44,051 --> 00:08:47,283
away or try a new model that is not yet

133
00:08:47,323 --> 00:08:51,023
available in the paid APIs.

134
00:08:51,963 --> 00:08:56,795
Those are some of the advantages when having self

135
00:08:56,859 --> 00:09:00,750
hosted large language models. So if you're wondering

136
00:09:00,902 --> 00:09:04,478
how this would look like when dealing with self

137
00:09:04,526 --> 00:09:07,902
hosted large language models, an environment

138
00:09:08,038 --> 00:09:11,374
where we do our experiments and

139
00:09:11,534 --> 00:09:14,714
that environment is where we prepare our scripts,

140
00:09:15,094 --> 00:09:18,790
we prepare the data, and in some cases it would

141
00:09:18,822 --> 00:09:21,554
be used to launch new resources.

142
00:09:22,014 --> 00:09:25,904
So whether they're fine tuning jobs or that's

143
00:09:25,944 --> 00:09:29,960
where the scripts are run. However, the actual infrastructure where

144
00:09:29,992 --> 00:09:33,568
the model is trained and deployed, it would be separate because

145
00:09:33,616 --> 00:09:36,524
that would involve a much larger infrastructure.

146
00:09:36,944 --> 00:09:41,728
So in terms of best practice, number one, it is recommended that

147
00:09:41,776 --> 00:09:46,888
the always on data science environment has

148
00:09:46,936 --> 00:09:50,538
a very small instance size or type,

149
00:09:50,736 --> 00:09:54,318
so that the per hour costs would be low and

150
00:09:54,366 --> 00:09:57,814
that the actual large language model training and

151
00:09:57,854 --> 00:10:01,754
deployment the servers there would be the

152
00:10:02,134 --> 00:10:05,474
actual large ones. So for the training,

153
00:10:06,374 --> 00:10:10,038
which could be expensive, as long as the billing is per

154
00:10:10,086 --> 00:10:13,630
second or per hour. If the actual training job

155
00:10:13,742 --> 00:10:17,014
runs only for, let's say 30 seconds,

156
00:10:17,174 --> 00:10:20,974
then you only have to pay for 30 seconds because after 31

157
00:10:21,014 --> 00:10:24,434
seconds the server would automatically get deleted

158
00:10:25,094 --> 00:10:27,862
deployment endpoints, of course that's a different story,

159
00:10:28,038 --> 00:10:31,594
but generally in some cases

160
00:10:32,494 --> 00:10:36,478
the deployment servers have smaller

161
00:10:36,526 --> 00:10:40,670
instance types compared to the training servers

162
00:10:40,742 --> 00:10:44,614
needed. So as long as we're able to allocate and

163
00:10:44,654 --> 00:10:48,102
assign different instance types and different environments

164
00:10:48,238 --> 00:10:51,594
depending on the type of task

165
00:10:52,054 --> 00:10:55,670
for the self hosted large language model, then it should help

166
00:10:55,702 --> 00:10:59,022
us manage the cost long term. So once we

167
00:10:59,038 --> 00:11:02,646
have that model trained and deployed, of course we have

168
00:11:02,670 --> 00:11:05,998
to prepare the other parts of the infrastructure,

169
00:11:06,166 --> 00:11:09,998
because after setting up and deploying the

170
00:11:10,046 --> 00:11:13,968
actual model, it's just probably one fourth

171
00:11:14,056 --> 00:11:18,016
or one fifth or even one 6th of the overall application.

172
00:11:18,200 --> 00:11:21,872
We have to set up the front end, we have

173
00:11:21,888 --> 00:11:25,208
to set up the back end, and you also have to set

174
00:11:25,256 --> 00:11:28,416
up the database and connect all of them.

175
00:11:28,600 --> 00:11:31,648
So one of the possible ways to deal

176
00:11:31,696 --> 00:11:35,520
with this type of requirement is when you're in a rush. It may make

177
00:11:35,552 --> 00:11:39,876
sense at the start to utilize a serverless implementation,

178
00:11:40,060 --> 00:11:43,476
even if the actual deployed model endpoint

179
00:11:43,540 --> 00:11:46,344
is not serverless. So in this case,

180
00:11:47,244 --> 00:11:51,228
70% of this architecture makes use

181
00:11:51,276 --> 00:11:55,052
of various managed and serverless components.

182
00:11:55,188 --> 00:11:58,308
So for example, a serverless function like AWS

183
00:11:58,356 --> 00:12:02,224
lambda and a serverless API gateway

184
00:12:02,544 --> 00:12:06,792
which would invoke the lambda function, which would then invoke

185
00:12:06,928 --> 00:12:10,604
and pass the inputs to the large language model.

186
00:12:11,224 --> 00:12:15,120
With this type of setup, we're able to

187
00:12:15,152 --> 00:12:18,352
allocate more time on the model part

188
00:12:18,528 --> 00:12:22,728
because the cloud provider has already been

189
00:12:22,776 --> 00:12:27,128
taking care of the infrastructure for the serverless

190
00:12:27,256 --> 00:12:31,250
components so that we don't have to worry about the system administration

191
00:12:31,362 --> 00:12:34,374
part of those resources.

192
00:12:35,314 --> 00:12:39,174
Now for the exciting part, let's now dive deeper

193
00:12:39,714 --> 00:12:44,306
into the various other best practices we'll

194
00:12:44,330 --> 00:12:47,578
be discussing in the next few minutes. Let's start with

195
00:12:47,626 --> 00:12:50,454
security. When dealing with security,

196
00:12:50,994 --> 00:12:54,106
whether it's self hosted or not, we have to

197
00:12:54,130 --> 00:12:57,514
worry about the security of the setup.

198
00:12:58,214 --> 00:13:02,046
When dealing with security, we have to always think

199
00:13:02,230 --> 00:13:05,806
and position ourselves in the point of view of

200
00:13:05,830 --> 00:13:09,998
an attacker. And it's not just the best practices that

201
00:13:10,046 --> 00:13:13,674
we have to worry about, because no amount of best practices

202
00:13:14,214 --> 00:13:17,622
can really replace an actual attacker.

203
00:13:17,758 --> 00:13:22,058
Trying to find ways to

204
00:13:22,146 --> 00:13:25,578
use the model and have it misbehave,

205
00:13:25,746 --> 00:13:29,294
let's say, have it send spam emails to various users.

206
00:13:30,034 --> 00:13:34,018
So just in case that you're interested in learning

207
00:13:34,066 --> 00:13:37,854
more about the security aspect, we actually have

208
00:13:38,514 --> 00:13:41,810
on how to build an

209
00:13:41,842 --> 00:13:45,814
LM vulnerability scanner here in Conf 42,

210
00:13:46,234 --> 00:13:50,204
just assume that when dealing with large language models

211
00:13:50,664 --> 00:13:55,128
and having a self hosted open source

212
00:13:55,256 --> 00:13:58,864
setup, we have to take care of

213
00:13:59,024 --> 00:14:01,444
the different attacks like prompt injection.

214
00:14:01,904 --> 00:14:05,552
We have to worry about data poisoning because all

215
00:14:05,568 --> 00:14:08,576
of that is in our control. What if, for example,

216
00:14:08,640 --> 00:14:12,040
a storage bucket containing the data which will be

217
00:14:12,072 --> 00:14:15,778
used to train or tune? What if that is

218
00:14:15,826 --> 00:14:18,746
modified or altered by an attacker?

219
00:14:18,890 --> 00:14:21,134
So if the process is automated,

220
00:14:21,754 --> 00:14:25,674
then it's possible for the

221
00:14:25,714 --> 00:14:29,186
final output, the final version,

222
00:14:29,370 --> 00:14:32,842
to produce results which would of course

223
00:14:32,898 --> 00:14:37,066
be influenced data which has been modified already

224
00:14:37,250 --> 00:14:41,234
by the attacker. So this means that even if the attacker has

225
00:14:41,274 --> 00:14:45,422
no direct access to the deployed large language

226
00:14:45,478 --> 00:14:49,566
model or models, then there's still some way for the attacker

227
00:14:49,590 --> 00:14:52,646
to cause harm. When dealing with

228
00:14:52,790 --> 00:14:55,954
large language models and even machine learning models in general,

229
00:14:56,414 --> 00:15:00,502
it gets trickier once you have to deal with distributed setups

230
00:15:00,638 --> 00:15:05,254
and when you have multiple resources running, and when you have multiple experiments

231
00:15:05,414 --> 00:15:08,458
running at the same time. When having

232
00:15:08,646 --> 00:15:12,694
those types of experiments and actions and operations,

233
00:15:12,994 --> 00:15:16,370
it's critical to use a dedicated debugger which

234
00:15:16,402 --> 00:15:20,454
allows you to add breakpoints and allow you to troubleshoot

235
00:15:20,954 --> 00:15:24,974
in a different way where you're able to

236
00:15:25,394 --> 00:15:29,722
manage the situation, where the

237
00:15:29,778 --> 00:15:33,810
script is actually not running real time in the same

238
00:15:34,002 --> 00:15:39,294
environment, but it's running somewhere else in a different container

239
00:15:39,454 --> 00:15:43,166
or server. This means that there

240
00:15:43,190 --> 00:15:46,590
should be different ways to control the debugger,

241
00:15:46,742 --> 00:15:49,942
and there should be ways for you to get notified when

242
00:15:49,998 --> 00:15:53,638
something gets detected. This will help you manage

243
00:15:53,726 --> 00:15:57,150
issues much earlier, and you're able

244
00:15:57,222 --> 00:16:00,662
to save on cost and save money, because you

245
00:16:00,678 --> 00:16:04,206
don't have to wait for, let's say, five r to six r's only to realize

246
00:16:04,390 --> 00:16:07,510
that you ended up with a failed

247
00:16:07,542 --> 00:16:11,394
training job. Next would be model monitoring.

248
00:16:11,854 --> 00:16:15,190
Model monitoring is important because for one thing,

249
00:16:15,342 --> 00:16:18,870
if you're not able to save or capture the request and

250
00:16:18,902 --> 00:16:22,926
response, then it would be tricky

251
00:16:22,950 --> 00:16:26,742
for you to analyze and review whether the responses

252
00:16:26,798 --> 00:16:29,998
are actually acceptable or not. And there are

253
00:16:30,046 --> 00:16:33,600
various ways to analyze the data, and you cannot analyze any

254
00:16:33,632 --> 00:16:37,240
data if it's not being collected at all. So model monitoring

255
00:16:37,272 --> 00:16:41,024
is a large topic of its own. So being able to deep dive

256
00:16:41,064 --> 00:16:45,080
on model monitoring and have sophisticated and

257
00:16:45,112 --> 00:16:48,800
mature tools to help monitor and analyze the model's

258
00:16:48,832 --> 00:16:51,484
performance, every version of it,

259
00:16:52,024 --> 00:16:55,368
it's critical to the long term success of

260
00:16:55,416 --> 00:16:58,616
your deployment setup. Versioning,

261
00:16:58,800 --> 00:17:02,320
sort of. Versioning is critical because we're

262
00:17:02,352 --> 00:17:05,688
all pretty sure that your system is going to evolve.

263
00:17:05,856 --> 00:17:09,512
And the tricky part here is that it's not a

264
00:17:09,528 --> 00:17:13,672
single component which would change. It's the entire infrastructure,

265
00:17:13,808 --> 00:17:17,256
it's the entire implementation. In the first version,

266
00:17:17,320 --> 00:17:20,544
there may not be any sort of retrieval

267
00:17:20,584 --> 00:17:24,564
augmented generation setup, but in the second version there's

268
00:17:24,644 --> 00:17:29,188
rag set up. So how do you even compare

269
00:17:29,236 --> 00:17:33,076
those? Are those even comparable? Do you even need the same set

270
00:17:33,100 --> 00:17:36,548
of models? Do you need a less stronger

271
00:17:36,596 --> 00:17:40,532
model for the rag version and have a vector database? So a

272
00:17:40,548 --> 00:17:44,148
lot of people just think that this is a linear process. But setup

273
00:17:44,196 --> 00:17:48,028
versioning is somewhat not

274
00:17:48,076 --> 00:17:51,084
that straightforward when dealing with open source,

275
00:17:51,124 --> 00:17:53,184
large language model deployments.

276
00:17:54,014 --> 00:17:57,710
Deployment options machine learning models may be able

277
00:17:57,742 --> 00:18:01,034
to respond within an acceptable range of maybe

278
00:18:01,854 --> 00:18:05,566
half a second to maybe 30 seconds or

279
00:18:05,590 --> 00:18:09,294
40 seconds. 40 seconds already at this point

280
00:18:09,414 --> 00:18:13,154
may take a bit of time, and in some cases,

281
00:18:13,654 --> 00:18:16,830
some models may even respond in more than two or three

282
00:18:16,862 --> 00:18:20,384
minutes or even five minutes, depending on the type of of process

283
00:18:20,464 --> 00:18:24,160
that's being run, the type of request that's being processed

284
00:18:24,192 --> 00:18:27,960
by the model. Given that there's

285
00:18:27,992 --> 00:18:31,648
a big chance that serverless might not be a supported deployment

286
00:18:31,696 --> 00:18:35,784
option, then asynchronous invocations can

287
00:18:35,824 --> 00:18:39,312
be a viable option, and you should be able to utilize

288
00:18:39,368 --> 00:18:44,232
the event driven nature of the cloud, the asynchronous

289
00:18:44,288 --> 00:18:48,698
deployment option. So again, there could be real time deployment

290
00:18:48,746 --> 00:18:52,330
option where when you do a request, you just wait for

291
00:18:52,362 --> 00:18:55,746
maybe ten to 20 seconds or 30 seconds and then you get

292
00:18:55,770 --> 00:18:59,546
a response. But if there's no guarantee that something

293
00:18:59,610 --> 00:19:03,218
will get returned in 30 seconds, and the response

294
00:19:03,266 --> 00:19:07,054
might happen after, let's say six minutes or eight minutes,

295
00:19:07,434 --> 00:19:11,386
then it may make sense to review the other deployment types

296
00:19:11,410 --> 00:19:14,812
or options available in whatever framework or service

297
00:19:14,868 --> 00:19:17,820
that you're using. Then finally,

298
00:19:17,972 --> 00:19:21,460
automation. Automation is critical because

299
00:19:21,612 --> 00:19:25,628
given the number of tasks and the types of duties

300
00:19:25,676 --> 00:19:29,064
that you'll have to worry about and your team has to worry about,

301
00:19:29,484 --> 00:19:32,764
you have to find ways to iteratively improve

302
00:19:32,844 --> 00:19:36,876
the workflow. And while it's not a

303
00:19:36,900 --> 00:19:40,384
good idea to automate all the steps all in one go,

304
00:19:40,524 --> 00:19:44,952
it's about identifying which steps would probably not change and

305
00:19:45,008 --> 00:19:48,320
which steps could easily be automated so that

306
00:19:48,392 --> 00:19:51,768
you would be able to speed up the process bit by bit.

307
00:19:51,896 --> 00:19:55,920
So for example, if you're able to start with small scripts to

308
00:19:55,952 --> 00:19:59,936
automate some data processing tasks, some data cleaning tasks,

309
00:20:00,080 --> 00:20:04,224
and then later on convert them into more formal processing

310
00:20:04,264 --> 00:20:07,068
jobs, then that's the way to automate.

311
00:20:07,256 --> 00:20:10,772
So there, that's a good summary of some of

312
00:20:10,788 --> 00:20:14,188
the best practices when dealing with open source

313
00:20:14,356 --> 00:20:17,980
large language model setup. So that's pretty much

314
00:20:18,012 --> 00:20:21,652
it. We're able to talk about a

315
00:20:21,668 --> 00:20:25,156
lot of things. We are able to start with a

316
00:20:25,180 --> 00:20:28,700
quick introduction of the concepts such as machine

317
00:20:28,732 --> 00:20:33,256
learning AI and also generative

318
00:20:33,320 --> 00:20:37,272
AI, where we're able to easily

319
00:20:37,448 --> 00:20:40,928
know that with generative AI we can generate text,

320
00:20:41,016 --> 00:20:44,448
we can generate sounds, or you can even

321
00:20:44,496 --> 00:20:48,112
generate images. And again,

322
00:20:48,248 --> 00:20:52,004
the focus is LLMs set of concepts

323
00:20:52,744 --> 00:20:57,524
really relevant in helping us dive deeper into

324
00:20:57,864 --> 00:21:01,194
the more advanced best

325
00:21:01,234 --> 00:21:05,666
practices and strategies in order for us to handle a

326
00:21:05,690 --> 00:21:09,018
more specific set of projects,

327
00:21:09,106 --> 00:21:12,794
especially those which involve open

328
00:21:12,874 --> 00:21:15,634
source large language models,

329
00:21:15,754 --> 00:21:19,514
projects and hope you learned something new.

