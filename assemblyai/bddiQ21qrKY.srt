1
00:00:27,240 --> 00:00:31,332
Hello everyone. My name is I'm Tommy Fernandez. I'm a technical writer

2
00:00:31,388 --> 00:00:35,244
at Semaphore, and this talk is based on a workshop

3
00:00:35,324 --> 00:00:38,764
we did on Belgrade about machine learning,

4
00:00:38,884 --> 00:00:42,548
DevOps and mlops. Since we don't

5
00:00:42,596 --> 00:00:45,884
have as much time, I will assume you have

6
00:00:45,924 --> 00:00:49,860
the basics of machine learning and cover all the tools

7
00:00:49,892 --> 00:00:53,996
and practices you need to take a model from Jupyter

8
00:00:54,060 --> 00:00:57,264
notebooks and deployed in production in a

9
00:00:57,304 --> 00:01:01,720
safe and automated way. So why use mlops?

10
00:01:01,912 --> 00:01:05,632
First, because automation means there's less work

11
00:01:05,688 --> 00:01:08,884
for us to do. We can do more in less time.

12
00:01:09,464 --> 00:01:13,008
The automation means we also can scale our work,

13
00:01:13,136 --> 00:01:16,872
take the same workload, and use bigger machines

14
00:01:16,968 --> 00:01:21,416
without additional work. It also provides consistencies

15
00:01:21,480 --> 00:01:24,680
because we track everything that goes into the

16
00:01:24,712 --> 00:01:28,944
model. We can know exactly what data points were

17
00:01:29,024 --> 00:01:33,152
used for training, what scripts and parameters

18
00:01:33,208 --> 00:01:36,184
were used to train our model. And finally,

19
00:01:36,224 --> 00:01:39,704
we have traceability. We know exactly what

20
00:01:39,744 --> 00:01:43,484
data sets went into our training and fine tuning.

21
00:01:44,024 --> 00:01:48,152
Let's start with a quick overview of the machine learning

22
00:01:48,328 --> 00:01:51,918
model we are going to use. This is oncaggle.com.

23
00:01:52,006 --> 00:01:55,822
i will leave a link in the slides so you can

24
00:01:55,998 --> 00:02:00,422
check out the code for yourself. We're using REsnet 34,

25
00:02:00,518 --> 00:02:04,006
a computational neural network, and using the

26
00:02:04,070 --> 00:02:07,350
optfor pets dataset to fine

27
00:02:07,382 --> 00:02:11,438
tune this model to recognize cats and dogs.

28
00:02:11,526 --> 00:02:15,430
I don't want to spend a lot of time here explaining

29
00:02:15,502 --> 00:02:19,698
the code, because you probably know this very well.

30
00:02:19,806 --> 00:02:23,974
The main problem I think Jupyter notebook have

31
00:02:24,314 --> 00:02:28,082
is that they work like Excel, they work great in

32
00:02:28,098 --> 00:02:31,234
your machine. They are great way to experiment and

33
00:02:31,274 --> 00:02:34,170
to build and discover data.

34
00:02:34,362 --> 00:02:38,854
But when you need to deploy the application, the model into

35
00:02:39,154 --> 00:02:42,810
the general public is not feasible to

36
00:02:42,842 --> 00:02:46,518
use it with Jupyter notebooks. So we're going to

37
00:02:46,566 --> 00:02:50,470
take everything we have here and put it in pure Python

38
00:02:50,582 --> 00:02:53,582
and train the model using continuous integration.

39
00:02:53,678 --> 00:02:57,382
So this is the project. I will leave a link to

40
00:02:57,398 --> 00:03:01,622
the repository in the slides. This project has

41
00:03:01,718 --> 00:03:06,430
all the python code we need to fine

42
00:03:06,462 --> 00:03:09,874
tune the model, test it, and deploy it.

43
00:03:10,294 --> 00:03:14,302
The code is the same. We found the Jupyter notebook with

44
00:03:14,358 --> 00:03:18,154
some small modifications.

45
00:03:18,694 --> 00:03:21,814
The first thing we're going to need is to download

46
00:03:21,854 --> 00:03:26,114
the training data set. I'm doing that here on a terminal,

47
00:03:26,414 --> 00:03:29,854
and this is the first problem we're going to encounter when

48
00:03:29,894 --> 00:03:33,998
using DevOps practices on machine learning

49
00:03:34,086 --> 00:03:38,678
workflows. The data is big, it's about 800

50
00:03:38,766 --> 00:03:42,570
megabytes, and we can't use something like

51
00:03:42,642 --> 00:03:45,802
git to track this. In theory we can,

52
00:03:45,898 --> 00:03:49,458
because git support like large files,

53
00:03:49,586 --> 00:03:53,722
but we're going to easily reach the maximum amount

54
00:03:53,778 --> 00:03:58,354
of data very easily. So we need a different alternative.

55
00:03:58,514 --> 00:04:02,850
And to manage the data and to later create

56
00:04:02,922 --> 00:04:06,738
the machine learning pipelines we're using a

57
00:04:06,786 --> 00:04:10,656
tool called, called DVC. So DVC is an open

58
00:04:10,720 --> 00:04:13,864
source tool. I don't work for DVC. This is not an

59
00:04:13,904 --> 00:04:17,288
endorsement. It's just a tool that I find useful.

60
00:04:17,456 --> 00:04:20,792
And it's useful because it lets me track the

61
00:04:20,848 --> 00:04:24,920
data sets in git without actually having to

62
00:04:24,992 --> 00:04:29,600
upload the files into git. It uses hashes and

63
00:04:29,792 --> 00:04:33,338
special files to track what data goes in

64
00:04:33,376 --> 00:04:37,214
into the model. DVC comes for

65
00:04:37,294 --> 00:04:41,006
all macOS, windows and Linux. So you

66
00:04:41,030 --> 00:04:44,114
can install it's a command line tool.

67
00:04:44,774 --> 00:04:48,062
And as you can see we have here the file.

68
00:04:48,118 --> 00:04:51,310
This is a git repository. So visual

69
00:04:51,382 --> 00:04:55,350
studio code is going to mark this file as pending

70
00:04:55,422 --> 00:04:58,726
to upload. The problem

71
00:04:58,790 --> 00:05:02,388
is, as I said, we don't want to upload this

72
00:05:02,436 --> 00:05:06,108
big file into the git repository. So instead

73
00:05:06,156 --> 00:05:10,588
we're going to run DVC add and

74
00:05:10,676 --> 00:05:14,100
file. We are going to execute this.

75
00:05:14,252 --> 00:05:18,260
So this is going to do a number of things. It's going to create a

76
00:05:18,292 --> 00:05:22,156
new file which has the extension DVC.

77
00:05:22,300 --> 00:05:25,590
And this file contains the hash for the original

78
00:05:25,662 --> 00:05:29,382
file, the size and the path to the

79
00:05:29,438 --> 00:05:31,954
original file with the data.

80
00:05:32,974 --> 00:05:36,822
And it also has updated Gitignore and

81
00:05:36,878 --> 00:05:40,486
added the file. So it's no longer going to be

82
00:05:40,550 --> 00:05:44,334
pushed into repository, it's going to be ignored.

83
00:05:44,494 --> 00:05:48,190
The other thing it does is to create a cachet

84
00:05:48,262 --> 00:05:51,726
directory into the DbC

85
00:05:51,910 --> 00:05:55,694
folder. This folder is not to be checked in into

86
00:05:55,774 --> 00:05:59,254
the repository. It's also be ignored. But what

87
00:05:59,294 --> 00:06:02,822
DvC does is to move the original file into

88
00:06:02,878 --> 00:06:06,038
this cache directory and then link

89
00:06:06,086 --> 00:06:09,894
it back to the original location. Is going

90
00:06:09,934 --> 00:06:13,934
to use either ref links, hard links or

91
00:06:13,974 --> 00:06:17,722
symlinks depending on the file system you have

92
00:06:17,778 --> 00:06:21,450
on your computer. In case of macOS, it's going

93
00:06:21,482 --> 00:06:24,854
to use ref links, meaning that both

94
00:06:25,234 --> 00:06:28,898
entry files point to the same part of the

95
00:06:28,946 --> 00:06:32,298
disk so the file is not duplicated. And now

96
00:06:32,346 --> 00:06:36,298
we, what we need to do is to check out the Gitignore

97
00:06:36,466 --> 00:06:40,474
and the DVC file. So once we

98
00:06:40,514 --> 00:06:44,290
push this, we track in in our repo

99
00:06:44,482 --> 00:06:48,334
which data we are using in our training.

100
00:06:48,994 --> 00:06:52,522
So let's pause and check how DVC works.

101
00:06:52,658 --> 00:06:56,082
It follows the git syntax and workflow.

102
00:06:56,138 --> 00:06:59,414
It ties into the git way of working.

103
00:06:59,994 --> 00:07:04,370
Each time you run a DVC add, it will hash

104
00:07:04,402 --> 00:07:07,994
the file, move it into the cachet and create that

105
00:07:08,114 --> 00:07:11,506
DVC file as a pointer to the

106
00:07:11,530 --> 00:07:15,066
original file. And when we do a DVC

107
00:07:15,130 --> 00:07:18,602
checkout, DVC will pull that

108
00:07:18,658 --> 00:07:21,978
file from the cache and link it into our

109
00:07:22,066 --> 00:07:26,626
working directory. So we can have different branches

110
00:07:26,690 --> 00:07:31,762
in our git repository, each with different DBC

111
00:07:31,818 --> 00:07:35,212
files pointing to different data sets.

112
00:07:35,338 --> 00:07:39,072
And all the datasets will be stored eventually in

113
00:07:39,088 --> 00:07:42,656
the cachet, and we will pull from the

114
00:07:42,680 --> 00:07:46,536
cache using DVC checkout the correct files

115
00:07:46,640 --> 00:07:50,096
every time. So another cool feature about

116
00:07:50,240 --> 00:07:53,792
DBC are machine learning pipelines.

117
00:07:53,928 --> 00:07:57,080
Pipelines are like make files for machine

118
00:07:57,152 --> 00:08:01,272
learning. They are versioned with git, so the whole process

119
00:08:01,368 --> 00:08:04,644
to build and train model is stored

120
00:08:04,724 --> 00:08:07,940
on git and tracked there. And all the results,

121
00:08:08,092 --> 00:08:11,164
all the intermediate files, all the models,

122
00:08:11,244 --> 00:08:14,420
all the transform datasets are stored

123
00:08:14,492 --> 00:08:18,404
and cached. DVC will keep track of all

124
00:08:18,444 --> 00:08:22,396
the changes and will reuse intermediate files

125
00:08:22,500 --> 00:08:26,644
as needed. This is the syntax. To add stage

126
00:08:26,764 --> 00:08:29,704
we put a name which is arbitrary,

127
00:08:29,884 --> 00:08:33,736
the dependencies which are the inputs files.

128
00:08:33,800 --> 00:08:36,288
They can be source code files,

129
00:08:36,376 --> 00:08:40,560
database data files, and the outputs. We can also

130
00:08:40,712 --> 00:08:44,496
store metrics as a separate entity.

131
00:08:44,640 --> 00:08:48,200
And finally we have the command that runs

132
00:08:48,312 --> 00:08:52,000
in this case is a Python program that cleans up the

133
00:08:52,032 --> 00:08:55,456
input data. The stages are stored in

134
00:08:55,480 --> 00:08:58,596
a component file called DBC YAML.

135
00:08:58,660 --> 00:09:02,140
It can be checked in into git and it tracks all

136
00:09:02,172 --> 00:09:05,860
the stages, the inputs, outputs, and computes the

137
00:09:05,892 --> 00:09:09,780
dependency graph. The pipeline is stored in a file called

138
00:09:09,852 --> 00:09:13,612
DBC YAML, which can be tracked with git

139
00:09:13,788 --> 00:09:17,884
and it tracks the inputs, outputs, and builds

140
00:09:17,964 --> 00:09:21,060
the dependency graph automatically.

141
00:09:21,212 --> 00:09:24,472
Okay, let's see pipelines in action.

142
00:09:24,668 --> 00:09:28,872
Instead of tracking the image torbo, I'm going

143
00:09:28,928 --> 00:09:32,044
to track the output of these files.

144
00:09:32,384 --> 00:09:36,232
So what I have here is a prepare script

145
00:09:36,368 --> 00:09:39,608
which basically unpacks the turbo

146
00:09:39,736 --> 00:09:43,824
into separate images. I'll start by removing

147
00:09:43,944 --> 00:09:48,008
the turbo from the DVC cachet. We need

148
00:09:48,056 --> 00:09:51,590
to remove the DVC file which tracks

149
00:09:51,662 --> 00:09:55,566
the image and this will update

150
00:09:55,630 --> 00:09:59,310
Gitignore. And now the file

151
00:09:59,342 --> 00:10:03,874
is removed from the cache and these files is again,

152
00:10:04,814 --> 00:10:07,754
since we don't want to track this file,

153
00:10:08,254 --> 00:10:11,510
add it into the now

154
00:10:11,582 --> 00:10:14,958
I'm going to add to run the

155
00:10:15,046 --> 00:10:18,766
prepared script, the inputs

156
00:10:18,830 --> 00:10:22,334
is images, dot, tar, dot. So I

157
00:10:22,374 --> 00:10:26,950
put that as a dependency here. The output

158
00:10:27,062 --> 00:10:31,086
is this directory data images will

159
00:10:31,190 --> 00:10:35,046
contain the individual images and

160
00:10:35,070 --> 00:10:38,998
we call the script one pack. This is the command that will

161
00:10:39,166 --> 00:10:42,354
take this input and create these outputs.

162
00:10:42,914 --> 00:10:46,794
So this step added images to Gitignore

163
00:10:46,874 --> 00:10:50,570
so the files inside these folders are not tracked by

164
00:10:50,602 --> 00:10:55,170
git and has created a new file called DVC

165
00:10:55,362 --> 00:10:59,214
YAML. As we can see here, we have

166
00:10:59,914 --> 00:11:03,874
name of the stage, the command that we run the

167
00:11:03,914 --> 00:11:07,442
inputs and the output. This file DBC

168
00:11:07,538 --> 00:11:11,022
YAML would be tracked with git. So we should

169
00:11:11,078 --> 00:11:14,470
add it to the repository and to run the

170
00:11:14,502 --> 00:11:18,206
pipeline we're going to run DBC Repro

171
00:11:18,390 --> 00:11:22,134
and it will detect what is missing. We only had

172
00:11:22,174 --> 00:11:26,474
one state and it's going to see there's no

173
00:11:26,894 --> 00:11:30,926
images in the images folder. So I'm going to run

174
00:11:30,990 --> 00:11:34,258
script. The script unpacks and

175
00:11:34,306 --> 00:11:38,546
once it's unpacked DVC will cache.

176
00:11:38,650 --> 00:11:42,098
Every one of these images which are located

177
00:11:42,226 --> 00:11:46,210
here are going to be stored in the DBC

178
00:11:46,282 --> 00:11:49,618
cachet and they are going to be linked back into my

179
00:11:49,666 --> 00:11:54,330
working directory. So now we are tracking

180
00:11:54,442 --> 00:11:58,018
this file, these individual images files which

181
00:11:58,066 --> 00:12:01,880
are training. That does it. The other

182
00:12:01,952 --> 00:12:05,616
thing that happened is that DVC created this file,

183
00:12:05,680 --> 00:12:09,464
DVC lock. This file saves

184
00:12:09,584 --> 00:12:13,256
the output of the DVC repro

185
00:12:13,440 --> 00:12:17,064
so we know which is the state, the final state

186
00:12:17,144 --> 00:12:21,124
of running this script. We have the numbers files

187
00:12:21,864 --> 00:12:25,888
and the total size. It also tracks

188
00:12:25,976 --> 00:12:30,250
the hash of the script. So if we modify

189
00:12:30,362 --> 00:12:34,434
the prepare script is going to rerun the

190
00:12:34,514 --> 00:12:38,274
this stage to confirm that everything is okay.

191
00:12:38,314 --> 00:12:42,066
We can run DC repro again and this time

192
00:12:42,210 --> 00:12:46,066
it will not do anything because nothing has changed.

193
00:12:46,170 --> 00:12:48,974
The input script is the same,

194
00:12:49,394 --> 00:12:52,762
the output files are all the same,

195
00:12:52,818 --> 00:12:55,932
we haven't changed them now what

196
00:12:55,988 --> 00:12:59,428
happens if we change the output? If we delete

197
00:12:59,476 --> 00:13:04,020
one of these files? If we run DVC repro

198
00:13:04,092 --> 00:13:07,956
again, it's going to find that there are

199
00:13:07,980 --> 00:13:11,572
some missing files and it's going to pull

200
00:13:11,628 --> 00:13:15,052
that file from the cache. It's going to check out

201
00:13:15,188 --> 00:13:18,544
automatically the output from the last run

202
00:13:18,884 --> 00:13:22,660
and the files will be recreated. As you can

203
00:13:22,692 --> 00:13:26,660
see we have the same files I have deleted again. So these

204
00:13:26,692 --> 00:13:31,132
were stored initially in the cache and are now relinked

205
00:13:31,228 --> 00:13:34,612
into my working directory. Now let's add

206
00:13:34,668 --> 00:13:38,348
another stage. This case we're going to add a train

207
00:13:38,436 --> 00:13:42,164
stage that will fine tune a convolutional

208
00:13:42,244 --> 00:13:46,180
neural network. This is the same code we find in

209
00:13:46,212 --> 00:13:50,384
the Jupyter notebook, pulls a pre trained network

210
00:13:50,684 --> 00:13:54,780
and uses fine tuning to categorize

211
00:13:54,852 --> 00:13:58,220
inputs as images, as cats or

212
00:13:58,252 --> 00:14:02,572
dogs. This script

213
00:14:02,668 --> 00:14:06,692
also outputs a few graphics, the confusion

214
00:14:06,748 --> 00:14:10,460
metrics and the top losses and the fine tune

215
00:14:10,492 --> 00:14:13,716
results. These are all plots that

216
00:14:13,860 --> 00:14:17,556
evaluate the error of the model. In order to

217
00:14:17,620 --> 00:14:21,340
add this stage, in order to add this stage,

218
00:14:21,412 --> 00:14:25,180
we're going to call DVC stage at we're

219
00:14:25,212 --> 00:14:29,820
going to call this stage train. As inputs we have the train script.

220
00:14:30,012 --> 00:14:33,508
The images in the images folder

221
00:14:33,636 --> 00:14:37,348
and the outputs are two file that are the

222
00:14:37,436 --> 00:14:40,932
models. We can supply the plots as outputs or

223
00:14:40,988 --> 00:14:44,940
with the plots keyword this will treat

224
00:14:45,012 --> 00:14:48,988
the outputs differently because it will let DVC know

225
00:14:49,036 --> 00:14:52,756
that these are things that we can compare across

226
00:14:52,860 --> 00:14:56,252
different iterations of our training. So if

227
00:14:56,268 --> 00:14:59,820
you have different trainings you can compare the plots with

228
00:14:59,852 --> 00:15:03,556
different training data and parameters. Plots are

229
00:15:03,700 --> 00:15:07,692
usually images and we can also use the metrics

230
00:15:07,748 --> 00:15:10,940
keyword to add files like CSV

231
00:15:11,052 --> 00:15:15,356
or TXT files and also compare that those benchmarks

232
00:15:15,420 --> 00:15:19,124
across different runs. And finally we call the

233
00:15:19,164 --> 00:15:22,740
train script to take these inputs and grade

234
00:15:22,772 --> 00:15:26,300
this out. We can see that DVC YAML

235
00:15:26,372 --> 00:15:30,236
is modified. A new stage is here

236
00:15:30,300 --> 00:15:34,344
with all the inputs and outputs. And to run

237
00:15:34,764 --> 00:15:38,812
this stage we're going to run the PVC wrapper.

238
00:15:38,948 --> 00:15:42,796
This will skip the prepared stage because nothing

239
00:15:42,860 --> 00:15:46,516
has changed. This process will take some time so

240
00:15:46,580 --> 00:15:49,224
I will speed up the recording.

241
00:15:49,764 --> 00:15:52,864
So the whole process took about 15 minutes.

242
00:15:53,444 --> 00:15:57,260
I'm running this on my laptop, so it's not the best machine for

243
00:15:57,292 --> 00:16:00,940
this task, but hopefully you're using a more powerful

244
00:16:01,012 --> 00:16:04,454
machine. Let's check the DVC lock.

245
00:16:04,604 --> 00:16:08,450
We are going to see new files here due

246
00:16:08,482 --> 00:16:12,074
to the train stage. We're going to be the

247
00:16:12,114 --> 00:16:16,090
outputs which are the bots. The models are

248
00:16:16,122 --> 00:16:19,394
located in the model directory. These files,

249
00:16:19,514 --> 00:16:23,266
because I mark them as outputs in my stage

250
00:16:23,410 --> 00:16:27,330
are also ignored so they won't be uploaded

251
00:16:27,402 --> 00:16:30,980
into GitHub. Same things for the

252
00:16:31,012 --> 00:16:35,116
files in the metrics, the images that our

253
00:16:35,180 --> 00:16:39,140
training generated. Now if I want to run BBC repro

254
00:16:39,172 --> 00:16:43,132
again, going to skip both stages because

255
00:16:43,228 --> 00:16:46,900
nothing has changed. Let's try deleting some files.

256
00:16:47,012 --> 00:16:51,660
We can delete one of these plots

257
00:16:51,772 --> 00:16:55,212
and let's also delete the model files.

258
00:16:55,308 --> 00:16:58,738
And if we run DVC repro

259
00:16:58,786 --> 00:17:02,474
again going to find these files are

260
00:17:02,514 --> 00:17:06,186
missing and pull them from the cache. So here they are

261
00:17:06,210 --> 00:17:10,018
again and they are safe in our PVC cache.

262
00:17:10,106 --> 00:17:13,722
So to finish we have the test file. The test

263
00:17:13,778 --> 00:17:17,922
file loads some images from Wikipedia

264
00:17:18,098 --> 00:17:21,226
and loads the model and tries to run

265
00:17:21,290 --> 00:17:24,722
the prediction. Let's add the test stage.

266
00:17:24,818 --> 00:17:28,496
We're going to call call it test. The input is

267
00:17:28,520 --> 00:17:32,480
the test file, the model files and

268
00:17:32,512 --> 00:17:35,912
there are no outputs. The idea is that the test

269
00:17:35,968 --> 00:17:39,720
file will return with an exit code different

270
00:17:39,912 --> 00:17:43,160
from zero when there's an error, and it's going

271
00:17:43,192 --> 00:17:47,232
to exit with a serious test code when it

272
00:17:47,368 --> 00:17:51,688
passes. So again, running dvd reprocess

273
00:17:51,846 --> 00:17:55,236
will only run the test and we

274
00:17:55,260 --> 00:17:58,924
don't have any output, but we can check the status

275
00:17:58,964 --> 00:18:02,668
code which is zero as always. The stage

276
00:18:02,796 --> 00:18:06,740
are shown here in the DVC YAML.

277
00:18:06,892 --> 00:18:10,944
We can also visualize the stages

278
00:18:11,924 --> 00:18:13,504
calling DVC.

279
00:18:14,844 --> 00:18:18,406
It will create a graph with all the

280
00:18:18,430 --> 00:18:22,502
stages and dependencies. And we can also find here

281
00:18:22,598 --> 00:18:26,342
in DBC log the inputs and the outputs

282
00:18:26,398 --> 00:18:29,994
all hash. So we want to also

283
00:18:30,574 --> 00:18:34,366
go here into our repository. We are going

284
00:18:34,430 --> 00:18:37,974
to add the DVC lock. It's going to

285
00:18:38,014 --> 00:18:40,718
be tracked in git, DVC,

286
00:18:40,766 --> 00:18:44,586
YAML or git ignores these files we

287
00:18:44,610 --> 00:18:48,818
have deleted so we don't need to check them. And this one

288
00:18:48,866 --> 00:18:52,778
change that I made on the prepared script is superfluous.

289
00:18:52,826 --> 00:18:56,530
So we can, we can undo that. So that's

290
00:18:56,562 --> 00:19:00,330
it. We have this, we have tracked all

291
00:19:00,402 --> 00:19:04,714
our process. We, the data that came into the

292
00:19:04,834 --> 00:19:08,682
training script, the outputs, the models

293
00:19:08,778 --> 00:19:12,874
and the results of the test are all tracked

294
00:19:13,034 --> 00:19:16,586
using metadata in our git repository.

295
00:19:16,690 --> 00:19:20,010
Now let's see how we can run this application.

296
00:19:20,202 --> 00:19:24,194
We have an application file here. We are using

297
00:19:24,274 --> 00:19:27,658
the stream lead library for that.

298
00:19:27,786 --> 00:19:31,818
And this is a very easy way to quickly run

299
00:19:31,906 --> 00:19:35,414
a model in a browser. The ST

300
00:19:35,714 --> 00:19:39,698
namespace is from streamlit and we

301
00:19:39,826 --> 00:19:42,582
are using different methods here.

302
00:19:42,678 --> 00:19:46,222
One for the title. To set a title we can

303
00:19:46,278 --> 00:19:50,234
create widget to upload images.

304
00:19:50,574 --> 00:19:54,414
This widget will show the image on screen and

305
00:19:54,454 --> 00:19:58,094
we have a button to run the prediction.

306
00:19:58,254 --> 00:20:01,614
This will call make prediction, which loads the

307
00:20:01,654 --> 00:20:05,782
model and returns the probability

308
00:20:05,958 --> 00:20:09,222
the model will return. True or false? If it's true,

309
00:20:09,278 --> 00:20:12,914
it's yet. If it's false, it's a doc. So it's going

310
00:20:12,954 --> 00:20:16,066
to show me that message. Auto run

311
00:20:16,130 --> 00:20:19,426
this model we call streamlet run on

312
00:20:19,450 --> 00:20:23,202
the file. Now the application is running on my machine.

313
00:20:23,338 --> 00:20:27,178
Let's try it by uploading. One picture

314
00:20:27,266 --> 00:20:31,294
of my cat here she has just woken up

315
00:20:31,594 --> 00:20:36,458
and let's try the prediction. It's 99%

316
00:20:36,546 --> 00:20:39,794
certain that's cat and I think that's dude.

317
00:20:40,094 --> 00:20:43,534
One other thing we may want to do is to

318
00:20:43,694 --> 00:20:47,294
put this model inside a docker container.

319
00:20:47,454 --> 00:20:51,134
And here we have the basic docker file.

320
00:20:51,174 --> 00:20:55,034
To do this, we start from a by ten

321
00:20:55,374 --> 00:20:59,574
container. We add an application

322
00:20:59,654 --> 00:21:02,862
user to not run the application as root and

323
00:21:02,918 --> 00:21:06,686
copy the requirements, install them and,

324
00:21:06,750 --> 00:21:10,542
and then copy. Basically what we need here is the models,

325
00:21:10,678 --> 00:21:13,854
the source file, and then run the application

326
00:21:13,934 --> 00:21:17,742
with streamlit. Now that this tab is complete

327
00:21:17,798 --> 00:21:21,870
and we have committed all the files, what if we want to

328
00:21:21,902 --> 00:21:25,270
share this cache with my colleagues, with other

329
00:21:25,342 --> 00:21:28,790
team members? This is where remote storage

330
00:21:28,902 --> 00:21:33,516
comes in. DVC supports remote storage

331
00:21:33,710 --> 00:21:37,800
and when we add a remote in the same vein

332
00:21:37,872 --> 00:21:42,152
as it, we add the remote using a similar syntax

333
00:21:42,328 --> 00:21:45,696
and we can upload these files,

334
00:21:45,800 --> 00:21:49,496
push the files into our remote repository and

335
00:21:49,520 --> 00:21:52,804
other people can connect to that storage.

336
00:21:53,424 --> 00:21:56,928
But we have a common cache for everyone in the

337
00:21:56,976 --> 00:22:01,166
team and this will remember have all the versions,

338
00:22:01,270 --> 00:22:04,886
all the iterations that everyone had done during

339
00:22:04,950 --> 00:22:09,886
their work all in one place. DVC supports

340
00:22:10,070 --> 00:22:13,830
by default several cloud providers,

341
00:22:13,982 --> 00:22:17,942
and you can also bring your own. If you have a server

342
00:22:17,998 --> 00:22:21,710
you can connect via SSh or using different

343
00:22:21,782 --> 00:22:26,190
protocols. But in my case I will use AWS

344
00:22:26,302 --> 00:22:30,296
and s three buckets. I have created an

345
00:22:30,360 --> 00:22:33,840
s three bucket only for for storing

346
00:22:33,912 --> 00:22:36,004
this example.

347
00:22:37,864 --> 00:22:41,240
So the syntax to other modes very similar.

348
00:22:41,312 --> 00:22:44,704
To keep DBC remote, add a

349
00:22:44,744 --> 00:22:48,200
name. In this case I'm going to call it origin, just to

350
00:22:48,232 --> 00:22:51,736
keep the convention, but it can be anything. And then

351
00:22:51,800 --> 00:22:55,426
since I'm using s three, I'm going to prepend

352
00:22:55,610 --> 00:22:59,114
their packet with s three and the

353
00:22:59,154 --> 00:23:02,322
name of the bucket. Once we added the cache,

354
00:23:02,378 --> 00:23:05,922
we need to run PVC remote

355
00:23:06,058 --> 00:23:11,018
default and the name of the remote

356
00:23:11,106 --> 00:23:14,610
which I called origin and this will make that

357
00:23:14,642 --> 00:23:18,114
the default. And now we can push the files.

358
00:23:18,234 --> 00:23:22,112
If we run DB, push will connect to the street

359
00:23:22,168 --> 00:23:25,804
bucket, see what's missing and push the changes.

360
00:23:26,344 --> 00:23:29,792
In this case the bucket is empty, so it will

361
00:23:29,888 --> 00:23:33,856
push everything we have into the

362
00:23:34,040 --> 00:23:37,920
remote cachet. So it's starting to to do that right

363
00:23:37,952 --> 00:23:41,448
now. So once we have everything

364
00:23:41,576 --> 00:23:45,192
in our repository, it's we can share

365
00:23:45,248 --> 00:23:48,572
or work with other people. They only need

366
00:23:48,668 --> 00:23:52,596
to add the repository at the remote and then run

367
00:23:52,660 --> 00:23:56,524
DBC. And this will pull all the changes

368
00:23:56,604 --> 00:24:00,064
into our local file system.

369
00:24:00,884 --> 00:24:04,744
Here we can see the complete workflow.

370
00:24:05,204 --> 00:24:09,076
We have our code and our pointers to

371
00:24:09,100 --> 00:24:12,116
the files in our repository,

372
00:24:12,180 --> 00:24:16,704
git hub, Bitpacket, GitLab, any git

373
00:24:16,784 --> 00:24:20,448
provider and we run git pull. This pulls all the

374
00:24:20,496 --> 00:24:24,216
files that are the code,

375
00:24:24,320 --> 00:24:27,552
the pointers, the hashes, the DVC files,

376
00:24:27,608 --> 00:24:31,472
the DVC YAML, everything that preserves state.

377
00:24:31,648 --> 00:24:35,272
Then we run DBC pool. This will connect to

378
00:24:35,328 --> 00:24:39,144
their mode storage and pull all the big

379
00:24:39,224 --> 00:24:42,846
files that stored there and they're

380
00:24:42,870 --> 00:24:46,834
going to be stored in our cache. Any changes

381
00:24:47,414 --> 00:24:51,314
that were made will also be synced with our local cache.

382
00:24:51,734 --> 00:24:55,198
Then we will run EVC rePl, that will

383
00:24:55,246 --> 00:24:58,534
run our training, fine tuning, testing, everything we

384
00:24:58,574 --> 00:25:02,150
want. We can try different parameters, then we commit

385
00:25:02,222 --> 00:25:05,646
all the changes. This will contain any changes with it

386
00:25:05,750 --> 00:25:09,210
to the code and all the new reference to

387
00:25:09,242 --> 00:25:12,410
the new outputs, models, plots,

388
00:25:12,562 --> 00:25:15,938
metrics, everything that is stored in our local

389
00:25:15,986 --> 00:25:19,394
cache. And this will store

390
00:25:19,514 --> 00:25:23,226
push all this reference into git. And then when

391
00:25:23,250 --> 00:25:27,146
we run DVC push, it will actually push the copy

392
00:25:27,170 --> 00:25:30,794
of our cache into the remote storage.

393
00:25:30,954 --> 00:25:35,546
Now that we have everything in a remote repository

394
00:25:35,610 --> 00:25:39,148
and decoding git, we can set up continuous

395
00:25:39,196 --> 00:25:42,980
integration. You can use any continuous integration

396
00:25:43,052 --> 00:25:46,484
product. I'm going to use semaphore because I work

397
00:25:46,524 --> 00:25:49,624
for Semaphore and it's the tool that I know best.

398
00:25:50,164 --> 00:25:54,148
So here we have our workflow

399
00:25:54,196 --> 00:25:58,012
editor. This lets us configure our

400
00:25:58,068 --> 00:26:02,188
commands. First we're going to open the pipeline and

401
00:26:02,276 --> 00:26:05,794
select one of the machines that are available.

402
00:26:06,414 --> 00:26:09,654
And here are the commands that are going

403
00:26:09,694 --> 00:26:13,470
to run before each of my jobs. We're going

404
00:26:13,502 --> 00:26:17,354
to set the Python version, install DVC

405
00:26:18,054 --> 00:26:22,438
and install the dependencies

406
00:26:22,526 --> 00:26:26,070
of python and pull everything from the

407
00:26:26,182 --> 00:26:29,798
cache. This checkout pulls the code

408
00:26:29,846 --> 00:26:33,928
from git. Then if we go to the train

409
00:26:34,016 --> 00:26:36,884
step, we're going to run report train.

410
00:26:37,344 --> 00:26:41,168
This will show only the change in the envelope file in the logs.

411
00:26:41,216 --> 00:26:44,896
And then we're going to push the new models into

412
00:26:44,960 --> 00:26:48,872
the DVC cache and the train command will use

413
00:26:48,928 --> 00:26:52,376
DVC repro test. This will be the only command in

414
00:26:52,400 --> 00:26:56,224
this test. Remember that we put DVC pool

415
00:26:56,304 --> 00:26:59,990
as a common command in the pipeline.

416
00:27:00,062 --> 00:27:04,594
These are all the commands that are going to run before

417
00:27:05,734 --> 00:27:10,034
any of the jobs. So basically this

418
00:27:10,334 --> 00:27:13,950
job pulls the models and run the

419
00:27:13,982 --> 00:27:17,830
test stage. And then we have two continuous

420
00:27:17,902 --> 00:27:21,334
delivery pipelines. We have one for Docker that

421
00:27:21,374 --> 00:27:24,854
will build the docker image and

422
00:27:24,894 --> 00:27:28,960
pushes to Docker hub. Then we have a second

423
00:27:29,032 --> 00:27:32,776
pipeline that deploys the model into

424
00:27:32,880 --> 00:27:36,392
hacking phase using streamlet. And this will

425
00:27:36,528 --> 00:27:40,576
decode the call the code, pull the

426
00:27:40,600 --> 00:27:43,976
cache with all the models and run as deploy

427
00:27:44,040 --> 00:27:47,568
script. We are providing

428
00:27:47,656 --> 00:27:51,270
environment variables. One is the address of the space

429
00:27:51,432 --> 00:27:54,746
and the other is a private ssh key that we

430
00:27:54,770 --> 00:27:59,074
use to push the changes into the hacking

431
00:27:59,114 --> 00:28:03,058
face space. Hiking face uses git

432
00:28:03,146 --> 00:28:06,426
and lfs to support

433
00:28:06,530 --> 00:28:11,174
large files. So basically this job joins

434
00:28:11,794 --> 00:28:14,906
everything into one repository and pushes the

435
00:28:14,930 --> 00:28:18,610
bundle using SSH into the hangface

436
00:28:18,682 --> 00:28:23,152
repository. You can check the code in detail in

437
00:28:23,168 --> 00:28:26,448
the repository that I'm going to share in the slides.

438
00:28:26,496 --> 00:28:29,968
So you can download that and you will find links to blog

439
00:28:30,016 --> 00:28:33,440
post, to the source code

440
00:28:33,552 --> 00:28:37,488
and to the pipeline so you can replicate

441
00:28:37,536 --> 00:28:40,244
that into your CI CD system.

442
00:28:40,744 --> 00:28:43,960
So once I run the continuous integration

443
00:28:44,032 --> 00:28:47,980
pipeline and continue deployment pipeline, I have

444
00:28:48,092 --> 00:28:51,612
the application running on hiking face. This is

445
00:28:51,668 --> 00:28:56,476
by the way for free. You can host your models here

446
00:28:56,540 --> 00:29:01,620
on hiking face for free using different frameworks.

447
00:29:01,772 --> 00:29:05,380
I'm using streamlit which is supported by hiking

448
00:29:05,412 --> 00:29:09,076
face. And now let me again

449
00:29:09,140 --> 00:29:13,132
the picture just to ensure that's working same as

450
00:29:13,188 --> 00:29:16,480
before. And yeah, this case is even

451
00:29:16,552 --> 00:29:20,144
more serious than scan. So yeah, this is one

452
00:29:20,184 --> 00:29:23,448
way we can deploy quickly. And this is all running

453
00:29:23,616 --> 00:29:27,424
in automation. We don't need to deploy manually. We just need

454
00:29:27,504 --> 00:29:31,536
to push our changes into the git repository

455
00:29:31,600 --> 00:29:35,764
and let the A CD

456
00:29:36,264 --> 00:29:40,012
system to take over, drain and deploy for us.

457
00:29:40,168 --> 00:29:43,860
So that's all I have. Thank you for watching this talk.

458
00:29:43,932 --> 00:29:47,484
I hope it help you incorporate different

459
00:29:47,644 --> 00:29:51,916
rules or practices into your ML

460
00:29:52,060 --> 00:29:55,348
workflows. And if you want to contact me,

461
00:29:55,396 --> 00:29:58,628
here's my contact information so I will. Happy to

462
00:29:58,756 --> 00:30:02,708
talk to you. Thank you for watching and have a nice

463
00:30:02,756 --> 00:30:03,604
conference. Thank you.

