1
00:00:21,200 --> 00:00:24,302
Today I am going to share with you some of the lessons learned from multiple

2
00:00:24,358 --> 00:00:28,342
AI chatbot projects where we utilized large language models

3
00:00:28,518 --> 00:00:32,310
and doing that is actually quite tricky. So by the end of the presentation

4
00:00:32,382 --> 00:00:34,998
you will have a list of what to pay attention to,

5
00:00:35,126 --> 00:00:39,022
sometimes critical issues, sometimes tiny little details

6
00:00:39,158 --> 00:00:41,834
which are still important in the project success.

7
00:00:42,694 --> 00:00:46,766
And we will start with the introduction to rag, what it is

8
00:00:46,910 --> 00:00:50,754
and what kind of challenges you can expect when building complex applications.

9
00:00:51,534 --> 00:00:55,616
Then we will talk about hallucinations, but also how

10
00:00:55,680 --> 00:00:59,312
we control the scope of the conversation. So if we

11
00:00:59,328 --> 00:01:02,968
are dealing with customer issue, we dont start talking about

12
00:01:03,016 --> 00:01:06,696
us presidency, election or any other issue which is not

13
00:01:06,840 --> 00:01:10,520
relevant. We will also cover the cost,

14
00:01:10,712 --> 00:01:13,856
how to calculate it and whats important in

15
00:01:13,880 --> 00:01:17,584
various scenarios. And at the end ill briefly

16
00:01:17,624 --> 00:01:21,830
describe privacy issues related to llms and the consequences

17
00:01:21,902 --> 00:01:26,086
of various decisions. My name is Martin,

18
00:01:26,230 --> 00:01:29,710
my background is in data engineering and mlobs and I'm

19
00:01:29,742 --> 00:01:34,230
running a team specialized in everything data. At Tantus data and

20
00:01:34,262 --> 00:01:37,622
at Tantus data we help our customers with setting up data

21
00:01:37,678 --> 00:01:41,238
infrastructure, building data pipelines, and machine learning

22
00:01:41,286 --> 00:01:44,654
and genai driven applications. So during that

23
00:01:44,694 --> 00:01:47,934
presentation I will share lessons learned from some

24
00:01:47,974 --> 00:01:51,694
of our projects. And a little disclaimer before

25
00:01:51,734 --> 00:01:55,694
we get started. We need to be aware that the entire area of

26
00:01:55,734 --> 00:01:59,102
Genai is moving incredibly fast. The models

27
00:01:59,158 --> 00:02:02,714
improve over time, the libraries, the tools improve,

28
00:02:03,054 --> 00:02:06,454
some of them die. So it's really hard

29
00:02:06,494 --> 00:02:09,470
to keep track of all that. And because of that,

30
00:02:09,582 --> 00:02:13,222
be aware that some of the tools I'm referring to might

31
00:02:13,278 --> 00:02:16,554
be outdated by the time you listen

32
00:02:16,594 --> 00:02:20,410
to it. And I'll try not to

33
00:02:20,522 --> 00:02:23,970
focus on specific tools, but more on problems, solutions,

34
00:02:24,042 --> 00:02:27,014
techniques and general ideas.

35
00:02:27,354 --> 00:02:30,906
But since there are so much going on

36
00:02:30,970 --> 00:02:34,170
in the area of Genai after the presentation,

37
00:02:34,242 --> 00:02:37,602
I would be really happy to hear from you about your

38
00:02:37,658 --> 00:02:41,166
findings, your experience. So don't be

39
00:02:41,190 --> 00:02:44,114
shy and let's connect on LinkedIn.

40
00:02:44,734 --> 00:02:48,550
Okay, so let's get started. Let's think about concrete business

41
00:02:48,622 --> 00:02:51,870
problem you would like to solve. And let's think about

42
00:02:51,902 --> 00:02:55,394
a chat which is travel assistant on vacation rental website.

43
00:02:55,774 --> 00:02:59,846
And let's say the customer comes and asks,

44
00:03:00,030 --> 00:03:03,798
I need an apartment in London

45
00:03:03,846 --> 00:03:07,716
with elevator. How do we know what the

46
00:03:07,740 --> 00:03:09,264
customer is asking for?

47
00:03:12,084 --> 00:03:15,356
How do we come up with specific information and use that in the

48
00:03:15,380 --> 00:03:18,940
chat? So one of the

49
00:03:19,052 --> 00:03:23,108
very common answer to these kind of questions is vector embeddings

50
00:03:23,156 --> 00:03:27,180
and vector databases. So let's quickly define what they

51
00:03:27,212 --> 00:03:31,444
are and why are they good in natural language

52
00:03:31,484 --> 00:03:34,850
problems. But then I will show you

53
00:03:34,882 --> 00:03:38,786
some examples of when they do not work that well and what

54
00:03:38,890 --> 00:03:42,922
we can do. So the promise about

55
00:03:42,978 --> 00:03:46,826
vector embeddings is very simple. First of all,

56
00:03:46,970 --> 00:03:51,074
you transform the text into a vector and the vector represents

57
00:03:51,114 --> 00:03:54,938
the semantic meaning of the text. So two

58
00:03:54,986 --> 00:03:59,266
texts which have similar meaning will be transformed into two vectors

59
00:03:59,450 --> 00:04:02,974
which are also close to each other.

60
00:04:03,474 --> 00:04:05,654
And let's have a look at examples.

61
00:04:06,354 --> 00:04:10,034
This is one of the very classical examples.

62
00:04:10,154 --> 00:04:13,650
King and queen are somewhat the same

63
00:04:13,762 --> 00:04:17,654
role, you can say, and the only difference is gender.

64
00:04:18,034 --> 00:04:22,386
So in the perfect vector space, the distance between king and queen

65
00:04:22,570 --> 00:04:25,134
should be the same as between men.

66
00:04:26,654 --> 00:04:30,070
And you should be even able to do this

67
00:04:30,102 --> 00:04:33,390
kind of math like queen equals king,

68
00:04:33,542 --> 00:04:35,954
man plus woman.

69
00:04:36,694 --> 00:04:40,166
And this is another example. The words red,

70
00:04:40,230 --> 00:04:43,510
orange and yellow represents colors, so they are close to each

71
00:04:43,542 --> 00:04:47,246
other. Then king and queen are also close to

72
00:04:47,270 --> 00:04:51,154
each other and car is somewhere completely else.

73
00:04:51,564 --> 00:04:55,068
And this is very flat and simplified

74
00:04:55,236 --> 00:04:58,780
dummy example of vector embeddings, because in reality

75
00:04:58,852 --> 00:05:01,984
they have hundreds or thousands of dimensions.

76
00:05:02,524 --> 00:05:06,028
But the idea, the promise from embeddings

77
00:05:06,116 --> 00:05:09,596
is that the vectors are close to each other if

78
00:05:09,660 --> 00:05:12,144
the text has similar meaning.

79
00:05:12,444 --> 00:05:16,100
So it's not a surprise that for searching information needed

80
00:05:16,132 --> 00:05:20,174
by LLM in a chatbot, we likely want to try vector

81
00:05:20,214 --> 00:05:23,710
database. So the super basic idea is that you transform your

82
00:05:23,742 --> 00:05:27,510
documents into vector, you store them into vector database

83
00:05:27,662 --> 00:05:31,474
and you serve the relevant documents to the LLM.

84
00:05:32,694 --> 00:05:36,310
And more general version of this diagram is

85
00:05:36,342 --> 00:05:40,118
that one when we provide an LLM with access

86
00:05:40,206 --> 00:05:43,532
to our documents, databases, API,

87
00:05:43,678 --> 00:05:47,564
basically everything needed in order to understand our domain information.

88
00:05:47,984 --> 00:05:51,696
And this technique is called RaG, stands for retrieval

89
00:05:51,800 --> 00:05:55,432
augmented generation. But once again,

90
00:05:55,608 --> 00:05:59,400
why are we doing this? We need to remember that

91
00:05:59,432 --> 00:06:02,656
the main ability of LLM is not really the knowledge it

92
00:06:02,680 --> 00:06:06,680
comes with, but the ability to work with texts, with texts

93
00:06:06,712 --> 00:06:10,406
written in natural language, and ability to follow

94
00:06:10,470 --> 00:06:13,846
instructions related to these texts. So LLM has

95
00:06:13,870 --> 00:06:17,462
a chance to know only about the information it was

96
00:06:17,518 --> 00:06:21,750
trained on and we need to provide it with our specific domain

97
00:06:21,822 --> 00:06:25,766
knowledge. Let's get back to our example, our business

98
00:06:25,830 --> 00:06:29,414
problem. How do we use that technique? How do we use vector

99
00:06:29,454 --> 00:06:32,598
databases for our I need

100
00:06:32,646 --> 00:06:35,554
an apartment with elevator in London query.

101
00:06:36,344 --> 00:06:39,624
If we have our apartment descriptions in the vector

102
00:06:39,664 --> 00:06:43,256
database, what we could do, we could just check if the vector

103
00:06:43,320 --> 00:06:47,848
representing our query is close to any of our property description.

104
00:06:48,016 --> 00:06:51,592
And what we hope for is that I

105
00:06:51,648 --> 00:06:55,472
need an apartment with elevator in London. Vector will be

106
00:06:55,488 --> 00:06:59,680
close to an apartment with description apartment

107
00:06:59,752 --> 00:07:03,424
with elevator in London and not that

108
00:07:03,464 --> 00:07:07,216
close to apartment with description apartment in

109
00:07:07,240 --> 00:07:10,204
London. So no elevator mentioned.

110
00:07:10,624 --> 00:07:14,376
But once again, this example is way too simplistic.

111
00:07:14,480 --> 00:07:18,264
This is perfectly valid technique, and it describes what vector

112
00:07:18,304 --> 00:07:21,696
embeddings and vector databases could be used for. But I would

113
00:07:21,720 --> 00:07:25,640
like to focus on the challenges which you might face. So first

114
00:07:25,672 --> 00:07:29,104
of all, the apartment description will never be just a single sentence.

115
00:07:29,184 --> 00:07:32,696
They will look more like this. This is

116
00:07:32,720 --> 00:07:35,648
an example cottage in Cornwall,

117
00:07:35,776 --> 00:07:39,184
western England, and it's not that expensive.

118
00:07:39,264 --> 00:07:42,760
We have one more very similar one. So you can see the descriptions are

119
00:07:42,792 --> 00:07:46,568
quite long and you have some extra information about them.

120
00:07:46,736 --> 00:07:51,056
But we have also some completely different properties.

121
00:07:51,120 --> 00:07:54,560
We have another one in London. It's not a cottage anymore,

122
00:07:54,592 --> 00:07:58,308
it's an apartment that is much more expensive. So completely

123
00:07:58,396 --> 00:08:01,860
different type of property and one more

124
00:08:01,972 --> 00:08:05,580
similar. And then if we take all four properties I

125
00:08:05,612 --> 00:08:09,900
have just shown, get the description and take the

126
00:08:10,012 --> 00:08:13,436
vector embeddings, then we end up with something like this.

127
00:08:13,580 --> 00:08:17,876
I do understand that this is not very readable, but this

128
00:08:18,020 --> 00:08:21,964
actually represents very well the problem an engineer

129
00:08:22,004 --> 00:08:25,936
is struggling with. So we have four different properties both,

130
00:08:25,940 --> 00:08:30,072
but in the world of vectors, they are very close to each other,

131
00:08:30,208 --> 00:08:34,240
simply because the wording used in the description

132
00:08:34,272 --> 00:08:37,816
is very similar. It's very far away from the

133
00:08:37,840 --> 00:08:41,164
word banana, it's very far away from some sentence of

134
00:08:42,104 --> 00:08:45,832
some sentence about constitution, but it

135
00:08:45,888 --> 00:08:50,096
doesn't really help us because the property description itself

136
00:08:50,160 --> 00:08:54,288
is very close to each other. So it's very hard to distinguish

137
00:08:54,416 --> 00:08:58,088
what is what in very hard to make a good

138
00:08:58,136 --> 00:09:01,768
proposal for the customer. And the reason for that

139
00:09:01,816 --> 00:09:05,344
is that if we are using general vector

140
00:09:05,384 --> 00:09:09,296
embeddings, they are good in general language, but they are not specialized

141
00:09:09,360 --> 00:09:12,928
in a specific domain, and the specialization in a specific domain

142
00:09:13,056 --> 00:09:16,640
is usually what we want. So maybe that

143
00:09:16,672 --> 00:09:20,480
will come up as a surprise. But magic does not exist. There is

144
00:09:20,512 --> 00:09:24,228
no such thing as a silver bullet. So vector databases

145
00:09:24,316 --> 00:09:28,044
are useful, but you need to test what will work for you.

146
00:09:28,204 --> 00:09:31,644
Maybe you will need to fine tune the embedding so they are specialized

147
00:09:31,684 --> 00:09:35,820
to the domain. For sure, you will need to do splitting of long documents

148
00:09:35,892 --> 00:09:39,756
because of the context length limitation of vector embedding models,

149
00:09:39,940 --> 00:09:43,508
they can accept text up to specific limit,

150
00:09:43,636 --> 00:09:46,220
few hundreds, up to few thousand at most.

151
00:09:46,412 --> 00:09:50,616
But even if you can fit long text in the embedding model,

152
00:09:50,740 --> 00:09:54,200
it does not mean the longer text will work better for

153
00:09:54,232 --> 00:09:57,764
you. This is something to be tested.

154
00:09:58,384 --> 00:10:01,688
One of the secret ingredients making

155
00:10:01,736 --> 00:10:05,320
your chat better is splitting the documents you are working with

156
00:10:05,352 --> 00:10:09,376
into digestible chunks, that's for sure. But if

157
00:10:09,440 --> 00:10:13,124
I tell you, if I tell you,

158
00:10:13,784 --> 00:10:16,992
get the document description, get the apartment description,

159
00:10:17,048 --> 00:10:20,084
get a PDF document, and split it into chunks,

160
00:10:20,434 --> 00:10:23,730
it will be a bit too simplistic. It will

161
00:10:23,762 --> 00:10:26,922
be somewhat like saying,

162
00:10:27,098 --> 00:10:31,250
just draw two circles, complete the owl,

163
00:10:31,442 --> 00:10:34,690
something is missing. So what we do,

164
00:10:34,762 --> 00:10:38,066
what we pay attention to when splitting documents, let's have a

165
00:10:38,090 --> 00:10:41,494
look, let's have a look at some of the solutions.

166
00:10:42,314 --> 00:10:46,010
When you split the document, what will matter for sure is the size.

167
00:10:46,162 --> 00:10:49,634
And all I can say for sure is that very big chunk will not work

168
00:10:49,674 --> 00:10:53,170
very well and it's kind of intuitive. The vector size is

169
00:10:53,202 --> 00:10:56,914
static and if you try to squeeze too much information into

170
00:10:56,954 --> 00:10:59,894
it, you will lose some of it.

171
00:11:00,274 --> 00:11:04,122
But other than that, when you split the document, you need

172
00:11:04,178 --> 00:11:07,914
to know something about the context. And a good

173
00:11:07,954 --> 00:11:11,266
example would be a large PDF, and having just a chunk

174
00:11:11,290 --> 00:11:14,490
of it without knowing which chapter or which section it

175
00:11:14,522 --> 00:11:18,212
comes from, it will not be very helpful.

176
00:11:18,388 --> 00:11:22,372
That's why it's important to keep the relevant information as part

177
00:11:22,468 --> 00:11:27,184
of the chunk or as part of the metadata.

178
00:11:27,804 --> 00:11:31,516
And if you Google search for what, if the data is too

179
00:11:31,540 --> 00:11:35,124
large for LLM context, or if you just scan the QR code,

180
00:11:35,244 --> 00:11:39,108
you will get to one of our articles describing these kind of problems.

181
00:11:39,236 --> 00:11:42,554
But we also described there a mechanism called self

182
00:11:42,594 --> 00:11:45,890
core retriever and it's super useful in situations

183
00:11:45,922 --> 00:11:50,002
when you have a granular split with all the details necessary.

184
00:11:50,178 --> 00:11:53,850
But still the vector similarity of multiple chunks is too

185
00:11:53,882 --> 00:11:57,810
close to each other and it's hard to distinct

186
00:11:57,922 --> 00:12:01,810
which one is the best in a given situation. And in

187
00:12:01,842 --> 00:12:06,162
such cases it's good to try the mechanism and

188
00:12:06,298 --> 00:12:10,664
what it does. It's basically a tool in LangChain

189
00:12:10,704 --> 00:12:14,600
which allows us to come up with structured query

190
00:12:14,792 --> 00:12:19,120
for specific attributes you predefined. So let's say from

191
00:12:19,152 --> 00:12:23,408
a PDF chunk you will extract price or

192
00:12:23,496 --> 00:12:27,640
offer name. And if you predefined them,

193
00:12:27,792 --> 00:12:31,736
you can have another LLM call for better understanding

194
00:12:31,760 --> 00:12:35,788
of values of these kind of attributes. So you,

195
00:12:35,816 --> 00:12:39,540
you can make better decision. You can make a better decision

196
00:12:39,612 --> 00:12:44,180
about what answer to present to the user and it's very useful.

197
00:12:44,372 --> 00:12:46,824
I recommend you to read up on that.

198
00:12:47,324 --> 00:12:51,236
But let's move on. One more

199
00:12:51,260 --> 00:12:54,684
disclaimer, one more disclaimer about PDF files I mentioned.

200
00:12:54,844 --> 00:12:58,180
The disclaimer is that a lot will depend on the format

201
00:12:58,292 --> 00:13:02,232
and how exactly you parse the PDF. Sometimes you

202
00:13:02,248 --> 00:13:05,680
need to just find a specific parser for a specific document,

203
00:13:05,832 --> 00:13:09,456
but sometimes maybe it's worth looking around.

204
00:13:09,560 --> 00:13:12,960
Maybe you have a chance to get the data you need from a source

205
00:13:12,992 --> 00:13:16,936
which has structure just better than the PDF file.

206
00:13:17,080 --> 00:13:20,604
Maybe the same data exists in better format.

207
00:13:21,264 --> 00:13:24,600
So far we've been focusing on how we can improve

208
00:13:24,752 --> 00:13:28,164
the vector search by splitting the documents.

209
00:13:28,544 --> 00:13:31,872
But what else we can do in order to improve

210
00:13:31,928 --> 00:13:35,416
the vector search, you can use something else instead of

211
00:13:35,440 --> 00:13:39,824
vector search. And I just wanted to say that vector databases

212
00:13:39,904 --> 00:13:43,080
are very popular. They are growing, they are very

213
00:13:43,232 --> 00:13:46,776
natural to be used in context of natural language processing.

214
00:13:46,960 --> 00:13:50,896
But just the fact that they are popular, just the fact that

215
00:13:50,960 --> 00:13:54,218
they are very much connected with our lens,

216
00:13:54,376 --> 00:13:57,954
does not mean this is the only tool you can use.

217
00:13:58,254 --> 00:14:01,334
So for instance, if you have an elasticsearch,

218
00:14:01,454 --> 00:14:04,726
or if you have some search API in your company, there is

219
00:14:04,750 --> 00:14:07,830
really no reason not to use it, not to try,

220
00:14:07,902 --> 00:14:11,662
if it can provide you with relevant info. And at the

221
00:14:11,678 --> 00:14:15,038
same time, most of the vector databases, they come

222
00:14:15,086 --> 00:14:18,594
up with not only the vector search ability,

223
00:14:18,974 --> 00:14:21,734
but they have hybrid search ability.

224
00:14:21,814 --> 00:14:25,532
So on top of vector search, you can

225
00:14:25,668 --> 00:14:29,060
enable more traditional keyword search, for example

226
00:14:29,092 --> 00:14:32,580
BM 25, and you can verify which results

227
00:14:32,652 --> 00:14:36,100
are better. Maybe you can mix them together. Maybe you can

228
00:14:36,132 --> 00:14:39,908
use both results. And once

229
00:14:39,956 --> 00:14:43,948
you mix them together, once you utilize data from multiple

230
00:14:43,996 --> 00:14:47,580
search methods, what you can do is you can re

231
00:14:47,612 --> 00:14:51,332
rank the responses you received. So in many

232
00:14:51,388 --> 00:14:55,340
of our cases we have implemented, we realized

233
00:14:55,452 --> 00:14:58,852
that it makes a lot of sense to blend

234
00:14:58,908 --> 00:15:02,828
multiple sources, multiple results,

235
00:15:02,956 --> 00:15:07,184
blend them together. And what you can consider, except of

236
00:15:07,764 --> 00:15:11,708
vector databases, is data coming directly from backend database,

237
00:15:11,756 --> 00:15:15,884
from data lake, from data warehouse, internal APIs,

238
00:15:15,924 --> 00:15:19,648
but also external APIs like panel

239
00:15:19,696 --> 00:15:23,004
data, or from Google search.

240
00:15:23,704 --> 00:15:27,568
So far we've been focusing on how we can improve the

241
00:15:27,616 --> 00:15:30,604
vector search by splitting the documents.

242
00:15:30,984 --> 00:15:34,328
But what else we can do in order to improve

243
00:15:34,376 --> 00:15:38,264
the vector search, you can use something else instead of vector

244
00:15:38,304 --> 00:15:41,688
search. And I just wanted to say that vector

245
00:15:41,736 --> 00:15:45,530
databases are very popular, they are growing, they are very

246
00:15:45,682 --> 00:15:49,266
natural to be used in context of natural language processing.

247
00:15:49,450 --> 00:15:52,162
But just the fact that they are popular,

248
00:15:52,298 --> 00:15:56,642
just the fact that they are very much connected with llms

249
00:15:56,818 --> 00:16:00,414
does not mean this is the only tool you can use.

250
00:16:00,714 --> 00:16:04,346
So for instance, if you have an elasticsearch, or if

251
00:16:04,370 --> 00:16:08,322
you have some search API in your company, there is really no reason

252
00:16:08,378 --> 00:16:11,784
not to use it, not to try, if it can provide you with

253
00:16:11,824 --> 00:16:15,584
relevant info. And at the same time, most of

254
00:16:15,624 --> 00:16:19,464
the vector databases, they come up with not only the

255
00:16:19,584 --> 00:16:23,424
vector search ability, but they have hybrid

256
00:16:23,464 --> 00:16:27,152
search ability. So on top of vector search,

257
00:16:27,208 --> 00:16:30,792
you can enable more traditional keyword search,

258
00:16:30,928 --> 00:16:34,208
for example BM 25, and you can verify

259
00:16:34,336 --> 00:16:37,802
which results are better. Maybe you can mix them together.

260
00:16:37,898 --> 00:16:42,138
Maybe you can use both results. And once

261
00:16:42,186 --> 00:16:45,682
you mix them together, once you utilize data from

262
00:16:45,738 --> 00:16:49,618
multiple search methods. What you can do is you can

263
00:16:49,666 --> 00:16:53,562
rerank the responses you received. So in many

264
00:16:53,618 --> 00:16:56,122
of our cases we have implemented,

265
00:16:56,298 --> 00:17:00,138
we realized that it makes a lot of

266
00:17:00,186 --> 00:17:05,088
sense to blend multiple sources, multiple result,

267
00:17:05,216 --> 00:17:08,400
blend them together. And what you can consider,

268
00:17:08,472 --> 00:17:12,664
except of vector databases, is data coming directly

269
00:17:12,704 --> 00:17:16,032
from backend database, from data lake, from data

270
00:17:16,088 --> 00:17:20,512
warehouse, internal APIs, but also external

271
00:17:20,568 --> 00:17:23,912
APIs like panel data or from

272
00:17:24,088 --> 00:17:27,712
Google search. And then on top of

273
00:17:27,768 --> 00:17:31,364
quite aggressive query query, which is providing us with many

274
00:17:31,404 --> 00:17:34,788
results, what we do, we do re rank and we select

275
00:17:34,836 --> 00:17:38,544
the best candidates, the candidates which are the most promising.

276
00:17:38,964 --> 00:17:42,972
So the chatbot can utilize the information from the most promising ones

277
00:17:43,108 --> 00:17:46,384
in coming up with the most relevant answer.

278
00:17:47,044 --> 00:17:50,308
The last technique I wanted to mention, and it's actually quite

279
00:17:50,476 --> 00:17:54,396
simple but still quite powerful, is preprocessing

280
00:17:54,460 --> 00:17:58,184
using large language models. So let's say you

281
00:17:58,224 --> 00:18:02,000
have some metadata, but in your metadata, you don't

282
00:18:02,072 --> 00:18:06,160
have any information if an apartment has an elevator

283
00:18:06,232 --> 00:18:09,576
or not. But the customers are looking for this kind

284
00:18:09,600 --> 00:18:13,792
of information. And you do have that information in

285
00:18:13,808 --> 00:18:17,328
the description in a free text. So what you can

286
00:18:17,376 --> 00:18:20,544
do a batch preprocessing

287
00:18:20,584 --> 00:18:24,196
using LLM in search for specific metadata,

288
00:18:24,360 --> 00:18:28,340
you know, users are often looking for. And then

289
00:18:28,532 --> 00:18:32,612
once you extract the metadata, you can just save it. You can enrich

290
00:18:32,628 --> 00:18:36,100
your database and use it in your queries.

291
00:18:36,172 --> 00:18:40,380
So basically, you are utilizing the fact that LLMs are

292
00:18:40,452 --> 00:18:43,732
very, very good in tasks like sentiment analysis,

293
00:18:43,788 --> 00:18:47,588
text categorization and so on. You just

294
00:18:47,716 --> 00:18:50,744
tell them which category you are looking for,

295
00:18:51,054 --> 00:18:54,646
what information you are looking for, and they do it for you. They do

296
00:18:54,670 --> 00:18:57,910
it basically out of the box. They are

297
00:18:57,942 --> 00:19:01,470
good at this kind of tasks, out of the box. So there is really

298
00:19:01,502 --> 00:19:04,234
no reason not to, not to use that fact.

299
00:19:04,934 --> 00:19:08,886
Okay, so we've been talking about techniques which leads us to providing the

300
00:19:08,950 --> 00:19:12,446
most relevant information to the chatbot. But even if

301
00:19:12,470 --> 00:19:16,496
you provide it with very, very relevant information, it's still

302
00:19:16,630 --> 00:19:20,700
can make a mistake. It still can hallucinate. So yes,

303
00:19:20,892 --> 00:19:24,492
one way of preventing or limiting the hallucination is to provide

304
00:19:24,548 --> 00:19:27,772
it with relevant info, but there is really

305
00:19:27,828 --> 00:19:31,756
no guarantee that the answer chat comes up with based on the prompt,

306
00:19:31,820 --> 00:19:34,804
the data you provided provided with,

307
00:19:34,964 --> 00:19:38,452
there is no guarantee that the information produced by the

308
00:19:38,468 --> 00:19:42,628
chatbot is correct. So I will show you a very quick demo of what

309
00:19:42,676 --> 00:19:46,102
the hallucination looks like and one specific technique

310
00:19:46,158 --> 00:19:49,446
which you could use in your project in

311
00:19:49,470 --> 00:19:53,374
order to prevent the hallucination. So let's have a look at the demo I

312
00:19:53,414 --> 00:19:57,394
recorded. Let's have a look.

313
00:19:57,854 --> 00:20:01,798
What we have is Python code where we import a

314
00:20:01,846 --> 00:20:05,714
tool called nemo guardrails. It's a tool created by Nvidia.

315
00:20:06,254 --> 00:20:09,990
And we have a text file with some questions.

316
00:20:10,022 --> 00:20:13,332
We'll have a look at it in a second. And then we define that we

317
00:20:13,348 --> 00:20:17,184
want to use an old OpenAI model text, davinci zero three.

318
00:20:18,044 --> 00:20:21,332
And then in the file we define some

319
00:20:21,388 --> 00:20:25,076
questions. The first question

320
00:20:25,180 --> 00:20:28,580
we define is when did

321
00:20:28,612 --> 00:20:32,452
the roman empire collapse? So we want to ask that question

322
00:20:32,508 --> 00:20:36,300
to the model. And I am asking the question about the

323
00:20:36,332 --> 00:20:40,296
roman empire because it's a common knowledge. And the second

324
00:20:40,360 --> 00:20:43,704
question I'm asking is how

325
00:20:43,744 --> 00:20:47,404
many goals has been scored in polish extraclass in a specific season?

326
00:20:47,704 --> 00:20:51,184
So since the first question is a common knowledge and the second one

327
00:20:51,224 --> 00:20:54,936
is not, I expect one of the questions not to be

328
00:20:54,960 --> 00:20:58,272
the hallucination, one of the answer to the question not to be the

329
00:20:58,288 --> 00:21:01,720
hallucination, and one of them. For one of them,

330
00:21:01,752 --> 00:21:04,920
I do expect the model to hallucinate. And let's see if the

331
00:21:04,952 --> 00:21:09,004
tool can spot what is hallucination, what is not.

332
00:21:09,464 --> 00:21:14,592
So let's see, we run the code and

333
00:21:14,608 --> 00:21:18,728
we will have a lot of logs. And once

334
00:21:18,776 --> 00:21:22,624
we scroll all the way up, after it completes,

335
00:21:22,784 --> 00:21:26,640
after it completes, we can see the

336
00:21:26,672 --> 00:21:29,464
first question, when did the roman empire collapse?

337
00:21:29,624 --> 00:21:32,868
And we get a bottle some bot responses

338
00:21:32,916 --> 00:21:35,664
and it's getting flagged as not hallucination.

339
00:21:36,164 --> 00:21:39,364
But how exactly did the tool spot

340
00:21:39,404 --> 00:21:43,012
that? Let's have a look into the details. Using the second question

341
00:21:43,148 --> 00:21:47,184
as an example, how many goals has been scored in polish extraclassa?

342
00:21:47,484 --> 00:21:51,684
The bot response we are receiving is 1800.

343
00:21:51,804 --> 00:21:55,444
I have no idea if it's correct or not, but the whole point is that

344
00:21:55,484 --> 00:21:59,504
what the tool is doing, it's asking exactly the same question for the second time,

345
00:21:59,944 --> 00:22:03,004
and then we get completely different response,

346
00:22:03,464 --> 00:22:07,168
and then the tool is asking the same question for the third

347
00:22:07,216 --> 00:22:10,528
time and you're getting, once again different

348
00:22:10,696 --> 00:22:14,328
response. And then what the tool

349
00:22:14,376 --> 00:22:19,000
is doing is actually checking if the answers

350
00:22:19,072 --> 00:22:23,400
we are getting are in sync, if the

351
00:22:23,432 --> 00:22:27,216
meaning of them is exactly the same. So it's actually doing another prompt

352
00:22:27,280 --> 00:22:30,362
to the model. And the prompt is

353
00:22:30,538 --> 00:22:34,434
you are given a task to identify if the hypothesis

354
00:22:34,514 --> 00:22:37,842
is in agreement with the context below and

355
00:22:37,858 --> 00:22:42,378
the hypothesis is the original answer

356
00:22:42,426 --> 00:22:46,186
we received. So the answer to the first

357
00:22:46,250 --> 00:22:50,254
time we ask that question and then context

358
00:22:51,194 --> 00:22:55,002
are two extra responses we have received because the

359
00:22:55,058 --> 00:22:59,254
tool was asking the same question three times and the

360
00:22:59,294 --> 00:23:02,594
answer from the model is no

361
00:23:03,374 --> 00:23:06,854
informations are not, which means

362
00:23:07,014 --> 00:23:11,198
we flag it as hallucination. So yeah,

363
00:23:11,326 --> 00:23:14,902
there are ways of preventing the hallucinations. It's good to

364
00:23:14,918 --> 00:23:18,158
be aware of them, but at the same time it's good to be aware of

365
00:23:18,206 --> 00:23:21,558
consequences of these kind of techniques, because there is no such thing

366
00:23:21,606 --> 00:23:25,674
as free lunch. First of all, you need to be aware of the costs

367
00:23:25,714 --> 00:23:29,170
associated with that. The cost of us dollars you

368
00:23:29,202 --> 00:23:32,802
pay for the extra API, call the cost of

369
00:23:32,858 --> 00:23:36,586
slower system because you make extra API, so you introduce an

370
00:23:36,610 --> 00:23:40,058
extra delay, but also the cost of false positive,

371
00:23:40,226 --> 00:23:44,570
because there is really no guarantee that this kind of technique always

372
00:23:44,762 --> 00:23:48,526
works. But all that,

373
00:23:48,690 --> 00:23:52,302
the existence of hallucinations, the fact that we

374
00:23:52,358 --> 00:23:56,174
have to deal with them, but also how we have to experiment

375
00:23:56,214 --> 00:23:59,278
with cutting the documents, how we have to tune the search

376
00:23:59,326 --> 00:24:02,966
engine, all of that can lead to the conclusion that we are back to

377
00:24:02,990 --> 00:24:07,310
square one to some extent, and that there is really no shortcut.

378
00:24:07,462 --> 00:24:11,342
And even though LLMs are really impressive,

379
00:24:11,518 --> 00:24:14,622
you cannot avoid working on the data quality

380
00:24:14,678 --> 00:24:18,814
or just careful engineering. Tools like llms

381
00:24:18,934 --> 00:24:23,074
are impressive, but you still have to do your homework.

382
00:24:23,854 --> 00:24:27,846
The good news is that there are many tools which could

383
00:24:27,910 --> 00:24:31,406
help you to some extent. So I mentioned Nemo guardiols,

384
00:24:31,470 --> 00:24:35,342
but it's worth looking into memgpt weaviate but

385
00:24:35,478 --> 00:24:39,222
at the same time, do not expect that some tool will solve

386
00:24:39,358 --> 00:24:43,024
all your problems. Do not expect

387
00:24:43,684 --> 00:24:47,716
that you buy some tool which will magically solve everything.

388
00:24:47,900 --> 00:24:51,228
This approach, shut up and take my money will

389
00:24:51,356 --> 00:24:55,060
probably not work. It's not gonna happen. The tool might

390
00:24:55,092 --> 00:24:58,364
be helpful, but the tools themselves are coming

391
00:24:58,444 --> 00:25:02,316
with their own problems. The tools themselves are quite

392
00:25:02,460 --> 00:25:06,942
immature because basically the entire area of large language models,

393
00:25:07,108 --> 00:25:10,618
chatbots and so on, is quite

394
00:25:10,786 --> 00:25:14,386
new, quite fresh. And just

395
00:25:14,530 --> 00:25:17,634
to show you an example of how the

396
00:25:17,714 --> 00:25:21,466
tools are changing, this is the history of code in Lancranc

397
00:25:21,490 --> 00:25:25,066
project. And there are tons of changes, which on

398
00:25:25,090 --> 00:25:28,602
one hand is a good thing because the project is evolving and it's actually

399
00:25:28,658 --> 00:25:31,842
impressive how fast it's growing. But on

400
00:25:31,858 --> 00:25:35,136
the other hand, that means you have to be aware

401
00:25:35,160 --> 00:25:39,112
of the updates, upcoming changes, there will be some bugs introduced,

402
00:25:39,248 --> 00:25:42,968
there will be some breaking changes over time, and you just

403
00:25:43,016 --> 00:25:46,200
need to be ready for that. You just need to be aware

404
00:25:46,232 --> 00:25:49,976
of that. So we have all the tools which are helpful,

405
00:25:50,040 --> 00:25:53,568
but not very stable yet, and we are working with a completely

406
00:25:53,616 --> 00:25:56,808
new area and there is a lot of unknown here. And that is why it

407
00:25:56,816 --> 00:26:00,246
is really important that you do the testing. And testing of

408
00:26:00,270 --> 00:26:03,718
LLM project is really, really tricky. So what you

409
00:26:03,726 --> 00:26:07,486
can do for sure, and what you should do is testing of

410
00:26:07,510 --> 00:26:11,246
the retrieval because this is fully under your control and

411
00:26:11,270 --> 00:26:14,634
this is quite predictable. So it's easy to define the test condition,

412
00:26:15,094 --> 00:26:18,390
but you should also test

413
00:26:18,462 --> 00:26:21,914
the LLM actions wherever you can.

414
00:26:22,214 --> 00:26:26,230
And I say wherever you can because it's actually quite tricky

415
00:26:26,262 --> 00:26:29,640
and it's very hard to define reliable tests,

416
00:26:29,752 --> 00:26:33,284
reliable tests which cover most of the possibilities.

417
00:26:33,664 --> 00:26:37,376
And one of the problem with testing llms is that even if

418
00:26:37,400 --> 00:26:40,576
you have exactly the same input in

419
00:26:40,600 --> 00:26:44,324
your test, the output could vary.

420
00:26:45,184 --> 00:26:48,600
So there is this post on OpenAI forum, and I really recommend

421
00:26:48,632 --> 00:26:52,004
you to read the question of determinism.

422
00:26:52,464 --> 00:26:56,332
The bottom line is that large language model action is not really

423
00:26:56,488 --> 00:27:00,180
deterministic. So yeah, you can have the parameters

424
00:27:00,212 --> 00:27:03,980
like temperature, you can set it, and this should control

425
00:27:04,052 --> 00:27:07,580
how creative the model is. But there is this misconception

426
00:27:07,612 --> 00:27:11,084
that if you set it to zero, LLM will be behaving

427
00:27:11,124 --> 00:27:14,820
in exactly the same way. In reality it will be

428
00:27:14,852 --> 00:27:18,052
just kind of less creative.

429
00:27:18,108 --> 00:27:21,516
But it still might provide you with various results,

430
00:27:21,700 --> 00:27:25,278
mostly because of the hardware it's being physically run on. But also

431
00:27:25,366 --> 00:27:29,674
you can always end up with two tokens which have exactly the same probability.

432
00:27:30,014 --> 00:27:33,994
So one or the other will be randomly selected in your result.

433
00:27:34,334 --> 00:27:37,654
So keep that in mind when you write the test,

434
00:27:37,734 --> 00:27:41,734
and it's always worth checking the lang chain utils

435
00:27:41,894 --> 00:27:45,774
lang chain utils for testing because they take this kind of

436
00:27:45,814 --> 00:27:50,214
lack of determinism into consideration and they aim to mitigate

437
00:27:50,254 --> 00:27:54,352
it during testing. But what is critical

438
00:27:54,528 --> 00:27:58,368
when you move to production is that you collect the data from your

439
00:27:58,416 --> 00:28:01,568
run, from your run with real users,

440
00:28:01,696 --> 00:28:05,128
because that is really something which gives you the real feedback

441
00:28:05,176 --> 00:28:09,444
about how it is going, how the users are using the application,

442
00:28:09,744 --> 00:28:13,256
whether they are happy with that or not. Make sure you

443
00:28:13,280 --> 00:28:16,600
collect the data. Make sure you analyze that, especially in the

444
00:28:16,632 --> 00:28:18,704
early phases of of the project.

445
00:28:19,564 --> 00:28:23,664
Let's have a look at legal and privacy aspects of llms.

446
00:28:23,964 --> 00:28:27,116
What we need to understand is that whenever we

447
00:28:27,140 --> 00:28:30,444
pull the data from any database and then process

448
00:28:30,524 --> 00:28:34,636
the data and then eventually pass

449
00:28:34,700 --> 00:28:38,060
the data to LLM, our data is being sent to the

450
00:28:38,092 --> 00:28:41,164
LLM provider, to OpenAI, to Microsoft,

451
00:28:41,244 --> 00:28:45,180
Google, and in some cases it's perfectly fine. But there

452
00:28:45,212 --> 00:28:48,716
are cases that you don't want to send the data

453
00:28:48,780 --> 00:28:52,548
anywhere because it's too sensitive. And that means that

454
00:28:52,596 --> 00:28:56,692
you might want to use an open source LLM installed in

455
00:28:56,708 --> 00:28:58,664
a data center you own.

456
00:29:00,084 --> 00:29:03,524
Keep in mind that in situations when LLM over API

457
00:29:03,564 --> 00:29:07,460
is not possible, you not only have to have a private

458
00:29:07,532 --> 00:29:10,660
LLM installation, you also need to have

459
00:29:10,692 --> 00:29:14,230
your private embedding, private vector, DB and

460
00:29:14,262 --> 00:29:17,902
so on. And installing all that is not a rocket science,

461
00:29:18,078 --> 00:29:22,014
but at the same time. It increases the complexity of your ecosystem

462
00:29:22,134 --> 00:29:25,914
and there is a lot more that you have to maintain.

463
00:29:26,774 --> 00:29:30,510
And let's keep in mind that privacy and where the data is being sent

464
00:29:30,582 --> 00:29:34,630
is just one aspect of legal concerns when it comes to llms.

465
00:29:34,822 --> 00:29:38,702
I would really recommend to read the license terms of

466
00:29:38,878 --> 00:29:42,374
the ones you plan to use. For instance, you should not

467
00:29:42,414 --> 00:29:46,350
get misled by the term open source. Open source

468
00:29:46,462 --> 00:29:49,958
does not automatically mean that you can do everything with it.

469
00:29:50,086 --> 00:29:53,726
Some of open source licenses are limiting how you can use the

470
00:29:53,750 --> 00:29:57,726
data produced by the LLM. So for instance, you won't

471
00:29:57,830 --> 00:30:01,622
be able to use the data you collected for training another

472
00:30:01,798 --> 00:30:05,784
LLM in case you decide to change the model.

473
00:30:05,904 --> 00:30:09,884
So you collect the data from the chatbot. You cannot use that in the future

474
00:30:10,424 --> 00:30:13,496
for the training purposes. Similarly,

475
00:30:13,600 --> 00:30:17,056
generating synthetic data for machine learning model is very

476
00:30:17,240 --> 00:30:21,040
blurry area when it gets to llms. So once

477
00:30:21,072 --> 00:30:24,464
again, don't assume too much and make

478
00:30:24,504 --> 00:30:28,404
sure you don't get into the unpleasant surprises.

479
00:30:29,344 --> 00:30:33,054
Another very important consideration when starting a project

480
00:30:33,224 --> 00:30:36,826
and deciding which LLM to use is cost. And you might think

481
00:30:36,930 --> 00:30:40,762
open source is cheaper because you basically don't pay for the API call.

482
00:30:40,898 --> 00:30:44,354
But in context of Llm it's not that obvious.

483
00:30:44,514 --> 00:30:47,434
And why is that? First of all,

484
00:30:47,514 --> 00:30:50,614
because simple math is not that simple anymore.

485
00:30:50,994 --> 00:30:53,534
And what do I mean by the simple math?

486
00:30:54,194 --> 00:30:57,266
Let's start with the API calls. For instance,

487
00:30:57,330 --> 00:31:00,534
when you are using GPT 3.5, you pay

488
00:31:00,614 --> 00:31:03,774
half a dollar per million tokens in the input and then

489
00:31:03,814 --> 00:31:07,382
$1.5 for million tokens in the output.

490
00:31:07,558 --> 00:31:10,990
But then for GPT four you pay 30

491
00:31:11,062 --> 00:31:14,910
and 60 respectively. So already order of magnitude

492
00:31:14,942 --> 00:31:18,294
more. And in general you have a price list.

493
00:31:18,334 --> 00:31:21,950
And based on that you can estimate how much single interaction

494
00:31:21,982 --> 00:31:25,318
with user can cost and then you can multiply it by number

495
00:31:25,366 --> 00:31:29,496
of expected interactions. But there

496
00:31:29,520 --> 00:31:33,608
will be a few small asterisks to remember about. So first of all,

497
00:31:33,736 --> 00:31:37,416
the math will depend not only on the number of tokens

498
00:31:37,440 --> 00:31:41,056
in general, but also on our understanding of

499
00:31:41,160 --> 00:31:43,324
what is the balance between input.

500
00:31:43,824 --> 00:31:47,564
Cloud is cheaper for input but more expensive for output.

501
00:31:48,104 --> 00:31:51,552
And in most cases it's good enough assumption that token

502
00:31:51,608 --> 00:31:55,098
is a word. But if you are in situation that

503
00:31:55,146 --> 00:31:58,530
small difference matters. Then it's worth looking closer

504
00:31:58,562 --> 00:32:02,354
at the tokenizers. It's worth looking

505
00:32:02,474 --> 00:32:05,450
closer at them because the models use different tokenizers,

506
00:32:05,482 --> 00:32:09,994
and number of tokens consumed for the same text

507
00:32:10,154 --> 00:32:14,010
by cloud is different, actually a bit larger

508
00:32:14,162 --> 00:32:18,194
than the one from chat GPT. So to make it even more

509
00:32:18,234 --> 00:32:22,036
confusing, Google Gemini charges not per token

510
00:32:22,180 --> 00:32:26,116
but per character. So the math

511
00:32:26,260 --> 00:32:29,796
is a little bit tricky already. But doing

512
00:32:29,900 --> 00:32:33,428
back of the envelope calculation should give us close enough

513
00:32:33,556 --> 00:32:37,444
number, and it becomes much more

514
00:32:37,484 --> 00:32:41,108
complex when we try to do the math for open source.

515
00:32:41,196 --> 00:32:44,184
For open source LLM, we host ourselves.

516
00:32:44,584 --> 00:32:48,392
Then you don't calculate the cost per token or characters

517
00:32:48,448 --> 00:32:52,400
produced, but you start with the price of the machine,

518
00:32:52,472 --> 00:32:55,728
price of the GPU, the price for maintenance,

519
00:32:55,896 --> 00:32:59,604
and then you need to estimate the expected traffic.

520
00:33:00,384 --> 00:33:03,760
If your traffic is low, the cost per

521
00:33:03,792 --> 00:33:07,544
request will be extremely high. So it's not obvious math.

522
00:33:07,624 --> 00:33:11,176
It's prone to errors. In many cases, it will be more expensive

523
00:33:11,240 --> 00:33:14,908
than using APIs, or, or at least the return of

524
00:33:14,956 --> 00:33:18,388
investment won't be. I briefly mentioned open

525
00:33:18,436 --> 00:33:22,532
source models, and I'm actually coming from the background where I've always been using

526
00:33:22,588 --> 00:33:25,852
open source, open source databases, open source

527
00:33:25,908 --> 00:33:29,596
data tools, and I really like them. But it

528
00:33:29,620 --> 00:33:33,060
was kind of comfortable working with the open source products

529
00:33:33,132 --> 00:33:36,884
because the open source was actually ahead. They were leading

530
00:33:36,924 --> 00:33:40,684
the innovation and then at some point the cloud providers

531
00:33:40,724 --> 00:33:44,724
came. They were to some extent kind of wrapping

532
00:33:44,844 --> 00:33:48,916
the open source innovation into more convenient way of using

533
00:33:48,980 --> 00:33:53,068
it. But now, I'm a bit sad to say that,

534
00:33:53,196 --> 00:33:56,384
but the open source llms are still behind

535
00:33:56,684 --> 00:34:00,428
and they don't perform as good as the commercial

536
00:34:00,476 --> 00:34:03,612
ones. They are good, they are improving,

537
00:34:03,788 --> 00:34:07,468
but be prepared for extra effort if you want to tune

538
00:34:07,516 --> 00:34:11,116
specific use case with open source LLM. And of course

539
00:34:11,260 --> 00:34:15,052
you can fine tune the model. But before you even do that,

540
00:34:15,148 --> 00:34:17,624
make sure that your data is in good shape.

541
00:34:18,004 --> 00:34:21,700
Data will be the starting point for you anyway, and the

542
00:34:21,732 --> 00:34:25,116
easiest way to start is with rag application instead of fine

543
00:34:25,140 --> 00:34:28,876
tuning. So starting with simplerag can provide you with

544
00:34:28,980 --> 00:34:32,828
much faster result and much faster feedback

545
00:34:32,996 --> 00:34:36,884
from the customer. But if at some

546
00:34:36,924 --> 00:34:41,228
point you decide to tune the model itself, beware that

547
00:34:41,396 --> 00:34:45,020
there are various types of tuning and they differ.

548
00:34:45,092 --> 00:34:48,444
They differ in the sense of how much data you need,

549
00:34:48,484 --> 00:34:52,984
what kind of results you can expect, and whether they introduce extra latency.

550
00:34:53,884 --> 00:34:57,500
All things considered, building chatbots is an area

551
00:34:57,572 --> 00:35:01,532
where you need to experiment a lot. But when you experiment, make sure

552
00:35:01,628 --> 00:35:05,204
you don't get overwhelmed by that. Make sure you have

553
00:35:05,244 --> 00:35:09,508
business goal in mind all the time, because it's very easy to get lost

554
00:35:09,636 --> 00:35:12,144
and end up in never ending experiments.

555
00:35:12,644 --> 00:35:15,716
In most cases, you are not creating a research company.

556
00:35:15,900 --> 00:35:19,644
In most cases you want to solve some specific business problems.

557
00:35:19,684 --> 00:35:23,300
So keep that in mind. So working with

558
00:35:23,332 --> 00:35:26,612
llms is a very, very nice,

559
00:35:26,708 --> 00:35:30,544
interesting job. But at the same time you need to stay focused

560
00:35:30,844 --> 00:35:35,144
on the business goal and make sure you are pragmatic.

561
00:35:36,484 --> 00:35:39,608
Thanks a lot. If you have any questions,

562
00:35:39,776 --> 00:35:43,152
drop me an email or drop me a line on LinkedIn. I'm always

563
00:35:43,288 --> 00:35:45,024
happy to chat. Thank you.

