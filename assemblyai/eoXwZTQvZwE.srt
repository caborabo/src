1
00:00:20,730 --> 00:00:24,458
Everyone, welcome to Montopon. Enhancing data refinement with polars

2
00:00:24,554 --> 00:00:28,306
today we dive into how polars leverage SQL in Python for efficient

3
00:00:28,338 --> 00:00:32,198
data manipulation. I hope you all enjoy this talk and let's start.

4
00:00:32,364 --> 00:00:35,910
So our agenda for today is first we're going to answer the question,

5
00:00:35,980 --> 00:00:39,138
why polars? We're going to do introduction to SQL

6
00:00:39,154 --> 00:00:42,614
context in polars. We're going to learn how to utilize data frames

7
00:00:42,662 --> 00:00:46,010
for big data sets using SQL context in polars.

8
00:00:46,430 --> 00:00:49,890
We're going to learn about common table expressions, which is a nice feature.

9
00:00:49,990 --> 00:00:55,198
Polars sales with SQL concept so

10
00:00:55,284 --> 00:00:58,638
why polars? I think first we need to answer the question

11
00:00:58,724 --> 00:01:02,494
what Polars is? So Polars is a data frame interface on

12
00:01:02,532 --> 00:01:06,366
top of an online analytical processing query engine. It's implemented

13
00:01:06,398 --> 00:01:09,620
in Rust using Apache arrow as the memory model.

14
00:01:10,470 --> 00:01:14,260
Now why should choose polars when you have so many options out there?

15
00:01:14,710 --> 00:01:18,930
So firstly, by performance, Polars is designed for speed. It's leveraging

16
00:01:19,010 --> 00:01:22,242
rust efficiency and safety to perform data operations.

17
00:01:22,306 --> 00:01:26,146
First you can visit this link in order to see detailed

18
00:01:26,178 --> 00:01:29,830
polar's benchmark against other known libraries such as pandas,

19
00:01:30,170 --> 00:01:33,606
we have memory efficiency. Since Polars is written

20
00:01:33,638 --> 00:01:37,254
in Rust, it's using an enhanced memory management without having a garbage

21
00:01:37,302 --> 00:01:40,862
collector. We have IO polar support or common

22
00:01:40,916 --> 00:01:44,270
data storage layers such as local and cloud storage.

23
00:01:45,410 --> 00:01:48,638
We have lazy evaluation. We're going to

24
00:01:48,644 --> 00:01:51,886
go into this more deeply in

25
00:01:51,908 --> 00:01:55,434
this talk. But in short, Polars offers lazy

26
00:01:55,482 --> 00:01:59,650
frame that allows for lazy evaluation to reduce memory usage.

27
00:01:59,990 --> 00:02:03,666
We have easy to use API despite its

28
00:02:03,688 --> 00:02:07,910
high performance, Polars provides a user friendly API making

29
00:02:07,980 --> 00:02:11,986
data manipulation and analysis straightforward for Python users,

30
00:02:12,098 --> 00:02:16,150
especially photos coming from other data processing libraries.

31
00:02:16,730 --> 00:02:19,900
We have SQL contracts, which is our subject for today,

32
00:02:20,430 --> 00:02:23,046
and we have versatile data manipulation.

33
00:02:23,238 --> 00:02:26,614
Polars supports wide range of operation, includes filtering,

34
00:02:26,662 --> 00:02:30,140
sorting, joining, grouping and aggregating data.

35
00:02:31,870 --> 00:02:35,994
We have interoperability so polars can interact easily

36
00:02:36,042 --> 00:02:39,534
with other Python data science tool and libraries again

37
00:02:39,572 --> 00:02:43,738
such as pandas. It's allowing for seamless integration into existing

38
00:02:43,914 --> 00:02:47,466
data science workflows and we have open source.

39
00:02:47,498 --> 00:02:50,866
So Polars is an open source project right now and according to the owners it

40
00:02:50,888 --> 00:02:54,286
will always be an open source project. It is driving by an active

41
00:02:54,318 --> 00:02:58,518
community of developers and everyone is encouraged to add new features and

42
00:02:58,684 --> 00:03:02,546
contribute to the project. You can visit the official

43
00:03:02,658 --> 00:03:06,454
user guide with this link. If you need more details about

44
00:03:06,492 --> 00:03:09,658
polars, I recommend it. Okay,

45
00:03:09,744 --> 00:03:13,418
so let's start with introduction to SQL context. We're going

46
00:03:13,424 --> 00:03:17,194
to go and sound right away. So we

47
00:03:17,232 --> 00:03:21,082
have this block of code. First we are importing

48
00:03:21,146 --> 00:03:24,910
polars, then we're creating a new instance

49
00:03:25,570 --> 00:03:29,214
of s two context. So that's the syntax of using

50
00:03:29,252 --> 00:03:33,050
that. Then we're creating a small data frame.

51
00:03:33,210 --> 00:03:36,370
Here we have a small data frame with three columns id,

52
00:03:36,440 --> 00:03:40,146
name and badges. Id is one, two, three. We have Charlie, Bob and

53
00:03:40,168 --> 00:03:43,570
Addis and each one has a certain badges one,

54
00:03:43,640 --> 00:03:47,606
five and eight. 10th, we are registering a

55
00:03:47,628 --> 00:03:51,442
new table in our SQL context.

56
00:03:51,506 --> 00:03:55,574
So the syntax for that is register. Our first

57
00:03:55,612 --> 00:03:58,518
argument is the table name, so it's going to be trainers.

58
00:03:58,614 --> 00:04:01,814
And our second argument is either a data frame or a lazy

59
00:04:01,862 --> 00:04:05,946
frame. Now we're using a data frame and then

60
00:04:05,968 --> 00:04:09,926
we are executing an SQL query over our SQL

61
00:04:09,958 --> 00:04:13,534
contest. So we are using select id name from

62
00:04:13,572 --> 00:04:16,798
trainers where badges equal eight. So a simple

63
00:04:16,884 --> 00:04:20,174
SQL query that will fetch us all the id and

64
00:04:20,212 --> 00:04:24,190
names from our trainers data set that has eight badges.

65
00:04:24,630 --> 00:04:28,882
Here we are using the argument eager equal true. Eager basically means

66
00:04:28,936 --> 00:04:32,766
that polars will return data frame

67
00:04:32,798 --> 00:04:36,422
as the result and not laser frame which is the default. In case

68
00:04:36,476 --> 00:04:39,554
our result is small and can fit our memory.

69
00:04:39,602 --> 00:04:43,266
We can use eager equal true and then we are printing

70
00:04:43,298 --> 00:04:46,760
the result. So let's look at our result here.

71
00:04:47,530 --> 00:04:51,434
So we have a small data set with the columns id

72
00:04:51,472 --> 00:04:54,730
name. Id is an integer and name is mastering.

73
00:04:55,550 --> 00:04:59,194
And we have Alice which is the only one that match our

74
00:04:59,232 --> 00:05:03,146
query because she has eight beds. So test SQL

75
00:05:03,178 --> 00:05:06,794
context in polars. Now let's go over lazy

76
00:05:06,842 --> 00:05:10,954
stream. So what are lazy frames? Lazy frames

77
00:05:11,002 --> 00:05:14,642
in polars are a clever approach to data analysis, allowing the entire

78
00:05:14,696 --> 00:05:18,798
query to be reviewed before execution. This method

79
00:05:18,894 --> 00:05:23,042
not only enables better efficiency and optimization, but also

80
00:05:23,096 --> 00:05:26,322
supports working with data sets larger than your machine memory.

81
00:05:26,386 --> 00:05:29,794
By using streaming, this means polars

82
00:05:29,842 --> 00:05:33,442
can process huge amounts of data very smoothly

83
00:05:33,506 --> 00:05:35,430
on almost every machine.

84
00:05:37,290 --> 00:05:40,886
So let's go a bit hands on on how

85
00:05:40,908 --> 00:05:44,886
to use polars laser frames with SQL

86
00:05:44,918 --> 00:05:45,690
context.

87
00:05:47,950 --> 00:05:51,150
So first we have two data sets.

88
00:05:51,490 --> 00:05:54,862
One is Pokemon and the other one

89
00:05:54,916 --> 00:05:58,094
is trainers. So as you can see,

90
00:05:58,132 --> 00:06:01,534
Pokemon is a very big data set. It has

91
00:06:01,572 --> 00:06:05,154
200 million rows with

92
00:06:05,192 --> 00:06:08,658
four columns id, name, type and trainer. And we

93
00:06:08,664 --> 00:06:11,966
have another data set for trainers which contains

94
00:06:11,998 --> 00:06:14,050
three columns id, name and badges.

95
00:06:15,370 --> 00:06:18,802
So Pokemon cannot fit in memory

96
00:06:18,946 --> 00:06:22,914
in a lot of machines. So here we want to utilize data frames

97
00:06:22,962 --> 00:06:26,182
in order to query over this big data

98
00:06:26,236 --> 00:06:29,546
set. So let's go a bit hands on and see how

99
00:06:29,568 --> 00:06:33,402
we can do that with polars. So first

100
00:06:33,456 --> 00:06:37,370
we have the scan csV. Scan csV. When you use it,

101
00:06:37,440 --> 00:06:41,214
it scan a file and rather than reading the entire CSV into

102
00:06:41,252 --> 00:06:44,734
memory, polars script a laserframe with reference to

103
00:06:44,772 --> 00:06:48,942
the file data, so no processing of the data happens until

104
00:06:48,996 --> 00:06:52,942
you explicitly execute a query over it. Here we're using

105
00:06:52,996 --> 00:06:54,990
the infrastructure argument.

106
00:06:55,570 --> 00:06:59,410
Infrastemalence makes our life easier

107
00:06:59,750 --> 00:07:03,938
and if we don't know the schema of our

108
00:07:04,024 --> 00:07:07,702
data set, polars basically scans the amount

109
00:07:07,756 --> 00:07:11,334
of force it takes as argument and set

110
00:07:11,372 --> 00:07:14,902
the schema itself. So with bigger number,

111
00:07:15,036 --> 00:07:18,390
usually inferring the scammer will be slower.

112
00:07:18,890 --> 00:07:22,442
So we were reading underdrows in order to understand

113
00:07:22,496 --> 00:07:26,534
our schema. And as you can see we have another scan CSV

114
00:07:26,582 --> 00:07:30,134
for our trainers. Here we are not using interscama lens,

115
00:07:30,262 --> 00:07:34,734
but we are telling followers what the schema of our CSV is.

116
00:07:34,852 --> 00:07:38,462
So we are telling id is an integer, name is string and

117
00:07:38,516 --> 00:07:42,314
Vegas is also an integer. Then we're creating

118
00:07:42,362 --> 00:07:45,682
a new SQL context same way as we

119
00:07:45,736 --> 00:07:49,266
did before. Now we are using register many.

120
00:07:49,368 --> 00:07:53,170
So register many allow us to register more than one table at a time.

121
00:07:53,320 --> 00:07:57,278
So now we are registering two tables. One is Pokemon,

122
00:07:57,374 --> 00:08:01,014
which pointing to our Pokemon Slayframe and

123
00:08:01,052 --> 00:08:04,774
the other one is trainers. And now

124
00:08:04,812 --> 00:08:08,374
we're printing our schema in order to see what

125
00:08:08,412 --> 00:08:11,962
our result will be. And we are executing a very simple

126
00:08:12,016 --> 00:08:15,386
SQL query quote show terrace. Now we

127
00:08:15,408 --> 00:08:19,180
are also using giga two because we want to data frame as the result.

128
00:08:19,630 --> 00:08:22,218
So let's see the result. First,

129
00:08:22,304 --> 00:08:26,430
our schema planes give us two order digs.

130
00:08:27,650 --> 00:08:31,194
As you can see, polars inferred the schema

131
00:08:31,242 --> 00:08:34,650
pretty well. Id is integer, name is string,

132
00:08:34,730 --> 00:08:38,514
type is also string and trainer is also string. So polars is

133
00:08:38,552 --> 00:08:42,738
doing a very good job with installing schema itself. And of

134
00:08:42,744 --> 00:08:46,550
course trainer has the schema we configure. Then let's see

135
00:08:46,620 --> 00:08:49,974
what short tables will give us. And here

136
00:08:50,012 --> 00:08:54,134
we have a very small data frame with only one column name

137
00:08:54,252 --> 00:08:57,510
and we have Pokemon and trainers as our tables.

138
00:08:58,110 --> 00:09:01,514
So far so good. Now let's see how can we

139
00:09:01,552 --> 00:09:05,434
run SQL queries over our two new

140
00:09:05,472 --> 00:09:06,410
tables.

141
00:09:09,310 --> 00:09:13,194
So here we are using our SQL context that contains

142
00:09:13,242 --> 00:09:16,718
two tables, Pokemons and trainers. We are running a

143
00:09:16,724 --> 00:09:20,734
simple SQL query select id trainer from Pokemon where

144
00:09:20,772 --> 00:09:24,350
Pokemon's name is Charizard. Why? Because everyone

145
00:09:24,420 --> 00:09:28,062
loves charizard and we're using sync CSV

146
00:09:28,206 --> 00:09:31,874
with one argument called result CSV. So since

147
00:09:31,912 --> 00:09:35,138
we are not using eager equal tool like we used before,

148
00:09:35,304 --> 00:09:38,370
the result of our query is going to be raised frame.

149
00:09:38,530 --> 00:09:43,330
So in order to dump it we are using sync CSV

150
00:09:43,490 --> 00:09:46,806
which is evaluating the query in spamming mode and writing to a new

151
00:09:46,828 --> 00:09:50,298
CSV file. So here we are dumping all the result of

152
00:09:50,304 --> 00:09:53,946
this query into result CSV. Now let's look at the

153
00:09:53,968 --> 00:09:58,310
result. So that's how resort

154
00:09:58,390 --> 00:10:02,750
csv look like. We have the ideas of the Pokemon

155
00:10:04,930 --> 00:10:09,038
that are charizards and we have the trainer that

156
00:10:09,124 --> 00:10:12,606
the charizard belongs to. Okay,

157
00:10:12,708 --> 00:10:16,674
now that we went over a very simple SQL query, let's go

158
00:10:16,712 --> 00:10:21,070
over common table expressions or CTE. So common table expressions

159
00:10:21,150 --> 00:10:25,410
are a feature of SQL that allows you to define a temporary

160
00:10:25,750 --> 00:10:29,254
named result set that can be referenced within

161
00:10:29,292 --> 00:10:30,630
a SQL statement.

162
00:10:31,290 --> 00:10:35,170
Sds provide a way to break down complex

163
00:10:35,250 --> 00:10:38,870
SQL queries into smaller, more manageable

164
00:10:38,950 --> 00:10:42,218
story pieces, making them easier to read,

165
00:10:42,304 --> 00:10:46,454
write or maintain. So let's

166
00:10:46,502 --> 00:10:50,490
look at an example of SCTe.

167
00:10:51,170 --> 00:10:54,270
So as you can see, the query is a bit more complex.

168
00:10:55,330 --> 00:11:00,122
So we're using width as the keyword

169
00:11:00,186 --> 00:11:04,160
in order to create a common table expression. We're going to call

170
00:11:06,290 --> 00:11:10,194
our new temple champion and it's using to contain all

171
00:11:10,232 --> 00:11:14,274
the trainer names and trainer badges from our trainer data set

172
00:11:14,472 --> 00:11:17,782
that has eight

173
00:11:17,836 --> 00:11:21,382
badges. So it's going to give us all our trainers that

174
00:11:21,516 --> 00:11:25,240
has eight badges. And after we get a result,

175
00:11:26,330 --> 00:11:29,962
by the way, we call it champions because if you ever played a Pokemon game,

176
00:11:30,096 --> 00:11:34,140
you know that in order to complete the game and

177
00:11:34,590 --> 00:11:38,362
mastering the league, you have to get eight badges. So after

178
00:11:38,416 --> 00:11:42,974
we have our champions, we're going to select our

179
00:11:43,012 --> 00:11:46,538
trainer from our Pokemon data set. Here we're

180
00:11:46,554 --> 00:11:50,014
using an alias called trainer name and we are also

181
00:11:50,052 --> 00:11:53,258
going to select all the Pokemon from this specific trainer.

182
00:11:53,434 --> 00:11:56,954
So that's why we're using Joiner.

183
00:11:57,002 --> 00:12:00,466
We are going to join on the champion's name which is the

184
00:12:00,488 --> 00:12:04,526
trainer name, and on our Pokemon trainer.

185
00:12:04,718 --> 00:12:08,840
And we're also using limit which is another nice feature that

186
00:12:09,290 --> 00:12:12,818
Polar has it support limit and order using.

187
00:12:12,994 --> 00:12:18,150
So we're going to get our six pokemons of every

188
00:12:18,220 --> 00:12:21,626
champions that exist in

189
00:12:21,648 --> 00:12:26,170
our trainers data set. Now here we are using collect.

190
00:12:26,910 --> 00:12:30,380
Collect is another way of getting our data from a query result.

191
00:12:30,750 --> 00:12:34,634
We are using stream l equal two because we don't want to collect

192
00:12:34,762 --> 00:12:38,586
all the results into our memory in case it's

193
00:12:38,778 --> 00:12:42,462
a very large result. So we're using streaming. Streaming basically

194
00:12:42,516 --> 00:12:45,682
repents a generator then we're using to dix in order

195
00:12:45,736 --> 00:12:48,814
to convert the result to row

196
00:12:48,862 --> 00:12:52,660
into a dictionary. Now let's see

197
00:12:53,270 --> 00:12:57,480
our result and how it's looking like. So we have

198
00:12:59,770 --> 00:13:03,014
six results because our limit was six and we only

199
00:13:03,052 --> 00:13:06,294
have one champion called Alex Bell and our

200
00:13:06,332 --> 00:13:10,598
Pokemon name in

201
00:13:10,604 --> 00:13:13,658
the first six results. So as you can see,

202
00:13:13,824 --> 00:13:19,450
the alias also extracted the dictionary test

203
00:13:19,600 --> 00:13:20,650
recipes.

204
00:13:24,190 --> 00:13:28,254
A quick disclaimer, please note that for us do not support all

205
00:13:28,292 --> 00:13:32,222
SQL operation. If you need more information about which

206
00:13:32,356 --> 00:13:36,026
SQL operations are supported, you can go over their user

207
00:13:36,058 --> 00:13:39,598
guide. There's a very nice list there that tells

208
00:13:39,614 --> 00:13:42,260
you all the operations are starting to support them.

209
00:13:43,110 --> 00:13:46,754
So that's it is. And that concludes

210
00:13:46,802 --> 00:13:49,560
our talk. So thank you very much.

211
00:13:50,490 --> 00:13:53,794
What we learned today Polar's

212
00:13:53,842 --> 00:13:56,994
SQL Quadrax offers a very unique and powerful

213
00:13:57,042 --> 00:14:00,934
tool for data manipulations, combining familiarity

214
00:14:00,982 --> 00:14:04,310
with SQL with the performance of rust powered

215
00:14:04,390 --> 00:14:07,882
operations in Apache O. Whatever you are a

216
00:14:07,936 --> 00:14:11,994
seasoned data professional or very new to the field, I encourage

217
00:14:12,042 --> 00:14:15,674
you to explore this library and experience the efficiency gains

218
00:14:15,722 --> 00:14:19,310
firsthand, both for small and large data sets.

219
00:14:19,650 --> 00:14:22,894
Thank you for your attention. I look forward to any

220
00:14:22,932 --> 00:14:26,910
questions you might have. You can mail me at tomnabi

221
00:14:27,730 --> 00:14:31,134
two@gmail.com I look

222
00:14:31,172 --> 00:14:34,780
forward to any questions and enjoy the rest of Python 42.

