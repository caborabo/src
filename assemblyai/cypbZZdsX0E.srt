1
00:00:27,240 --> 00:00:31,204
Hello everyone, I'm Akshay Jain, our engineering manager at Innovate UK.

2
00:00:31,334 --> 00:00:35,016
AWS UK is a government bank fund which help

3
00:00:35,120 --> 00:00:39,048
organization by providing more funding to execute their projects.

4
00:00:39,216 --> 00:00:42,984
At present I work with the data migration projects where the primary

5
00:00:43,024 --> 00:00:46,624
responsibility includes around executing a data migrations,

6
00:00:46,744 --> 00:00:50,536
data integrations and implementation of some of the NLP techniques

7
00:00:50,640 --> 00:00:53,804
to help them resolve some of the machine learning related use cases.

8
00:00:54,264 --> 00:00:58,240
Topic for today's talk is topic modeling for text documentation

9
00:00:58,272 --> 00:01:01,630
using NLP techniques. In this particular seminar I'm just

10
00:01:01,662 --> 00:01:05,174
going to share my journey how we are trying to solve some of the use

11
00:01:05,214 --> 00:01:08,726
cases that we have around the text document analysis and

12
00:01:08,750 --> 00:01:12,110
how we are solving it using the NLP techniques. So yeah,

13
00:01:12,182 --> 00:01:14,966
let me just walk you through to the some of the use cases and the

14
00:01:14,990 --> 00:01:18,166
challenges that we have and both that I will be just

15
00:01:18,230 --> 00:01:21,758
throwing at you some of the solutions and the things which we are implementing

16
00:01:21,886 --> 00:01:25,198
to meet those particular challenges. Yeah, let me just go back to

17
00:01:25,206 --> 00:01:28,752
the use cases. So on the use cases side, primarily we have

18
00:01:28,808 --> 00:01:32,560
four kind of use cases right now to solve. The first use case

19
00:01:32,592 --> 00:01:35,944
is how we can identify the entities name within

20
00:01:35,984 --> 00:01:39,512
the documentations. So generally what happened is that as it

21
00:01:39,528 --> 00:01:44,084
is a venture capitalist company, lots of people submit their applications

22
00:01:44,384 --> 00:01:47,944
in their intention to raise a fund. So lots of

23
00:01:47,984 --> 00:01:51,512
documentation comes in a way where people provide information such as

24
00:01:51,568 --> 00:01:55,000
what is the description, what is the purpose of their work and

25
00:01:55,032 --> 00:01:58,104
things like that. So all of those things come in a form of a

26
00:01:58,144 --> 00:02:01,392
large documentation and what we try to identify

27
00:02:01,448 --> 00:02:04,808
in the document is what is the kind of all different entities and things are

28
00:02:04,856 --> 00:02:08,288
involved just to identify. We are not putting government

29
00:02:08,376 --> 00:02:11,856
money to any of the sanctioned companies or sanctioned persons

30
00:02:11,880 --> 00:02:15,360
or something like that. The second kind of use cases, what we

31
00:02:15,392 --> 00:02:19,408
have is that whatever applications or things we are getting

32
00:02:19,576 --> 00:02:23,346
in that data, we may want to identify whether

33
00:02:23,410 --> 00:02:27,042
the documents have a certain similarity or not. We have

34
00:02:27,058 --> 00:02:30,874
a two purpose for it e to just identify what are the different segments

35
00:02:30,954 --> 00:02:34,850
or industry sectors in which we are getting a different application.

36
00:02:35,002 --> 00:02:38,394
And the second one is to identify the use cases where

37
00:02:38,434 --> 00:02:42,154
people are submitting our own applications multiple time with the word

38
00:02:42,194 --> 00:02:45,386
changes and things like that in a way.

39
00:02:45,490 --> 00:02:49,778
So we don't spend much effort to ss application

40
00:02:49,866 --> 00:02:53,474
again and again in a manner. So that's the kind of second use case we

41
00:02:53,514 --> 00:02:56,842
have where the purpose is to clean the application

42
00:02:56,898 --> 00:03:00,962
data and identify the similarity between the documents.

43
00:03:01,138 --> 00:03:05,322
The third use case, what we have is to understand

44
00:03:05,458 --> 00:03:09,274
what are the sectors and things in people are submitting

45
00:03:09,314 --> 00:03:13,066
their applications, how those applications are relate to each other from

46
00:03:13,090 --> 00:03:17,208
the industrial sector perspective and where the

47
00:03:17,296 --> 00:03:21,120
funding and things are coming in a way just to understand the

48
00:03:21,192 --> 00:03:25,404
market conditions. And the fourth use case we have is to

49
00:03:25,984 --> 00:03:29,768
support a kind of ecosystem where we can say that

50
00:03:29,896 --> 00:03:33,592
in which particular subcategory under the industry codes,

51
00:03:33,768 --> 00:03:37,312
the more and more money is getting funded, or more and

52
00:03:37,328 --> 00:03:40,704
more application which is coming in the market. So from all

53
00:03:40,744 --> 00:03:44,400
those aspects, we just try to identify and

54
00:03:44,432 --> 00:03:48,310
we try to analyze all the applications that we are getting. And for

55
00:03:48,342 --> 00:03:51,670
that purposes, we are trying to build a system which

56
00:03:51,702 --> 00:03:55,470
can help us resolve all of this kind of problem and

57
00:03:55,542 --> 00:03:59,238
give us a concrete solution around it in a particular way. So let

58
00:03:59,246 --> 00:04:02,630
me just walk you through the journey where we have solved some of these use

59
00:04:02,662 --> 00:04:06,558
cases, and some of the use cases are still undergoing appropriation.

60
00:04:06,686 --> 00:04:10,294
So I'll be just walking you through that journey in that particular sense. The first

61
00:04:10,334 --> 00:04:13,742
use case we have is to identify the entity documents.

62
00:04:13,878 --> 00:04:16,886
We generally have lots of textual information in our documents.

63
00:04:16,950 --> 00:04:20,436
What is the purpose of the fundraising, how it is going

64
00:04:20,460 --> 00:04:23,980
to help them, then, what they have written now, what kind of work they are

65
00:04:24,012 --> 00:04:27,364
building, what kind of partnerships they have, what are the people,

66
00:04:27,444 --> 00:04:30,556
what are the different peoples and entity that is involved in those things.

67
00:04:30,700 --> 00:04:34,228
So we get generally lots of actual information in the form of text

68
00:04:34,276 --> 00:04:37,588
and documentation from them on those kind of things.

69
00:04:37,636 --> 00:04:40,900
What we try to identify is that what is the kind of entities that are

70
00:04:40,932 --> 00:04:44,620
involving over here in terms of what is the country from which the funding is

71
00:04:44,652 --> 00:04:48,318
asked, what are the people who are asking for funding and

72
00:04:48,406 --> 00:04:52,118
some more textual information that we just want to identify from those documents

73
00:04:52,246 --> 00:04:55,638
is to ensure that, that we are working on the applications.

74
00:04:55,766 --> 00:04:59,174
As for the government guideline and no complacency

75
00:04:59,254 --> 00:05:03,014
issues and things are happening over there in that case. So in those particular cases,

76
00:05:03,094 --> 00:05:06,742
one of the things, what we have done is whatever documentation and text we

77
00:05:06,758 --> 00:05:09,382
are getting. Like this is one of the example of the text which I have

78
00:05:09,398 --> 00:05:12,960
just randomly from the Internet out of one of the news article.

79
00:05:13,112 --> 00:05:16,552
Well, they're specifically mentioning something about Dandy Murray and

80
00:05:16,608 --> 00:05:19,816
how the things are going on tennis side of the things. I just randomly took

81
00:05:19,840 --> 00:05:22,792
an extract out of it. And on this particular extract,

82
00:05:22,848 --> 00:05:26,528
if I want to identify that, what are the entities that has been one

83
00:05:26,576 --> 00:05:29,776
over here in terms of a people, country,

84
00:05:29,960 --> 00:05:33,320
date and other factors, what I can do is

85
00:05:33,352 --> 00:05:36,616
I can just use some of the available

86
00:05:36,720 --> 00:05:40,174
NLP libraries to identify those kind of information.

87
00:05:40,514 --> 00:05:43,746
One of the library that support identification of all

88
00:05:43,770 --> 00:05:47,330
this critical details with a very minimal usage of coding,

89
00:05:47,402 --> 00:05:50,650
is a spacy library. And in the Spacy library.

90
00:05:50,722 --> 00:05:54,458
What we can do is we can just take a spacy library. We can just

91
00:05:54,506 --> 00:05:57,794
load a model, whatever we like to use. Spacy library provide

92
00:05:57,874 --> 00:06:01,162
multiple models like here. In this example, I'm just showing you

93
00:06:01,178 --> 00:06:04,390
the usage of encore web large language model.

94
00:06:04,522 --> 00:06:07,806
You can use any other language model as well. Whatever works

95
00:06:07,830 --> 00:06:11,702
for you. There are specific language model that has been built to analyze the

96
00:06:11,838 --> 00:06:15,126
news related articles, web related details

97
00:06:15,150 --> 00:06:18,310
and things like that. So you can just choose what kind of

98
00:06:18,382 --> 00:06:21,710
LL model is working best for you and you can just basically load that model

99
00:06:21,742 --> 00:06:25,262
into the spacy. And after you just load that model, what you

100
00:06:25,278 --> 00:06:29,270
can do is you can very easily analyze the textual information

101
00:06:29,382 --> 00:06:33,328
using this particular library. And this library is going to provide your details

102
00:06:33,376 --> 00:06:37,288
that what is the kind of entities and entry types are involved

103
00:06:37,336 --> 00:06:41,552
in that particular document based on the supported entity types.

104
00:06:41,688 --> 00:06:44,888
So if I just go over here and run this particular code

105
00:06:44,936 --> 00:06:48,320
in the article that I have just shown you earlier,

106
00:06:48,512 --> 00:06:51,984
what it's going to do is it is going to provide me an output

107
00:06:52,104 --> 00:06:55,760
which is going to look something like this. Where this is going to shame me

108
00:06:55,792 --> 00:06:59,496
that in this particular article, these are the only different persons that

109
00:06:59,520 --> 00:07:03,194
are involved. These are the geopolitical locations that have been

110
00:07:03,274 --> 00:07:06,402
identified. In this particular article, it has identified the location

111
00:07:06,458 --> 00:07:09,962
as Dubai over here, as you can see event it

112
00:07:09,978 --> 00:07:13,602
is able to unidentify. For example, the Qatar open is one

113
00:07:13,618 --> 00:07:16,922
of the event which has identified.

114
00:07:17,098 --> 00:07:20,962
It is also providing the detail about the various dates, related details

115
00:07:21,018 --> 00:07:24,266
like whether it's a particular day or something

116
00:07:24,330 --> 00:07:28,236
related to d. Those kind of detail it is providing. Also it is able

117
00:07:28,260 --> 00:07:31,596
to identify some of the cardinals or the numerical information that we

118
00:07:31,620 --> 00:07:35,172
have within it, like two or six or any other numbers and

119
00:07:35,188 --> 00:07:38,564
things like that. Whatever is designed over here. So those kind of information

120
00:07:38,644 --> 00:07:42,884
are something which we can very easily identify. And once we identify, this information

121
00:07:42,964 --> 00:07:46,972
can be stored for acquiring purposes. Just to check whether a

122
00:07:46,988 --> 00:07:50,756
particular kind of entities or person or organization names or something

123
00:07:50,820 --> 00:07:54,514
like that is involved in a particular application or not.

124
00:07:54,684 --> 00:07:58,478
Spacy, inbuilt support all these kind of different entity types like

125
00:07:58,526 --> 00:08:02,174
person, organizations, geopolitical locations,

126
00:08:02,334 --> 00:08:05,846
products, law, date, time, etcetera. You would

127
00:08:05,870 --> 00:08:09,742
be able to differentiate whatever textual information you have within

128
00:08:09,798 --> 00:08:13,606
this particular categories. And based on that, basically you

129
00:08:13,670 --> 00:08:17,030
would be able to store this data and can be used, and further

130
00:08:17,102 --> 00:08:20,286
for querying purposes to perform some of the

131
00:08:20,310 --> 00:08:23,646
compliance related checks and things like that. This is like the

132
00:08:23,710 --> 00:08:27,246
way in which we have started the thing using innovate and

133
00:08:27,310 --> 00:08:31,198
we are just progressing further on it to use the information in this particular map.

134
00:08:31,326 --> 00:08:34,774
Now, the next use case, what we have is

135
00:08:34,814 --> 00:08:38,334
to identify the similar documents. As I mentioned in the scenario,

136
00:08:38,374 --> 00:08:41,998
what happens is that the application gets submitted and that application

137
00:08:42,046 --> 00:08:45,478
generally go through a cycle where the application get reviewed and

138
00:08:45,606 --> 00:08:49,494
reviewed by the multiple subject matter expert, depending on in which field

139
00:08:49,574 --> 00:08:53,416
people are looking for funding for and things like that. Basically analyze

140
00:08:53,440 --> 00:08:56,696
in that particular way and based on that general decision is

141
00:08:56,720 --> 00:08:59,968
taken whether to give a funding or not and things like that.

142
00:09:00,136 --> 00:09:04,000
What the general scenario, what we see is that people submit

143
00:09:04,032 --> 00:09:07,792
the applications, if the application got rejected, they just go

144
00:09:07,848 --> 00:09:11,000
and make a wordy changes over here and there.

145
00:09:11,072 --> 00:09:14,520
They change some words, they make a paragraph over here and there

146
00:09:14,672 --> 00:09:17,976
and add some more additional details over here and then, and then resubmit

147
00:09:18,000 --> 00:09:21,188
the applications. So what we basically try to identify over

148
00:09:21,236 --> 00:09:24,684
here is that how two applications are similar to each other,

149
00:09:24,844 --> 00:09:28,596
and if those applications are similar, or if those applications are

150
00:09:28,740 --> 00:09:32,348
submitted across the multiple categories or things like that,

151
00:09:32,476 --> 00:09:36,652
then we just want to identify those kind of applications. So we basically

152
00:09:36,748 --> 00:09:40,236
arrange and manage them properly. So in order to identify again

153
00:09:40,300 --> 00:09:43,956
those kind of textual informations and the details, we basically

154
00:09:44,020 --> 00:09:47,876
use a two kind of methodology. The first methodology is to use

155
00:09:48,060 --> 00:09:51,220
inbuilt functionality of a specific kind of library,

156
00:09:51,372 --> 00:09:54,716
where, you know, we are again going to use some kind of language model.

157
00:09:54,780 --> 00:09:58,452
And in that language model, when we are going to provide a particular

158
00:09:58,628 --> 00:10:02,244
document as an input on those particular documents, it basically

159
00:10:02,324 --> 00:10:05,956
process the documentation, apply the inbuilt algorithm on

160
00:10:05,980 --> 00:10:09,828
it, which is primarily towards the TF IDF kind of algorithm

161
00:10:09,876 --> 00:10:13,456
based scenarios. And then possibly application or vectorization on

162
00:10:13,480 --> 00:10:17,376
it. And based on that as an output, provide us a detail

163
00:10:17,560 --> 00:10:21,936
that how this two documents or this row information which

164
00:10:21,960 --> 00:10:25,312
is mentioned on those test documents are similar

165
00:10:25,368 --> 00:10:29,376
to each other. And if that particular match has

166
00:10:29,520 --> 00:10:33,144
crossed a certain threshold, then we just flag that these

167
00:10:33,184 --> 00:10:36,480
two applications can dissimilar across categories.

168
00:10:36,672 --> 00:10:40,538
Or if, let's say in the future, an application is rejected

169
00:10:40,696 --> 00:10:43,406
which has a similarity to the current application,

170
00:10:43,590 --> 00:10:46,846
then we basically process it in a different way to go further,

171
00:10:46,910 --> 00:10:50,078
deep dive on it. That whether this application is again

172
00:10:50,126 --> 00:10:53,310
submitted from the same source with some changes, or it is a new

173
00:10:53,342 --> 00:10:56,750
application altogether. So those kind of use cases which

174
00:10:56,782 --> 00:11:00,822
we can just solve with this kind of functionality, and as

175
00:11:00,838 --> 00:11:04,478
a part of a specific library, this particular functionality comes in build.

176
00:11:04,606 --> 00:11:08,622
So without writing a very minimal amount of code, you basically solve

177
00:11:08,798 --> 00:11:12,630
this particular problem and identify a solution in a particular way.

178
00:11:12,742 --> 00:11:16,294
So like here in this example, you can see that the first sentence is

179
00:11:16,334 --> 00:11:19,622
I like salty fries and hamburgers and the second

180
00:11:19,678 --> 00:11:22,886
one is fast food tastes very good.

181
00:11:22,990 --> 00:11:25,894
So in this particular way, it basically take those words,

182
00:11:25,974 --> 00:11:29,254
apply the laminizations and the localization

183
00:11:29,334 --> 00:11:33,302
related techniques on it to identify word

184
00:11:33,358 --> 00:11:36,972
like how they are similar to each other and bring the forms and

185
00:11:37,068 --> 00:11:40,132
bring the words in its original format and then it will

186
00:11:40,148 --> 00:11:43,652
just calculate the similarity score on it. So based on that you can just see

187
00:11:43,668 --> 00:11:47,780
the similarity scores that it has generated and the score is something that

188
00:11:47,812 --> 00:11:51,252
can be used further to identify the things

189
00:11:51,348 --> 00:11:55,004
at a particular level. Generally what happen is that

190
00:11:55,044 --> 00:11:58,020
when we go and we apply any kind of this kind of processes,

191
00:11:58,092 --> 00:12:01,492
if you are applying with a spacy libraries or things like that,

192
00:12:01,628 --> 00:12:05,448
it is good to apply directly on the raw data to a certain extent,

193
00:12:05,596 --> 00:12:09,000
because to a certain way specific library use some of the internal

194
00:12:09,032 --> 00:12:12,872
algorithms to implement the tokenization and the lamentations

195
00:12:13,008 --> 00:12:16,696
and the laminization kind of functionality before applying the

196
00:12:16,720 --> 00:12:19,920
respective algorithms. So the things can work

197
00:12:19,952 --> 00:12:23,512
out very smooth over there. But in general use cases, whenever,

198
00:12:23,568 --> 00:12:27,064
if we are going for a more further use cases where let's say we trying

199
00:12:27,104 --> 00:12:30,640
to implement some advanced logic over there to identify the

200
00:12:30,672 --> 00:12:35,180
text documentation based out of the cosine transformation related algorithms

201
00:12:35,252 --> 00:12:38,468
and the vectorization techniques like that, in those cases,

202
00:12:38,556 --> 00:12:42,060
the initial thing, what we generally do is we pre process

203
00:12:42,132 --> 00:12:45,668
the text information and that text information. We basically

204
00:12:45,756 --> 00:12:49,308
apply some kind of data cleansing in terms of making

205
00:12:49,356 --> 00:12:52,740
ensure that that everything is in lowercase. We stop the

206
00:12:52,772 --> 00:12:56,884
punctuations and the generally used words. We also apply the

207
00:12:56,924 --> 00:13:01,028
tokenizations and the laminarization. So laminization, what it generally do

208
00:13:01,076 --> 00:13:05,192
is it basically bring the words within a sentence to its

209
00:13:05,288 --> 00:13:08,632
root. So that when we are comparing our words and things

210
00:13:08,688 --> 00:13:12,032
like that particular command become on the same level playing

211
00:13:12,088 --> 00:13:15,784
field. And further, that if you are applying any kind of algorithm,

212
00:13:15,824 --> 00:13:18,684
like for example, let's say we are applying a DfIDf,

213
00:13:19,304 --> 00:13:22,880
some kind of n gram texting or something like that, then those

214
00:13:22,912 --> 00:13:26,160
kind of calculations become very effective over there to

215
00:13:26,192 --> 00:13:29,816
calculate the score and the further usage. There are some of the techniques which

216
00:13:29,840 --> 00:13:33,072
we can be used and for that, like NLTK is a general

217
00:13:33,128 --> 00:13:36,480
library which provides lots of functionality to implement those kind

218
00:13:36,512 --> 00:13:40,440
of use cases with a very minimal amount of coding and things like that.

219
00:13:40,592 --> 00:13:44,000
Now once we basically clean the data, after that we

220
00:13:44,032 --> 00:13:48,232
go to the other use cases, where in this particular use case, what generally happen

221
00:13:48,288 --> 00:13:52,104
is that at the next level what we try to identify is

222
00:13:52,224 --> 00:13:56,120
that whatever applications and the details and the textual information we are

223
00:13:56,152 --> 00:14:00,088
getting, how that particular information is

224
00:14:00,136 --> 00:14:03,224
segregated across the different industry sectors

225
00:14:03,264 --> 00:14:06,728
and things like that. So we understand that from which

226
00:14:06,776 --> 00:14:10,080
particular sectors more and more funding requirements are

227
00:14:10,112 --> 00:14:13,496
coming, or what kind of growth we are seeing. So a,

228
00:14:13,520 --> 00:14:17,592
we can understand the industry trend and b, we can basically manage

229
00:14:17,648 --> 00:14:21,392
our capacity to access those applications and things in a

230
00:14:21,408 --> 00:14:25,224
particular way. So in order to perform those kind of things,

231
00:14:25,524 --> 00:14:29,172
we basically manage, or we are building a

232
00:14:29,188 --> 00:14:32,144
kind of techniques using a clustering algorithms,

233
00:14:32,524 --> 00:14:35,988
which basically help us identify that whatever textual

234
00:14:36,036 --> 00:14:39,916
information we are getting, that actual informations belong to

235
00:14:39,980 --> 00:14:43,644
which particular clusters, in order to build and take

236
00:14:43,684 --> 00:14:47,412
this particular count, as I mentioned earlier, basically first

237
00:14:47,468 --> 00:14:50,814
go and we basically first clean the data. After doing this

238
00:14:50,854 --> 00:14:54,534
data, we basically apply some kind of actorizations on it.

239
00:14:54,654 --> 00:14:59,030
And after we do this particularizations, we basically go for algorithms

240
00:14:59,182 --> 00:15:02,566
to cluster them in a way, to see, you know, what kind of

241
00:15:02,590 --> 00:15:05,974
clustering is working best for us. We eventually started

242
00:15:06,094 --> 00:15:09,742
with the k means clustering. Then the k means clustering.

243
00:15:09,878 --> 00:15:13,694
This is one of the output which is there on the with some sample data,

244
00:15:13,854 --> 00:15:17,350
where we can see very clearly that with the k means clustering we are able

245
00:15:17,382 --> 00:15:20,766
to basically that data and you are able to

246
00:15:20,790 --> 00:15:23,942
basically tell that what are the kind of clusters and things,

247
00:15:23,998 --> 00:15:26,910
those applications belong in a particular way.

248
00:15:27,062 --> 00:15:30,422
And based on that, basically that can be used further,

249
00:15:30,558 --> 00:15:34,182
and the segmentation can just help us to understand the application in a proper

250
00:15:34,238 --> 00:15:37,830
way. The other things, what happen is, or why we need this

251
00:15:37,862 --> 00:15:41,398
particular thing is because there are chances that a company is working

252
00:15:41,446 --> 00:15:45,126
in a sector e, but when they basically submit the applications

253
00:15:45,190 --> 00:15:49,022
and the details whose application may belong to sector b.

254
00:15:49,198 --> 00:15:52,718
So those kind of things are very general in nature, because companies

255
00:15:52,806 --> 00:15:56,366
are doing generally an innovation either in their field or in some other fields

256
00:15:56,390 --> 00:15:59,822
as well. So basically try to capture those things. And we also try to

257
00:15:59,838 --> 00:16:02,886
understand that what are the kind of overlap we

258
00:16:02,910 --> 00:16:06,302
are seeing between the industries in terms of innovation and

259
00:16:06,318 --> 00:16:10,198
the kind of things they are doing. So this kind of mustering techniques basically help

260
00:16:10,246 --> 00:16:14,046
us identify those kind of things and provide answer more on that side.

261
00:16:14,190 --> 00:16:17,462
So here, basically, Keymans is one of the clustering techniques that we

262
00:16:17,478 --> 00:16:21,270
have used. And in that way, basically it has worked very well out

263
00:16:21,302 --> 00:16:24,806
of us to segregate our data in a certain way.

264
00:16:24,990 --> 00:16:28,182
Other thing, what we have tried out is a fuzzy seaming clustering,

265
00:16:28,278 --> 00:16:32,214
which has comparatively generated a better output for our data sets

266
00:16:32,334 --> 00:16:36,286
because of the characteristic of the data and how the words in

267
00:16:36,310 --> 00:16:39,630
the textual information is connected to each other

268
00:16:39,702 --> 00:16:43,232
in the vector arrays. So based on those particular inputs, we were

269
00:16:43,248 --> 00:16:46,424
able to basically generate a clusters and the things out of it.

270
00:16:46,544 --> 00:16:50,672
To understand those particular things in a more appropriate manner.

271
00:16:50,848 --> 00:16:54,440
Now, based out of this particular information, we are now

272
00:16:54,472 --> 00:16:58,120
able to cognitively basically categorize the data. And we can just say that,

273
00:16:58,152 --> 00:17:01,928
yeah, these are the kind of industry categories and the things that

274
00:17:02,016 --> 00:17:05,632
these applications belong to, and they can be used further on

275
00:17:05,648 --> 00:17:09,704
that side. All of these things are implemented generally using a Python library.

276
00:17:09,784 --> 00:17:13,760
With that, basically we can just build the pipelines on the top of the AWS

277
00:17:13,792 --> 00:17:18,072
using the sagemaker notebooks or something like that. And in that way, basically the

278
00:17:18,088 --> 00:17:21,528
spill dynamics can be solved in a particular manner along

279
00:17:21,576 --> 00:17:25,312
with this thing. The other thing, what also we want to identify is that

280
00:17:25,368 --> 00:17:29,164
whatever things we are getting and whatever things are getting in our cluster,

281
00:17:29,864 --> 00:17:33,896
those things are similar to each other. And how those things are in

282
00:17:33,920 --> 00:17:37,648
general helping us out to identify the things in a particular

283
00:17:37,736 --> 00:17:42,066
way, in order to identify those kind of details and

284
00:17:42,130 --> 00:17:45,346
some of the scenarios around that. What we generally do is that

285
00:17:45,410 --> 00:17:49,114
on, after doing a clustering of those applications, we also

286
00:17:49,154 --> 00:17:53,050
basically go and generate diagrams and triagrams on the top of it

287
00:17:53,202 --> 00:17:56,522
to identify the frequently used keywords and the things

288
00:17:56,618 --> 00:17:59,650
that are present on those particular clusters.

289
00:17:59,842 --> 00:18:03,266
And using the libraries and

290
00:18:03,290 --> 00:18:07,066
the techniques around it, we are basically able to implement those kind

291
00:18:07,090 --> 00:18:10,656
of details and identify what are the common or keywords

292
00:18:10,680 --> 00:18:14,480
and things we are seeing. And there are certain set of keywords which

293
00:18:14,512 --> 00:18:18,216
we basically take out of it, just to understand that these are the

294
00:18:18,240 --> 00:18:21,992
highest highly user keywords in this applications and the things like that.

295
00:18:22,128 --> 00:18:25,664
So this we can just implement using this diagram and diagram techniques.

296
00:18:25,704 --> 00:18:29,376
And we can also around that get us some kind of informations

297
00:18:29,440 --> 00:18:32,408
on dummy data, like the number you can just see over here for some of

298
00:18:32,416 --> 00:18:35,872
the dummy data that, you know, how those particular eggs are being used.

299
00:18:35,968 --> 00:18:39,832
What are the kind of instances we are seeing over here? Generally the term frequencies

300
00:18:39,888 --> 00:18:43,472
and the inverse from frequencies kind of scenarios in this manner. And we basically

301
00:18:43,528 --> 00:18:47,184
just see that how those terms are being used across application to

302
00:18:47,344 --> 00:18:50,864
understand the application similarity, dependency and the

303
00:18:50,944 --> 00:18:54,360
sector's influence on each other. So in that way, basically it

304
00:18:54,392 --> 00:18:58,640
help us get those information and process further on

305
00:18:58,672 --> 00:19:02,360
those particular sites. So once you basically get this particular

306
00:19:02,432 --> 00:19:06,264
information at the next level, this is something which is in progress.

307
00:19:06,344 --> 00:19:09,646
We are still working on it, and this is something like what we're trying to

308
00:19:09,670 --> 00:19:12,926
identify. Is that, okay, where we have identified what are the

309
00:19:12,950 --> 00:19:16,394
clusters we are having, we have identified like what is the kind of

310
00:19:16,694 --> 00:19:19,798
topics we are having in that particular clusters.

311
00:19:19,966 --> 00:19:23,406
But the missing part is that what is the kind of hierarchy

312
00:19:23,470 --> 00:19:27,182
that exists between those particular topics or cluster

313
00:19:27,278 --> 00:19:31,014
that has been present over there in that particular way. So for that

314
00:19:31,054 --> 00:19:34,630
purpose, we are just experimenting with some of the algorithms and the techniques

315
00:19:34,662 --> 00:19:38,336
around it to identify those things. Like one of the rhythm which we have

316
00:19:38,360 --> 00:19:41,864
used right now is related to the accommodative clustering techniques.

317
00:19:42,024 --> 00:19:45,400
So with this technique we are just trying to understand that how these

318
00:19:45,472 --> 00:19:48,592
topics and things are related to each other and whether we are able to build

319
00:19:48,648 --> 00:19:52,160
some kind of a graph around graph around those

320
00:19:52,192 --> 00:19:55,840
things or not, where the root node, maybe we can just see a cluster

321
00:19:55,912 --> 00:19:59,232
as a category. At a sub level we can see this

322
00:19:59,288 --> 00:20:02,852
different hierarchy, and at the end or at

323
00:20:02,868 --> 00:20:06,556
the root level we can just see the different tags or topics that has

324
00:20:06,580 --> 00:20:10,052
been identified within that cluster. So we can just use it in a

325
00:20:10,068 --> 00:20:13,988
particular manner. So this again, your progress. And mostly

326
00:20:14,036 --> 00:20:17,860
once we crack it in a future video related to machine learning or something like

327
00:20:17,892 --> 00:20:21,596
that, I will be happy to share those kind of intenses in terms of

328
00:20:21,620 --> 00:20:24,676
what is work out and what is thought. Right now, the results which we are

329
00:20:24,700 --> 00:20:28,216
seeing on this particular technique using algorithm clustering is

330
00:20:28,240 --> 00:20:31,584
not that much satisfactory. We are trying to improve it with other things and things

331
00:20:31,624 --> 00:20:35,272
like that. Yeah, so that's how we are basically trying to solve

332
00:20:35,288 --> 00:20:39,208
the use cases around the machine learning, using a topic modeling.

333
00:20:39,256 --> 00:20:42,832
And in order to build this particular things, we are generally using most of

334
00:20:42,848 --> 00:20:46,884
the open source Python libraries. And in order to perform this particular

335
00:20:47,944 --> 00:20:52,104
techniques or clustering with the diagrams and the diagrams technique,

336
00:20:52,224 --> 00:20:55,480
we also got an exposure to a different libraries like keyboard or

337
00:20:55,512 --> 00:20:59,242
something. And those libraries are generating a reasonable amount of results.

338
00:20:59,418 --> 00:21:03,018
But one of the things, what we identified is that rather than going for keyword

339
00:21:03,066 --> 00:21:06,786
libraries and other logging page and respective library,

340
00:21:06,930 --> 00:21:10,210
we in our experience, we basically, for a particular domain,

341
00:21:10,322 --> 00:21:13,874
we got a more positive results by using the basic

342
00:21:13,914 --> 00:21:17,978
python techniques around the malgram and grams and the

343
00:21:18,106 --> 00:21:21,370
frequency idea related techniques and things like

344
00:21:21,402 --> 00:21:24,676
that. So yeah, that's all about this particular presentation.

345
00:21:24,780 --> 00:21:28,180
And my goal was just to provide you in a detail in terms

346
00:21:28,212 --> 00:21:31,452
of what other kind of use cases exist and what are

347
00:21:31,468 --> 00:21:34,596
the kind of approach and things generally work out in the industry in that

348
00:21:34,620 --> 00:21:38,180
particular manner. So yeah, that's all about, that's all from me in

349
00:21:38,252 --> 00:21:41,708
today's presentation. If you guys want to learn more about it or

350
00:21:41,756 --> 00:21:45,452
just want to be in touch, please send my contact detail. Feel free

351
00:21:45,468 --> 00:21:48,732
to connect with me on the LinkedIn, or feel free to connect with me

352
00:21:48,748 --> 00:21:52,682
on the email to discuss any of the possible challenges or things that you

353
00:21:52,698 --> 00:21:56,554
would like to discuss around the use of topic

354
00:21:56,594 --> 00:21:59,770
modeling and the analytic data around us, and I would be

355
00:21:59,802 --> 00:22:03,562
happy to connect and share more details over there in

356
00:22:03,578 --> 00:22:06,962
that particular way. So thank you. Thank you for your time, and thank you

357
00:22:06,978 --> 00:22:09,674
for listening this particular session. Thank you.

