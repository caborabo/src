1
00:00:24,570 --> 00:00:28,790
Hello everyone. One welcome to comfort two cloud native today

2
00:00:28,860 --> 00:00:31,922
we are happy to talk about Chidori AI and ML

3
00:00:31,986 --> 00:00:35,746
cluster management developed by Incorta. Let me introduce

4
00:00:35,778 --> 00:00:39,538
myself first. I am Ahmed Gaber but you can call me Biga.

5
00:00:39,634 --> 00:00:42,982
I am a cloud engineering manager at Incorta and

6
00:00:43,036 --> 00:00:47,430
also I have today with me Nadine Khaled, cloud engineer at Incorta.

7
00:00:47,850 --> 00:00:52,430
In our agenda today we will talk about how spark operates in Kubernetes

8
00:00:53,010 --> 00:00:57,038
and did five summit challenges involve it and explain

9
00:00:57,124 --> 00:01:00,750
how Chidori can address these challenges. And also

10
00:01:00,820 --> 00:01:03,140
we prepare a good demo for you.

11
00:01:04,870 --> 00:01:08,050
Let's dive into how spark work on Kubernetes.

12
00:01:08,710 --> 00:01:12,546
As you see in this diagram, the client will start to submit the

13
00:01:12,568 --> 00:01:16,066
spark job into Kubernetes to run as a spark driver in

14
00:01:16,088 --> 00:01:19,794
Kubernetes we have two modes. First one is a client mode

15
00:01:19,842 --> 00:01:23,766
which means the driver will still running in

16
00:01:23,788 --> 00:01:27,030
the client side. The second mode is

17
00:01:27,100 --> 00:01:30,538
cluster mode which means the driver will be running

18
00:01:30,624 --> 00:01:34,550
as a BoD inside Kubernetes. Once the driver

19
00:01:34,630 --> 00:01:37,786
started, it requests from Kubernetes to start the

20
00:01:37,808 --> 00:01:41,114
executor bots. So the Kubernetes scheduler will

21
00:01:41,152 --> 00:01:44,730
start to allocate the executor costs inside Kubernetes.

22
00:01:45,230 --> 00:01:48,986
After this exactor costs created, the driver

23
00:01:49,018 --> 00:01:53,078
will be get notified to start to schedule jobs in these exactors.

24
00:01:53,274 --> 00:01:57,474
So as you see here, Spark will get benefit from

25
00:01:57,592 --> 00:02:01,620
the Kubernetes scalability features like the

26
00:02:02,470 --> 00:02:05,602
cluster horizontal scaling for the node itself.

27
00:02:05,736 --> 00:02:09,710
So if you have costs to be allocated

28
00:02:09,870 --> 00:02:13,878
inside the nodes and there isn't enough equity, cluster will be

29
00:02:13,964 --> 00:02:18,646
started to scale up to add new nodes to

30
00:02:18,668 --> 00:02:23,174
be more flexible with your job. And also you

31
00:02:23,212 --> 00:02:26,630
have like a resource management to ensure that

32
00:02:26,700 --> 00:02:30,170
the driver and executor running within your capacity.

33
00:02:30,290 --> 00:02:34,254
We figure out some of good insights on that

34
00:02:34,292 --> 00:02:37,706
model. First one, as I said, the cluster auto scaling cluster

35
00:02:37,738 --> 00:02:41,134
auto scaling give you the flexibility to get

36
00:02:41,172 --> 00:02:46,366
the better performance with low cost. So you don't have to have

37
00:02:46,388 --> 00:02:49,874
like a node to be up and running almost the

38
00:02:49,912 --> 00:02:53,826
time. Also we find that enable the

39
00:02:53,848 --> 00:02:57,142
dynamic allocation give you a flexibility inside

40
00:02:57,196 --> 00:03:01,400
the application itself to scale in and out. The executor itself

41
00:03:02,970 --> 00:03:06,482
also uses spot nodes with Spark workload

42
00:03:06,626 --> 00:03:08,540
will save a cost a lot.

43
00:03:10,030 --> 00:03:13,306
This will give you a flexibility to get

44
00:03:13,488 --> 00:03:15,900
higher performance with low cost.

45
00:03:16,830 --> 00:03:20,794
Also we notice most of spark bottlenecks

46
00:03:20,842 --> 00:03:24,254
come from the shuffling issues.

47
00:03:24,452 --> 00:03:28,254
So to optimize your spark job

48
00:03:28,372 --> 00:03:33,218
you have to attach the spark job into fast

49
00:03:33,384 --> 00:03:37,006
local SSD based on your cloud vendor

50
00:03:37,118 --> 00:03:41,410
to optimize the spark scratch space. In summary running

51
00:03:41,480 --> 00:03:45,338
Spark on Kubernetes not only optimize resource utilization

52
00:03:45,454 --> 00:03:48,882
and reduce cost, but also enhance the overall

53
00:03:48,946 --> 00:03:51,480
performance for your spark application.

54
00:03:51,930 --> 00:03:55,366
While running Spark on Kubernetes bring us a lot of

55
00:03:55,468 --> 00:03:58,906
benefits like better resource use and the

56
00:03:58,928 --> 00:04:02,950
cost saving. It is not without its challenges.

57
00:04:03,110 --> 00:04:06,620
Let's dive into some of these challenges you might face.

58
00:04:08,270 --> 00:04:11,822
Firstly the Nikode boot which

59
00:04:11,876 --> 00:04:15,902
mean like when

60
00:04:15,956 --> 00:04:20,362
spin spark a job as a cluster mode inside Kubernetes.

61
00:04:20,426 --> 00:04:23,538
As I said before, the driver will start

62
00:04:23,624 --> 00:04:26,770
running as a separate BoD inside Kubernetes.

63
00:04:27,270 --> 00:04:31,246
This BoD is not controlled by any of replica controller

64
00:04:31,358 --> 00:04:35,394
or stateful set or deployment object in Kubernetes and

65
00:04:35,432 --> 00:04:39,762
this creates some kind of availability issue for

66
00:04:39,816 --> 00:04:43,170
this BoD and it would be like act as a single point of failure.

67
00:04:43,250 --> 00:04:47,446
So which means if this driver down for

68
00:04:47,468 --> 00:04:51,146
any something your job will completely fail and

69
00:04:51,248 --> 00:04:55,014
kubernetes will not have any controller to spin this driver

70
00:04:55,062 --> 00:04:58,886
again. The second challenge is around driver

71
00:04:58,918 --> 00:05:03,162
bods distribution across nodes and its implications

72
00:05:03,306 --> 00:05:06,430
for cost. Driver bods are allocated across

73
00:05:06,500 --> 00:05:09,610
node based on resource request and node offenses.

74
00:05:09,690 --> 00:05:13,310
However, as job conclude you may observe

75
00:05:13,390 --> 00:05:17,438
scattering of some bods across the nodes due

76
00:05:17,454 --> 00:05:20,562
to the constraint imposed by the naked BoD issue.

77
00:05:20,696 --> 00:05:25,086
This distribution can prevent the scaling down of

78
00:05:25,128 --> 00:05:27,240
a node to optimize running cost.

79
00:05:28,090 --> 00:05:31,442
Another challenge is around startup time overhead.

80
00:05:31,586 --> 00:05:35,590
This time will come from two factors. First one

81
00:05:35,740 --> 00:05:38,934
is if a new node

82
00:05:38,982 --> 00:05:42,118
is required from a driver to be allocated or executor,

83
00:05:42,214 --> 00:05:45,894
so there is a latency to wait this node

84
00:05:45,942 --> 00:05:49,660
to become available. Also another factor is the

85
00:05:50,370 --> 00:05:54,314
startup time of the BoD itself for the driver. If you're

86
00:05:54,362 --> 00:05:57,534
using heavily bison libraries to start

87
00:05:57,572 --> 00:06:01,674
your job. So you will wait to install these libraries

88
00:06:01,802 --> 00:06:05,454
and configure some configuration

89
00:06:05,502 --> 00:06:08,766
for this job before the bot

90
00:06:08,798 --> 00:06:12,994
become available. So this will impact also the time to

91
00:06:13,032 --> 00:06:16,150
job to be executing once it's submitted.

92
00:06:18,170 --> 00:06:22,086
Another challenge is the Kubernetes scatter itself.

93
00:06:22,268 --> 00:06:25,494
To understand this issue we

94
00:06:25,532 --> 00:06:29,446
must understand how Kubernetes allocate bot to the

95
00:06:29,468 --> 00:06:33,418
node. This scaddler is called Kubescadular is

96
00:06:33,504 --> 00:06:37,206
watch that from the abi server

97
00:06:37,238 --> 00:06:41,082
in Kubernetes once the master get

98
00:06:41,136 --> 00:06:44,542
requested to create a bot so the scheduler will

99
00:06:44,596 --> 00:06:48,458
start looking to available node to be hosted this node

100
00:06:48,554 --> 00:06:52,126
based on the resources constraint defined by the

101
00:06:52,148 --> 00:06:55,630
bot definition itself and also node affinities.

102
00:06:56,530 --> 00:07:00,286
Once the scador find feasible nodes

103
00:07:00,318 --> 00:07:04,002
to costs this bot, it will have like a scoring to

104
00:07:04,056 --> 00:07:06,658
find the best match for this node.

105
00:07:06,834 --> 00:07:11,874
Also, if the scheduler didn't found any feasible

106
00:07:11,922 --> 00:07:15,494
node to costs this BoD, the BoD will

107
00:07:15,612 --> 00:07:19,302
remain unscadbled until the schedule find

108
00:07:19,436 --> 00:07:22,886
best match node for free. So what is missing?

109
00:07:22,918 --> 00:07:26,454
Kubernetes will build for running microservice

110
00:07:26,502 --> 00:07:30,274
with scale out architecture in mind. The default Kubernetes schedule

111
00:07:30,342 --> 00:07:34,346
is not ideal for AI and ML workload lacking

112
00:07:34,378 --> 00:07:37,774
critical high performance scheduling component like

113
00:07:37,972 --> 00:07:41,630
batch scheduling permission and multiple queues for

114
00:07:41,700 --> 00:07:44,782
efficiency. In addition,

115
00:07:44,926 --> 00:07:48,766
Kubernetes is missing gang scheduling for scaling

116
00:07:48,798 --> 00:07:52,782
up burial processing AI workload to multiple

117
00:07:52,846 --> 00:07:54,450
distributed nodes.

118
00:07:57,710 --> 00:08:01,590
Also most of AI and the ML jobs

119
00:08:01,670 --> 00:08:05,222
require array of libraries and the framework including

120
00:08:05,286 --> 00:08:08,090
wheels, eggs, jars and framework.

121
00:08:08,610 --> 00:08:12,686
This diversity require a robust tracking system to ensure everything

122
00:08:12,788 --> 00:08:16,414
within our container image is up to date and function as

123
00:08:16,452 --> 00:08:20,966
expected. Moreover, the size of container images

124
00:08:21,098 --> 00:08:24,514
become critical consideration as we add

125
00:08:24,552 --> 00:08:28,450
more component the images grow larger which again

126
00:08:28,520 --> 00:08:31,922
slow down the deployment time and

127
00:08:31,976 --> 00:08:36,530
impact the efficiency and also adding to that managing compatibility

128
00:08:36,610 --> 00:08:38,550
and upgrade of these versions.

129
00:08:42,010 --> 00:08:45,702
So another challenge to run any spark or ML job

130
00:08:45,756 --> 00:08:49,622
inside Kubernetes is it's related to concept called

131
00:08:49,676 --> 00:08:53,894
twelve G awareness. Mainly scheduler will

132
00:08:53,932 --> 00:08:57,094
allocate the pod based on the resource request

133
00:08:57,222 --> 00:09:00,338
and pod affinity which is defined by the BoD

134
00:09:00,374 --> 00:09:04,394
itself. However, the node state itself is managed

135
00:09:04,442 --> 00:09:08,462
by kubernetes. It's another agent running inside each

136
00:09:08,516 --> 00:09:12,786
node in your cluster to know the state of the

137
00:09:12,808 --> 00:09:16,066
node itself. So for example if this node have

138
00:09:16,088 --> 00:09:20,702
a disk pressure or some kind of throttling

139
00:09:20,766 --> 00:09:25,110
in some resource. So you need to have like awareness

140
00:09:25,610 --> 00:09:28,834
before you allocate this job into node.

141
00:09:28,962 --> 00:09:33,042
Also you must utilize the node affinities and boot affinities

142
00:09:33,106 --> 00:09:36,874
together to get the best match of allocating bods to

143
00:09:36,912 --> 00:09:41,254
nodes with Kubernetes scheduler. Another challenge

144
00:09:41,302 --> 00:09:45,594
is related to integration and monitoring in

145
00:09:45,632 --> 00:09:49,514
Spark. In Kubernetes to monitor job you have to use Spark

146
00:09:49,562 --> 00:09:52,430
Ui master or using Spark history server.

147
00:09:52,930 --> 00:09:56,878
This tool is mainly concerned about the job.

148
00:09:56,964 --> 00:10:01,206
It's job focused and it's concerned only about the tasks

149
00:10:01,258 --> 00:10:04,434
or in the stages and some kind

150
00:10:04,472 --> 00:10:08,014
of resource monitoring of infrastructure. But it's missing

151
00:10:08,062 --> 00:10:11,230
the correlation between the cluster behavior,

152
00:10:11,310 --> 00:10:15,238
the Kubernetes behavior with this job. So you will find

153
00:10:15,324 --> 00:10:19,094
some difficulty to troubleshooting some issues you may

154
00:10:19,132 --> 00:10:23,110
face. Also the integration with third party tools

155
00:10:23,690 --> 00:10:27,786
the current way to submit any job as we see in the first slide to

156
00:10:27,808 --> 00:10:31,770
use the Spark submit command which is a CLI command in

157
00:10:31,840 --> 00:10:35,910
Spark. So it's not friendly to be integrated

158
00:10:35,990 --> 00:10:39,470
with other tools. So after

159
00:10:39,540 --> 00:10:43,070
addressing all of these challenges we

160
00:10:43,220 --> 00:10:47,674
starting to build our beloved solution is chidori.

161
00:10:47,802 --> 00:10:51,534
So we started build chidori with a mindset to solve all

162
00:10:51,572 --> 00:10:55,394
issues that I listed in the previous slides by solving the

163
00:10:55,432 --> 00:10:59,282
naked boot availability issue, provide more stable framework could

164
00:10:59,336 --> 00:11:04,082
run the spark or email jobs inside Kubernetes and

165
00:11:04,136 --> 00:11:07,702
also provide well integrated rest API with third

166
00:11:07,756 --> 00:11:11,366
parties and provide more clear monitoring to

167
00:11:11,388 --> 00:11:15,046
the create different factors to

168
00:11:15,068 --> 00:11:18,730
have a good troubleshooting for resource jobs inside kubernetes.

169
00:11:19,790 --> 00:11:23,114
So in this diagram I will explain the high level

170
00:11:23,152 --> 00:11:27,530
design of chidori. So let us start with chidori server.

171
00:11:27,870 --> 00:11:31,950
So as I said in first slide

172
00:11:32,290 --> 00:11:35,866
of the issues that we have naked

173
00:11:35,898 --> 00:11:40,430
bot availability issues when we run our driver inside Kubernetes.

174
00:11:40,850 --> 00:11:44,514
So we build Shuduri with a concept to

175
00:11:44,552 --> 00:11:48,786
be like a hosting for spark drivers inside

176
00:11:48,888 --> 00:11:53,438
Kubernetes. So Shaduri will costs the spark driver and Shaduri

177
00:11:53,454 --> 00:11:57,026
itself is a Kubernetes deployment so it's

178
00:11:57,058 --> 00:12:00,406
totally managed by Kubernetes to guarantee the high

179
00:12:00,428 --> 00:12:04,306
availability and disability also. So we build API

180
00:12:04,338 --> 00:12:07,814
server that provide multiple APIs to be deal with spark

181
00:12:07,862 --> 00:12:11,642
in kubernetes like create job, delete job listing jobs and

182
00:12:11,696 --> 00:12:16,998
get logs. So this is BI will be integrated

183
00:12:17,094 --> 00:12:20,938
with the spark submit client and also integrated

184
00:12:21,114 --> 00:12:24,942
with any third parties that can be integrated with

185
00:12:24,996 --> 00:12:28,846
Spark on Kubernetes server. So once

186
00:12:28,948 --> 00:12:33,582
we receive a job inside Shaduri it will be queue and

187
00:12:33,636 --> 00:12:37,590
we build the queuing because we want shudderi

188
00:12:37,610 --> 00:12:41,314
to be controlled how much driver can be run at a time.

189
00:12:41,512 --> 00:12:44,914
So the admin can configure the maximum

190
00:12:44,962 --> 00:12:48,790
number of jobs and maximum allowed

191
00:12:49,130 --> 00:12:52,502
memory and CPU to be consumed at a time.

192
00:12:52,636 --> 00:12:55,766
So once the job received in the bias server it

193
00:12:55,788 --> 00:12:59,194
will be stored in our queuing system.

194
00:12:59,312 --> 00:13:02,602
We provide interface for multiple queuing system

195
00:13:02,656 --> 00:13:06,234
like rabbit, meq, cloudbubsub and

196
00:13:06,432 --> 00:13:10,522
Azure. So once the job queued

197
00:13:10,586 --> 00:13:13,998
if there enough capacity to job to be run,

198
00:13:14,084 --> 00:13:18,126
the scheduler will fetch this job from the queue and start

199
00:13:18,308 --> 00:13:22,510
go routine function to run this job and

200
00:13:22,660 --> 00:13:26,340
the core engine will start tracking this job

201
00:13:26,870 --> 00:13:30,274
to manage all the lifecycle of job and all

202
00:13:30,312 --> 00:13:34,034
of this metadata stored in our backend store for

203
00:13:34,072 --> 00:13:37,954
monitoring purpose and auditing. Also we

204
00:13:37,992 --> 00:13:41,602
build the story to be like interface with many of Spark

205
00:13:41,666 --> 00:13:46,190
vendor provider like Incorta, Kubernetes and databricks.

206
00:13:46,290 --> 00:13:50,342
So you can use chidori to submit jobs to incorta

207
00:13:50,486 --> 00:13:53,914
or your own cluster in Kubernetes or

208
00:13:53,952 --> 00:13:57,194
your cluster in databricks. Also we build like

209
00:13:57,232 --> 00:14:01,118
a monitoring to monitor the jobs running jobs and get

210
00:14:01,204 --> 00:14:05,114
full monitoring capabilities to create different factors

211
00:14:05,242 --> 00:14:09,306
while you troubleshooting your jobs.

212
00:14:09,498 --> 00:14:13,442
Also we have a connect layer that

213
00:14:13,496 --> 00:14:16,770
provides spark connect interface with other

214
00:14:16,840 --> 00:14:20,434
parties in the client side. Also we will provide our

215
00:14:20,472 --> 00:14:23,790
Spark summit chidori version that easily integrated

216
00:14:23,870 --> 00:14:27,794
with our Chidori server. So you can

217
00:14:27,832 --> 00:14:31,254
consider Chidori is a full AI and

218
00:14:31,292 --> 00:14:35,014
ML cluster manager in Kubernetes that provide

219
00:14:35,212 --> 00:14:39,002
a full integration with Kubernetes and also

220
00:14:39,136 --> 00:14:42,374
different tools in ML ecosystem

221
00:14:42,422 --> 00:14:46,042
like MLflow and Kieflow. So we can focus on your

222
00:14:46,096 --> 00:14:49,418
business logic by developing your model, training,

223
00:14:49,584 --> 00:14:53,134
deployment and serving. And Shduri will take

224
00:14:53,172 --> 00:14:56,862
care about infrastructure management. So now

225
00:14:56,916 --> 00:15:00,046
is the demo part hi everyone, as mentioned by

226
00:15:00,068 --> 00:15:03,946
Bega, this is Nadine Khaled from Incorta Cloud team and today I'm going to demonstrate

227
00:15:03,978 --> 00:15:07,602
with you how to install Shizuri on your environment and put it into

228
00:15:07,656 --> 00:15:10,786
action. So as you can

229
00:15:10,808 --> 00:15:14,846
see here, we provide hand charts for easy installation into your namespace.

230
00:15:14,958 --> 00:15:18,406
So once chidori is installed you can verify if all

231
00:15:18,428 --> 00:15:21,634
the infrastructure components are created.

232
00:15:21,762 --> 00:15:25,314
And by infrastructure components here I mean spark

233
00:15:25,362 --> 00:15:29,158
server deployment which is in our case it's chidori.

234
00:15:29,254 --> 00:15:32,858
And we also have rabbit MQ sit for set which is responsible for

235
00:15:32,944 --> 00:15:36,534
queuing the jobs, and Shidoicore deployment

236
00:15:36,582 --> 00:15:40,270
which is responsible for monitoring the jobs that you had ran before.

237
00:15:40,420 --> 00:15:43,774
And also, as you can see here, we have created all the necessary services

238
00:15:43,892 --> 00:15:47,054
that are responsible for making the deployments communicate with each

239
00:15:47,092 --> 00:15:47,680
other.

240
00:15:49,970 --> 00:15:53,866
Also, Chidori simplifies the management of Python

241
00:15:53,898 --> 00:15:57,538
packages, so you can install the python packets that you want

242
00:15:57,704 --> 00:16:02,274
to install for your job execution and

243
00:16:02,312 --> 00:16:05,686
removing the hassle of installing it manually. As you can see

244
00:16:05,708 --> 00:16:09,410
here, I have installed Python package Tensorflow and all these packages

245
00:16:09,490 --> 00:16:13,462
are already pre installed in Chidori so you don't need to install them

246
00:16:13,516 --> 00:16:17,326
again. Also through Chidori

247
00:16:17,378 --> 00:16:21,398
you can submit your spark jobs through any spark master,

248
00:16:21,494 --> 00:16:25,366
whether it's kubernetes or data Brext or Databrock or Azure

249
00:16:25,398 --> 00:16:29,078
HD insight. Also, Chidori offers the flexibility

250
00:16:29,174 --> 00:16:32,926
in specifying the driver memory that you want. You can choose the size

251
00:16:33,108 --> 00:16:35,600
for the driver that you want.

252
00:16:36,370 --> 00:16:39,546
So let's go back to Chidori setup.

253
00:16:39,738 --> 00:16:43,602
Once you make sure that all the pods are up and running, you can start

254
00:16:43,656 --> 00:16:47,106
submitting your spark jobs through Spark submit and you

255
00:16:47,128 --> 00:16:52,146
can open Chidori monitoring to

256
00:16:52,168 --> 00:16:54,370
see the status of these jobs.

257
00:16:56,310 --> 00:16:59,640
As you can see here, all the jobs I have created before,

258
00:17:00,410 --> 00:17:04,290
I can filter by status, whether it's failed,

259
00:17:04,370 --> 00:17:08,554
whether it's succeeded. I can also filter by

260
00:17:08,672 --> 00:17:12,054
the date the job was created and I can filter

261
00:17:12,102 --> 00:17:16,042
by the schema name and the table name of the job that

262
00:17:16,096 --> 00:17:19,850
was created. Also you can preview

263
00:17:21,070 --> 00:17:24,426
the history of the job that you have created with the same schema

264
00:17:24,458 --> 00:17:28,346
name and table name as you can see here, these are the jobs

265
00:17:28,458 --> 00:17:32,306
that was created with the same schema name and table name and these are the

266
00:17:32,328 --> 00:17:34,980
status and all the information about them.

267
00:17:35,510 --> 00:17:39,634
You can also view this history in

268
00:17:39,672 --> 00:17:42,914
a short view. So you are

269
00:17:42,952 --> 00:17:46,182
going to see how

270
00:17:46,236 --> 00:17:50,086
long these jobs took in order to be

271
00:17:50,108 --> 00:17:53,830
loaded or created through shadowy.

272
00:17:54,250 --> 00:17:57,798
And also you can do some actions on the jobs.

273
00:17:57,894 --> 00:18:01,750
You can download Spark

274
00:18:01,910 --> 00:18:05,802
driver locks and also you can open this job in Spark history

275
00:18:05,856 --> 00:18:09,262
server. This will redirect you to the Spark history

276
00:18:09,316 --> 00:18:14,634
server. Also Chidori provides

277
00:18:14,682 --> 00:18:16,590
a shareable link feature.

278
00:18:18,530 --> 00:18:21,886
So when you want to share with someone the history of the

279
00:18:21,908 --> 00:18:25,106
job or any details or any information about the job,

280
00:18:25,208 --> 00:18:28,050
you can just give him this signed URL.

281
00:18:29,430 --> 00:18:32,958
So you will remove from them the hassle of logging to shadowy

282
00:18:32,974 --> 00:18:37,142
monitoring with credentials. So as you can see here,

283
00:18:37,276 --> 00:18:41,254
you just copy the URL, the shareable link and the

284
00:18:41,452 --> 00:18:44,694
person that you gave him this URL. He will open this

285
00:18:44,732 --> 00:18:48,490
link and he will be able to view the history

286
00:18:48,560 --> 00:18:52,074
of these jobs and

287
00:18:52,112 --> 00:18:55,562
he can display it as I said before in the chart view

288
00:18:55,696 --> 00:18:59,834
and he can display details about each job

289
00:18:59,952 --> 00:19:03,286
was created. So that was Chidori

290
00:19:03,318 --> 00:19:07,098
and I hope you enjoyed the demo. So thank you

291
00:19:07,184 --> 00:19:10,200
to attending our session and have a good day.

