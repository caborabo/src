1
00:00:27,520 --> 00:00:31,012
Let me give you an overview about open telemetry. Here's everything

2
00:00:31,068 --> 00:00:34,716
that I'll be walking you through today. I'll give you a

3
00:00:34,740 --> 00:00:38,292
brief background about open telemetry, the core concepts,

4
00:00:38,348 --> 00:00:41,700
the building blocks and architecture of open telemetry.

5
00:00:41,892 --> 00:00:45,412
We'll quickly dive into the instrumentation part where we'll

6
00:00:45,508 --> 00:00:49,612
see the code and we'll start instrumenting with

7
00:00:49,668 --> 00:00:53,308
traces, metrics and logs for a simple node JS application

8
00:00:53,396 --> 00:00:56,916
using the co op framework. It's a very basic application where

9
00:00:56,940 --> 00:01:00,874
we try to cover all these concepts and try to extract telemetry that

10
00:01:00,914 --> 00:01:04,826
makes sense to us. Lastly, I'll also cover about open element

11
00:01:04,890 --> 00:01:08,594
collector how you can get started, how it's beneficial

12
00:01:08,714 --> 00:01:12,338
and when you should be using that. And then again,

13
00:01:12,426 --> 00:01:16,210
we have heard from developers across the world it works in my machine.

14
00:01:16,362 --> 00:01:20,614
It's an Ops problem, Ops complaining that it's the app problem.

15
00:01:20,914 --> 00:01:24,106
Today I've even heard people complaining that it's my container

16
00:01:24,130 --> 00:01:27,454
is working fine, you're just not deploying my container correctly.

17
00:01:27,984 --> 00:01:31,464
Let's see how open telemetry or observability helps resolve this

18
00:01:31,504 --> 00:01:35,248
conflict in today's world. A quick background about

19
00:01:35,296 --> 00:01:38,912
open telemetry open telemetry today is an incubating

20
00:01:38,968 --> 00:01:42,096
project in CNCF landscape. There has already

21
00:01:42,160 --> 00:01:45,696
been a proposal made to make the project to a

22
00:01:45,720 --> 00:01:49,592
graduate state. It was originally formed in 2019

23
00:01:49,688 --> 00:01:53,724
by a merger of two famous projects, open tracing and mobile census.

24
00:01:54,314 --> 00:01:58,266
Open tracing was developed by Uber to monitor their

25
00:01:58,330 --> 00:02:02,042
terms of microservices and open sensors was developed by

26
00:02:02,098 --> 00:02:06,058
Google for the purpose of monitoring their microservices

27
00:02:06,146 --> 00:02:10,290
and telecommetrics. Some of the core goals of open telemetry

28
00:02:10,362 --> 00:02:13,146
is to provide a set of APIs, libraries,

29
00:02:13,250 --> 00:02:16,370
integrations to connect the telemetry from across

30
00:02:16,442 --> 00:02:18,294
your system and services.

31
00:02:19,174 --> 00:02:22,742
It helps you set the standard from to collect the telemetry from

32
00:02:22,838 --> 00:02:25,274
all your application and infrastructure.

33
00:02:25,894 --> 00:02:29,998
One of the best part about open telemetry is how it provides you

34
00:02:30,086 --> 00:02:33,846
an option to set all this telemetry collector to

35
00:02:33,870 --> 00:02:37,590
a choice of your observability back in, which means you're not logged

36
00:02:37,622 --> 00:02:40,838
into a single vendor or any specific tool.

37
00:02:41,006 --> 00:02:44,334
Regardless of how you instrument your application,

38
00:02:44,414 --> 00:02:48,310
services and infrastructure. With open telemetry, you are free

39
00:02:48,342 --> 00:02:51,454
to choose where you want to store your telemetry in house,

40
00:02:51,534 --> 00:02:55,302
third party, or a combination of both. You'll see in

41
00:02:55,318 --> 00:02:59,234
this in this chart how quickly open telemetry has risen.

42
00:02:59,574 --> 00:03:02,710
You'll see that today. Open telemetry is second

43
00:03:02,782 --> 00:03:06,502
most fastest growing project in CNCF space.

44
00:03:06,678 --> 00:03:10,742
It is right behind kubernetes with the number of contributions

45
00:03:10,878 --> 00:03:14,974
and adoption. This is because there is a strong

46
00:03:15,054 --> 00:03:18,662
interest in modern observability. There was

47
00:03:18,678 --> 00:03:22,838
a report by Gardner in 2022 that

48
00:03:22,966 --> 00:03:26,998
a lot of companies are looking to enhance open standards,

49
00:03:27,126 --> 00:03:31,234
which is what open telemetry, EBTF and Grafana are working towards.

50
00:03:32,054 --> 00:03:34,910
If you want to read more about it, you can scan the QR code on

51
00:03:34,942 --> 00:03:38,806
top right? Let's see some of the core concepts and the building blocks

52
00:03:38,830 --> 00:03:42,420
of open telemetry open telemetry is

53
00:03:42,532 --> 00:03:46,228
basically a specification. It's not as one specific

54
00:03:46,316 --> 00:03:48,784
framework language or an SDK.

55
00:03:49,324 --> 00:03:53,188
Open provides the specification with which each individual

56
00:03:53,236 --> 00:03:56,524
languages and frameworks develop their own set

57
00:03:56,564 --> 00:03:59,908
of SDKs. Now, these SDKs are built on top

58
00:03:59,956 --> 00:04:03,292
of the API specification provided by open telemetry.

59
00:04:03,468 --> 00:04:07,524
These APIs are based on tracing metrics and logging.

60
00:04:07,684 --> 00:04:11,476
All these APIs follow the same semantic convention across

61
00:04:11,660 --> 00:04:15,740
so that any language or framework that are being built for

62
00:04:15,772 --> 00:04:18,828
these using open telemetry remains standard.

63
00:04:18,956 --> 00:04:22,540
And today you might instrument Java application

64
00:04:22,652 --> 00:04:25,900
and tomorrow you have to instrument bay with

65
00:04:25,932 --> 00:04:29,364
that. Having the same specification and semantics.

66
00:04:29,484 --> 00:04:32,836
You do not have to worry or revisit the documentation

67
00:04:32,940 --> 00:04:36,422
or reinvent the vein often. Most SDKs

68
00:04:36,478 --> 00:04:39,834
today provide you with the option of automatic instrumentation.

69
00:04:40,134 --> 00:04:43,758
For example, no JS provides you an option of automatic

70
00:04:43,806 --> 00:04:47,678
instrumentation with libraries like Express core,

71
00:04:47,806 --> 00:04:51,150
MySQL, and any other common framework that are

72
00:04:51,182 --> 00:04:55,014
being used with node JS. We'll see about that in some

73
00:04:55,054 --> 00:04:57,954
time when we get into the hands on part of that.

74
00:04:58,374 --> 00:05:02,378
Lastly, one of the important protocols and part of open telemetry is open

75
00:05:02,426 --> 00:05:05,634
telemetry protocol. This protocol is used to

76
00:05:05,674 --> 00:05:08,834
send all the telemetry that is collected from your applications,

77
00:05:08,954 --> 00:05:12,586
infrastructure or services to the choice of your back end.

78
00:05:12,770 --> 00:05:16,562
OTLP works on two famous protocols.

79
00:05:16,618 --> 00:05:19,986
One is HTTP or GrPC.

80
00:05:20,170 --> 00:05:23,090
Depending on your system architecture or your requirements,

81
00:05:23,162 --> 00:05:26,306
you can choose to use either, or you can choose to

82
00:05:26,330 --> 00:05:30,010
use both. Let's quickly get into the hands on

83
00:05:30,042 --> 00:05:33,858
part and see how we can get started with instrumenting. Simple service

84
00:05:34,026 --> 00:05:38,026
as no. J's, the conventions remain same across

85
00:05:38,130 --> 00:05:41,682
other languages. The APIs and APIs are

86
00:05:41,738 --> 00:05:45,186
similar. Only thing that will change is the packages

87
00:05:45,290 --> 00:05:49,298
and some SDKs. Here's a very simple application

88
00:05:49,426 --> 00:05:53,700
which is built on top of known express using the core framework.

89
00:05:53,842 --> 00:05:57,544
Core is a very simple light framework similar to

90
00:05:57,584 --> 00:06:00,368
Express, which helps you write rest APIs.

91
00:06:00,456 --> 00:06:04,264
Quickly we'll get into each of the packages of open telemetry

92
00:06:04,304 --> 00:06:07,444
that we'll be using. Let's start with the tracing

93
00:06:07,984 --> 00:06:11,352
and automatic instrumentation with node for node JS,

94
00:06:11,528 --> 00:06:15,944
how we export that telemetry to our collector and

95
00:06:16,064 --> 00:06:19,588
moving towards metrics and

96
00:06:19,736 --> 00:06:23,444
combining all these three, sending it to collector and how

97
00:06:23,484 --> 00:06:26,740
collector exports all these metrics to Neuralick

98
00:06:26,892 --> 00:06:29,972
new relic is one of our observability backends.

99
00:06:30,068 --> 00:06:33,644
Neuralik helps you provide the contextual information by

100
00:06:33,684 --> 00:06:37,104
stitching together all the telemetry exported from the collector.

101
00:06:37,724 --> 00:06:41,572
This is a very simple application as I mentioned. You'll see

102
00:06:41,628 --> 00:06:45,196
that there's nothing much. It's just a very basic application

103
00:06:45,260 --> 00:06:48,816
which has a certain endpoint. Here I have

104
00:06:48,920 --> 00:06:53,560
at least four API endpoints which is one root path,

105
00:06:53,672 --> 00:06:57,664
a post request, and another request with get which

106
00:06:57,744 --> 00:07:01,864
accepts certain parameters. Each of these requests will be automatically

107
00:07:01,944 --> 00:07:05,480
capturing the traces with by using the open elementary

108
00:07:05,512 --> 00:07:09,048
SDKs. Now to start with the safest

109
00:07:09,136 --> 00:07:12,684
option and the easiest option that you can opt

110
00:07:12,784 --> 00:07:16,304
to get started with open telemetry is automatic instrumentation.

111
00:07:16,764 --> 00:07:22,588
We will not be modified anything from in the source code of this core.

112
00:07:22,756 --> 00:07:26,500
Instead, the recommendation from open telemetry for node JS

113
00:07:26,572 --> 00:07:30,036
is that you create a separate wrapper file which will

114
00:07:30,060 --> 00:07:34,184
be the primary module for you to start your Nord Express applications.

115
00:07:35,004 --> 00:07:38,332
We will start with setting up this file and adding all

116
00:07:38,348 --> 00:07:42,300
the packages. I'll walk you through about the details of each

117
00:07:42,332 --> 00:07:46,744
package that we are using and the topic that we will be focusing on.

118
00:07:47,164 --> 00:07:50,964
Firstly, we'll start with the tracing. For that we'll be focusing on

119
00:07:51,004 --> 00:07:55,264
a couple of packages which are automatic instrumentation for node

120
00:07:55,684 --> 00:07:59,544
SDK trace node SDK trace base

121
00:08:00,044 --> 00:08:03,444
and if you and you need to focus on

122
00:08:03,484 --> 00:08:07,344
what we are importing from each of these each of these different packages

123
00:08:07,974 --> 00:08:11,758
for the automatic instrumentation, it provides us an API called

124
00:08:11,846 --> 00:08:15,102
Get Node auto instrumentation which helps you capture

125
00:08:15,198 --> 00:08:19,094
the telemetry automatically from all of your node js and underlying

126
00:08:19,134 --> 00:08:22,942
libraries. There are some conventions for you to set up

127
00:08:22,958 --> 00:08:26,710
your open telemetry service, right? For that we'll use some helper packages

128
00:08:26,782 --> 00:08:30,502
like semantic convention resources which helps us

129
00:08:30,598 --> 00:08:33,830
configure our application name and other attributes of our

130
00:08:33,862 --> 00:08:37,669
application correctly. Let me quickly scroll down to the part

131
00:08:37,701 --> 00:08:40,513
where we are setting up the tracing for our application.

132
00:08:41,133 --> 00:08:45,533
We'll ignore everything else that's configured for now and point

133
00:08:45,573 --> 00:08:49,113
you towards what is important for you to get started quickly.

134
00:08:49,453 --> 00:08:52,405
Firstly, we require a tracer provider.

135
00:08:52,589 --> 00:08:56,285
A tracer provider is an API which will register

136
00:08:56,469 --> 00:08:59,621
your open register the application with the open telemetry

137
00:08:59,677 --> 00:09:03,818
API. Now here is where we provide our resource.

138
00:09:03,946 --> 00:09:08,042
Now this is the resource where we have set up our application name.

139
00:09:08,218 --> 00:09:11,922
This is the most basic configuration that we are setting up here.

140
00:09:12,018 --> 00:09:15,530
What we'll do is we'll just add our resource name which is will

141
00:09:15,562 --> 00:09:18,922
be our open elementary service name. Once we

142
00:09:18,938 --> 00:09:22,794
have added our name, we can set the configuration of how frequently

143
00:09:22,874 --> 00:09:26,186
we want to flush our traces. Flush basically

144
00:09:26,250 --> 00:09:29,582
tells the SDK how frequently you want the telemetry

145
00:09:29,678 --> 00:09:33,774
to be sent out. Once we have the provider API

146
00:09:33,814 --> 00:09:37,654
configured, we have to add a span processor. Now traces

147
00:09:37,734 --> 00:09:41,510
are built of multiple spans. Each span

148
00:09:41,582 --> 00:09:45,086
is a is an operation within your application that

149
00:09:45,110 --> 00:09:48,286
can pull information about its execution period, what was

150
00:09:48,310 --> 00:09:52,678
the function, and anything that happened in the specific operation

151
00:09:52,726 --> 00:09:58,062
like error or exception. Span contains all that information and

152
00:09:58,158 --> 00:10:01,934
building stitching all these spans is what is called as a straight

153
00:10:01,974 --> 00:10:06,118
as a trace. Now in this trace provider we

154
00:10:06,166 --> 00:10:10,118
add a span processor. A span processor basically sets

155
00:10:10,166 --> 00:10:13,342
up an information. A span processor

156
00:10:13,398 --> 00:10:16,750
basically sets up and tells the SDK how each of

157
00:10:16,782 --> 00:10:20,722
these spans should be processed from this application. For this

158
00:10:20,778 --> 00:10:24,074
particular example, we'll be using the batch band processor. It is

159
00:10:24,114 --> 00:10:28,082
also a recommended processor so that there is not much

160
00:10:28,218 --> 00:10:32,494
frequent there is not much frequent export

161
00:10:33,194 --> 00:10:37,106
so that there is not much operational load on SDK

162
00:10:37,250 --> 00:10:40,642
for processing the span. The batch span takes

163
00:10:40,698 --> 00:10:44,066
a number takes a few optional settings which are default.

164
00:10:44,170 --> 00:10:47,506
All the values that you see here are the default values. You can increase or

165
00:10:47,530 --> 00:10:50,944
reduce this as per your requirements. Basically batch plan

166
00:10:50,984 --> 00:10:54,520
processor collects all your span and processes them in a

167
00:10:54,552 --> 00:10:57,720
batch. It takes another parameter which is

168
00:10:57,752 --> 00:11:01,344
an exporter which says where all these process spans

169
00:11:01,424 --> 00:11:05,008
have to be exported to. For this one I've configured

170
00:11:05,056 --> 00:11:08,944
it to a simple character. Character is will be running in my

171
00:11:08,984 --> 00:11:12,392
local setup in a container. I talk about container towards the

172
00:11:12,408 --> 00:11:16,224
very end. Once we have instrumented our application covering

173
00:11:16,264 --> 00:11:19,764
the topics of traces, metrics and logs,

174
00:11:20,104 --> 00:11:23,560
all of these will be exported to our collector and our collector

175
00:11:23,592 --> 00:11:27,032
will be exporting it to other to our observability backend which

176
00:11:27,048 --> 00:11:31,056
will be neurofic. Now in the collector there's only one configuration that's

177
00:11:31,080 --> 00:11:34,968
required because the collector is running locally. A simple localhost URL

178
00:11:35,056 --> 00:11:38,800
which is the default URL for open telemetry collector traces

179
00:11:38,872 --> 00:11:43,754
API and once we have done that, added our batch span processor,

180
00:11:44,294 --> 00:11:47,550
we can optionally register certain propagators.

181
00:11:47,742 --> 00:11:51,758
Now in the same trace provider will register w

182
00:11:51,806 --> 00:11:55,174
three c baggage propagator and trace context propagator.

183
00:11:55,334 --> 00:11:59,126
These propagators helps us find the origin and stitch the

184
00:11:59,150 --> 00:12:02,634
request that is hopping through multiple services.

185
00:12:02,974 --> 00:12:07,070
I'll show this in example of what it looks like once we have included these

186
00:12:07,142 --> 00:12:11,016
propagators. These basically helps you give you the overview and

187
00:12:11,040 --> 00:12:14,976
complete picture of how many different services your request

188
00:12:15,040 --> 00:12:18,576
has opt on and what was the operation or what was

189
00:12:18,600 --> 00:12:22,408
the problem at particular specific service that happened and

190
00:12:22,496 --> 00:12:25,604
helps you capture that information by stitching it all together.

191
00:12:26,224 --> 00:12:29,800
Once we have configured everything for our trace provider, we need

192
00:12:29,832 --> 00:12:32,444
to register our instrumentations.

193
00:12:33,424 --> 00:12:37,330
Register instrumentation is part of a open

194
00:12:37,362 --> 00:12:40,650
telemetry instrumentation library and you can import

195
00:12:40,722 --> 00:12:43,174
that and start adding our instrumentation.

196
00:12:43,874 --> 00:12:47,330
Basically, register instrumentation tells you what

197
00:12:47,442 --> 00:12:51,226
what is that you want to focus on this application's

198
00:12:51,250 --> 00:12:54,882
instrumentation. First thing we need to provide it provider

199
00:12:54,978 --> 00:12:58,210
for us. The tracer provider is our trace provider. That which is

200
00:12:58,242 --> 00:13:01,806
configured and in the instrumentations list,

201
00:13:01,930 --> 00:13:05,714
which is an array, will provide everything that we want to focus on.

202
00:13:06,014 --> 00:13:09,486
The get known auto instrumentation library has tons

203
00:13:09,510 --> 00:13:12,950
of libraries which can automatically capture

204
00:13:13,062 --> 00:13:16,830
metrics and traces from your application. I do not want

205
00:13:16,862 --> 00:13:20,750
to capture any of the file system operations that node Js or my

206
00:13:20,782 --> 00:13:24,158
Goa framework or co op framework does

207
00:13:24,206 --> 00:13:27,634
any operations on. I do want to capture

208
00:13:28,534 --> 00:13:32,286
that anything that's happening with respect to core framework

209
00:13:32,390 --> 00:13:36,358
be captured and in the same auto

210
00:13:36,446 --> 00:13:40,734
get node auto instrumentation library instrumentation and

211
00:13:40,774 --> 00:13:44,030
in the same get node auto instrumentation configuration

212
00:13:44,142 --> 00:13:47,174
you can add much more. For example, if you have my

213
00:13:47,214 --> 00:13:50,870
sequel or anything, you have tons of different libraries

214
00:13:50,942 --> 00:13:54,658
which are prepackaged and all you have to do is add it to this

215
00:13:54,766 --> 00:13:59,014
register instrumentation list and it will start capturing that information.

216
00:13:59,874 --> 00:14:03,410
Now we'll focus on this part later. Once we go through the

217
00:14:03,442 --> 00:14:07,658
logs part, this specific configuration will be focused focusing

218
00:14:07,706 --> 00:14:11,458
on decorating our logs. Now, this is all the

219
00:14:11,586 --> 00:14:15,066
configuration that's required for you to start capturing your

220
00:14:15,130 --> 00:14:18,906
traces from your node application. Let's quickly see

221
00:14:18,970 --> 00:14:22,160
in the console window what the traces looks like once we

222
00:14:22,192 --> 00:14:25,616
start instrumenting. Once we start utilizing

223
00:14:25,680 --> 00:14:29,136
our wrapper with our node application. Now in

224
00:14:29,160 --> 00:14:33,664
my terminal, before I start my application, I'm busking a few environment variables.

225
00:14:33,824 --> 00:14:37,384
These are the two environment variables which are helper,

226
00:14:37,544 --> 00:14:41,720
which are basically helper configurations. For me to start this application,

227
00:14:41,912 --> 00:14:45,792
the hotel service name is what will be referred by the wrapper,

228
00:14:45,968 --> 00:14:49,644
which is going to provide you a name of that specific service.

229
00:14:50,344 --> 00:14:54,240
And the portal log level basically helps us debug all our

230
00:14:54,272 --> 00:14:57,724
configuration that we just did in our wrapper file.

231
00:14:58,144 --> 00:15:01,904
This particular command basically tells node that before loading

232
00:15:01,944 --> 00:15:04,484
the main file, which is our index js,

233
00:15:04,784 --> 00:15:08,240
load the portal wrapper and then load the index js.

234
00:15:08,392 --> 00:15:12,064
This basically loads hotel wrapper as the primary file and then

235
00:15:12,144 --> 00:15:15,400
executes our index js. This way we are able to capture

236
00:15:15,432 --> 00:15:18,134
the metrics from the start of our application.

237
00:15:18,714 --> 00:15:22,594
Now, my application is running successfully on port 3000.

238
00:15:22,714 --> 00:15:26,506
Now this is a typical behavior of any basic Nord express or

239
00:15:26,530 --> 00:15:29,770
node core of node core service.

240
00:15:29,962 --> 00:15:33,506
Let me add another send some request to

241
00:15:33,530 --> 00:15:37,642
this particular service that is being run. I hit the roots

242
00:15:37,818 --> 00:15:41,626
in I hit the root of this specific API and

243
00:15:41,730 --> 00:15:45,408
you see I hit the root API of this service

244
00:15:45,456 --> 00:15:49,600
that's running and you'll see that my API

245
00:15:49,672 --> 00:15:53,616
responded and got executed. I got a console log

246
00:15:53,680 --> 00:15:57,376
from my application which is saying my service name, what is

247
00:15:57,400 --> 00:16:00,952
the host name of where it is running, what is the message and the

248
00:16:00,968 --> 00:16:04,616
timestamp. This is a typical logging of console

249
00:16:04,640 --> 00:16:07,768
dot log or any logging library that you are using.

250
00:16:07,936 --> 00:16:11,752
For this instance, I'm using the bunion library for my logging application

251
00:16:11,888 --> 00:16:15,224
and it adds certain attributes. The rest

252
00:16:15,264 --> 00:16:18,240
of the output that you are seeing is not from the API,

253
00:16:18,352 --> 00:16:21,560
but it is actually from open territories.

254
00:16:21,672 --> 00:16:25,536
Debug logs this is a typical trace that is,

255
00:16:25,640 --> 00:16:30,088
that gets exported and this is, these are the attributes

256
00:16:30,136 --> 00:16:33,744
that are automatically attached. You'll notice the trace id and span

257
00:16:33,784 --> 00:16:37,124
id that got attached to this specific resource.

258
00:16:37,644 --> 00:16:41,116
Let me call on the API now. You'll see I got

259
00:16:41,140 --> 00:16:44,484
the response here and the debug output

260
00:16:44,524 --> 00:16:47,788
from my orbital metric. The SDK

261
00:16:47,876 --> 00:16:51,164
prints out, the SDK prints out the debug

262
00:16:51,204 --> 00:16:54,504
logs for all of the requests that are coming in now.

263
00:16:55,084 --> 00:16:59,164
Be mindful that we did not modify anything in our actual source code,

264
00:16:59,204 --> 00:17:03,304
which is our index J's. All we have done is added a wrapper

265
00:17:03,984 --> 00:17:07,672
wrapper around it and all these attributes are being captured

266
00:17:07,728 --> 00:17:10,684
by SDK using the automatic instrumentation.

267
00:17:11,264 --> 00:17:15,552
I'll start my application with certain environment variables that I require

268
00:17:15,688 --> 00:17:20,248
for my hotel SDK, one of which is hotel service name.

269
00:17:20,416 --> 00:17:24,064
Auto service name is basically telling what the service of my

270
00:17:24,104 --> 00:17:27,632
name should be when I start executing this or export it to

271
00:17:27,688 --> 00:17:30,944
any observatory backlight. The other variable that I have here

272
00:17:30,984 --> 00:17:34,272
is auto log level. This helps me debug any of the

273
00:17:34,288 --> 00:17:38,088
problems that are occurring in the hotel configuration

274
00:17:38,176 --> 00:17:41,752
as part of the wrapper file. And the command here just basically

275
00:17:41,808 --> 00:17:45,744
tells node to load the wrapper file before loading the main module, which is

276
00:17:45,784 --> 00:17:49,216
the index js. Let's execute this file and

277
00:17:49,240 --> 00:17:52,960
see what happens. You'll see that there are a lot of debug

278
00:17:53,072 --> 00:17:56,572
statements that got printed. This is all because we

279
00:17:56,588 --> 00:18:00,116
have set the log level to debug. It says that it's trying to

280
00:18:00,140 --> 00:18:04,104
load the instrumentation for all these libraries, but there is not many,

281
00:18:05,444 --> 00:18:09,300
there are not many libraries that it's found. Only the library that it finds

282
00:18:09,372 --> 00:18:12,828
is for Node J's module and HTTP

283
00:18:12,956 --> 00:18:15,908
and also for core, and it's applying patch for that.

284
00:18:15,996 --> 00:18:20,338
Basically the libraries which are pre built as part of automatic instrumentation,

285
00:18:20,476 --> 00:18:24,494
it patches onto that. So any requests that are going or any operation

286
00:18:24,534 --> 00:18:28,342
that have been made from these libraries or any operations that

287
00:18:28,358 --> 00:18:31,234
are happening as part of these libraries are captured.

288
00:18:32,014 --> 00:18:36,142
Now you'll see that there are a couple of libraries which is bunny are

289
00:18:36,198 --> 00:18:39,670
being also been patched. We can do it how the logging works,

290
00:18:39,742 --> 00:18:43,526
but for now the logging is disabled. Once we have enabled

291
00:18:43,550 --> 00:18:47,488
that, you'll see that the middleware framework is being patch,

292
00:18:47,536 --> 00:18:50,920
which is our core, and some more

293
00:18:50,952 --> 00:18:54,640
debugging statements that are being generated. Let's quickly scroll

294
00:18:54,672 --> 00:18:58,368
down and we make a request to our service here.

295
00:18:58,416 --> 00:19:01,936
What I'll do is make a simple call to the root API

296
00:19:02,080 --> 00:19:05,480
that are configured. It's just going to return a simple hello world hello

297
00:19:05,512 --> 00:19:09,120
world response and let's see how the trace is being generated

298
00:19:09,192 --> 00:19:12,744
from open telemetry. So this is my logging line from

299
00:19:12,784 --> 00:19:15,908
my bunny logger, and let's see what happens after

300
00:19:15,956 --> 00:19:19,612
that. All this output that we are seeing right now is

301
00:19:19,708 --> 00:19:24,164
the actual trace that is being generated from our particular request.

302
00:19:24,324 --> 00:19:27,580
So if I scroll to the top, this is my log line that

303
00:19:27,612 --> 00:19:31,492
got hit and generated as soon as I hit my API.

304
00:19:31,668 --> 00:19:35,364
And these are the spans that got created, which is

305
00:19:35,404 --> 00:19:39,004
basically capturing all the information of execution of

306
00:19:39,044 --> 00:19:42,542
this particular API. You'll see it's capturing the

307
00:19:42,558 --> 00:19:46,550
metrics from the core library. It's having certain attributes.

308
00:19:46,662 --> 00:19:49,982
What kind of span processor are being used? What is the different

309
00:19:50,038 --> 00:19:53,382
body parsers that have been used that we are using in our application,

310
00:19:53,558 --> 00:19:57,174
the different span ids, as mentioned earlier,

311
00:19:57,294 --> 00:20:01,074
multiple spans and build a complete trace.

312
00:20:01,494 --> 00:20:05,174
And it provides a parent span id. So whatever the first request

313
00:20:05,294 --> 00:20:08,910
had, the parents, what are the first requests that came into the

314
00:20:08,942 --> 00:20:12,462
system becomes the parent span, and that id is

315
00:20:12,518 --> 00:20:16,954
attached to all the rest of the lifecycle of that particular request.

316
00:20:17,654 --> 00:20:20,894
There's a lot of output, again, a lot of debugging output which we will not

317
00:20:20,934 --> 00:20:24,406
focus on. We see how this output looks like

318
00:20:24,510 --> 00:20:28,206
in our observability back end once we have exported. For now it's

319
00:20:28,230 --> 00:20:31,262
getting exported to collector and routed to a backend.

320
00:20:31,438 --> 00:20:34,554
I'll show you directly how it looks like in the backend.

321
00:20:35,204 --> 00:20:39,064
I'll cover deeply about connector and how we can configure that

322
00:20:39,444 --> 00:20:43,164
we are exporting all our telemetry to Neuralink using the OTLP

323
00:20:43,204 --> 00:20:47,452
protocol. Let me click on my services and

324
00:20:47,508 --> 00:20:50,884
my service convert 42 hotel is already available here.

325
00:20:51,044 --> 00:20:53,704
You'll see some metrics are already coming in.

326
00:20:54,284 --> 00:20:58,260
Since I'm not capturing any metrics for my service yet, I switch

327
00:20:58,292 --> 00:21:02,184
to spans that are the cache captured from my service using the hotel

328
00:21:02,224 --> 00:21:05,720
SDK. We'll see. Some of the requests and response time

329
00:21:05,792 --> 00:21:09,288
are already being available here. I'll quickly just

330
00:21:09,336 --> 00:21:12,704
switch to distributed tracing to look at the rest APIs

331
00:21:12,744 --> 00:21:16,320
and the traces and analyze them. I'll click on this

332
00:21:16,352 --> 00:21:20,232
first one and start to see that there is this particular

333
00:21:20,328 --> 00:21:24,016
request. Let me click on the first request in

334
00:21:24,040 --> 00:21:27,700
this list. This is giving me an overview of what

335
00:21:27,732 --> 00:21:30,612
the request response time cycle was.

336
00:21:30,748 --> 00:21:34,084
It took 3.5 milliseconds for this request

337
00:21:34,124 --> 00:21:38,292
to get completed. There are certain operations that happen and

338
00:21:38,388 --> 00:21:41,516
if I just click on this particular trace you'll

339
00:21:41,540 --> 00:21:44,676
see there are attributes that got attached which we have seen most of

340
00:21:44,700 --> 00:21:47,556
it in the debug window, the entertainment,

341
00:21:47,660 --> 00:21:50,772
the duration, what was the SDB flavor,

342
00:21:50,868 --> 00:21:54,780
what was the host name, the target? So for

343
00:21:54,852 --> 00:21:58,620
we made a request on as a root rest API and

344
00:21:58,652 --> 00:22:02,388
that's what's being captured, the id of this particular trace,

345
00:22:02,556 --> 00:22:06,372
what was the type of request, and you

346
00:22:06,428 --> 00:22:09,904
also see how this actually was captured.

347
00:22:10,604 --> 00:22:14,028
The instrumentation hyphen HTTP this library

348
00:22:14,076 --> 00:22:18,056
was patched and it has captured this particular request and

349
00:22:18,080 --> 00:22:21,552
the library version. There are certain other attributes which

350
00:22:21,568 --> 00:22:24,952
are of help. If you have attached any custom attributes,

351
00:22:25,048 --> 00:22:27,484
it should also get listed in the same space.

352
00:22:28,184 --> 00:22:31,952
This is regardless of where you export your telemetry. It should give you an experience

353
00:22:32,048 --> 00:22:36,152
similar to this. Neuralink helps us get to the point quickly

354
00:22:36,248 --> 00:22:40,088
and that's why we're able to materialize all the traces

355
00:22:40,256 --> 00:22:43,538
fairly quickly. Let's look at some other trace.

356
00:22:43,696 --> 00:22:47,034
Let me make some two different APIs.

357
00:22:47,574 --> 00:22:51,034
I have the endpoint to which I'll make a request

358
00:22:51,574 --> 00:22:55,246
which is giving me basically weather information about

359
00:22:55,270 --> 00:22:58,694
the particular location. So I'll change the location to where

360
00:22:58,734 --> 00:23:01,910
I'm currently at and the API

361
00:23:01,942 --> 00:23:05,542
responded very quickly. So this particular endpoint

362
00:23:05,718 --> 00:23:09,022
basically makes a request to an external service which

363
00:23:09,038 --> 00:23:12,584
is also instrumented using open telemetry. But that

364
00:23:12,624 --> 00:23:16,912
service is not on my localhost, it is actually deployed

365
00:23:17,008 --> 00:23:20,360
elsewhere on an AWS instance. Let's see

366
00:23:20,432 --> 00:23:24,704
how open captures the essence of this request and

367
00:23:24,824 --> 00:23:29,096
helps us get the stigma information to give us a complete picture.

368
00:23:29,240 --> 00:23:33,152
I'll make a few more requests so that I'm able to have sizeable

369
00:23:33,208 --> 00:23:37,296
data that we can go through. I'll also make a request

370
00:23:37,360 --> 00:23:40,526
that can that should fail, with which we'll see what the

371
00:23:40,550 --> 00:23:44,286
errors look like. Once we start implementing our application

372
00:23:44,390 --> 00:23:48,194
using open telemetry. I'll just give a blah blah land

373
00:23:48,574 --> 00:23:52,354
and we'll see this specific request returned with 404.

374
00:23:52,854 --> 00:23:56,070
Now to show you quickly what happened, I'll just show

375
00:23:56,102 --> 00:23:59,630
you in the terminal. How many requests

376
00:23:59,702 --> 00:24:02,814
happened, what were the debug logs and if there were any

377
00:24:02,854 --> 00:24:07,012
errors. So you'll see this specific request failed

378
00:24:07,068 --> 00:24:11,436
and had a 404. So my request has completed, my trees got generated

379
00:24:11,580 --> 00:24:14,812
and this was the parameter that I have called. That was my

380
00:24:14,828 --> 00:24:18,524
HTTP target. Keep in mind we have not modified the source

381
00:24:18,564 --> 00:24:22,252
code of our application. We have just added a wrapper around

382
00:24:22,308 --> 00:24:25,940
our main application. So this is helpful if you want to quickly get

383
00:24:25,972 --> 00:24:29,772
started with open elementary so you don't disturb the existing application

384
00:24:29,828 --> 00:24:32,934
code and start experimenting with SDKs.

385
00:24:33,314 --> 00:24:37,354
Let's look at this specific trace in our back end coming

386
00:24:37,394 --> 00:24:41,434
back to our distributed tracing. I'll click on trace groups now

387
00:24:41,594 --> 00:24:45,010
I see that there are few more trace. Let me click

388
00:24:45,042 --> 00:24:47,094
on this and get into it further.

389
00:24:47,714 --> 00:24:50,962
Now there is this one request on Toggle,

390
00:24:51,058 --> 00:24:54,938
which was the most recent request and it has the most duration of

391
00:24:54,986 --> 00:24:58,506
all the other requests. I'm assuming this should be the request

392
00:24:58,570 --> 00:25:02,546
for weather. Let me click on this and as

393
00:25:02,570 --> 00:25:06,874
you see here, you'll see a map of the journey of your API

394
00:25:06,914 --> 00:25:11,434
request and how many other services it has topped my

395
00:25:11,474 --> 00:25:14,938
original service node js, from which I'm from which I made the

396
00:25:14,946 --> 00:25:18,498
request, and it has made an external call to an

397
00:25:18,546 --> 00:25:21,690
external service which is instrumented with open telemetry,

398
00:25:21,882 --> 00:25:25,762
which is in turn making another external request which is making at

399
00:25:25,778 --> 00:25:29,546
least eight different calls. And now that is unknown because that service is

400
00:25:29,570 --> 00:25:34,254
not incremented. Let's expand and see what happened underneath.

401
00:25:34,674 --> 00:25:38,974
You'll see the information of all the operations that happen underneath.

402
00:25:39,554 --> 00:25:42,658
There was a get request which we made from our system.

403
00:25:42,786 --> 00:25:46,018
It went to API weather request of

404
00:25:46,066 --> 00:25:49,660
Node express service, which is the Olodexpress portal,

405
00:25:49,812 --> 00:25:53,556
and there was some middleware operations and it also made a

406
00:25:53,580 --> 00:25:56,344
get request which is an external call.

407
00:25:56,764 --> 00:26:00,452
We can check what is the service that this particular

408
00:26:00,508 --> 00:26:04,932
service made a request to. You'll be able to understand what

409
00:26:04,988 --> 00:26:08,812
service external services are causing slowness in your application

410
00:26:08,948 --> 00:26:13,394
so that you are able to improve that particular area and aspect of your application.

411
00:26:13,564 --> 00:26:17,554
So basically this service is the open weather map application,

412
00:26:17,934 --> 00:26:22,014
open weather map service, and from which from this particular

413
00:26:22,094 --> 00:26:24,714
service it's requesting all the weather information.

414
00:26:25,454 --> 00:26:29,046
Once we have all this information, it becomes easier

415
00:26:29,110 --> 00:26:32,974
for us to triage and understand the behavior of our application,

416
00:26:33,094 --> 00:26:36,702
not just in happy scenarios, but also in problematic scenarios.

417
00:26:36,838 --> 00:26:40,014
Let me quickly go back and click click on the errors.

418
00:26:40,554 --> 00:26:44,378
You'll see that this particular request which failed at 404. Now this

419
00:26:44,426 --> 00:26:48,146
is being highlighted here. I quickly want to

420
00:26:48,170 --> 00:26:51,786
understand what has failed and new link provides you a

421
00:26:51,810 --> 00:26:55,134
good map and overview of which services were impacted.

422
00:26:55,674 --> 00:26:59,226
In this particular map, both the services are highlighted as red,

423
00:26:59,290 --> 00:27:03,354
which means both these services had some form of errors. We have seen

424
00:27:03,394 --> 00:27:06,894
that individual operations being captured in the spans,

425
00:27:07,054 --> 00:27:10,694
but if you want to focus on your errors, there's this convenient checkbox

426
00:27:10,734 --> 00:27:14,646
that you can just click and you'll see that there is a get request

427
00:27:14,710 --> 00:27:18,126
which actually failed and there was an error. Now this,

428
00:27:18,190 --> 00:27:21,854
since this is just automatic instrumentation and an external service,

429
00:27:21,974 --> 00:27:26,358
there is not much details. Now this is where manual instrumentation

430
00:27:26,406 --> 00:27:30,294
comes into picture. Once you have identified the areas, or once you

431
00:27:30,334 --> 00:27:34,112
know what areas you want to instrument, you can use the manual instrumentation

432
00:27:34,208 --> 00:27:38,000
and customize your error message, or even add additional spans

433
00:27:38,112 --> 00:27:41,592
to support your debugging and analysis journey of your

434
00:27:41,688 --> 00:27:45,244
applications. That's all about the tracing.

435
00:27:45,624 --> 00:27:48,976
We have set our tracing for unreal application using the

436
00:27:49,000 --> 00:27:53,304
automatic instrumentation trace provider and stress

437
00:27:53,344 --> 00:27:57,120
train the instrumentation using the pre built pre package

438
00:27:57,192 --> 00:28:00,754
library release as part of the automatic instrumentation.

439
00:28:01,054 --> 00:28:04,878
That's as simple as it to get started with automatic instrumentation

440
00:28:04,966 --> 00:28:08,234
for your node j's applications,

441
00:28:09,094 --> 00:28:12,654
let's get back to our code and

442
00:28:12,814 --> 00:28:16,390
include a metric instrumentation open

443
00:28:16,462 --> 00:28:19,814
industry provides us with packages to help

444
00:28:19,894 --> 00:28:22,634
us capture the metrics from our applications.

445
00:28:23,534 --> 00:28:26,778
In this part, we'll focus on configuring our

446
00:28:26,826 --> 00:28:31,094
metrics and extracting the metrics of our application over

447
00:28:31,794 --> 00:28:35,354
similar to the traces, there are a couple of packages

448
00:28:35,434 --> 00:28:38,786
that we need to be aware of, one of which is

449
00:28:38,850 --> 00:28:42,954
SDK matrix. This particular SDK provides

450
00:28:42,994 --> 00:28:46,690
us with meter provider, the exporting readers and

451
00:28:46,842 --> 00:28:50,694
helper function for debugging, which is console metric exporter.

452
00:28:51,174 --> 00:28:54,558
Similar to setting up a trace provider. What we do

453
00:28:54,606 --> 00:28:58,430
is we start a periodic exporting meter where

454
00:28:58,462 --> 00:29:01,734
we are configuring our meter provider to

455
00:29:01,774 --> 00:29:05,594
send all the metrics being captured to a console.

456
00:29:05,894 --> 00:29:09,774
But first, what we want to do is capture all these metrics from our

457
00:29:09,814 --> 00:29:12,750
application and export it to a packet.

458
00:29:12,902 --> 00:29:16,206
In this case, we are setting up a OTLP metric exporter

459
00:29:16,310 --> 00:29:19,540
without any particular particular URL. One of the default

460
00:29:19,572 --> 00:29:23,236
settings of any exporter SDKs for any of these traces,

461
00:29:23,300 --> 00:29:28,052
metrics or logs is that it always points to localhost 4318

462
00:29:28,228 --> 00:29:31,756
or 4317 depending on the protocol available.

463
00:29:31,900 --> 00:29:35,164
It tries to export it directly to that. The connector

464
00:29:35,204 --> 00:29:39,188
supports both 4317 which is receiving on GRPC,

465
00:29:39,316 --> 00:29:43,136
and 431, which receives on HTTP. Now for

466
00:29:43,160 --> 00:29:47,104
meter, once we have our meter exporter, we'll start

467
00:29:47,184 --> 00:29:50,576
with setting up our provider as it

468
00:29:50,600 --> 00:29:54,504
races require a trace provider. Meter also requires

469
00:29:54,584 --> 00:29:58,152
a meter provider. What we'll configure here is part of a

470
00:29:58,168 --> 00:30:01,800
meter provider. We just supply a name again, a resource which

471
00:30:01,832 --> 00:30:05,616
has a service name and the number of readers that we can add.

472
00:30:05,720 --> 00:30:09,244
This can be an array or a single value. Here I'm

473
00:30:09,284 --> 00:30:13,004
adding both a console metric reader and a metric reader exporter

474
00:30:13,084 --> 00:30:16,784
which is going to send it and export it to our observability packet.

475
00:30:17,164 --> 00:30:21,076
Once we have our service provider once we

476
00:30:21,100 --> 00:30:25,644
have our meter provider service, we can register it globally

477
00:30:25,764 --> 00:30:30,164
in two methods. One is using the open telemetry API

478
00:30:30,324 --> 00:30:33,676
which is open telemetry metrics and setting a global meter

479
00:30:33,700 --> 00:30:37,288
provider. We can use this we

480
00:30:37,296 --> 00:30:40,304
can use this in cases where you do not have

481
00:30:40,344 --> 00:30:43,844
trace provider or register instrumentations API available.

482
00:30:44,384 --> 00:30:48,128
With this you are able to configure just the metrics provider.

483
00:30:48,296 --> 00:30:51,840
But since we are using the automatic instrumentation API and

484
00:30:51,912 --> 00:30:55,320
a trace provider with registered instrumentation, we are going to

485
00:30:55,352 --> 00:30:59,512
enable this in the larger scope of our application as

486
00:30:59,528 --> 00:31:03,664
part of the list of instrumentations. The API that also accepts is

487
00:31:03,784 --> 00:31:06,984
meter provider. Here we can specify the provider that we just

488
00:31:07,024 --> 00:31:10,152
configured, which is our my service provider which contains the

489
00:31:10,168 --> 00:31:14,224
console metric reader as well as metric reader exporter, one which

490
00:31:14,264 --> 00:31:17,800
exports it to our console, another which exports it to a bucket.

491
00:31:17,992 --> 00:31:22,244
That is all the configuration that's required for us to enable the meter provider.

492
00:31:22,624 --> 00:31:26,864
Let's look at how the output changes for all our open

493
00:31:26,904 --> 00:31:30,984
telemetry metrics. I will disable the

494
00:31:31,144 --> 00:31:34,504
tricks debug information so we are only able to

495
00:31:34,544 --> 00:31:37,760
see the information from our

496
00:31:37,792 --> 00:31:41,444
meter providers. Let me switch back to console

497
00:31:41,944 --> 00:31:46,044
and restart my application quickly clear the console.

498
00:31:46,624 --> 00:31:49,936
I'll retain the logging level since I have modified it directly

499
00:31:49,960 --> 00:31:53,944
in the go. Everything else remains the same. Now we

500
00:31:54,024 --> 00:31:57,286
see a lot of debug output because we have just disabled

501
00:31:57,310 --> 00:32:01,078
that, but we will start seeing the output from the meter provider

502
00:32:01,126 --> 00:32:04,846
once we start hitting any of our endpoints. Let me quickly hit some

503
00:32:04,870 --> 00:32:07,394
of the endpoints and see what it looks like.

504
00:32:07,974 --> 00:32:11,262
One of the configuration that we have enabled is the flush

505
00:32:11,318 --> 00:32:14,594
timings for the console we have set it to,

506
00:32:15,254 --> 00:32:18,782
but you can go as aggressive as 1 second. It is

507
00:32:18,878 --> 00:32:23,124
the default settings is set to 60 seconds. Since meter can

508
00:32:23,164 --> 00:32:25,948
become too aggressive and EW cpu cycles,

509
00:32:26,076 --> 00:32:29,904
it is recommended that you configure it and tune it as per your requirements.

510
00:32:30,684 --> 00:32:34,028
Now this is the output from our meter provider which is capturing

511
00:32:34,076 --> 00:32:37,708
the information for histogram and it is capturing the duration

512
00:32:37,756 --> 00:32:41,628
of our inbound HTTP request, which is in this case is

513
00:32:41,676 --> 00:32:45,092
our endpoints that we are just calling. There are some data

514
00:32:45,148 --> 00:32:48,870
points and values that it attaches, but it's basically

515
00:32:48,942 --> 00:32:52,274
easier to visualize this rather than reading the raw data,

516
00:32:52,814 --> 00:32:56,150
but it also helps you understand what kind of data is being

517
00:32:56,182 --> 00:32:59,614
captured with these console outputs. If I make

518
00:32:59,654 --> 00:33:03,118
a few more calls to different endpoints, I'll start

519
00:33:03,166 --> 00:33:06,654
seeing similar output. There's not much difference except

520
00:33:06,774 --> 00:33:10,434
for the value and start and end time of that request

521
00:33:11,174 --> 00:33:14,716
registered or the application level basically captures all all the

522
00:33:14,740 --> 00:33:18,372
operations for available libraries. Since we also registered

523
00:33:18,388 --> 00:33:21,984
this as part of our automatic instrumentation, it's going to capture

524
00:33:22,524 --> 00:33:25,804
all the operation for each of these endpoints that happen in

525
00:33:25,844 --> 00:33:30,076
our let's switch back to our observability

526
00:33:30,140 --> 00:33:33,984
backend which is neuralink, and see how the metrics are being reflected.

527
00:33:34,564 --> 00:33:38,516
As earlier, all the traces that were being captured

528
00:33:38,620 --> 00:33:42,090
were exported via collector. Metric is also been sent

529
00:33:42,122 --> 00:33:45,954
into the new relay via collector. I'll switch to metrics

530
00:33:46,034 --> 00:33:49,370
in this instance and you see now the metrics chart has

531
00:33:49,402 --> 00:33:52,690
started getting populated metrics. Capturing these

532
00:33:52,722 --> 00:33:56,354
metrics helps you populate these charts, which gives you insight about your

533
00:33:56,394 --> 00:33:59,810
response time, your throughput, and if there are any errors,

534
00:33:59,882 --> 00:34:03,642
you also start capturing those details. Additionally, if you want to

535
00:34:03,698 --> 00:34:07,200
dig further into metrics, what are the different metrics that you have captured?

536
00:34:07,322 --> 00:34:10,344
You can go to the matrix Explorer and see for yourself.

537
00:34:11,004 --> 00:34:14,260
I'll come back to my goal and now that we

538
00:34:14,292 --> 00:34:17,548
have captured the traces and metrics, it's time

539
00:34:17,596 --> 00:34:20,732
to focus on one of the most important telemetry

540
00:34:20,868 --> 00:34:24,260
logs. One of my previous mentors had a famous

541
00:34:24,292 --> 00:34:27,860
saying for all the engineers loves logs.

542
00:34:28,052 --> 00:34:32,140
If there are no logs, there's no life. And that's particularly true

543
00:34:32,252 --> 00:34:35,350
especially for DevOps and SRE engineers. If the services go down,

544
00:34:35,382 --> 00:34:39,358
they start digging to the logs and try to identify what has actually gone wrong

545
00:34:39,526 --> 00:34:42,742
before they can recover the services. Let's focus on

546
00:34:42,758 --> 00:34:46,062
the logs aspect of instrumentation. For our node

547
00:34:46,078 --> 00:34:49,518
J's service we have focused on metrics, which was

548
00:34:49,566 --> 00:34:53,318
fairly simple. All you require is a SDK metrics and the

549
00:34:53,366 --> 00:34:56,594
exporter. For logs it's similar,

550
00:34:56,894 --> 00:35:00,694
but requires a few more steps than setting up your meter or your matrix

551
00:35:00,734 --> 00:35:04,148
provider. For the logs we focus mainly on the API

552
00:35:04,196 --> 00:35:08,084
logs, SDK and another package

553
00:35:08,124 --> 00:35:11,620
which is SDK logs. The SDK logs provides us with logger

554
00:35:11,652 --> 00:35:14,860
provider, the processor and the

555
00:35:14,892 --> 00:35:19,144
log exporter. These APIs helps us set up our application

556
00:35:19,484 --> 00:35:22,988
to properly set up the logs attached with all the

557
00:35:23,036 --> 00:35:26,700
information with regards to traces and metrics. We'll see

558
00:35:26,732 --> 00:35:30,326
how all this ties up towards the end. The library that

559
00:35:30,350 --> 00:35:33,486
I'm using as part of, as part of our application here

560
00:35:33,550 --> 00:35:37,550
is Bunyan. Bunyan is simple logging library,

561
00:35:37,622 --> 00:35:41,006
which is a very famous library for adding any any kind

562
00:35:41,030 --> 00:35:44,834
of logger for simple service. There is

563
00:35:45,174 --> 00:35:48,654
a library available already. If you're using Bernie.

564
00:35:48,734 --> 00:35:52,854
There is a library called Instrumentation Bunyip which helps you capture logs

565
00:35:52,894 --> 00:35:56,772
in open telemetry format. We've seen in the console that logger

566
00:35:56,828 --> 00:36:00,580
log format of bunion is slightly different. We'll see how

567
00:36:00,612 --> 00:36:03,972
that changes automatically. Without modifying any of our application

568
00:36:04,028 --> 00:36:07,852
code. Using this package, we'll set up our logger

569
00:36:07,948 --> 00:36:11,364
to start using and transforming our logs into standard

570
00:36:11,404 --> 00:36:14,684
open dimension format. Now firstly, we require

571
00:36:14,724 --> 00:36:18,996
a logger provider which again accepts a resource and

572
00:36:19,060 --> 00:36:22,436
our resource is again the same global object where we are setting up the

573
00:36:22,460 --> 00:36:25,586
service name. This is particularly important if you want

574
00:36:25,650 --> 00:36:29,562
all these material traces, metrics, traces and logs attached to

575
00:36:29,578 --> 00:36:33,034
the same service. If you do not provide the name, it's assumed as

576
00:36:33,074 --> 00:36:36,234
unknown service. That's the default name that it accepts.

577
00:36:36,314 --> 00:36:40,546
It's always a good practice to add your own service name and the

578
00:36:40,570 --> 00:36:44,282
default value for exporter is this endpoint which

579
00:36:44,298 --> 00:36:47,466
is localhost 4318 version one logs

580
00:36:47,610 --> 00:36:51,990
each of the exporter endpoints and each of the exporter exporter

581
00:36:52,022 --> 00:36:55,942
APIs for different SDKs have these dedicated endpoints configured.

582
00:36:56,078 --> 00:36:59,470
I've included here for your reference. If even if you do not

583
00:36:59,542 --> 00:37:03,886
add this particular endpoint is going to point it to the default connector

584
00:37:04,030 --> 00:37:07,394
receiver to the default endpoint.

585
00:37:08,094 --> 00:37:11,430
Once we have our exporter and provider, we can

586
00:37:11,462 --> 00:37:14,774
configure and attach the processors that we require.

587
00:37:14,894 --> 00:37:18,548
Similar to the span processing logs also have different processor

588
00:37:18,636 --> 00:37:21,884
which is simple and batch processor. I'm using a simple

589
00:37:21,924 --> 00:37:25,308
processor for the console exporter and batch processor

590
00:37:25,436 --> 00:37:29,624
for exporting it to our back end where I'm using the exporter

591
00:37:30,164 --> 00:37:33,584
similar to the metrics provider where we can register it globally.

592
00:37:34,404 --> 00:37:37,620
If you want to capture only the logs from your application, you can use

593
00:37:37,652 --> 00:37:41,188
the portal logs API which is open telemetry API with

594
00:37:41,236 --> 00:37:44,528
logs and use the global logger provider.

595
00:37:44,696 --> 00:37:48,384
But since we are using the automatic instrumentation, I'll go ahead and register it

596
00:37:48,424 --> 00:37:51,896
as part of the register instrumentation and that is

597
00:37:51,920 --> 00:37:55,376
all that's required for you to successfully include logging with

598
00:37:55,400 --> 00:37:59,392
open telemetry in your node application. Once we have enabled the

599
00:37:59,488 --> 00:38:02,848
logger application, we do not have to modify anything.

600
00:38:02,976 --> 00:38:06,816
Since I'm already using the bunion here, it automatically

601
00:38:06,880 --> 00:38:10,506
patches the instance of bunion with open telemetry logger.

602
00:38:10,680 --> 00:38:14,286
Once we enable our provider with the logger provider and register it with the list

603
00:38:14,310 --> 00:38:17,598
of instrumentation, we can add additional options

604
00:38:17,646 --> 00:38:20,454
that we want for the bunion instrumentation to modify.

605
00:38:20,614 --> 00:38:25,494
For example, it provides a log hook option with

606
00:38:25,534 --> 00:38:29,110
which we can modify the log record and

607
00:38:29,182 --> 00:38:33,438
attach any attributes that we want. For example here I'm attaching the resource

608
00:38:33,486 --> 00:38:36,794
attribute which is the service name from the trace provider.

609
00:38:37,234 --> 00:38:40,922
Any customization that you want to add to your log record with open telemetry

610
00:38:40,978 --> 00:38:44,762
and bernouin you can do so here. Once I have

611
00:38:44,898 --> 00:38:48,634
enabled this provider and added my instrumentation, let me restart my

612
00:38:48,674 --> 00:38:51,938
application. I keep the command as same

613
00:38:52,066 --> 00:38:55,386
and you'll see it's running. My application is now running. Let's hit

614
00:38:55,410 --> 00:38:58,934
the endpoint and see what the output looks like.

615
00:38:59,314 --> 00:39:02,434
Now you'll see apart from the default application log

616
00:39:02,474 --> 00:39:06,358
line which was coming, which is the bundling standard of logs,

617
00:39:06,526 --> 00:39:10,910
there is another output which is coming from the logger provider of logs

618
00:39:11,102 --> 00:39:15,726
and the information of all the trace id span ids, the severity

619
00:39:15,830 --> 00:39:19,510
is coming. This is because once

620
00:39:19,542 --> 00:39:23,166
we have included our log, it attaches

621
00:39:23,310 --> 00:39:26,390
all the other information for that particular trace and any additional

622
00:39:26,422 --> 00:39:30,346
custom attribute that we have included. Now in this case, the attribute

623
00:39:30,370 --> 00:39:33,794
that we have added as part of the logbook is the service name.

624
00:39:33,954 --> 00:39:37,066
This can be particularly helpful if your application is running on

625
00:39:37,090 --> 00:39:40,866
multiple hosts and you're streaming all the logs to a central location.

626
00:39:41,010 --> 00:39:45,242
This can be helpful to identify which particular service is breaking and

627
00:39:45,298 --> 00:39:48,570
where the location is. Once we have set

628
00:39:48,602 --> 00:39:52,250
up our logging we can see this in the backend. Let me

629
00:39:52,282 --> 00:39:57,006
quickly generate some more load so we can see all the logs for different I'll

630
00:39:57,030 --> 00:40:00,950
just hit it a couple of more times. One, two, three and

631
00:40:01,102 --> 00:40:04,310
let's switch to our back end there which is nearly and

632
00:40:04,342 --> 00:40:08,142
see what the logs look like. You'll see conveniently there is a logs

633
00:40:08,198 --> 00:40:12,014
option within the same screen which you can click on. Once I click

634
00:40:12,054 --> 00:40:16,054
on this you see that all the logs have already started to flow in

635
00:40:16,214 --> 00:40:19,806
and they have eight requests that I've already made. Once I click

636
00:40:19,830 --> 00:40:23,514
on any of this request you'll see that all the

637
00:40:23,554 --> 00:40:26,898
logs, the body, what is the service name, the span

638
00:40:26,946 --> 00:40:30,170
id, the trace id have been attached, but this is

639
00:40:30,202 --> 00:40:33,574
not present as part of the standard logging statement of bunny.

640
00:40:34,034 --> 00:40:37,450
Any logs that are being generated as part of our application

641
00:40:37,562 --> 00:40:41,306
is now patched with open elementary logs provider and it decorates

642
00:40:41,370 --> 00:40:44,614
our log message with all this additional metadata.

643
00:40:45,154 --> 00:40:49,046
Beauty of all of this setting up logs and traces and metrics

644
00:40:49,170 --> 00:40:52,670
comes into picture now to having to reach to the

645
00:40:52,702 --> 00:40:55,926
root cause of any problem, having the right context is very

646
00:40:55,990 --> 00:40:59,830
important. For example, let me make another request to my

647
00:40:59,862 --> 00:41:03,670
weather API where I'm going to fail it with a

648
00:41:03,702 --> 00:41:07,394
wrong parameter. Once my API has failed,

649
00:41:08,014 --> 00:41:11,190
I'll fail it a couple more times. Now I'll come

650
00:41:11,222 --> 00:41:14,950
back to my back end, which is neuralink, and I start seeing

651
00:41:15,022 --> 00:41:18,592
all this very stitched together and providing me all the

652
00:41:18,608 --> 00:41:22,160
context of the failed request. You see that information like

653
00:41:22,192 --> 00:41:25,896
matrix and span are all available already, but what I want

654
00:41:25,920 --> 00:41:29,336
to focus now is on the errors. We've seen the errors in

655
00:41:29,360 --> 00:41:33,600
the context of traces and how that looks like. What I'm particularly interested

656
00:41:33,632 --> 00:41:37,404
now is to see the relevant logs of that particular trace.

657
00:41:37,824 --> 00:41:42,238
Let me quickly switch back to the distributed tracing and switch

658
00:41:42,286 --> 00:41:45,990
clip on the errors. You see the three requests that just

659
00:41:46,142 --> 00:41:49,550
made have failed and there are three different errors.

660
00:41:49,702 --> 00:41:53,726
I click on this particular trace and I'm seeing the errors for

661
00:41:53,750 --> 00:41:57,262
these particular services. This is something we have already seen as

662
00:41:57,278 --> 00:42:01,270
part of the trace exploration. But now what I'm interested is to understand the

663
00:42:01,302 --> 00:42:04,790
logs related to this particular request. I do not

664
00:42:04,822 --> 00:42:08,142
have to navigate away from this screen now. This is particular to

665
00:42:08,158 --> 00:42:11,994
the neuralink platform, but this is also a beauty of open telemetry.

666
00:42:12,114 --> 00:42:15,810
The place id and span id that got attached to the log statements

667
00:42:15,922 --> 00:42:20,266
come into picture of being really helpful here you'll

668
00:42:20,290 --> 00:42:24,386
see a small logs tab on the top. And once I click on this,

669
00:42:24,570 --> 00:42:28,370
you'll see there are a couple of log lines which are having

670
00:42:28,562 --> 00:42:31,854
from particular request and that specific function.

671
00:42:32,314 --> 00:42:35,664
Now, in a scenario where you have tons of requests and

672
00:42:35,754 --> 00:42:38,996
that particular trace, and a particular trace fails, you would want

673
00:42:39,020 --> 00:42:42,900
to understand that particular log. And to go through the tons of

674
00:42:42,932 --> 00:42:46,356
logs is already tedious. Having the right context

675
00:42:46,460 --> 00:42:49,944
helps you get to the root cause really quick.

676
00:42:50,364 --> 00:42:54,348
In this case, I'm able to reach that particular log line without having to navigate

677
00:42:54,396 --> 00:42:58,500
away much. The other way to reach to this particular stage is through

678
00:42:58,532 --> 00:43:01,940
log screen. Let's say I'm exploring logs from all

679
00:43:01,972 --> 00:43:05,356
these services and I see there are a couple of errors and I want

680
00:43:05,380 --> 00:43:08,460
to understand what trace was this that actually

681
00:43:08,532 --> 00:43:12,436
failed. I see that trace id is already attached and

682
00:43:12,460 --> 00:43:16,028
there is a log message that's also available. But what

683
00:43:16,116 --> 00:43:19,692
is also available is getting to that specific request directory.

684
00:43:19,828 --> 00:43:23,684
Once I click on this request, you'll see that it opens directly in that trace.

685
00:43:23,804 --> 00:43:27,660
And this completes the cycle of combining

686
00:43:27,692 --> 00:43:31,704
the traces, logs and metrics. It provides you the complete metrics.

687
00:43:31,804 --> 00:43:35,576
The trace cycle and the logs thus

688
00:43:35,680 --> 00:43:39,024
operate can help you avoid pointing

689
00:43:39,064 --> 00:43:42,448
fingers to dev or ops. Also helps you reach

690
00:43:42,496 --> 00:43:46,564
to the root cause of your application problem very quickly.

691
00:43:47,024 --> 00:43:50,792
Now that we have seen how we get started with automatic instrumentation

692
00:43:50,848 --> 00:43:54,344
for load j's and capture traces, logs and metrics,

693
00:43:54,464 --> 00:43:58,612
let's look at another important piece which is open telemetry collector.

694
00:43:58,808 --> 00:44:02,276
The collector is a very important piece and part

695
00:44:02,300 --> 00:44:06,364
of open elementary which helps you capture information from

696
00:44:06,444 --> 00:44:10,236
your infrastructure as well as your microservices and different

697
00:44:10,300 --> 00:44:15,116
applications. The collector is built with three different components,

698
00:44:15,300 --> 00:44:19,316
receivers, processors and exporters. Receivers is

699
00:44:19,380 --> 00:44:23,076
where we define how we want to get the data into

700
00:44:23,180 --> 00:44:26,400
collector which can be push or pull with the

701
00:44:26,432 --> 00:44:29,816
application automatic instrumentation that we have covered. We are

702
00:44:29,840 --> 00:44:33,784
using the push based mechanism where we are sending all the telemetry using

703
00:44:33,824 --> 00:44:37,376
the SDKs processors is which where we

704
00:44:37,400 --> 00:44:40,560
have we can define how we want to process our telemetry.

705
00:44:40,672 --> 00:44:44,480
We can modify, attach any custom attribute or

706
00:44:44,512 --> 00:44:47,244
even talk attributes that we do not want.

707
00:44:47,544 --> 00:44:51,072
Exporters again work on the same principle of push or pull with

708
00:44:51,088 --> 00:44:54,240
where we can export the telemetry to multiple

709
00:44:54,312 --> 00:44:57,472
or single packets. Basically it acts as

710
00:44:57,488 --> 00:45:00,896
a proxy where multiple telemetry formats can work as

711
00:45:00,920 --> 00:45:04,472
an agent or a gateway. If you want to scale the character, you can send

712
00:45:04,488 --> 00:45:08,216
it up behind a load balancer and it

713
00:45:08,240 --> 00:45:12,096
can scale up as per your requires. Here's a simple example

714
00:45:12,120 --> 00:45:15,632
of open data collector where we are using the configuration

715
00:45:15,688 --> 00:45:19,708
file to add a receiver for collecting host metrics.

716
00:45:19,836 --> 00:45:23,412
You'll see once we add a simple host matrix block in a Yami file,

717
00:45:23,508 --> 00:45:26,844
we are able to capture all this information which is system dot memory

718
00:45:26,884 --> 00:45:30,500
utilization file system information, networking and

719
00:45:30,532 --> 00:45:34,116
aging information. There are tons of more information that you can capture.

720
00:45:34,260 --> 00:45:37,744
All you have to do is define in the receivers block for host memories.

721
00:45:38,444 --> 00:45:42,584
One of the important concepts with collector is how we sample our data.

722
00:45:43,184 --> 00:45:46,848
We have seen different processors for span and long which is simple,

723
00:45:46,896 --> 00:45:50,672
and batch processors with connector. The concept of sampling comes into

724
00:45:50,728 --> 00:45:54,032
picture of what we want to capture and how we want to

725
00:45:54,048 --> 00:45:57,016
capture. There are two most famous strategies,

726
00:45:57,160 --> 00:46:00,664
one of which is head based sampling and another is stain based sampling.

727
00:46:00,824 --> 00:46:04,368
Headbase sampling is the default that is enabled where it

728
00:46:04,416 --> 00:46:08,136
can just overall statistical sampling of all the requests that are

729
00:46:08,160 --> 00:46:12,546
coming through and the tail based sampling is something that

730
00:46:12,690 --> 00:46:16,146
captures and gives you the information for most actionable

731
00:46:16,210 --> 00:46:19,874
trace that are sampled. It helps you identify the

732
00:46:19,914 --> 00:46:23,218
portion of the trace data instead of the overall statistics of the

733
00:46:23,226 --> 00:46:26,786
data. Tail based sampling is recommended where you want to

734
00:46:26,810 --> 00:46:31,066
get only the right data instead of the tons of data that are coming through

735
00:46:31,250 --> 00:46:35,314
and flowing in from across your systems. You'll see

736
00:46:35,694 --> 00:46:39,342
how it changes depending on the sampling strategy on the left hand

737
00:46:39,358 --> 00:46:42,926
side in the configuration you will see in the processors block, we are defining a

738
00:46:42,950 --> 00:46:46,430
policy to enable the database sampling. You'll see the

739
00:46:46,542 --> 00:46:50,262
throughput that has changed before and after. Before we

740
00:46:50,278 --> 00:46:53,686
apply the database sampling, there has been a lot of throughput and cpu cycles

741
00:46:53,750 --> 00:46:57,886
that collector was consuming. This is because it tries to process all

742
00:46:57,910 --> 00:47:01,694
the information that is coming in and tries to export that.

743
00:47:02,154 --> 00:47:06,034
With tb sampling we can reduce that throughput

744
00:47:06,114 --> 00:47:09,898
and also reduce the number of spans that we are sending, which becomes

745
00:47:09,946 --> 00:47:13,434
easier for any engineer to start debugging and

746
00:47:13,474 --> 00:47:16,746
see only the actionable samples. There is

747
00:47:16,770 --> 00:47:20,266
another form of sampling which is probabilistic sampling. This is

748
00:47:20,330 --> 00:47:23,294
entirely different from head based and tail based sampling.

749
00:47:23,594 --> 00:47:27,254
In probabilistic sampling, you set the number of

750
00:47:27,334 --> 00:47:30,966
the set the sampling percentage of how many samples

751
00:47:31,070 --> 00:47:35,806
you want to capture from that particular system. It can be 15%

752
00:47:35,870 --> 00:47:39,950
to 60% or even 100%. This is particularly

753
00:47:39,982 --> 00:47:43,022
recommended, in my own opinion, that you can start using

754
00:47:43,078 --> 00:47:46,662
for any new projects that you are deploying. This helps you understand the

755
00:47:46,678 --> 00:47:50,094
behavior of your system and once you have understood what

756
00:47:50,134 --> 00:47:54,398
percentage of samples you want, you can switch to database sampling and

757
00:47:54,486 --> 00:47:57,790
refine your policies to get the most actionable samples from

758
00:47:57,822 --> 00:48:01,806
that particular service or infrastructure. Let me show you the

759
00:48:01,830 --> 00:48:05,974
configuration file of the collector that I was using locally to

760
00:48:06,054 --> 00:48:09,638
export all the telemetry from our node j's application and

761
00:48:09,686 --> 00:48:13,446
also how you can get started with collector by

762
00:48:13,590 --> 00:48:17,110
running it simply in a docker container. You can

763
00:48:17,142 --> 00:48:21,420
start using the open telemetry collector locally by running the docker image.

764
00:48:21,612 --> 00:48:25,180
There are various versions of open telemetry collector image that are available

765
00:48:25,252 --> 00:48:28,500
on the Docker hub. One particular image that you should

766
00:48:28,532 --> 00:48:33,004
be using is open telemetry collector iPhone contrib. This contains

767
00:48:33,124 --> 00:48:36,924
most of the processors, exporters and receivers

768
00:48:37,084 --> 00:48:40,500
which are not available in the primary mainstream branch

769
00:48:40,532 --> 00:48:44,556
which is open telemetry collector. The country version is where most

770
00:48:44,580 --> 00:48:47,780
of the community and the different plugins

771
00:48:47,852 --> 00:48:51,436
are available and are being contributed to. One thing

772
00:48:51,460 --> 00:48:55,044
that you need to be mindful is you need to have the ports 4317

773
00:48:55,124 --> 00:48:58,412
and 4318 open. You can get all this information

774
00:48:58,508 --> 00:49:02,524
directly from the open elementary country, GitHub, repo or the Docker

775
00:49:02,604 --> 00:49:06,124
page. Once you have this container up and running,

776
00:49:06,244 --> 00:49:10,104
you can start using the collector to receive the metrics and export it.

777
00:49:10,604 --> 00:49:14,450
Since we have already configured our application with instrumentation,

778
00:49:14,602 --> 00:49:18,930
let me show you what the configuration file that we use. The collector

779
00:49:19,002 --> 00:49:22,418
requires a config YAML to be present with which it's

780
00:49:22,466 --> 00:49:26,066
actually driven. There the three main components that we talked about,

781
00:49:26,130 --> 00:49:29,082
the processors, receivers and exporters.

782
00:49:29,218 --> 00:49:32,974
These. These are the three blocks that are made within the collector

783
00:49:33,434 --> 00:49:36,970
and with this we are able to configure how we

784
00:49:37,002 --> 00:49:40,450
want to receive the telemetry. What we

785
00:49:40,482 --> 00:49:44,226
want to do with the telemetry if you want to process that,

786
00:49:44,330 --> 00:49:48,098
attach any custom attributes and where we want to export it,

787
00:49:48,226 --> 00:49:51,826
we can see all the debug output with collector as well, similar to

788
00:49:51,850 --> 00:49:55,794
the SDKs. By adding the debug attribute and

789
00:49:55,914 --> 00:49:59,770
where we want to export it, here we've here I'm

790
00:49:59,802 --> 00:50:03,426
exporting it to newly. So I've added the exporter endpoint,

791
00:50:03,490 --> 00:50:07,910
which is OTLP endpoint dot in our data. With my particular license,

792
00:50:08,062 --> 00:50:11,790
you can add multiple exporters to different observability backends,

793
00:50:11,862 --> 00:50:15,486
or if you just want to extort it in time series database, you can

794
00:50:15,510 --> 00:50:19,554
do so too. The particular important

795
00:50:19,934 --> 00:50:22,874
block in the configuration is the service pipelines.

796
00:50:23,374 --> 00:50:26,874
Here is where you will enable all these processors,

797
00:50:27,494 --> 00:50:31,814
receivers and exporters. In the pipelines

798
00:50:31,894 --> 00:50:35,056
we we add what to enable, for example for receivers,

799
00:50:35,120 --> 00:50:38,808
I'm enabling OTLB for all of the traces, matrix and logs

800
00:50:38,936 --> 00:50:42,856
for processors. What processors to be used for individual telemetry

801
00:50:43,040 --> 00:50:46,544
and for logs for exporters. Which one you want to export?

802
00:50:46,704 --> 00:50:50,440
You may not want to export everything, but you just want to start debugging.

803
00:50:50,512 --> 00:50:54,016
You can remove the exporter from the pipeline and the collector will

804
00:50:54,040 --> 00:50:57,944
still process that without exporting it. For example, in the processors for

805
00:50:57,984 --> 00:51:01,164
logs and I'm attaching a custom attribute which is the environment,

806
00:51:01,744 --> 00:51:05,936
you can choose to add multiple processes for only one

807
00:51:05,960 --> 00:51:09,768
telemetry or all of it. With that, I want to

808
00:51:09,896 --> 00:51:13,244
conclude my topic of open telemetry 101 today

809
00:51:13,784 --> 00:51:17,536
to just recap and give you a few highlights of everything that I've covered.

810
00:51:17,680 --> 00:51:21,400
First of all, it's an exciting time for open source observability.

811
00:51:21,592 --> 00:51:25,288
Open elementary is growing and being adopted at a very rapid pace,

812
00:51:25,416 --> 00:51:29,032
and not just in terms of contribution from the community, but also in

813
00:51:29,048 --> 00:51:31,910
terms of adoption. Like GitHub,

814
00:51:32,062 --> 00:51:36,054
Microsoft, Neuralink are contributing heavily to include it

815
00:51:36,094 --> 00:51:40,422
in their own ecosystems. But you need to be mindful with your maturity

816
00:51:40,518 --> 00:51:44,086
and you have to plan ahead with the adoption of open telemetry. Start with

817
00:51:44,110 --> 00:51:48,630
automatic instrumentation and then advance towards manual instrumentation

818
00:51:48,742 --> 00:51:51,958
as a way to understand and mature of what is

819
00:51:52,006 --> 00:51:55,574
important within your system. Just having a form of automatic

820
00:51:55,614 --> 00:51:58,994
instrumentation and collecting telemetry is not observability.

821
00:51:59,444 --> 00:52:03,604
Your instrumentation should include a proper contextual information for traces,

822
00:52:03,684 --> 00:52:06,772
logs and metrics to improve observation. Remember the

823
00:52:06,788 --> 00:52:10,244
example that we covered where we are able to see logs, metrics,

824
00:52:10,284 --> 00:52:13,884
errors and traces all in a single place. That is

825
00:52:13,924 --> 00:52:17,300
the complete powerful observability system where

826
00:52:17,332 --> 00:52:20,904
you are able to reach to the root cause of your problems.

827
00:52:21,244 --> 00:52:25,036
You can deploy the collector easily. Together, there are multiple options that are

828
00:52:25,060 --> 00:52:28,516
available. You can deploy it as a standalone, as an agent,

829
00:52:28,620 --> 00:52:32,172
or as a gateway behind a load balancer. Or if

830
00:52:32,188 --> 00:52:35,732
you are using kubernetes you can also deploy it on Kubernetes in various

831
00:52:35,788 --> 00:52:39,460
modes of demonstrate, stateful, set or even as a Kubernetes

832
00:52:39,532 --> 00:52:43,572
operator. You can start collecting data from all your pipelines

833
00:52:43,628 --> 00:52:46,972
as well as multiple distributed systems which can help

834
00:52:47,028 --> 00:52:49,532
you with your MTDI,

835
00:52:49,628 --> 00:52:53,100
MTD and MTDR. One of my final advice

836
00:52:53,132 --> 00:52:56,444
that I would like to close with is there is a lot of active investment

837
00:52:56,524 --> 00:53:01,018
going on with open telemetry and it helps engineers

838
00:53:01,186 --> 00:53:04,714
work based on data and not opinion. Thank you

839
00:53:04,754 --> 00:53:05,514
and I'll see you next time.

