1
00:00:27,040 --> 00:00:31,068
Today I am going to talk about, from DevOps to Mlops,

2
00:00:31,236 --> 00:00:34,884
a journey of scaling machine learning models to 2 million

3
00:00:34,924 --> 00:00:38,500
API requests per day. So before we dive in

4
00:00:38,692 --> 00:00:42,460
a brief about me, I am Chinmay. You can find me on

5
00:00:42,492 --> 00:00:45,824
Twitter LinkedIn etcetera via Chinmay 185.

6
00:00:46,364 --> 00:00:49,852
I am a founder at company called 120 n where we help startup

7
00:00:49,908 --> 00:00:53,664
and enterprises with backend and site reliability engineering.

8
00:00:54,244 --> 00:00:57,964
I write stories of our work in

9
00:00:58,124 --> 00:01:01,564
what is called pragmatic software engineering. These stories, I published

10
00:01:01,604 --> 00:01:04,908
them on Twitter LinkedIn, etcetera. I love engineering,

11
00:01:04,996 --> 00:01:08,892
psychology, percussion, and I am a huge fan of

12
00:01:09,068 --> 00:01:12,676
a game called Age of Empires. All right, so let's start.

13
00:01:12,860 --> 00:01:16,564
So what's, what are we covering today? We are covering three

14
00:01:16,604 --> 00:01:19,624
things fundamentally. One is what is mlops?

15
00:01:20,324 --> 00:01:23,454
How do you think mlops for DevOps practitioners?

16
00:01:24,114 --> 00:01:28,026
Fundamentally, I want to talk more about, and spend more time in talking about

17
00:01:28,090 --> 00:01:31,794
a real world production case study that we worked on, which will

18
00:01:31,914 --> 00:01:35,494
talk about all the learnings that we had into

19
00:01:36,034 --> 00:01:39,146
in a case study kind of walkthrough. So what

20
00:01:39,170 --> 00:01:43,234
is mlops fundamentally? Mlops is operationalizing

21
00:01:43,274 --> 00:01:47,782
data science. We all know what DevOps is. DevOps is operationalizing

22
00:01:47,978 --> 00:01:50,438
software delivery, software engineering.

23
00:01:50,566 --> 00:01:54,734
Similarly, MlOps is equivalent to DevOps

24
00:01:54,814 --> 00:01:58,286
in a sense. It talks about operationalizing data science

25
00:01:58,350 --> 00:02:01,990
workloads. Think of machine learning AI ML

26
00:02:02,022 --> 00:02:05,950
workloads essentially, right? So that means it is all

27
00:02:05,982 --> 00:02:09,914
about moving machine learning workloads to production.

28
00:02:10,454 --> 00:02:14,306
Just like we have DevOps phases, we have various phases in mlops.

29
00:02:14,470 --> 00:02:17,794
Fundamentally, it's like build, where you build the models,

30
00:02:17,954 --> 00:02:21,290
you manage various versions of the models. For example,

31
00:02:21,442 --> 00:02:25,418
you deploy these models on production. You monitor, you take feedback,

32
00:02:25,506 --> 00:02:29,002
you continuously improve the models, etcetera. So these are various four steps

33
00:02:29,018 --> 00:02:32,450
of mlops. Let's look at some of them in more

34
00:02:32,482 --> 00:02:35,850
detail. Right? So fundamentally, just like

35
00:02:35,882 --> 00:02:39,134
software engineering is about building and shipping code to production,

36
00:02:39,594 --> 00:02:43,152
MLOps is, is about building machine learning models.

37
00:02:43,288 --> 00:02:46,964
Now, what do you need for building machine learning models? You need data.

38
00:02:47,544 --> 00:02:51,080
Data. You need to extract this data in various forms.

39
00:02:51,192 --> 00:02:54,152
You will need to analyze it. You will need to sort of prune some parts

40
00:02:54,168 --> 00:02:57,324
of data. Essentially, you are doing data preparation and gathering.

41
00:02:57,704 --> 00:03:01,296
Then you feed in this data to your ML model. You will

42
00:03:01,360 --> 00:03:04,744
train the model. You will evaluate models response, you will

43
00:03:04,784 --> 00:03:07,960
test and validate whether the model works correctly or not.

44
00:03:08,032 --> 00:03:11,304
You will fine tune this process over time. You know, you have

45
00:03:11,344 --> 00:03:15,044
test data segregation, you will have production data, stuff like that.

46
00:03:15,784 --> 00:03:19,264
This is all the machine learning part of it, which is what data scientists

47
00:03:19,344 --> 00:03:23,040
work on. Now, the operational parts of it are model

48
00:03:23,072 --> 00:03:26,152
serving, how do you serve this model to production? How do

49
00:03:26,168 --> 00:03:30,008
you run this on GPU's? Do you run this on cpu's? Which cloud provider do

50
00:03:30,016 --> 00:03:33,800
you want to use? How do you monitor the model, whether it's performing as

51
00:03:33,832 --> 00:03:37,140
per expectations or not? How do you manage scale up and scale down of that

52
00:03:37,172 --> 00:03:41,104
model? All of that is the operational concern, which is the ops part of it.

53
00:03:41,884 --> 00:03:44,668
Fundamentally, this is around a feedback loop,

54
00:03:44,836 --> 00:03:48,428
just like in software. We have Ci CD continuous integration and

55
00:03:48,476 --> 00:03:51,596
continuous delivery deployment. We have a third parameter,

56
00:03:51,660 --> 00:03:54,948
or third item in mlops called continuous

57
00:03:54,996 --> 00:03:58,716
testing and training, where you are going to continuously

58
00:03:58,820 --> 00:04:02,762
monitor and train the model and improve

59
00:04:02,818 --> 00:04:06,746
the model over the period of time. So here is

60
00:04:06,770 --> 00:04:09,786
what simplest mlops workflow looks

61
00:04:09,810 --> 00:04:12,826
like. This diagram is from sort of

62
00:04:12,850 --> 00:04:16,214
Google's mlops guide. You can find the link in the description.

63
00:04:16,874 --> 00:04:20,914
Fundamentally, again, it starts with getting data. So we are trying to map all these

64
00:04:20,954 --> 00:04:24,346
previous steps and phases that we looked at into this model.

65
00:04:24,490 --> 00:04:27,928
So we're going to get some data from various sources.

66
00:04:28,106 --> 00:04:31,372
It could be offline data, it could be real time data, things like that.

67
00:04:31,428 --> 00:04:35,276
For now, we're keeping the diagram very simple and just looking at some

68
00:04:35,340 --> 00:04:38,604
offline data. For example, we are going to extract this data,

69
00:04:38,724 --> 00:04:41,264
analyze it, prepare that data essentially.

70
00:04:41,604 --> 00:04:45,348
And there's a second step. Then the whole model

71
00:04:45,396 --> 00:04:48,940
training step appears where you are going to train the model, you're going to evaluate

72
00:04:48,972 --> 00:04:52,228
the model, you're going to check the performance, you're going to manage various

73
00:04:52,276 --> 00:04:55,454
versions, validations, etcetera. Finally, you have a train

74
00:04:55,494 --> 00:04:58,422
model which you put into model registry.

75
00:04:58,598 --> 00:05:01,670
Now once that model registry has

76
00:05:01,702 --> 00:05:05,070
the model, the operational part of the mlops comes

77
00:05:05,102 --> 00:05:08,342
into picture, which is serving the model, and then which is

78
00:05:08,358 --> 00:05:12,030
where you have, for example, a prediction service which you can

79
00:05:12,062 --> 00:05:15,654
run on production. Then you have to monitor, scale that service,

80
00:05:15,774 --> 00:05:18,990
run this on GPU's, figure out cloud cost optimizations,

81
00:05:19,062 --> 00:05:22,510
etcetera, around all of that. So that's operationalizing data

82
00:05:22,542 --> 00:05:26,438
science. That's the simplest mlops

83
00:05:26,526 --> 00:05:30,318
flow that you can think of. You can also map this into

84
00:05:30,446 --> 00:05:34,094
a classic DevOps Infinity loop. So the typical

85
00:05:34,134 --> 00:05:37,518
DevOps Infinity loop talks about your code, build, test,

86
00:05:37,646 --> 00:05:40,486
plan, then release, deploy, operate, monitor,

87
00:05:40,590 --> 00:05:44,470
and doing this in a loop consistently over long periods of time.

88
00:05:44,662 --> 00:05:48,776
So similarly for mlops, it kind of starts with

89
00:05:48,950 --> 00:05:52,292
having data preparation. You're going to prepare the data,

90
00:05:52,428 --> 00:05:55,236
you're going to train the model. Again, that's a build and a test part of

91
00:05:55,260 --> 00:05:58,904
it. Then the release will go into model registry.

92
00:05:59,524 --> 00:06:02,604
You're going to then monitor the model performance.

93
00:06:02,724 --> 00:06:04,944
You're going to deploy the model, monitor the performance,

94
00:06:05,404 --> 00:06:09,388
and all of this altogether would be continuous training

95
00:06:09,436 --> 00:06:13,140
and testing of the model. Right? Enough about theory.

96
00:06:13,212 --> 00:06:17,320
What I want to talk more about is a

97
00:06:17,352 --> 00:06:20,616
use case that we worked on. So this is a production work that we worked

98
00:06:20,640 --> 00:06:23,712
on. I'm going to cover the use case at

99
00:06:23,728 --> 00:06:26,976
a high level. I'm going to talk about what work we did,

100
00:06:27,120 --> 00:06:30,120
how we applied the DevOps and mlops practices,

101
00:06:30,272 --> 00:06:33,928
best practices in production, and the kind of issues

102
00:06:33,976 --> 00:06:36,952
that we faced during the production journey.

103
00:06:37,048 --> 00:06:40,392
Right. So let's start with the case study that we

104
00:06:40,408 --> 00:06:44,024
had in mind. We were working on.

105
00:06:44,484 --> 00:06:48,204
We were working with a company which was building Ekyc SaaS

106
00:06:48,244 --> 00:06:51,996
APIs, which was accessible to B two

107
00:06:52,020 --> 00:06:55,348
B and B two C customers. This needed to scale

108
00:06:55,436 --> 00:06:59,292
up to 2 million API requests per day to the model. Now,

109
00:06:59,428 --> 00:07:02,844
the SaaS APIs, these three or four APIs

110
00:07:02,884 --> 00:07:06,844
that we had, one was face matching API. Imagine you

111
00:07:06,884 --> 00:07:11,154
provide two images to the model. You're going to have to match

112
00:07:11,274 --> 00:07:14,946
the face between two images and model outputs

113
00:07:14,970 --> 00:07:18,370
a score between say zero and one. Based on the

114
00:07:18,402 --> 00:07:21,618
matching score, you can decide if the two images, if the two people

115
00:07:21,666 --> 00:07:25,882
in this images are same or not, and that's a face matching API,

116
00:07:26,018 --> 00:07:29,434
then we have face liveness detection, which is if you

117
00:07:29,474 --> 00:07:33,026
have an image of a face, is this face of a

118
00:07:33,050 --> 00:07:36,494
live person, or is this a face of a non alive person?

119
00:07:37,094 --> 00:07:40,254
Then we had an OCR or optical character recognition

120
00:07:40,414 --> 00:07:44,486
from an image. For example, you upload a photo of a passport or any

121
00:07:44,630 --> 00:07:48,366
identity card, you would be able to extract the text

122
00:07:48,430 --> 00:07:52,214
information from it. So that imagine this Kyc

123
00:07:52,254 --> 00:07:55,686
use case for an insurance or a telecom or any other domain.

124
00:07:55,830 --> 00:07:59,758
People would have to manually enter a lot of information for the user, like their

125
00:07:59,806 --> 00:08:02,724
name, their date of birth, their address, etcetera.

126
00:08:02,854 --> 00:08:07,000
All of this information gets captured via the OCR

127
00:08:07,032 --> 00:08:11,192
API and you get that information returned via response in a structured fashion.

128
00:08:11,328 --> 00:08:14,884
So that eliminates having to type and mistype information.

129
00:08:16,344 --> 00:08:20,120
So similarly, we had some other small APIs as well, which I'll ignore for now.

130
00:08:20,272 --> 00:08:24,096
So fundamentally, we had this ML system,

131
00:08:24,200 --> 00:08:27,568
and the architecture of that system, along with other components was something

132
00:08:27,616 --> 00:08:31,140
like this. So it was served mainly for

133
00:08:31,172 --> 00:08:34,124
b two b use cases. We also had some things for b, two c,

134
00:08:34,164 --> 00:08:37,380
but again, we'll ignore that for now. So imagine from a b two b use

135
00:08:37,412 --> 00:08:40,636
case. I'm an insurance or a telecom company.

136
00:08:40,820 --> 00:08:44,564
I have my own app, and there is a client SDK in that app

137
00:08:44,604 --> 00:08:48,244
that I need to install and run. I need to package the client SDK

138
00:08:48,284 --> 00:08:51,764
as part of my app, now this SDK talks to our backend

139
00:08:51,804 --> 00:08:55,892
APIs, which are these ML APIs exposed via an

140
00:08:56,028 --> 00:09:00,228
HTTP API, for example. So it connects to for example load balancer.

141
00:09:00,356 --> 00:09:04,316
We have an API layer then to

142
00:09:04,340 --> 00:09:08,132
be able to serve these models. We had a RabbitMQ as another like a

143
00:09:08,148 --> 00:09:11,820
queue mechanism where we would push messages.

144
00:09:12,012 --> 00:09:15,596
For example, we want to map match two images,

145
00:09:15,660 --> 00:09:18,812
right? Face recognition or face matching across two images.

146
00:09:18,988 --> 00:09:22,748
We would create a message in RabbitMQ, push that message on the

147
00:09:22,756 --> 00:09:26,212
rabbitmQ. And there is these background workers, which are these ML

148
00:09:26,268 --> 00:09:30,228
workers, which would run, which would accept message from the RabbitMQ.

149
00:09:30,356 --> 00:09:33,700
They would do the processing, they would update the results in database.

150
00:09:33,812 --> 00:09:36,940
Maybe they would even save the results in a cache

151
00:09:36,972 --> 00:09:40,484
like redis, which we had. And the images themselves

152
00:09:40,524 --> 00:09:44,052
can be stored in a distributed file store. Could be minio,

153
00:09:44,108 --> 00:09:46,104
could be s, three things like that.

154
00:09:47,504 --> 00:09:50,600
Essentially these workers would perform the bulk of the task and

155
00:09:50,632 --> 00:09:53,744
then the API would return the results to the user.

156
00:09:53,864 --> 00:09:57,400
So this is the kind of architecture we had. Now what were

157
00:09:57,432 --> 00:10:00,216
the requirements from a slo point of view?

158
00:10:00,400 --> 00:10:03,576
So we set out for achieving at least

159
00:10:03,600 --> 00:10:06,884
like two nines of availability, that is, two nines of uptime

160
00:10:07,264 --> 00:10:10,496
during peak hours or during our business hours. Because we were

161
00:10:10,520 --> 00:10:13,896
dealing with a b two b company, there typically would be

162
00:10:13,920 --> 00:10:17,680
business hours. Typically stores would open at 09:00 a.m. In the morning and

163
00:10:17,712 --> 00:10:20,760
would go on till like 10:00 a.m. 10:00 p.m. In the night, for example.

164
00:10:20,792 --> 00:10:24,848
Right, the local time. So we had promised like two lines of uptime

165
00:10:24,936 --> 00:10:29,040
during that. In terms of SLO, we obviously

166
00:10:29,072 --> 00:10:32,160
had to worry about costs and optimizing the costs as

167
00:10:32,192 --> 00:10:36,248
an important requirement. From SLo point of view, we hadn't defined specific

168
00:10:36,296 --> 00:10:40,352
metrics, but we'll get to that later. And then from an API point of view,

169
00:10:40,488 --> 00:10:43,776
these were synchronous APIs as far as the user and the SDK is

170
00:10:43,800 --> 00:10:47,128
concerned. So less than three second API latency for

171
00:10:47,176 --> 00:10:50,804
95th percentile. That was our goal that we had set out.

172
00:10:51,344 --> 00:10:55,160
Now, given this, let's think about our architecture

173
00:10:55,272 --> 00:10:58,520
and we set out to build a cloud agnostic architecture.

174
00:10:58,552 --> 00:11:01,444
I'll cover more of that soon. And why that is.

175
00:11:02,104 --> 00:11:05,864
So, for example, for the storing of actual

176
00:11:05,904 --> 00:11:09,392
images, which were ephemeral for short time and whatnot, because again we're

177
00:11:09,408 --> 00:11:13,460
dealing with sensitive data. We were using s three

178
00:11:13,612 --> 00:11:17,156
if we are deployed on AWS, we were using gcs if we were deployed

179
00:11:17,180 --> 00:11:20,516
on GCP, and Minio if we were deployed on on

180
00:11:20,540 --> 00:11:24,624
premise. One of the reason was that we wanted to create,

181
00:11:25,404 --> 00:11:29,156
we wanted to have same code, could use different type

182
00:11:29,180 --> 00:11:32,340
of image store without having to change the code a lot

183
00:11:32,372 --> 00:11:35,748
or without no change to the code at all. Why? Because then

184
00:11:35,796 --> 00:11:39,380
we could deploy this entire stack on any cloud.

185
00:11:39,452 --> 00:11:42,876
We could run it on on premise. We could even run it on customers premise.

186
00:11:42,940 --> 00:11:46,464
We could run it on AWS, GCP or any other cloud for that matter.

187
00:11:46,884 --> 00:11:50,636
This was the main point that we wanted to achieve. That's why

188
00:11:50,660 --> 00:11:53,964
we set out to have a cloud agnostic architecture where we don't

189
00:11:54,004 --> 00:11:58,396
use a very cloud specific component and then we are tied to that particular cloud

190
00:11:58,540 --> 00:12:01,996
as a code dependency. Now that's

191
00:12:02,020 --> 00:12:05,532
for Binayo. For Redis we could either go with

192
00:12:05,548 --> 00:12:09,508
self hosted redis which is basically a cache for storing

193
00:12:09,636 --> 00:12:13,052
bunch of latest computation. That API can quickly return the

194
00:12:13,068 --> 00:12:16,812
results to the users. We could do this as a self hosted

195
00:12:16,948 --> 00:12:20,704
redis or elasticache if you are on one of the cloud providers.

196
00:12:21,124 --> 00:12:24,604
Now for RabbitMQ, we again choose RabbitMQ

197
00:12:24,684 --> 00:12:27,700
purely so that we could run this on premise easily.

198
00:12:27,812 --> 00:12:31,160
And if you were on the cloud, we could use something like

199
00:12:31,192 --> 00:12:35,576
sqs or like equivalent in India cloud for

200
00:12:35,680 --> 00:12:39,760
GPU's and workers, which were predominantly GPU workload,

201
00:12:39,872 --> 00:12:43,344
we would use them on one of the cloud providers,

202
00:12:43,384 --> 00:12:46,864
or we could get our own custom GPU's, et cetera. For this

203
00:12:46,904 --> 00:12:50,352
production use case we were on one of the cloud providers and so we

204
00:12:50,368 --> 00:12:54,232
use most of the cloud components, but our code was such that it was

205
00:12:54,368 --> 00:12:58,244
not tightly coupled to cloud at all. And for database, again,

206
00:12:58,664 --> 00:13:02,104
we could use postgres. It could be RDS or cloud

207
00:13:02,144 --> 00:13:05,964
SQL or something else depending on the cloud provider.

208
00:13:07,944 --> 00:13:11,752
We fundamentally had Nomad as the orchestrator which would orchestrate

209
00:13:11,808 --> 00:13:15,568
all the deployments and scaling

210
00:13:15,616 --> 00:13:19,568
of components. Back then we were not using kubernetes purely,

211
00:13:19,616 --> 00:13:23,464
again from a simplicity point of view that we wanted to deploy this whole

212
00:13:23,504 --> 00:13:27,124
stack with the orchestrator on premise and

213
00:13:27,244 --> 00:13:30,396
we didn't want to in the team. We did not have a lot

214
00:13:30,420 --> 00:13:34,556
of Kubernetes expertise to be able to manage self hosted kubernetes

215
00:13:34,700 --> 00:13:38,444
on premise ourselves. So that's where we chose Hashicorp stack,

216
00:13:38,484 --> 00:13:41,732
which is fairly single binary, easy to manage and

217
00:13:41,748 --> 00:13:45,344
easy to run, and we already had expertise in the team for that.

218
00:13:45,884 --> 00:13:49,380
Why cloud agnostic? I think it's a very important point that I want to highlight

219
00:13:49,492 --> 00:13:53,006
because we were cloud agnostic, we could package the same wine

220
00:13:53,030 --> 00:13:56,926
in a different bottle, for example, so we

221
00:13:56,950 --> 00:14:00,366
could package the same stack and run it on any environment

222
00:14:00,430 --> 00:14:04,246
that we wanted to. We could do air gap environments if we had to,

223
00:14:04,390 --> 00:14:07,526
things like that. So this was the main reason why we

224
00:14:07,550 --> 00:14:11,198
went cloud agnostic. And I think one of the lessons to learn

225
00:14:11,246 --> 00:14:15,134
is to build more cloud agnostic systems. That way

226
00:14:15,174 --> 00:14:18,810
you're not tied to any of the cloud providers, although using

227
00:14:18,842 --> 00:14:22,330
cloud providers obviously simplifies a lot of things for you. But you would want

228
00:14:22,362 --> 00:14:25,954
to have your architecture and code not coupled

229
00:14:25,994 --> 00:14:30,018
with the cloud provider so that you can change freely and

230
00:14:30,106 --> 00:14:33,346
migrate to a different cloud if you want to without having to redo

231
00:14:33,370 --> 00:14:37,602
a lot of effort. What did our scaling journey and

232
00:14:37,778 --> 00:14:41,426
how did we go from zero requests to 2 million API requests

233
00:14:41,450 --> 00:14:44,884
per day? Let's talk about that. Obviously it wasn't zero

234
00:14:44,924 --> 00:14:48,564
request one day and 2 million the next day. It was a gradual

235
00:14:48,644 --> 00:14:50,784
scaling journey, something like this.

236
00:14:51,524 --> 00:14:54,700
So we would roll out on few regions

237
00:14:54,732 --> 00:14:58,460
or few stores, and then we would slowly

238
00:14:58,492 --> 00:15:02,300
increment the traffic, we would observe the traffic and so on, so forth.

239
00:15:02,492 --> 00:15:06,348
So fundamentally from our scaling journey, I want to talk about

240
00:15:06,476 --> 00:15:09,724
four or five important points and then drill down on each one of them

241
00:15:09,764 --> 00:15:13,584
as we go through the talk. One is the elimination of single

242
00:15:13,624 --> 00:15:17,496
points of failure to be able to scale. We want to have

243
00:15:17,680 --> 00:15:21,536
zero or no single point of failure so that your system is more

244
00:15:21,600 --> 00:15:25,224
resilient to changes, resilient to failures. We also

245
00:15:25,264 --> 00:15:28,880
need to do good capacity planning so that you are able

246
00:15:28,912 --> 00:15:32,848
to scale up and down very easily and you can save on cloud

247
00:15:32,896 --> 00:15:36,712
costs. Otherwise, if it requires you to scale and you are

248
00:15:36,808 --> 00:15:40,480
having to do a changes to architecture, it causes problems.

249
00:15:40,552 --> 00:15:43,608
So having good capacity planning and how we went about that,

250
00:15:43,696 --> 00:15:47,496
I'll also cover that. Obviously, cost optimization and auto scaling goes

251
00:15:47,520 --> 00:15:50,848
hand in hand with capacity planning. So I'll cover that.

252
00:15:50,976 --> 00:15:54,160
Then comes around a lot of operational aspects about

253
00:15:54,272 --> 00:15:57,968
deployments, observability, being able to debug something,

254
00:15:58,136 --> 00:16:01,816
dealing with production issues, stuff like that. And obviously all

255
00:16:01,840 --> 00:16:04,776
of this journey wasn't very straightforward.

256
00:16:04,920 --> 00:16:08,048
It was fraught with some challenges that we encountered. So I'm going

257
00:16:08,056 --> 00:16:11,616
to cover like two interesting challenges that we encountered along the way. So hopefully

258
00:16:11,640 --> 00:16:14,992
you all can learn from it. So in the next part

259
00:16:15,008 --> 00:16:17,984
of the talk, I'm going to take each one of these points and then go

260
00:16:18,024 --> 00:16:22,280
drill down on each one of them. So let's talk about eliminating

261
00:16:22,392 --> 00:16:25,472
single points of failure. We had

262
00:16:25,608 --> 00:16:29,820
this architecture and just showing the architecture here as, as a, in the background.

263
00:16:29,972 --> 00:16:33,612
So one of the things we did is we added high availability mode

264
00:16:33,628 --> 00:16:37,444
for RabbitMQ. What does that mean? It means we have queue

265
00:16:37,484 --> 00:16:40,980
replication. So whichever queue is there on one machine, it gets

266
00:16:41,012 --> 00:16:45,028
replicated or mirrored onto the other machine. We were running RabbitmQ in

267
00:16:45,036 --> 00:16:48,620
a three node cluster instead of a single single node, for example.

268
00:16:48,812 --> 00:16:52,404
We also had cross AZ deployment for Rabbitmq. So the

269
00:16:52,444 --> 00:16:55,588
three nodes of RapidMQ, each one would run its own easy,

270
00:16:55,676 --> 00:16:59,110
for example. Obviously this was

271
00:16:59,142 --> 00:17:02,846
on premise or a setup where we wanted to host and

272
00:17:02,870 --> 00:17:06,694
manage RabbitMQ ourselves. But if it were a cloud

273
00:17:06,774 --> 00:17:11,030
managed service that we would use, we would use something like sqs, for example.

274
00:17:11,222 --> 00:17:14,982
One of the other things that we did to eliminate single points of failure

275
00:17:15,118 --> 00:17:18,918
is to run ML workloads in multiple azs.

276
00:17:19,046 --> 00:17:22,986
Now back then we had only two azs where we

277
00:17:23,010 --> 00:17:26,962
could have ML GPU's available. The third zone

278
00:17:27,098 --> 00:17:30,362
from the cloud provider did not yet provide the GPU's.

279
00:17:30,458 --> 00:17:34,162
So we had to tweak our logic and deployment and

280
00:17:34,178 --> 00:17:38,178
automation to be able to spin up and load balance between these two acs.

281
00:17:38,266 --> 00:17:42,074
So we would have to fix auto scaling, we would have to fix deployment

282
00:17:42,114 --> 00:17:45,426
automation to be able to run workloads

283
00:17:45,490 --> 00:17:49,024
only on two zones instead of three. For most of the other two cloud

284
00:17:49,064 --> 00:17:53,080
components or most of the other components in the architecture, we would have workloads

285
00:17:53,112 --> 00:17:56,880
run on all three acs. Other thing that

286
00:17:56,992 --> 00:18:00,624
we did is wherever possible, we used SaaS offering for

287
00:18:00,704 --> 00:18:04,016
some of the important stateful systems, like databases,

288
00:18:04,120 --> 00:18:06,944
for example, redis and postgres,

289
00:18:07,104 --> 00:18:10,320
just to make sure that we don't have to manage and

290
00:18:10,352 --> 00:18:13,728
scale those components. Also. And managing and scaling ML

291
00:18:13,816 --> 00:18:17,720
was one of the bigger challenges. So we wanted to offload some of the lower

292
00:18:17,752 --> 00:18:21,910
hanging fruits to the cloud providers. Fundamentally, the idea again is that

293
00:18:21,982 --> 00:18:25,918
scaling and managing stateful components is bit hard and

294
00:18:25,966 --> 00:18:28,994
stateless is much more easier. So wherever possible,

295
00:18:29,454 --> 00:18:33,342
it's easy to automate stateless application scaling, component scaling

296
00:18:33,438 --> 00:18:36,926
and stateful becomes difficult. So that's

297
00:18:36,950 --> 00:18:39,886
on the eliminating single point of failure.

298
00:18:40,030 --> 00:18:43,902
Let's talk about capacity planning. So always when

299
00:18:43,918 --> 00:18:46,898
you think about capacity planning, you think of the bottleneck,

300
00:18:47,046 --> 00:18:50,322
because if the strength of the link is

301
00:18:50,338 --> 00:18:53,786
the strength of the weakest component in the link, for example,

302
00:18:53,890 --> 00:18:57,026
strength of the chain is the strength of the weakest component in the chain.

303
00:18:57,050 --> 00:19:00,618
So you want to find out what's the weakest component and improve

304
00:19:00,706 --> 00:19:04,690
the strength of that component. So for example, if you think about various components

305
00:19:04,722 --> 00:19:08,466
from the architecture, we have API, which is simple

306
00:19:08,650 --> 00:19:11,850
API which is does talk to database and get the results

307
00:19:11,922 --> 00:19:15,258
from database. For example, there is database, which is stateful

308
00:19:15,306 --> 00:19:18,630
component. It could be redis, rabbit, postgres, etcetera,

309
00:19:18,782 --> 00:19:22,118
then mlworkers and something else.

310
00:19:22,166 --> 00:19:25,182
So where do we think is the bottleneck.

311
00:19:25,318 --> 00:19:28,886
Obviously it was on the mlworker side, because that's the component

312
00:19:28,910 --> 00:19:32,750
which takes most time in the request path. So we set out

313
00:19:32,782 --> 00:19:36,574
to figure out, for example, how many ML model requests

314
00:19:36,654 --> 00:19:39,582
can a single node handle. So for example,

315
00:19:39,718 --> 00:19:43,478
if you have a single node with say one gpu with 16 gigs of

316
00:19:43,526 --> 00:19:46,818
GPU memory and a GPU with whatever

317
00:19:46,866 --> 00:19:50,250
few cores, how many workers can I run on that? And how many

318
00:19:50,282 --> 00:19:53,894
requests per second or per hour can I get out of that?

319
00:19:54,754 --> 00:19:58,146
Now, each model, ML model may give

320
00:19:58,170 --> 00:20:02,010
us different results. So for example, face matching may be faster than OCR,

321
00:20:02,162 --> 00:20:06,130
or face liveness detection could be faster than face matching,

322
00:20:06,162 --> 00:20:09,970
for example. So we would run load test and

323
00:20:10,162 --> 00:20:13,626
we would run each of these models, each of the

324
00:20:13,650 --> 00:20:16,986
nodes, and we would run them via a load test to be able to find

325
00:20:17,090 --> 00:20:20,434
the maximum throughput that we can get over a long period of time,

326
00:20:20,474 --> 00:20:24,106
say an hour or two, for example. So again,

327
00:20:24,170 --> 00:20:27,698
I've broken this down into more detail and even more generic format

328
00:20:27,786 --> 00:20:31,226
in another talk that I gave, which is optimizing application performance.

329
00:20:31,330 --> 00:20:34,706
How do you go about it from first principle? So if you're interested, you can

330
00:20:34,730 --> 00:20:39,128
check that out. I'll provide the link in the description, hopefully cost

331
00:20:39,176 --> 00:20:42,936
optimization auto scaling, that is one of the pet peeves given the

332
00:20:42,960 --> 00:20:46,728
current market scenario. So mostly you will have seen, if you

333
00:20:46,736 --> 00:20:50,280
use GPU in cloud, it costs a lot. So how do you go

334
00:20:50,312 --> 00:20:54,272
about optimizing and auto scaling? So you

335
00:20:54,288 --> 00:20:57,520
have to think about what is the parameter on which you can auto scale like,

336
00:20:57,552 --> 00:21:00,912
is it the utilization of CPU or GPU? Could it

337
00:21:00,928 --> 00:21:04,560
be based on memory utilization or number of incoming requests,

338
00:21:04,592 --> 00:21:07,988
for example? Or it could be depth of the queue in case

339
00:21:08,116 --> 00:21:11,572
we were using rapid MQ. So could it be queue depth or

340
00:21:11,628 --> 00:21:15,460
something else? Now we kind of used a combination

341
00:21:15,532 --> 00:21:19,268
of some of these components. So I'll talk about how we went about.

342
00:21:19,396 --> 00:21:22,748
So this is the cost optimization auto scaling,

343
00:21:22,796 --> 00:21:25,956
like our auto scaler, how it works, right? So on the left you

344
00:21:25,980 --> 00:21:29,972
have top left you have a current request rate. This is

345
00:21:29,988 --> 00:21:34,284
the graph where we would have, what's the number of requests we are getting over

346
00:21:34,784 --> 00:21:38,752
last 20 or 30 minutes interval? And we

347
00:21:38,768 --> 00:21:42,856
would have a capacity predictor component which would run every 20 minutes,

348
00:21:43,000 --> 00:21:46,404
which would fetch this request rate, or it would have this information

349
00:21:46,744 --> 00:21:50,400
and it would also get the current node count or current worker

350
00:21:50,432 --> 00:21:53,920
count. So again, imagine we are running these gpu's on workers.

351
00:21:54,112 --> 00:21:57,606
What's the current number of workers that we have currently? So you would

352
00:21:57,630 --> 00:22:01,382
get the current workload, you would get the current

353
00:22:01,518 --> 00:22:05,030
request count. And based on that, based on

354
00:22:05,062 --> 00:22:08,554
the request number of requests and the growth rate of that,

355
00:22:09,094 --> 00:22:12,662
the capacitor, the capacity predictor would kind

356
00:22:12,678 --> 00:22:16,302
of predict, using just simple linear regression, the desired

357
00:22:16,358 --> 00:22:19,926
node count. So for example, if we, for example, open the stores at

358
00:22:19,950 --> 00:22:23,270
09:00 a.m. And we know that we at least need like

359
00:22:23,302 --> 00:22:26,706
50 machines at that point, so we would have time based auto scaling

360
00:22:26,730 --> 00:22:29,890
and we should just spin up 50 machines. You have them ready

361
00:22:30,082 --> 00:22:33,002
at like before ten minutes. The stores open, right?

362
00:22:33,098 --> 00:22:36,650
But the stores open and you continue to see increase in traffic,

363
00:22:36,762 --> 00:22:40,074
you would want to spin up more nodes. If you see decrease in traffic,

364
00:22:40,114 --> 00:22:43,594
typically during lunchtime, you would want to spin down a couple of nodes,

365
00:22:43,634 --> 00:22:47,274
for example, right? So we had this capacity predictor component

366
00:22:47,314 --> 00:22:50,682
which would take this request rate of growth or

367
00:22:50,698 --> 00:22:54,868
rate of decline, you would take the current node count, and based

368
00:22:54,916 --> 00:22:58,748
on the linear regression math and some other parameters that we talked about,

369
00:22:58,916 --> 00:23:02,636
it would predict the desired node count. Now this is

370
00:23:02,660 --> 00:23:06,220
the count that would go as an input to auto scalar. The autoscaler

371
00:23:06,252 --> 00:23:09,884
would then do couple of things. It would update Nomad.

372
00:23:09,924 --> 00:23:13,100
It would tell Nomad, hey, can you please spin up those many number

373
00:23:13,132 --> 00:23:16,888
of components or those many number of workers? So the Nomad

374
00:23:17,076 --> 00:23:20,856
update cluster configuration would run and the Nomad would correspondingly

375
00:23:20,880 --> 00:23:24,184
spin up more nodes. It will deploy the model on those nodes,

376
00:23:24,224 --> 00:23:27,224
etcetera, and it will also update, for example, if you're using slack.

377
00:23:27,264 --> 00:23:30,600
So it will also update us on slack. That, yeah, we've got two more

378
00:23:30,632 --> 00:23:34,664
nodes added, or we've got two nodes destroyed because it was less traffic

379
00:23:34,704 --> 00:23:38,008
time. For example, one of the reason we built

380
00:23:38,056 --> 00:23:42,288
this kind of a system is so that we have a manual override

381
00:23:42,416 --> 00:23:45,576
at any point. If we knew that there is a big campaign going on,

382
00:23:45,600 --> 00:23:49,600
are we going to scale, we are going to have to scale the machines

383
00:23:49,752 --> 00:23:53,424
at a particular time or due to some other kind of business

384
00:23:53,584 --> 00:23:57,288
constraints, we would be able to manually overwrite that value and

385
00:23:57,416 --> 00:24:00,672
change the configuration to be able to spin up those many

386
00:24:00,728 --> 00:24:04,024
number of nodes. This gave us a lot of control and

387
00:24:04,064 --> 00:24:07,800
we've been using this autoscaler for years now and it's just been

388
00:24:07,832 --> 00:24:11,320
working very well. It's a very simple, less effort work,

389
00:24:11,352 --> 00:24:14,784
but it just works flawlessly for us.

390
00:24:15,204 --> 00:24:18,024
So we've never had issues with auto scalar as such.

391
00:24:18,844 --> 00:24:22,924
There's more that we've written about in our, one of the recent blog posts

392
00:24:23,044 --> 00:24:26,356
and case studies. You should check it out if you're interested in this kind of

393
00:24:26,380 --> 00:24:29,932
stuff. Now, our journey wasn't

394
00:24:30,108 --> 00:24:33,468
smooth, right. It was fraught with some issues and errors.

395
00:24:33,516 --> 00:24:36,788
So I'm going to talk about some of the issues we encountered and how we

396
00:24:36,836 --> 00:24:40,236
navigated those, and what kind of impact it had on downtime,

397
00:24:40,300 --> 00:24:44,540
slo, etcetera. So one of the issue that we encountered was

398
00:24:44,692 --> 00:24:49,068
GPU utilization in Nomad. For example, imagine this

399
00:24:49,236 --> 00:24:52,892
top box to be a GPU. It has GPU cache memory

400
00:24:52,948 --> 00:24:56,660
and cores. We would be able to run a ML worker

401
00:24:56,692 --> 00:24:59,860
using Nomad, using Docker driver. So we would run one

402
00:24:59,892 --> 00:25:03,436
ML worker per GPU. What we notice is that the

403
00:25:03,460 --> 00:25:07,408
GPU wasn't utilized fully. It was with one worker, it was just 20%

404
00:25:07,456 --> 00:25:11,104
utilized, and lot of other

405
00:25:11,224 --> 00:25:15,224
resources of that machine were just left unused. We did load

406
00:25:15,264 --> 00:25:18,432
tests to be able to figure out how much throughput we can get out of

407
00:25:18,448 --> 00:25:21,672
a single machine running single ML worker. It's a

408
00:25:21,688 --> 00:25:25,408
docker container, and it wasn't impressive with

409
00:25:25,456 --> 00:25:28,776
this. If we just ran with this kind of

410
00:25:28,800 --> 00:25:32,472
hardware, our cost was going through the roof, and we

411
00:25:32,488 --> 00:25:36,896
had to really figure out how do we fix this. So we

412
00:25:37,040 --> 00:25:41,384
dug deep into Nomad. GitHub issues, some pull requests,

413
00:25:41,504 --> 00:25:45,064
some parts into reading obscure documentation and figuring out.

414
00:25:45,104 --> 00:25:48,880
And fundamentally later, what we discovered is one

415
00:25:48,912 --> 00:25:52,496
workaround which we can use, which is instead of using docker

416
00:25:52,520 --> 00:25:56,232
driver, if we use raw exec driver, which allows you

417
00:25:56,248 --> 00:25:59,446
to run any kind of component, it doesn't

418
00:25:59,560 --> 00:26:02,602
guarantee any. Like you

419
00:26:02,618 --> 00:26:06,658
have to worry about a lot of scheduling yourself when you use raw exec driver.

420
00:26:06,826 --> 00:26:10,346
But with raw exec driver, you could run docker compose. And we

421
00:26:10,370 --> 00:26:13,610
spin up multiple docker containers using docker

422
00:26:13,642 --> 00:26:17,162
compose via raw exec driver in Nomad. So what that allowed

423
00:26:17,178 --> 00:26:19,854
us to do is the same GPU machine,

424
00:26:20,274 --> 00:26:23,778
we could run not one, but four workers,

425
00:26:23,946 --> 00:26:27,772
right? Using docker composer. Now that led us

426
00:26:27,788 --> 00:26:30,624
to having about 80% utilization.

427
00:26:31,444 --> 00:26:34,908
It straight away brought our cost down by four x. So imagine if you

428
00:26:34,916 --> 00:26:39,348
had to have $100,000 per month on just on GPU,

429
00:26:39,476 --> 00:26:43,500
we would slash it by one fourth directly and have like 400%

430
00:26:43,572 --> 00:26:47,116
impact, essentially. So this was one way we solved it.

431
00:26:47,260 --> 00:26:50,556
This is as of today, last I checked, it is still open,

432
00:26:50,620 --> 00:26:53,860
this issue, and you would see this pull request, or this

433
00:26:53,892 --> 00:26:57,090
issue is still open. And one of the solutions or

434
00:26:57,122 --> 00:27:01,202
workarounds that we've used, I've highlighted that here for

435
00:27:01,218 --> 00:27:04,994
you to look at. One thing that we noticed, if you use raw exec

436
00:27:05,034 --> 00:27:08,258
driver, you're going to have to worry about

437
00:27:08,426 --> 00:27:11,250
the shutdown part of it yourself.

438
00:27:11,442 --> 00:27:15,138
So otherwise, nomad generally handles sick

439
00:27:15,186 --> 00:27:19,818
term, and it handles the graceful termination of resources

440
00:27:19,866 --> 00:27:23,082
or components. In this case you will have to have

441
00:27:23,138 --> 00:27:26,634
waits and timeouts and you have to do some magic

442
00:27:26,714 --> 00:27:29,994
and work. You have to put in some work to be able to tear

443
00:27:30,034 --> 00:27:33,506
down the components correctly. So we invested in that and we wrote

444
00:27:33,530 --> 00:27:35,694
some bash script to be able to run,

445
00:27:37,114 --> 00:27:40,826
which could run the components and also tear them down easily when we

446
00:27:40,850 --> 00:27:44,610
wanted to. One other issue that we encountered is

447
00:27:44,722 --> 00:27:48,154
high latency. So again we have

448
00:27:48,194 --> 00:27:52,010
steady traffic, new regions or new stores are opening up and

449
00:27:52,122 --> 00:27:55,746
we are getting them migrated to use our APIs. And the rollout

450
00:27:55,770 --> 00:27:58,986
is happening suddenly on one of the days. What we see is

451
00:27:59,010 --> 00:28:02,378
that more than 25 2nd response time.

452
00:28:02,506 --> 00:28:05,650
Now imagine like our sla is less than 3 seconds response

453
00:28:05,682 --> 00:28:08,930
time for 90th percentile or 95th percentile.

454
00:28:09,122 --> 00:28:12,426
Now we suddenly get 25 seconds of response time.

455
00:28:12,490 --> 00:28:16,690
That's unacceptable. So we debug

456
00:28:16,722 --> 00:28:20,154
this issue. We try to find out, oh, there must be some problem with Mlworker,

457
00:28:20,194 --> 00:28:24,114
right? Because that's the slowest component in the chain. But we realize

458
00:28:24,154 --> 00:28:27,610
that there is no queue depth. The workers are doing their job,

459
00:28:27,682 --> 00:28:30,906
there is no extra jobs for them to be processed.

460
00:28:31,010 --> 00:28:35,154
So the worker scaling or auto scaling is not a problem. But then

461
00:28:35,194 --> 00:28:39,378
why do we have this much latency on the API side?

462
00:28:39,546 --> 00:28:42,644
There is no processing lag on the worker. Why do we have that?

463
00:28:43,504 --> 00:28:47,296
We spent a lot of time debugging this issue and ultimately

464
00:28:47,320 --> 00:28:51,072
we discovered that the number of go routines that we had on

465
00:28:51,088 --> 00:28:54,568
the API side which would process the results from RabbitMQ.

466
00:28:54,616 --> 00:28:58,224
So for example, our flow was that

467
00:28:58,384 --> 00:29:02,124
we would have the client SDK call our NLB or AlB.

468
00:29:02,584 --> 00:29:06,160
The API would send a message to Rabbitmq. The workers would

469
00:29:06,192 --> 00:29:09,668
then consume that message, produce the result.

470
00:29:09,756 --> 00:29:13,076
It would be updated on database and it will also send another

471
00:29:13,140 --> 00:29:17,268
message on to RabbitMQ that the processing is done and some other metadata.

472
00:29:17,436 --> 00:29:21,500
Now there is this workers that were running on the API which

473
00:29:21,532 --> 00:29:24,916
would then consume this metadata via the RabbitMQ message

474
00:29:25,020 --> 00:29:28,100
and then it would return the response to the user or do something else.

475
00:29:28,132 --> 00:29:32,020
Right now that's the part. We were just running five coroutines.

476
00:29:32,212 --> 00:29:36,188
Now what we realize is that we were also running three nodes or three

477
00:29:36,276 --> 00:29:40,452
docker containers for workers. And we never faced any issue with worker,

478
00:29:40,548 --> 00:29:44,436
the cpu utilized API layer. Sorry, we never faced

479
00:29:44,460 --> 00:29:47,140
any cpu utilization or any issue with API layer.

480
00:29:47,252 --> 00:29:50,836
But when the request count increased,

481
00:29:50,900 --> 00:29:54,324
what we realized is that these five go routines were not enough and

482
00:29:54,364 --> 00:29:57,716
which is where we're actually seeing queue depth on the API side.

483
00:29:57,860 --> 00:30:01,466
The API go routines that we're trying to consume from Arabic MQ.

484
00:30:01,620 --> 00:30:04,894
That's where we saw the queue depth. And it took us a while to figure

485
00:30:04,934 --> 00:30:09,134
this out and to fix this, because our preconceived notion and the first response

486
00:30:09,174 --> 00:30:12,742
was to look at workers as a problem. Now we

487
00:30:12,758 --> 00:30:15,942
change the go routines to 30 and suddenly the flip,

488
00:30:16,118 --> 00:30:19,486
the traffic goes normal and we have less than 3 seconds response

489
00:30:19,510 --> 00:30:23,166
time. So that really shows how you want to,

490
00:30:23,310 --> 00:30:26,782
how you should understand the components and how data pipeline works,

491
00:30:26,878 --> 00:30:30,880
and you need to really think about where the latency is being introduced.

492
00:30:31,072 --> 00:30:34,404
After that, we also added a lot of other observability

493
00:30:35,184 --> 00:30:38,736
signals and metrics to be able to track this issue and further any

494
00:30:38,760 --> 00:30:42,824
other issues even even further. So we invested a lot in

495
00:30:42,944 --> 00:30:47,464
tracking cross service latencies

496
00:30:47,544 --> 00:30:50,888
and stuff like that. So I've written about this as

497
00:30:50,976 --> 00:30:53,084
a form of pragmatic engineering story.

498
00:30:54,244 --> 00:30:58,132
You can check it out on Twitter if you follow me there. I want to

499
00:30:58,148 --> 00:31:01,756
conclude this session by talking about some of the lessons that we learned along the

500
00:31:01,780 --> 00:31:05,156
way. So one is for ML workloads,

501
00:31:05,340 --> 00:31:08,700
it goes without saying, but it's worth repeating that.

502
00:31:08,732 --> 00:31:12,500
Data quality and training the model is super important. Like a lot

503
00:31:12,532 --> 00:31:15,732
of it depends on the quality of data and the volume of the

504
00:31:15,748 --> 00:31:19,220
data and how you train your models, how you carve out

505
00:31:19,332 --> 00:31:23,070
the test data versus the data that you

506
00:31:23,102 --> 00:31:27,166
run the model on and you want to check and how you label and

507
00:31:27,310 --> 00:31:31,926
data. So a lot of it depends on data quality and training in

508
00:31:31,950 --> 00:31:35,846
our case, and I would highly recommend to use cloud agnostic architecture to

509
00:31:35,870 --> 00:31:39,314
be able to build scalable systems that can

510
00:31:39,934 --> 00:31:43,702
give you that flexibility to deploy them on any type of workload or any type

511
00:31:43,718 --> 00:31:47,348
of underlying cloud. For us, that has been the massive win.

512
00:31:47,446 --> 00:31:51,604
And I would highly encourage you to think about building cloud agnostic systems.

513
00:31:52,104 --> 00:31:55,368
One of the things that I've seen majorly is people do not

514
00:31:55,416 --> 00:31:59,344
treat operational workloads as first class citizens. They just slap on

515
00:31:59,384 --> 00:32:02,864
top of the existing software and just say that yeah,

516
00:32:02,904 --> 00:32:06,264
somebody will manage this. But treating operational work as

517
00:32:06,304 --> 00:32:09,672
first class citizen really helps in automation of a lot

518
00:32:09,688 --> 00:32:13,172
of your day to day tasks, and it gives you,

519
00:32:13,368 --> 00:32:17,396
especially when you are first launching, you should really treat production,

520
00:32:17,580 --> 00:32:21,396
maintenance and operations as a first class citizen in

521
00:32:21,420 --> 00:32:23,344
software building and delivery process.

522
00:32:24,084 --> 00:32:28,580
Lastly, from a team point of view, we had really good collaboration

523
00:32:28,692 --> 00:32:32,404
with various teams, data scientists, the mobile

524
00:32:32,444 --> 00:32:35,732
SDK team, the backend engineering team, sres,

525
00:32:35,868 --> 00:32:39,746
and even the business folks. So for example, whenever there is a new campaign

526
00:32:39,810 --> 00:32:43,410
or we would get some info from business team that a new region or

527
00:32:43,442 --> 00:32:46,094
new bunch of stores are being onboarded,

528
00:32:46,634 --> 00:32:50,394
preemptively scaled the nodes to be able to handle that traffic,

529
00:32:50,434 --> 00:32:54,258
for example. So being able to closely collaborate with data scientists,

530
00:32:54,386 --> 00:32:57,698
we also optimize, for example, one of the case, we also optimize

531
00:32:57,746 --> 00:33:01,306
the docker image size for the models. Earlier, we would have all

532
00:33:01,330 --> 00:33:05,118
the versions of the models in our final

533
00:33:05,166 --> 00:33:08,566
Docker image, which would mean the Docker image itself would be like tens

534
00:33:08,590 --> 00:33:12,598
of gb. So we'll have ten GB Docker image later. We optimize that

535
00:33:12,686 --> 00:33:16,654
with close collaboration with the data scientists to less than like three gb

536
00:33:16,734 --> 00:33:20,462
of model. So that just speeds up a lot of

537
00:33:20,598 --> 00:33:24,918
warming up of nodes. It just speeds up the deployment process and

538
00:33:25,046 --> 00:33:29,526
the time it takes for nodes to be ready to serve traffic. So ensuring

539
00:33:29,590 --> 00:33:32,754
good collaboration between teams is super, super important.

540
00:33:33,314 --> 00:33:36,810
I think that's it from my side, what I would say is connect with

541
00:33:36,842 --> 00:33:40,506
me on LinkedIn, Twitter, et cetera. You should check out our go

542
00:33:40,570 --> 00:33:44,546
or SRE bootcamp that we have built at 120 n, along with

543
00:33:44,570 --> 00:33:47,986
the software engineering stories that are right. Here's the QR code. You can

544
00:33:48,090 --> 00:33:49,354
scan this and you can check this out.

