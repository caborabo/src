1
00:00:27,120 --> 00:00:31,204
Hi everyone. Good day. Let's talk about resilience.

2
00:00:32,504 --> 00:00:35,904
My name is Dima. For the past five years

3
00:00:35,944 --> 00:00:39,336
I've been developing high load fintech applications that

4
00:00:39,360 --> 00:00:42,840
process a huge amount of user money and have the

5
00:00:42,872 --> 00:00:46,724
highest requirements for stability and availability.

6
00:00:47,624 --> 00:00:51,344
During this time, me and my team has encountered a lot of problems

7
00:00:51,384 --> 00:00:55,416
and corner cases, and we've came up with our own

8
00:00:55,480 --> 00:00:59,138
recipes for resilience patterns, which ones work

9
00:00:59,186 --> 00:01:02,866
best for us, and how we prefer to cook them so we can sleep peacefully

10
00:01:02,890 --> 00:01:06,314
at night, knowing that our systems are equipped to survive

11
00:01:06,394 --> 00:01:09,922
any disaster. What is resilience?

12
00:01:10,058 --> 00:01:14,162
Resilience patterns are practices that help software

13
00:01:14,218 --> 00:01:18,146
survive when things go wrong. They like safety

14
00:01:18,170 --> 00:01:22,074
nets that make sure your application keeps running even

15
00:01:22,114 --> 00:01:25,172
if they are facing some problems. So let's take a

16
00:01:25,188 --> 00:01:29,184
look at them and see if they can help your software be better.

17
00:01:30,364 --> 00:01:33,820
Starting with Bulkhead imagine we have

18
00:01:33,852 --> 00:01:37,676
a simple, ordinary application. We have several

19
00:01:37,740 --> 00:01:42,148
backends behind us to get some data from and inside

20
00:01:42,236 --> 00:01:46,436
our application. We have several HTTP clients connected

21
00:01:46,540 --> 00:01:50,276
to those backends. What can go wrong? Simple application,

22
00:01:50,340 --> 00:01:53,890
right? But it turns out they

23
00:01:53,962 --> 00:01:56,906
all share the same connection pool,

24
00:01:57,090 --> 00:02:00,474
and they share other resources like cpu and

25
00:02:00,514 --> 00:02:04,738
ram. What will happen if one of backends experiences

26
00:02:04,786 --> 00:02:08,002
some sort of problems resulting in high request latency?

27
00:02:08,058 --> 00:02:11,682
For example, due to high

28
00:02:11,818 --> 00:02:15,594
response time, the entire connection pool will be completely

29
00:02:15,634 --> 00:02:19,614
filled by requests awaiting responses from backend one

30
00:02:19,794 --> 00:02:22,966
and request to healthy backend two or backend three,

31
00:02:23,110 --> 00:02:26,514
won't be able to be sent because the pool is exhausted.

32
00:02:27,054 --> 00:02:30,390
So a failure of one of our backends will result

33
00:02:30,502 --> 00:02:33,950
in a failure across the entire functionality of

34
00:02:33,982 --> 00:02:36,902
our application. But we don't want that.

35
00:02:36,998 --> 00:02:40,782
We would like the only functionality associated with a

36
00:02:40,878 --> 00:02:45,098
failing backend to experience degradation, but the rest of the application

37
00:02:45,286 --> 00:02:47,654
must continue operating normally.

38
00:02:48,714 --> 00:02:51,954
To protect our application from this problem,

39
00:02:52,114 --> 00:02:56,226
we can use the bulkhead pattern. This pattern originating

40
00:02:56,290 --> 00:03:00,362
from shipbuilding, such as creating several compartments within

41
00:03:00,418 --> 00:03:04,306
a ship isolated from each other. If a leak happens

42
00:03:04,450 --> 00:03:07,362
in one compartment, it fills with water,

43
00:03:07,458 --> 00:03:10,694
but the other compartments remain undamaged.

44
00:03:11,614 --> 00:03:15,034
How can we apply this idea? To our example?

45
00:03:15,694 --> 00:03:19,294
We can introduce individual limits on the number of

46
00:03:19,374 --> 00:03:23,114
concurrent requests for each HTTP client.

47
00:03:23,694 --> 00:03:27,782
Therefore, if one backend starts to slow down,

48
00:03:27,838 --> 00:03:31,286
it will lead to degraded functionality related

49
00:03:31,390 --> 00:03:35,190
only to that HTTP client, but the

50
00:03:35,222 --> 00:03:39,114
rest of the application will continue to operate normally.

51
00:03:39,764 --> 00:03:43,500
Bulkhead can be used to isolate any kind of resources.

52
00:03:43,692 --> 00:03:47,692
For instance, you can limit the resources consumed by background activities

53
00:03:47,868 --> 00:03:51,636
in your application, or you can even set restrictions

54
00:03:51,700 --> 00:03:55,784
on the number of incoming requests coming to your application.

55
00:03:56,244 --> 00:03:59,464
Upstream service or frontend may also experience

56
00:03:59,964 --> 00:04:03,636
some kind of failure. For example, reset with caches

57
00:04:03,740 --> 00:04:07,864
and it may lead to critical traffic increase. Coming to your APK

58
00:04:08,564 --> 00:04:12,076
input limit ensures that your application won't crash

59
00:04:12,140 --> 00:04:16,180
due to out of memory issues and will continue to function

60
00:04:16,332 --> 00:04:20,500
even though it will be responding with errors to request exceeding

61
00:04:20,572 --> 00:04:24,588
the limit. You can implement simple bulkhead

62
00:04:24,636 --> 00:04:28,036
just by using simple semaphore. Here's an

63
00:04:28,060 --> 00:04:31,820
example of scala code that uses one semaphore to limit

64
00:04:31,852 --> 00:04:35,648
concurrent requests and another semaphore to create

65
00:04:35,696 --> 00:04:39,000
a queue of painting requests. Such queue can

66
00:04:39,032 --> 00:04:42,656
be useful for smoothing out short term spikes

67
00:04:42,680 --> 00:04:45,604
in traffic and avoiding error spikes.

68
00:04:46,104 --> 00:04:49,416
So that was bulkhead. Bulkheads can help you

69
00:04:49,440 --> 00:04:53,768
isolate failures in teach your application to gracefully

70
00:04:53,816 --> 00:04:56,284
degrade in case of emergencies.

71
00:04:57,504 --> 00:05:00,950
Next pattern is cache caching is a vast topic

72
00:05:00,982 --> 00:05:04,614
in software engineering, so today we will focus

73
00:05:04,694 --> 00:05:08,878
only on application level caching. Let's return

74
00:05:08,926 --> 00:05:12,238
to our example. We still have an application that communicates

75
00:05:12,286 --> 00:05:14,634
with several other applications.

76
00:05:15,334 --> 00:05:19,086
Let's assume we have sufficiently high SSLA

77
00:05:19,190 --> 00:05:22,270
in our backends and they have very low probability

78
00:05:22,342 --> 00:05:25,374
of error. Let's consider a

79
00:05:25,414 --> 00:05:28,982
scenario where an operation requires querying all

80
00:05:29,038 --> 00:05:31,664
these backends to be completed.

81
00:05:31,824 --> 00:05:35,336
Each of the backends can independently respond with

82
00:05:35,360 --> 00:05:39,248
an error due to error potentially happening independently.

83
00:05:39,336 --> 00:05:42,800
The resulting probability of error in our application will

84
00:05:42,832 --> 00:05:46,352
be higher than the probability of error in each individual

85
00:05:46,408 --> 00:05:49,576
backend. We can increase

86
00:05:49,640 --> 00:05:52,776
the reliability of file application by adding a

87
00:05:52,800 --> 00:05:56,724
cache. Cache is a high speed data buffer

88
00:05:56,824 --> 00:06:00,284
that stores the most frequently used data, allowing us

89
00:06:00,324 --> 00:06:04,292
to avoid fetching it from potential slow source

90
00:06:04,348 --> 00:06:07,916
every time. Caches stored in memory have zero

91
00:06:07,980 --> 00:06:11,464
chance of error. Unlike fetching data over the network,

92
00:06:12,564 --> 00:06:16,036
caching also reduces network traffic, lowering the

93
00:06:16,060 --> 00:06:19,892
chance of error even more. As a result, we can

94
00:06:19,948 --> 00:06:23,170
achieve even lower error rate in our application than

95
00:06:23,202 --> 00:06:26,722
in our backends. Additionally, in memory, caches are

96
00:06:26,738 --> 00:06:29,946
much faster than the network, which reduces the

97
00:06:29,970 --> 00:06:33,614
latency of our application. It's a small bonus.

98
00:06:35,074 --> 00:06:38,834
Such caches are excellent for non personalized data,

99
00:06:38,914 --> 00:06:42,226
such as news feed or some other data that are the

100
00:06:42,250 --> 00:06:45,826
same for all our users. But what if

101
00:06:45,850 --> 00:06:49,266
we want to use memory caches for personalized data,

102
00:06:49,370 --> 00:06:53,218
for user profiles, for example, or personalized recommendation

103
00:06:53,306 --> 00:06:56,546
or something like that? In that case, we need

104
00:06:56,570 --> 00:07:00,402
some sort of sticky sessions to ensure that all requests coming

105
00:07:00,458 --> 00:07:04,894
from a user always go to the same instance of our application

106
00:07:05,354 --> 00:07:09,658
to be able to utilize caches of personalized data

107
00:07:09,706 --> 00:07:12,842
of its user. Good news is

108
00:07:12,938 --> 00:07:16,850
for this scenario we don't need any complex ticket session mechanism.

109
00:07:17,002 --> 00:07:20,690
We can handle some sort of minor corner cases and

110
00:07:20,722 --> 00:07:24,634
minor traffic rebalancing. Therefore, it will suffice only

111
00:07:24,674 --> 00:07:28,506
to use, for example, stable load balancing algorithm at

112
00:07:28,530 --> 00:07:32,298
your balancer without the need for any complex systems

113
00:07:32,346 --> 00:07:36,454
to manage sticky sessions. For example, we can use consistent hashing

114
00:07:37,074 --> 00:07:40,602
in the event of node failure. Consistent hashing ensures

115
00:07:40,658 --> 00:07:44,210
that the only users who are tied to the failed node

116
00:07:44,282 --> 00:07:48,414
will be rebalanced, rather than rebalancing all users.

117
00:07:50,804 --> 00:07:54,404
That's it. Now we can use our caches for all types of our

118
00:07:54,444 --> 00:07:57,744
data for personalized and non personalized.

119
00:07:58,604 --> 00:08:02,324
But let's take a look at another scenario. What if

120
00:08:02,364 --> 00:08:06,064
the data we want to cache is used in every request we handle?

121
00:08:06,604 --> 00:08:09,988
It could be information about access policies or

122
00:08:10,036 --> 00:08:14,304
subscription plans, or any other crucial

123
00:08:16,924 --> 00:08:20,228
let's take a look at another scenario. What if the data we

124
00:08:20,276 --> 00:08:24,348
want to cache is used in every request we handle? It could be information about

125
00:08:24,396 --> 00:08:27,932
access policies or subscription plans, or any

126
00:08:27,988 --> 00:08:30,664
other crucial entity in our domain.

127
00:08:31,484 --> 00:08:35,108
In this case, the source of this data can become a

128
00:08:35,156 --> 00:08:39,060
major point of failure in our system, even if we will be fetching

129
00:08:39,092 --> 00:08:42,472
it through the cache. If the volume

130
00:08:42,488 --> 00:08:45,840
of data in the source is not very large, we can consider fully

131
00:08:45,872 --> 00:08:49,724
replicating this data directly into the memory of our applications.

132
00:08:50,304 --> 00:08:53,704
At the start of our application, we download

133
00:08:53,744 --> 00:08:57,192
a snapshot of this data and then we receive updates from

134
00:08:57,248 --> 00:09:01,560
some sort of topic. This way we can increase the reliability

135
00:09:01,672 --> 00:09:05,136
of accessing this data because every retrieval will

136
00:09:05,160 --> 00:09:08,764
be done from the memory. Zero error probability,

137
00:09:08,884 --> 00:09:12,064
and it is still very fast because it is a memory.

138
00:09:13,204 --> 00:09:17,044
However, since our application will need to download some

139
00:09:17,124 --> 00:09:21,116
data at startup, we violate one of the principles

140
00:09:21,180 --> 00:09:24,996
of the twelve factor application, which states that the applications

141
00:09:25,100 --> 00:09:28,716
should start up quickly, but we don't want to give up

142
00:09:28,740 --> 00:09:32,384
on advantages we gain from using this type of cache.

143
00:09:33,354 --> 00:09:36,714
Let's think, if there anything we can do to avoid this

144
00:09:36,754 --> 00:09:40,574
issue. Why would we need fast startup in the first place?

145
00:09:41,674 --> 00:09:45,754
Fast app is needed for platforms like Kubernetes

146
00:09:45,834 --> 00:09:49,394
to be able to quickly migrate your application

147
00:09:49,554 --> 00:09:52,334
to another physical node, for example.

148
00:09:52,874 --> 00:09:56,426
But platforms like Kubernetes already can handle slow

149
00:09:56,530 --> 00:09:59,974
starting applications by using startup robots, for example.

150
00:10:01,214 --> 00:10:05,270
Another issue we may encounter is updating configurations of

151
00:10:05,302 --> 00:10:08,566
running applications. Often in order to fix some

152
00:10:08,590 --> 00:10:11,958
problems, we need to change cache times

153
00:10:12,086 --> 00:10:15,214
or request timeouts or some other configuration

154
00:10:15,254 --> 00:10:19,254
properties. And let's say we know how

155
00:10:19,334 --> 00:10:23,030
to quickly deliver updated configuration files to our application.

156
00:10:23,102 --> 00:10:26,820
But we still need to restart an application to apply our

157
00:10:26,852 --> 00:10:30,260
new configuration. And the rolling update can now take

158
00:10:30,292 --> 00:10:33,500
a very long time, and we don't want to make our

159
00:10:33,532 --> 00:10:37,544
users wait for a fix to be applied. What can we do?

160
00:10:38,564 --> 00:10:42,044
What if we can teach our application to reload configuration

161
00:10:42,124 --> 00:10:45,652
on the fly without restarting it.

162
00:10:45,668 --> 00:10:49,700
Turns out it is not so hard to do. We can store configuration

163
00:10:49,772 --> 00:10:54,572
in some concurrent variable and have another background thread periodically

164
00:10:54,668 --> 00:10:57,766
update this variable. Sounds simple, right?

165
00:10:57,950 --> 00:11:01,046
However, to ensure that certain parameters such

166
00:11:01,070 --> 00:11:04,182
as timeouts for HTTP clients take effect,

167
00:11:04,318 --> 00:11:08,422
we need to also reinitialize our HTTP clients

168
00:11:08,518 --> 00:11:11,798
or database clients when corresponding

169
00:11:11,886 --> 00:11:15,274
config changes, and it may be a challenge.

170
00:11:15,614 --> 00:11:18,982
But some clients like Cassandra for Java,

171
00:11:19,078 --> 00:11:23,072
already supports reloading of its configuration on

172
00:11:23,088 --> 00:11:27,064
the fly, so reloadable config can mitigate

173
00:11:27,104 --> 00:11:31,032
the negative impact of long application startup times.

174
00:11:31,168 --> 00:11:34,944
And as a small bonus, it has other use

175
00:11:34,984 --> 00:11:38,936
cases like we can use it to implement feature flags,

176
00:11:38,960 --> 00:11:42,768
for example. So next pattern

177
00:11:42,856 --> 00:11:46,080
next pattern is fallback. Let's take a look at

178
00:11:46,112 --> 00:11:49,764
another scenario. We receive a request send

179
00:11:49,804 --> 00:11:53,644
request to a backend or to a database and receive

180
00:11:53,684 --> 00:11:56,956
an error in return. Then we will respond to the

181
00:11:56,980 --> 00:12:01,116
user with an error message as well. However, it is often better to

182
00:12:01,220 --> 00:12:05,164
show an audited data with a message like we experiencing

183
00:12:05,244 --> 00:12:08,820
delaying in updating information or something like that,

184
00:12:08,972 --> 00:12:12,492
rather than displaying big red error message box for

185
00:12:12,548 --> 00:12:16,472
user. Right? So to improve our behavior

186
00:12:16,528 --> 00:12:19,864
in this case we can use the fallback pattern. The idea

187
00:12:19,904 --> 00:12:23,584
is to have a backup data source, probably with lower quality

188
00:12:23,624 --> 00:12:26,856
of data, from which the data can be retrieved when

189
00:12:26,880 --> 00:12:28,884
the primary source is unavailable.

190
00:12:29,504 --> 00:12:33,392
Often fallback cache is used as a backup data source.

191
00:12:33,568 --> 00:12:37,324
This cache stores last successful response from the service,

192
00:12:37,944 --> 00:12:41,488
and when domain service is unavailable we can use this

193
00:12:41,536 --> 00:12:44,980
last successful response to give

194
00:12:45,012 --> 00:12:48,324
to user. So now user will be able

195
00:12:48,364 --> 00:12:51,540
to see some data instead of an error and the team

196
00:12:51,612 --> 00:12:55,412
responsible for broken backend will have more time

197
00:12:55,508 --> 00:12:59,292
to fix the issue. Let's talk about

198
00:12:59,348 --> 00:13:03,012
retries. Let's rewind a

199
00:13:03,028 --> 00:13:06,724
little and go back to our example where we were trying to reduce probability

200
00:13:06,804 --> 00:13:10,056
of errors using caches. What if

201
00:13:10,080 --> 00:13:13,576
instead of caches in case of error, we simply send the

202
00:13:13,600 --> 00:13:17,584
request again? This is actually a pattern called retry,

203
00:13:17,744 --> 00:13:21,240
and it can also help us reduce the likelihood of errors

204
00:13:21,272 --> 00:13:24,744
in our application. Retries are often easier to

205
00:13:24,784 --> 00:13:27,624
implement because when you use caches there,

206
00:13:27,664 --> 00:13:31,280
often you need to invalidate them when the data changes.

207
00:13:31,352 --> 00:13:35,336
And cache invalidation can become very complex tasks for

208
00:13:35,360 --> 00:13:39,244
your system, it is considered one of the most challenging

209
00:13:39,284 --> 00:13:41,704
tasks in software engineering.

210
00:13:42,684 --> 00:13:46,100
That's why sometimes it's simply to just retrieve a failed

211
00:13:46,132 --> 00:13:50,060
request. However, what happens if one of our

212
00:13:50,092 --> 00:13:53,228
backends experience a failure? We will start

213
00:13:53,276 --> 00:13:57,104
retrying requests to that backend, which will increase the traffic

214
00:13:58,004 --> 00:14:01,324
increase will be by several times, and it is

215
00:14:01,364 --> 00:14:04,828
very likely that this backend wasn't designed to handle such

216
00:14:04,876 --> 00:14:08,828
a sudden spike in the traffic, so we will probably make the failure

217
00:14:08,916 --> 00:14:12,180
even worse. Therefore, along with

218
00:14:12,212 --> 00:14:15,104
a retry, the circuit breaker pattern must be used.

219
00:14:16,124 --> 00:14:19,748
It's a mechanism that detects an increase in error rate,

220
00:14:19,836 --> 00:14:22,964
and if error rate exceeds a certain threshold,

221
00:14:23,124 --> 00:14:26,452
requests to downstream service are blocked for

222
00:14:26,468 --> 00:14:30,724
a period of time. After this time, we try to

223
00:14:30,804 --> 00:14:34,140
let one or more requests through and check if service has

224
00:14:34,172 --> 00:14:37,714
recovered. If it has ok, we start allowing traffic again.

225
00:14:38,174 --> 00:14:41,654
Otherwise we will block requests for another period of

226
00:14:41,694 --> 00:14:45,150
time. Often retries and

227
00:14:45,182 --> 00:14:48,550
socket breakers are implemented at infrastructure level,

228
00:14:48,622 --> 00:14:51,114
at load balancers for example.

229
00:14:51,534 --> 00:14:55,014
However, infrastructure usually doesn't have full

230
00:14:55,094 --> 00:14:58,438
error context. For instance, it's not always

231
00:14:58,486 --> 00:15:02,478
possible to genetically determine if error can be retried

232
00:15:02,566 --> 00:15:05,768
or it should be counted as expected

233
00:15:05,896 --> 00:15:09,456
or bad error for the circuit

234
00:15:09,560 --> 00:15:13,472
breaker threshold. Therefore, sometimes it

235
00:15:13,488 --> 00:15:17,016
is necessary to move retries and circuit breakers inside of

236
00:15:17,040 --> 00:15:20,944
the application to have full context for making decisions

237
00:15:20,984 --> 00:15:23,484
regarding error classification.

238
00:15:25,104 --> 00:15:28,744
So, to summarize, by implementing

239
00:15:28,784 --> 00:15:32,568
these patterns, we can fortify our application from

240
00:15:32,616 --> 00:15:35,416
emergencies, maintain high availability,

241
00:15:35,560 --> 00:15:39,124
and deliver seamless experience to our users.

242
00:15:39,664 --> 00:15:43,552
Additionally, I would like to remind you not to overlook telemetry.

243
00:15:43,648 --> 00:15:47,616
Good looks and metrics can enhance the quality of

244
00:15:47,640 --> 00:15:49,644
your services by a lot.

245
00:15:51,464 --> 00:15:55,032
That's it. Thank you for watching. Thank you for

246
00:15:55,048 --> 00:15:57,284
your time. Cheers.

