1
00:00:24,650 --> 00:00:27,778
Hello all. Thank you for joining for the presentation.

2
00:00:27,954 --> 00:00:31,446
So myself, Deepak. I'm working as an associate director for

3
00:00:31,468 --> 00:00:35,634
data science and machine learning here at Novartis. I'm also responsible

4
00:00:35,682 --> 00:00:39,010
for generative AI product deliverables

5
00:00:39,170 --> 00:00:42,578
and building novel machine learning algorithms and deploying

6
00:00:42,594 --> 00:00:46,066
that into production. Today I'm

7
00:00:46,098 --> 00:00:49,766
going to talk about development of

8
00:00:49,788 --> 00:00:53,454
an machine learning model and productionizing the ML algorithm in cloud

9
00:00:53,492 --> 00:00:56,958
environment. So in the recent era, the most of the

10
00:00:56,964 --> 00:01:00,554
time the data scientists spend time in creating or developing

11
00:01:00,602 --> 00:01:03,962
machine learning model, it could be starting from linear regression,

12
00:01:04,026 --> 00:01:07,342
logistic regression, or nave bayesian or

13
00:01:07,396 --> 00:01:10,834
dradum forest decision trees. Any algorithms let them

14
00:01:10,872 --> 00:01:14,702
take. Finally, the ERa has been moved from traditional

15
00:01:14,766 --> 00:01:19,222
or classical machine learning algorithms to large

16
00:01:19,276 --> 00:01:23,314
language models. So now deploying the large language models

17
00:01:23,362 --> 00:01:27,890
or productionizing the large language models in cloud is the biggest challenge we have.

18
00:01:28,060 --> 00:01:31,914
Not only productionizing and how we scale with high

19
00:01:31,952 --> 00:01:35,260
inference is another important aspect to consider.

20
00:01:36,110 --> 00:01:40,246
All right, let's move to the next slide before

21
00:01:40,368 --> 00:01:43,806
getting into the development of algorithms. First, let's understand

22
00:01:43,908 --> 00:01:47,790
what is the development lifecycle of any machine learning algorithm.

23
00:01:48,530 --> 00:01:52,158
Now, to choose any problem

24
00:01:52,244 --> 00:01:55,826
statement, we have to clearly define what is a problem statement we

25
00:01:55,848 --> 00:01:59,022
are trying to solve. It could be an image classification,

26
00:01:59,166 --> 00:02:02,370
text classification or language translation

27
00:02:02,710 --> 00:02:06,834
question and answering or predicting the next sentence or document summarization,

28
00:02:06,962 --> 00:02:10,482
text summarization or there are many tasks involved

29
00:02:10,546 --> 00:02:14,534
in the problem statement. So before starting any

30
00:02:14,572 --> 00:02:17,882
machine learning algorithm development, we define the problem

31
00:02:17,936 --> 00:02:21,222
statement. Once we define the problem statement,

32
00:02:21,286 --> 00:02:25,066
then we have to start with the collectioning of data that is

33
00:02:25,088 --> 00:02:28,710
called a data collection or data acquisition and gathering.

34
00:02:28,870 --> 00:02:33,534
So typically in a domain based when

35
00:02:33,572 --> 00:02:37,870
we are working in an organization which is specific to

36
00:02:37,940 --> 00:02:43,594
healthcare or infrastructure financial investment

37
00:02:43,642 --> 00:02:46,834
banking, there are many scenarios where we cloud

38
00:02:46,872 --> 00:02:49,540
get some real time data for model training.

39
00:02:50,630 --> 00:02:54,578
Excuse me, but in the case of when you are trying to do some research,

40
00:02:54,664 --> 00:02:58,226
then we go for open source data set. But ideally we have to

41
00:02:58,248 --> 00:03:01,798
get the data set or we have to collect the data set and understand

42
00:03:01,884 --> 00:03:05,334
the data. As part of understanding the data, we may

43
00:03:05,372 --> 00:03:09,206
have to do some amount of data preprocessing techniques which we'll

44
00:03:09,238 --> 00:03:12,794
see later. After that we have to

45
00:03:12,832 --> 00:03:17,130
see what is the performance metrics you are going to define

46
00:03:17,710 --> 00:03:20,714
to achieve the objective which we have defined.

47
00:03:20,842 --> 00:03:23,680
Let's take a text classification problem here.

48
00:03:24,290 --> 00:03:27,962
The classification result can be measured in the performance

49
00:03:28,026 --> 00:03:31,854
metrics of recall, precision and

50
00:03:31,892 --> 00:03:35,886
accuracy. So these are all comes under confusion matrix.

51
00:03:35,998 --> 00:03:40,020
So we have to define what are all the performance metrics we do before

52
00:03:41,430 --> 00:03:45,314
performing any model training activity. Now further,

53
00:03:45,362 --> 00:03:48,806
we have to evaluate the procedures. When we

54
00:03:48,828 --> 00:03:52,022
take a model training process, we divide the data

55
00:03:52,076 --> 00:03:54,950
set into training, testing and validation.

56
00:03:56,250 --> 00:04:00,310
So we have a split of around 80 or 680

57
00:04:00,390 --> 00:04:04,586
2020 or 7015 in

58
00:04:04,608 --> 00:04:07,702
this kind. When I say the numbers are in percentage,

59
00:04:07,846 --> 00:04:11,466
right, we have to split the data in train test and validation,

60
00:04:11,578 --> 00:04:15,086
and we have to see whether the model can perform well with

61
00:04:15,108 --> 00:04:18,974
the validation and test data. Again, the metrics we use

62
00:04:19,092 --> 00:04:21,070
based on confusion metrics.

63
00:04:22,870 --> 00:04:26,370
Next, coming to data preprocessing and cleaning

64
00:04:27,430 --> 00:04:31,154
when it comes to data preprocessing, okay, we got the data.

65
00:04:31,272 --> 00:04:34,578
In case of text, what are all the process we follow?

66
00:04:34,744 --> 00:04:38,546
In case of natural language processing, we remove ASCII

67
00:04:38,578 --> 00:04:42,066
and special characters and we do stop words removal followed

68
00:04:42,098 --> 00:04:46,402
by stemming and lamatization. If required, we may go for parts

69
00:04:46,466 --> 00:04:50,646
of speech tagging or some kind of an embeddings, which may be required

70
00:04:50,758 --> 00:04:53,900
before building a model. Now,

71
00:04:54,830 --> 00:04:59,098
once we come to the construction of a baseline model, so when we are using

72
00:04:59,184 --> 00:05:03,374
models like random forest or bayesian network or

73
00:05:03,412 --> 00:05:05,630
generative adverseial networks,

74
00:05:06,530 --> 00:05:09,946
Ada boosting, exg boosting,

75
00:05:10,058 --> 00:05:14,334
when you're using models like this, we may have to think about

76
00:05:14,452 --> 00:05:17,906
the, we don't need to think about the baseline model because they are already a

77
00:05:17,928 --> 00:05:20,942
base models where we use a data set to further

78
00:05:21,006 --> 00:05:24,754
train and we do the prediction or classification with that

79
00:05:24,792 --> 00:05:28,070
algorithm. But the approach which we are going to talk about,

80
00:05:28,140 --> 00:05:31,080
baseline model is bit different. What I'm saying now,

81
00:05:32,250 --> 00:05:35,606
once I walk you through on the slides, you will understand what

82
00:05:35,628 --> 00:05:38,986
I mean by the typical base models. Then we have

83
00:05:39,008 --> 00:05:42,154
to fine tune a model. So the fine tuning a model comes

84
00:05:42,192 --> 00:05:45,674
up with hyperparameter tuning and that techniques also.

85
00:05:45,712 --> 00:05:49,466
I'll talk later here, but ideally we

86
00:05:49,488 --> 00:05:53,086
have to get a pretrained model and fine tune a model to perform a specific

87
00:05:53,188 --> 00:05:56,874
task. Then how do we deploy and operationalize

88
00:05:56,922 --> 00:06:00,110
or industrialize the model in cloud environment?

89
00:06:01,010 --> 00:06:04,606
All right, so we have discussed

90
00:06:04,638 --> 00:06:08,450
about the lifecycle of ML algorithm. Let's move to the next slide.

91
00:06:09,350 --> 00:06:12,798
Before getting into a model training activity,

92
00:06:12,894 --> 00:06:16,838
I clearly wanted to define the problem. What we are

93
00:06:16,844 --> 00:06:20,566
going to solve here is natural language processing tasks. As I

94
00:06:20,588 --> 00:06:24,354
said, natural language processing could be language

95
00:06:24,402 --> 00:06:27,974
translation or entity recognition, or it could be a spam

96
00:06:28,022 --> 00:06:32,134
detection, or we do some amount of part of speech tagging,

97
00:06:32,262 --> 00:06:35,446
text generation or document summarization.

98
00:06:35,558 --> 00:06:39,686
Question answering there are many natural language processing tasks involved,

99
00:06:39,798 --> 00:06:43,966
but ideally for this use case or for this demo, I'm going

100
00:06:43,988 --> 00:06:47,070
to walk you through or take you through on text classification.

101
00:06:48,210 --> 00:06:50,720
Right, let's move on.

102
00:06:51,890 --> 00:06:55,570
Now let's come to the natural language processing as a concept

103
00:06:56,630 --> 00:07:00,034
human understand English as a language or any other

104
00:07:00,072 --> 00:07:04,110
language which he has been known to from the birth.

105
00:07:04,270 --> 00:07:06,950
But when it comes to natural language processing,

106
00:07:07,610 --> 00:07:10,726
machine has to understand the language, right?

107
00:07:10,828 --> 00:07:14,806
So when I say machine, so whatever how humans interpret the

108
00:07:14,828 --> 00:07:18,490
language under response. Similar way we are building a machine learning

109
00:07:18,560 --> 00:07:22,282
or AI platform or AI machine to perform

110
00:07:22,336 --> 00:07:26,214
a specific job. That is what the natural language understanding

111
00:07:26,262 --> 00:07:30,086
has been given to the machine. So how human have an

112
00:07:30,128 --> 00:07:35,406
understanding by reading the text. Similarly by

113
00:07:35,428 --> 00:07:39,520
having or building a model, but to perform like a human

114
00:07:40,210 --> 00:07:43,646
to have a natural language understanding based on that,

115
00:07:43,668 --> 00:07:46,340
it determines the answer. Machine understand. Okay,

116
00:07:46,790 --> 00:07:50,418
this is the natural language understanding I got. Now what is the response I

117
00:07:50,424 --> 00:07:54,642
have to make which would be in human readable format? Again, we can make

118
00:07:54,696 --> 00:07:58,514
an output as a natural language generation, which could be text abstraction,

119
00:07:58,562 --> 00:08:03,000
text summarization, or we can do any natural language classification job.

120
00:08:03,450 --> 00:08:06,918
This is what I'm going to walk you through. So right now I put a

121
00:08:06,924 --> 00:08:10,774
Bert model here. If you could see in the center of the picture. But yes,

122
00:08:10,812 --> 00:08:15,130
I'll elaborately talk once I walk through the next couple of slides.

123
00:08:15,710 --> 00:08:19,674
Ideally we pass an input and we ask the model to

124
00:08:19,712 --> 00:08:23,002
classify. Then here it could be a spam

125
00:08:23,066 --> 00:08:26,560
or ham. So based on that it performs it.

126
00:08:27,890 --> 00:08:32,000
Let's move on to the next slide. Okay, hugging face

127
00:08:32,470 --> 00:08:35,906
now you would have heard this is getting very

128
00:08:35,928 --> 00:08:39,614
popular. Now. Hugging face is a framework or library to solve

129
00:08:39,662 --> 00:08:43,060
most of the NLP problems. They have built

130
00:08:44,710 --> 00:08:48,630
40,000 models around. They have built by now as of today

131
00:08:48,780 --> 00:08:52,642
which are having all as a pretrained model and some amount of instac

132
00:08:52,706 --> 00:08:56,440
based model or fine tuned model also available.

133
00:08:56,890 --> 00:09:00,602
Now we are going to use an agingface platform to perform our model

134
00:09:00,656 --> 00:09:04,458
training. Okay, now as I said, agingfare is

135
00:09:04,464 --> 00:09:09,066
the most popular framework which has been used by right

136
00:09:09,088 --> 00:09:14,014
now, sorry. It has around 4000 models or which

137
00:09:14,052 --> 00:09:18,490
can be deployed in cloud which is based on Pytorch or Tensorflow.

138
00:09:18,570 --> 00:09:21,546
Even Keras library are supported in hugging phase.

139
00:09:21,738 --> 00:09:25,214
Ideally we use a transformer based architecture models to

140
00:09:25,252 --> 00:09:28,706
develop our models. Now when you are

141
00:09:28,728 --> 00:09:31,778
talking about hugging phase, as I

142
00:09:31,784 --> 00:09:35,106
said, there are 4000 pretrained model and for each task they

143
00:09:35,128 --> 00:09:38,646
have a separate model. Let's say when we want to perform a text classification they

144
00:09:38,668 --> 00:09:41,586
have Bert, Robota, distal Bert XLM,

145
00:09:41,618 --> 00:09:45,074
Robota. Similarly for language translation they have Marian,

146
00:09:45,122 --> 00:09:49,314
Mt. Bard and T, five. For V and chat bots

147
00:09:49,362 --> 00:09:52,506
they have GPT, GPT-2 and now we

148
00:09:52,528 --> 00:09:55,114
would have got GPT-3 and four as well.

149
00:09:55,312 --> 00:09:58,826
When it comes to named entity recognition. Again, we can use the

150
00:09:58,848 --> 00:10:02,222
Bert model. Ideally, I'm going to talk more about the Bert model.

151
00:10:02,276 --> 00:10:05,966
The reason why I've kept Bert here is Bert is nothing but a

152
00:10:05,988 --> 00:10:09,626
bi directional encoder representation for transformer.

153
00:10:09,738 --> 00:10:13,194
It is based on the transform architecture or all the attention

154
00:10:13,242 --> 00:10:16,846
is you need based on that they have a transform architecture.

155
00:10:17,038 --> 00:10:20,594
In that way, Bert has been built once

156
00:10:20,632 --> 00:10:24,226
it came in 2018 or 19. Then it shook the

157
00:10:24,248 --> 00:10:28,318
industry to think about the whole machine learning development has been

158
00:10:28,344 --> 00:10:30,840
taken into a next space or next level.

159
00:10:31,450 --> 00:10:34,758
Okay, now, so we are going to use BERT model and we

160
00:10:34,764 --> 00:10:38,262
are going to fine tune the BerT model. Bert model has

161
00:10:38,316 --> 00:10:41,734
comes up with its own strength like it is based

162
00:10:41,772 --> 00:10:45,386
on masked language modeling and next sentence prediction. So if you want to

163
00:10:45,408 --> 00:10:48,954
know more about the Bert model, I have a separate video. Please go

164
00:10:48,992 --> 00:10:52,778
and have a look into that. Now. When coming to

165
00:10:52,784 --> 00:10:55,994
the Bert now, Bert can perform multiple tasks,

166
00:10:56,122 --> 00:11:00,090
but as a general model, you can do a downstream

167
00:11:00,250 --> 00:11:03,506
job to make it specific to a domain or specific to

168
00:11:03,528 --> 00:11:06,962
a task which has to be performed. So yes, Bird can perform

169
00:11:07,016 --> 00:11:10,674
text classification or text generation or next

170
00:11:10,712 --> 00:11:13,090
sentence productionize question and answering.

171
00:11:15,510 --> 00:11:19,074
Similar like a chatbot, it can also perform. But how do we fine

172
00:11:19,112 --> 00:11:22,214
tune the model? Right, we have a prechained model, then we fine

173
00:11:22,252 --> 00:11:26,438
tune a model based on the data set. Then we deploy the model.

174
00:11:26,604 --> 00:11:30,410
We deploy the fine tuned model in production in a cloud environment.

175
00:11:31,230 --> 00:11:34,794
All right, let me walk you through the as I said,

176
00:11:34,832 --> 00:11:38,506
we are going to take the text classification example. Our objective is

177
00:11:38,528 --> 00:11:41,978
to understand the sentiment. Let's say this is an amazing

178
00:11:42,064 --> 00:11:45,690
model. Then we are going to say whether it is positive or negative.

179
00:11:45,770 --> 00:11:49,738
That's what the classification job does. Now, I'm taking a binary classification

180
00:11:49,834 --> 00:11:53,022
here. Going to call that as a positive or negative here.

181
00:11:53,156 --> 00:11:56,402
So this is an example I'm going to take. Now I'm going to walk you

182
00:11:56,456 --> 00:12:00,798
through on how we can perform model chaining.

183
00:12:00,974 --> 00:12:04,514
But before getting into model chaining, I want to tell you

184
00:12:04,552 --> 00:12:08,130
about ML Flow. What is ML Flow?

185
00:12:08,290 --> 00:12:11,686
ML Flow is a platform or is

186
00:12:11,708 --> 00:12:15,350
an API library which can be injected into your model

187
00:12:15,420 --> 00:12:19,114
development process to perform all the model tracking and

188
00:12:19,152 --> 00:12:22,714
model experiments. What I mean by that,

189
00:12:22,912 --> 00:12:26,614
we can build many models and many iteration

190
00:12:26,662 --> 00:12:29,946
of models reasoning. We have to fine tune the model. We have

191
00:12:29,968 --> 00:12:33,306
to change the parameters of the model. Once we keep changing

192
00:12:33,338 --> 00:12:36,862
the parameters of the model every time, model will have a different

193
00:12:36,916 --> 00:12:42,138
outputs. What could be an output here? It could be an precision

194
00:12:42,234 --> 00:12:45,666
recall accuracy, f one score. F two score. There are

195
00:12:45,688 --> 00:12:49,982
many elements we consider as part of model development activity.

196
00:12:50,126 --> 00:12:54,546
So there could be a scenario if in

197
00:12:54,568 --> 00:12:58,454
case of text classification, we have seen positive or negative, how much

198
00:12:58,492 --> 00:13:02,514
I'm more inclined to positive. If I always wants a positive,

199
00:13:02,642 --> 00:13:07,010
I should not miss any false negative means. If algorithm

200
00:13:07,090 --> 00:13:10,140
says it is a

201
00:13:10,590 --> 00:13:14,250
negative, but actually it is a

202
00:13:14,320 --> 00:13:17,690
positive, I should not miss these kind of scenarios,

203
00:13:18,110 --> 00:13:21,278
right? So if I should not

204
00:13:21,284 --> 00:13:24,720
miss any false negative, then I'll be focusing more on

205
00:13:25,810 --> 00:13:29,610
recall. Similarly, when the algorithm

206
00:13:29,770 --> 00:13:32,990
says okay, it is a

207
00:13:33,060 --> 00:13:36,430
positive and algorithm says it is a negative,

208
00:13:36,590 --> 00:13:40,462
then again it comes under false positive, right? So it misses the crucial

209
00:13:40,526 --> 00:13:44,146
element. Right. Now, to handle these

210
00:13:44,168 --> 00:13:48,242
kind of scenarios, we need a tracking platform which is called ML flow,

211
00:13:48,386 --> 00:13:52,162
which is used to record and track all the experiments

212
00:13:52,226 --> 00:13:55,526
along with the results. But I can also show

213
00:13:55,548 --> 00:13:58,986
you a quick sample on how the code looks like by having an

214
00:13:59,008 --> 00:14:02,762
ML flow and without an ML flow before

215
00:14:02,816 --> 00:14:05,610
that, this is how the model experiment looks like.

216
00:14:05,760 --> 00:14:07,930
When I talk about model experiments,

217
00:14:08,990 --> 00:14:12,702
let's say I'm going to train an algorithm and I may

218
00:14:12,756 --> 00:14:16,222
train n number of times. So I wanted to know based

219
00:14:16,276 --> 00:14:19,546
on which seed and which parameter my model really performed.

220
00:14:19,578 --> 00:14:22,894
Well, considering the scenario, I'll take all

221
00:14:22,932 --> 00:14:26,306
the historical experiments which I performed that

222
00:14:26,328 --> 00:14:30,340
would be tracked in ML flow, which you can see each,

223
00:14:30,790 --> 00:14:34,386
along with the timestamp, you can see the model which I ran, and if

224
00:14:34,408 --> 00:14:38,914
you go deep and along with that features, what kind of features I configured,

225
00:14:39,042 --> 00:14:42,726
then I get an accuracy, precision, recall value, whatever. I have that in

226
00:14:42,748 --> 00:14:46,514
a metrics for confusion matrix. This really helps

227
00:14:46,562 --> 00:14:50,806
in performing in multiple iteration of experiments and to get the tracking

228
00:14:50,838 --> 00:14:54,154
of the models. All right, now coming

229
00:14:54,192 --> 00:14:57,418
to the code. So typically what we do to train and model,

230
00:14:57,504 --> 00:15:01,206
we load an input data and we extract some of the features.

231
00:15:01,318 --> 00:15:04,526
And I'm using an Ingrams to extract the features. Then I'm going to

232
00:15:04,548 --> 00:15:08,094
train and model, and I'm going to compute the accuracy. Now,

233
00:15:08,212 --> 00:15:11,920
what version of my code was this result from? No idea.

234
00:15:12,610 --> 00:15:15,886
To perform this, we need an ML flow tracking,

235
00:15:15,998 --> 00:15:19,262
which is ideally used to track all the experiment

236
00:15:19,326 --> 00:15:23,026
results. Now let's see how the code looks like with

237
00:15:23,048 --> 00:15:26,670
ML flow. So with pythonic way, by having a

238
00:15:26,760 --> 00:15:30,834
packages import having ML flow and ML Tensorflow,

239
00:15:30,962 --> 00:15:34,582
then we say ML flow start run as a run.

240
00:15:34,636 --> 00:15:38,762
Then we start to log the metrics. Then we keep training our

241
00:15:38,816 --> 00:15:42,154
model along with fine tuning the parameters. In this

242
00:15:42,192 --> 00:15:45,738
way, everything get stored in a

243
00:15:45,744 --> 00:15:49,386
database, ideally in cloud environment. We configure with an

244
00:15:49,408 --> 00:15:52,880
S three bucket of AWS service, Amazon Web service.

245
00:15:53,490 --> 00:15:57,326
Then once the setup is done, then we

246
00:15:57,348 --> 00:16:00,910
add an implementation accordingly to make an ML flow start,

247
00:16:00,980 --> 00:16:04,770
and then iterate the model multiple times

248
00:16:04,840 --> 00:16:07,810
and keep having experiment results get stored.

249
00:16:08,150 --> 00:16:11,442
ML flow comes with a default UI where all the model

250
00:16:11,496 --> 00:16:15,134
experiment can be visualized, which I've shown you in the earlier

251
00:16:15,182 --> 00:16:18,982
slide. Now coming to the model training.

252
00:16:19,116 --> 00:16:23,094
So now the reason why I kept explaining about the experiment and

253
00:16:23,132 --> 00:16:26,306
tracking and all. When you start the model training framework,

254
00:16:26,498 --> 00:16:29,686
you should have all the experiments needs to be tracked somehow.

255
00:16:29,798 --> 00:16:33,402
Right now I've taken a small example code of

256
00:16:33,456 --> 00:16:36,650
how do we perform the model training activity.

257
00:16:37,150 --> 00:16:40,774
So ideally we are going to use a BERT model which you can see somewhere,

258
00:16:40,822 --> 00:16:44,094
which I am using a pre trained BERT model. Then I'm using

259
00:16:44,132 --> 00:16:47,422
auto model for sequence classification reasoning. I can

260
00:16:47,476 --> 00:16:51,482
put this instead of BeRT model. I can try with robota

261
00:16:51,546 --> 00:16:54,430
XLM, robota distal, Bird, Biobird,

262
00:16:55,010 --> 00:16:58,686
GPT-2 or GPT-3 any of the pretrained models

263
00:16:58,718 --> 00:17:02,466
I can put here. So when I build a framework I

264
00:17:02,488 --> 00:17:06,366
have to call the transformer library. Then I use auto tokenizer

265
00:17:06,398 --> 00:17:10,066
and auto model for sequence classification and load the pretrained

266
00:17:10,098 --> 00:17:13,762
model and in the next line I'll show the code further.

267
00:17:13,826 --> 00:17:17,702
But before that we are using a GPU machine to run all this model

268
00:17:17,756 --> 00:17:21,418
training activity because this is a large language model.

269
00:17:21,504 --> 00:17:24,950
Then again when we are doing a fine tuning, it recurs a GPU machine

270
00:17:25,030 --> 00:17:28,170
with CUDA library to perform this activity,

271
00:17:29,070 --> 00:17:32,522
the model two device and the torch device,

272
00:17:32,586 --> 00:17:36,506
by using a CUDA library specifies

273
00:17:36,698 --> 00:17:40,746
the GPU would be let's say I'm using an eight GPU, the processor

274
00:17:40,778 --> 00:17:44,580
would get split into multiple GPUs and it starts to

275
00:17:44,950 --> 00:17:48,274
perform the model training activity. The reason why we're using

276
00:17:48,312 --> 00:17:52,340
auto model so this is a framework which we can build and

277
00:17:52,710 --> 00:17:56,402
by passing in the command line prompt the model framework.

278
00:17:56,546 --> 00:17:59,400
Based on that we can further train the model.

279
00:18:00,650 --> 00:18:04,022
Now coming to the model train

280
00:18:04,076 --> 00:18:07,286
which you can see is an abstract class to

281
00:18:07,308 --> 00:18:10,806
perform the model training which has been given by transformer

282
00:18:10,838 --> 00:18:14,938
architecture. Then I can start training

283
00:18:15,024 --> 00:18:18,362
model will get trained. Then I can keep changing my training.

284
00:18:18,416 --> 00:18:21,886
Hyperparameters it's based on learning rate, number of

285
00:18:21,908 --> 00:18:25,774
epochs and lock size.

286
00:18:25,892 --> 00:18:30,190
And there are additional parameters which we can use which

287
00:18:30,260 --> 00:18:34,354
mainly we use learning rate and number of epochs which

288
00:18:34,392 --> 00:18:37,598
is used for fine tuning the model parameters.

289
00:18:37,774 --> 00:18:41,298
Right? Now, once we train the model,

290
00:18:41,384 --> 00:18:45,634
then we use a data loader, then we use a model fit to

291
00:18:45,672 --> 00:18:49,430
train the model and along with that hyperparameters tuning,

292
00:18:50,090 --> 00:18:53,720
the fine tuning job is nothing but take a pretrained model

293
00:18:54,410 --> 00:18:57,786
and fine tune the model to a specific task. In Bert we are

294
00:18:57,808 --> 00:19:00,954
going to perform a text classification for that

295
00:19:01,152 --> 00:19:05,066
give an input data set and keep training the model

296
00:19:05,248 --> 00:19:09,126
until you get the accuracy or recall

297
00:19:09,158 --> 00:19:12,942
and precision to a certain benchmark. 85% or 95%.

298
00:19:12,996 --> 00:19:17,002
How much would you require based on the problem definition or problem statement

299
00:19:17,066 --> 00:19:20,606
which you defined? Now we

300
00:19:20,628 --> 00:19:23,534
have done with the model training, then we have to evaluate the model.

301
00:19:23,652 --> 00:19:27,802
As I said at the beginning of the conversation,

302
00:19:27,946 --> 00:19:31,182
when we get the model we have to do the model evaluation metrics,

303
00:19:31,246 --> 00:19:34,718
then split the data into train test and validation.

304
00:19:34,894 --> 00:19:38,062
Then once the training has been performed with the

305
00:19:38,216 --> 00:19:42,114
train data, we can use evaluation or validation

306
00:19:42,162 --> 00:19:45,590
data set to evaluate the model. Then further we can

307
00:19:45,660 --> 00:19:49,122
use a prediction logic which could be based on logics

308
00:19:49,186 --> 00:19:52,598
or softmax classifier or neural network in the behind. I don't

309
00:19:52,614 --> 00:19:56,570
want to go deeper in that. Our idea is to define a framework,

310
00:19:56,910 --> 00:20:00,810
do a model training and productionize the model, or deploy the model

311
00:20:00,880 --> 00:20:03,806
in cloud. That's where our focus is. If anything,

312
00:20:03,988 --> 00:20:07,646
please feel free to reach out to me after the presentation or after

313
00:20:07,668 --> 00:20:11,562
this live event. Then we can discuss or take the conversation

314
00:20:11,626 --> 00:20:15,726
further. Now talked about model training and model evaluation

315
00:20:15,838 --> 00:20:19,550
and we can use model accuracy, sorry, model prediction

316
00:20:19,630 --> 00:20:23,598
and based on the model prediction we can compute the confusion

317
00:20:23,614 --> 00:20:27,302
matrix score that can be used for

318
00:20:27,356 --> 00:20:31,254
taking the model to production. Now how do

319
00:20:31,292 --> 00:20:35,062
we take the model to production? That is another interesting problem which

320
00:20:35,116 --> 00:20:38,490
can be solved by using Pytorch serving,

321
00:20:39,150 --> 00:20:42,458
right? So now

322
00:20:42,624 --> 00:20:46,330
when we say PyTorch serving of ML models,

323
00:20:47,070 --> 00:20:50,346
Pytorch serving is nothing. But we have built a

324
00:20:50,368 --> 00:20:54,094
model in Pytorch and how do we serve the model in production to

325
00:20:54,132 --> 00:20:57,360
achieve the low latency and scalability problem,

326
00:20:57,810 --> 00:21:01,230
right? So as you could see, once you train

327
00:21:01,300 --> 00:21:04,660
the model and you evaluate the model, and if you feel the model

328
00:21:05,030 --> 00:21:07,890
is good enough to take to higher environments,

329
00:21:08,230 --> 00:21:11,966
then you have to convert the model into Mar file.

330
00:21:12,078 --> 00:21:15,954
So which is nothing but a torch serve which we've been

331
00:21:15,992 --> 00:21:19,910
using. And we have a model store where we have to convert the model

332
00:21:19,980 --> 00:21:23,634
into Mar file and the model has to be deployed

333
00:21:23,682 --> 00:21:26,200
into a pytot serving inference place.

334
00:21:27,050 --> 00:21:30,666
So there is a logic which we have to follow. We have to build the

335
00:21:30,688 --> 00:21:34,182
docker image. Once we have a model, then we have to deploy

336
00:21:34,246 --> 00:21:37,766
that into the ECs that we can take in the next slide.

337
00:21:37,878 --> 00:21:41,178
But overall torch serving will help us to

338
00:21:41,344 --> 00:21:44,878
deploy the Mar files inside a model store. Then it will have

339
00:21:44,884 --> 00:21:48,526
an inference API so that via invoking an API we can

340
00:21:48,548 --> 00:21:52,206
call the model prediction results. So internally this has

341
00:21:52,228 --> 00:21:56,074
an architecture where you can have multiple model can be served

342
00:21:56,122 --> 00:21:58,210
in a single python serving instance.

343
00:21:59,110 --> 00:22:02,606
Right. Now, ideally this can be used for an API invocation

344
00:22:02,718 --> 00:22:06,514
to call all the models which has been deployed inside the Python serving model

345
00:22:06,632 --> 00:22:10,086
again. For more questions please feel free to reach out to me after the

346
00:22:10,108 --> 00:22:13,158
event. Now once we have the

347
00:22:13,164 --> 00:22:17,414
Pytotch serving, this is the most interesting piece. So we

348
00:22:17,452 --> 00:22:21,078
have trained and deploy or sorry productionize the model

349
00:22:21,164 --> 00:22:24,434
in cloud right now whenever

350
00:22:24,482 --> 00:22:28,438
you talk about the model training activity. Once the Mar file

351
00:22:28,454 --> 00:22:31,706
is generated, we have to push inside the S three bucket because all

352
00:22:31,728 --> 00:22:34,958
the model can be stored in s three bucket because that

353
00:22:34,964 --> 00:22:37,722
is a huge file and is a blob storage or it's a file storage.

354
00:22:37,786 --> 00:22:41,706
S three bucket from Amazon can be used to store the models.

355
00:22:41,898 --> 00:22:45,006
Then we can use an ECR elastic container

356
00:22:45,038 --> 00:22:48,306
registry to push the model. Or we

357
00:22:48,328 --> 00:22:51,710
have to push the image into an ECS or EC two instance.

358
00:22:51,870 --> 00:22:56,126
There we could see we build every model as a docker image.

359
00:22:56,318 --> 00:23:00,710
Then once we have a docker image, then we have an EBS elastic

360
00:23:01,530 --> 00:23:04,998
load balancing server or EBS storage is used in

361
00:23:05,004 --> 00:23:08,246
the backend to connect to the ECS and the model can

362
00:23:08,268 --> 00:23:12,006
be stored over there in the model store. Then from there via inference

363
00:23:12,038 --> 00:23:15,494
API we can pick the model by writing a python function or Python

364
00:23:15,542 --> 00:23:19,046
code that will be deployed as a docker image or docker container.

365
00:23:19,238 --> 00:23:22,766
Then it can pull the model and it can perform the inference logic or

366
00:23:22,788 --> 00:23:26,558
it can do the prediction. So now you can see how the old model

367
00:23:26,644 --> 00:23:30,206
is getting developed from the time we

368
00:23:30,228 --> 00:23:34,590
start the model development to productionize the algorithms.

369
00:23:35,090 --> 00:23:38,206
Now let's talk more about the AWS cloud environment.

370
00:23:38,318 --> 00:23:42,078
So already we have a sage maker, but let's not use a sage maker

371
00:23:42,094 --> 00:23:46,446
or sage maker endpoint. But ideally we are saving a cost literally by

372
00:23:46,488 --> 00:23:50,182
having ECs, ECR and S three bucket and perform

373
00:23:50,236 --> 00:23:54,390
a model training in a GPU machine. And once the model has been trained,

374
00:23:55,050 --> 00:23:58,374
push the file with and we can write

375
00:23:58,412 --> 00:24:01,846
some scripts to push the trained model into S

376
00:24:01,868 --> 00:24:05,270
three bucket. Then once that is done we can use Jenkins

377
00:24:05,350 --> 00:24:08,714
Ci CD pipeline to

378
00:24:08,752 --> 00:24:12,746
push the docker image into an ECS container which underneath it

379
00:24:12,768 --> 00:24:16,750
uses Fargate or EC two. In my case I refer to EC two,

380
00:24:16,900 --> 00:24:20,938
right? As I said, once a model has been built, all the models

381
00:24:20,954 --> 00:24:24,786
will get stored under model store. Once the models are

382
00:24:24,808 --> 00:24:28,574
stored under model store, there is a management API, an inference API

383
00:24:28,702 --> 00:24:33,506
which by using an pytotch serving command where we

384
00:24:33,528 --> 00:24:38,626
can provide inference API for the applications

385
00:24:38,658 --> 00:24:42,262
to consume. I think this is a holistic step involved in

386
00:24:42,316 --> 00:24:45,730
creating the model or developing the machine learning algorithm

387
00:24:45,810 --> 00:24:48,870
or deploying the model in cloud environment

388
00:24:49,370 --> 00:24:53,014
right. Any further questions? As I said, you can always reach out to me after

389
00:24:53,052 --> 00:24:57,542
the event. All right. Thank you for

390
00:24:57,596 --> 00:25:00,540
watching my video. Have they had.

